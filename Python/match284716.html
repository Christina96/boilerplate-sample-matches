<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_netbox.py &amp; schedule.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_netbox.py &amp; schedule.py
      </h3>
<h1 align="center">
        0.6%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_netbox.py (1.4380531%)<th>schedule.py (0.4571027%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(1906-1915)<td><a href="#" name="0">(899-904)</a><td align="center"><font color="#ff0000">13</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_netbox.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
import pytest
import salt.pillar.netbox as netbox
from tests.support.mock import patch
@pytest.fixture
def default_kwargs():
    return {
        "minion_id": "minion1",
        "pillar": None,
        "api_url": "http://netbox.example.com",
        "api_token": "yeic5oocizei7owuichoesh8ooqu6oob3uWiey9a",
        "api_query_result_limit": 65535,
    }
@pytest.fixture
def headers():
    return {"Authorization": "Token quin1Di5MoRooChaiph3Aenaxais5EeY1gie6eev"}
@pytest.fixture
def device_results():
    return {
        "dict": {
            "count": 1,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 511,
                    "url": "https://netbox.example.com/api/dcim/devices/511/",
                    "name": "minion1",
                    "display_name": "minion1",
                    "device_type": {
                        "id": 4,
                        "url": "https://netbox.example.com/api/dcim/device-types/4/",
                        "manufacturer": {
                            "id": 1,
                            "url": "https://netbox.example.com/api/dcim/manufacturers/1/",
                            "name": "Cisco",
                            "slug": "cisco",
                        },
                        "model": "ISR2901",
                        "slug": "isr2901",
                        "display_name": "Cisco ISR2901",
                    },
                    "device_role": {
                        "id": 45,
                        "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                        "name": "Network",
                        "slug": "network",
                    },
                    "node_type": "device",
                    "tenant": None,
                    "platform": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/dcim/platforms/1/",
                        "name": "Cisco IOS",
                        "slug": "ios",
                    },
                    "serial": "",
                    "asset_tag": None,
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "rack": None,
                    "position": None,
                    "face": None,
                    "parent_device": None,
                    "status": {"value": "active", "label": "Active"},
                    "primary_ip": {
                        "id": 1146,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                        "family": 4,
                        "address": "192.0.2.1/24",
                    },
                    "primary_ip4": {
                        "id": 1146,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                        "family": 4,
                        "address": "192.0.2.1/24",
                    },
                    "primary_ip6": None,
                    "cluster": None,
                    "virtual_chassis": None,
                    "vc_position": None,
                    "vc_priority": None,
                    "comments": "",
                    "local_context_data": None,
                    "tags": [],
                    "custom_fields": {},
                    "config_context": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:04.171105Z",
                }
            ],
        }
    }
@pytest.fixture
def multiple_device_results():
    return {
        "dict": {
            "count": 2,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 511,
                    "url": "https://netbox.example.com/api/dcim/devices/511/",
                    "name": "minion1",
                    "display_name": "minion1",
                    "device_type": {
                        "id": 4,
                        "url": "https://netbox.example.com/api/dcim/device-types/4/",
                        "manufacturer": {
                            "id": 1,
                            "url": "https://netbox.example.com/api/dcim/manufacturers/1/",
                            "name": "Cisco",
                            "slug": "cisco",
                        },
                        "model": "ISR2901",
                        "slug": "isr2901",
                        "display_name": "Cisco ISR2901",
                    },
                    "device_role": {
                        "id": 45,
                        "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                        "name": "Network",
                        "slug": "network",
                    },
                    "node_type": "device",
                    "tenant": None,
                    "platform": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/dcim/platforms/1/",
                        "name": "Cisco IOS",
                        "slug": "ios",
                    },
                    "serial": "",
                    "asset_tag": None,
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "rack": None,
                    "position": None,
                    "face": None,
                    "parent_device": None,
                    "status": {"value": "active", "label": "Active"},
                    "primary_ip": {
                        "id": 1146,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                        "family": 4,
                        "address": "192.0.2.1/24",
                    },
                    "primary_ip4": {
                        "id": 1146,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                        "family": 4,
                        "address": "192.0.2.1/24",
                    },
                    "primary_ip6": None,
                    "cluster": None,
                    "virtual_chassis": None,
                    "vc_position": None,
                    "vc_priority": None,
                    "comments": "",
                    "local_context_data": None,
                    "tags": [],
                    "custom_fields": {},
                    "config_context": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:04.171105Z",
                },
                {
                    "id": 512,
                    "url": "https://netbox.example.com/api/dcim/devices/512/",
                    "name": "minion1",
                    "display_name": "minion1",
                    "device_type": {
                        "id": 4,
                        "url": "https://netbox.example.com/api/dcim/device-types/4/",
                        "manufacturer": {
                            "id": 1,
                            "url": "https://netbox.example.com/api/dcim/manufacturers/1/",
                            "name": "Cisco",
                            "slug": "cisco",
                        },
                        "model": "ISR2901",
                        "slug": "isr2901",
                        "display_name": "Cisco ISR2901",
                    },
                    "device_role": {
                        "id": 45,
                        "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                        "name": "Network",
                        "slug": "network",
                    },
                    "node_type": "device",
                    "tenant": None,
                    "platform": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/dcim/platforms/1/",
                        "name": "Cisco IOS",
                        "slug": "ios",
                    },
                    "serial": "",
                    "asset_tag": None,
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "rack": None,
                    "position": None,
                    "face": None,
                    "parent_device": None,
                    "status": {"value": "active", "label": "Active"},
                    "primary_ip": {
                        "id": 1150,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1150/",
                        "family": 4,
                        "address": "192.0.2.3/24",
                    },
                    "primary_ip4": {
                        "id": 1150,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1150/",
                        "family": 4,
                        "address": "192.0.2.3/24",
                    },
                    "primary_ip6": None,
                    "cluster": None,
                    "virtual_chassis": None,
                    "vc_position": None,
                    "vc_priority": None,
                    "comments": "",
                    "local_context_data": None,
                    "tags": [],
                    "custom_fields": {},
                    "config_context": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:04.171105Z",
                },
            ],
        }
    }
@pytest.fixture
def virtual_machine_results():
    return {
        "dict": {
            "count": 1,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 222,
                    "url": "https://netbox.example.com/api/virtualization/virtual-machines/222/",
                    "name": "minion1",
                    "status": {"value": "active", "label": "Active"},
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "cluster": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/virtualization/clusters/1/",
                        "name": "Cluster",
                    },
                    "role": {
                        "id": 45,
                        "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                        "name": "Network",
                        "slug": "network",
                    },
                    "node_type": "virtual-machine",
                    "tenant": None,
                    "platform": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/dcim/platforms/1/",
                        "name": "Cisco IOS",
                        "slug": "ios",
                    },
                    "primary_ip": {
                        "id": 1148,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1148/",
                        "family": 4,
                        "address": "192.0.2.2/24",
                    },
                    "primary_ip4": {
                        "id": 1148,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1148/",
                        "family": 4,
                        "address": "192.0.2.2/24",
                    },
                    "primary_ip6": None,
                    "vcpus": 1,
                    "memory": 1024,
                    "disk": 30,
                    "comments": "",
                    "local_context_data": None,
                    "tags": [],
                    "custom_fields": {},
                    "config_context": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:05.799541Z",
                }
            ],
        }
    }
@pytest.fixture
def multiple_virtual_machine_results():
    return {
        "dict": {
            "count": 1,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 222,
                    "url": "https://netbox.example.com/api/virtualization/virtual-machines/222/",
                    "name": "minion1",
                    "status": {"value": "active", "label": "Active"},
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "cluster": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/virtualization/clusters/1/",
                        "name": "Cluster",
                    },
                    "role": {
                        "id": 45,
                        "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                        "name": "Network",
                        "slug": "network",
                    },
                    "node_type": "virtual-machine",
                    "tenant": None,
                    "platform": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/dcim/platforms/1/",
                        "name": "Cisco IOS",
                        "slug": "ios",
                    },
                    "primary_ip": {
                        "id": 1148,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1148/",
                        "family": 4,
                        "address": "192.0.2.2/24",
                    },
                    "primary_ip4": {
                        "id": 1148,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1148/",
                        "family": 4,
                        "address": "192.0.2.2/24",
                    },
                    "primary_ip6": None,
                    "vcpus": 1,
                    "memory": 1024,
                    "disk": 30,
                    "comments": "",
                    "local_context_data": None,
                    "tags": [],
                    "custom_fields": {},
                    "config_context": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:05.799541Z",
                },
                {
                    "id": 223,
                    "url": "https://netbox.example.com/api/virtualization/virtual-machines/223/",
                    "name": "minion1",
                    "status": {"value": "active", "label": "Active"},
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "cluster": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/virtualization/clusters/1/",
                        "name": "Cluster",
                    },
                    "role": {
                        "id": 45,
                        "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                        "name": "Network",
                        "slug": "network",
                    },
                    "node_type": "virtual-machine",
                    "tenant": None,
                    "platform": {
                        "id": 1,
                        "url": "https://netbox.example.com/api/dcim/platforms/1/",
                        "name": "Cisco IOS",
                        "slug": "ios",
                    },
                    "primary_ip": {
                        "id": 1152,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1152/",
                        "family": 4,
                        "address": "192.0.2.4/24",
                    },
                    "primary_ip4": {
                        "id": 1152,
                        "url": "https://netbox.example.com/api/ipam/ip-addresses/1152/",
                        "family": 4,
                        "address": "192.0.2.4/24",
                    },
                    "primary_ip6": None,
                    "vcpus": 1,
                    "memory": 1024,
                    "disk": 30,
                    "comments": "",
                    "local_context_data": None,
                    "tags": [],
                    "custom_fields": {},
                    "config_context": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:05.799541Z",
                },
            ],
        }
    }
@pytest.fixture
def no_results():
    return {"dict": {"count": 0, "next": None, "previous": None, "results": []}}
@pytest.fixture
def http_error():
    return {"error": "HTTP 404: Not Found", "status": 404}
@pytest.fixture
def device_interface_results():
    return {
        "dict": {
            "count": 2,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 8158,
                    "url": "https://netbox.example.com/api/dcim/interfaces/8158/",
                    "device": {
                        "id": 511,
                        "url": "https://netbox.example.com/api/dcim/devices/511/",
                        "name": "minion1",
                        "display_name": "minion1",
                    },
                    "name": "GigabitEthernet0/0",
                    "label": "",
                    "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
                    "enabled": True,
                    "lag": None,
                    "mtu": None,
                    "mac_address": None,
                    "mgmt_only": False,
                    "description": "",
                    "mode": None,
                    "untagged_vlan": None,
                    "tagged_vlans": [],
                    "cable": None,
                    "cable_peer": None,
                    "cable_peer_type": None,
                    "connected_endpoint": None,
                    "connected_endpoint_type": None,
                    "connected_endpoint_reachable": None,
                    "tags": [],
                    "count_ipaddresses": 1,
                },
                {
                    "id": 8159,
                    "url": "https://netbox.example.com/api/dcim/interfaces/8159/",
                    "device": {
                        "id": 511,
                        "url": "https://netbox.example.com/api/dcim/devices/511/",
                        "name": "minion1",
                        "display_name": "minion1",
                    },
                    "name": "GigabitEthernet0/1",
                    "label": "",
                    "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
                    "enabled": True,
                    "lag": None,
                    "mtu": None,
                    "mac_address": None,
                    "mgmt_only": False,
                    "description": "",
                    "mode": None,
                    "untagged_vlan": None,
                    "tagged_vlans": [],
                    "cable": None,
                    "cable_peer": None,
                    "cable_peer_type": None,
                    "connected_endpoint": None,
                    "connected_endpoint_type": None,
                    "connected_endpoint_reachable": None,
                    "tags": [],
                    "count_ipaddresses": 1,
                },
            ],
        }
    }
@pytest.fixture
def device_interfaces_list():
    return [
        {
            "id": 8158,
            "url": "https://netbox.example.com/api/dcim/interfaces/8158/",
            "name": "GigabitEthernet0/0",
            "label": "",
            "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
            "enabled": True,
            "lag": None,
            "mtu": None,
            "mac_address": None,
            "mgmt_only": False,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "cable": None,
            "cable_peer": None,
            "cable_peer_type": None,
            "connected_endpoint": None,
            "connected_endpoint_type": None,
            "connected_endpoint_reachable": None,
            "tags": [],
            "count_ipaddresses": 1,
        },
        {
            "id": 8159,
            "url": "https://netbox.example.com/api/dcim/interfaces/8159/",
            "name": "GigabitEthernet0/1",
            "label": "",
            "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
            "enabled": True,
            "lag": None,
            "mtu": None,
            "mac_address": None,
            "mgmt_only": False,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "cable": None,
            "cable_peer": None,
            "cable_peer_type": None,
            "connected_endpoint": None,
            "connected_endpoint_type": None,
            "connected_endpoint_reachable": None,
            "tags": [],
            "count_ipaddresses": 1,
        },
    ]
@pytest.fixture
def virtual_machine_interface_results():
    return {
        "dict": {
            "count": 2,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 668,
                    "url": "https://netbox.example.com/api/virtualization/interfaces/668/",
                    "virtual_machine": {
                        "id": 222,
                        "url": "https://netbox.example.com/api/virtualization/virtual-machines/222/",
                        "name": "minion1",
                    },
                    "name": "GigabitEthernet0/0",
                    "enabled": True,
                    "mtu": None,
                    "mac_address": None,
                    "description": "",
                    "mode": None,
                    "untagged_vlan": None,
                    "tagged_vlans": [],
                    "tags": [],
                },
                {
                    "id": 669,
                    "url": "https://netbox.example.com/api/virtualization/interfaces/669/",
                    "virtual_machine": {
                        "id": 222,
                        "url": "https://netbox.example.com/api/virtualization/virtual-machines/222/",
                        "name": "minion1",
                    },
                    "name": "GigabitEthernet0/1",
                    "enabled": True,
                    "mtu": None,
                    "mac_address": None,
                    "description": "",
                    "mode": None,
                    "untagged_vlan": None,
                    "tagged_vlans": [],
                    "tags": [],
                },
            ],
        }
    }
@pytest.fixture
def virtual_machine_interfaces_list():
    return [
        {
            "id": 668,
            "url": "https://netbox.example.com/api/virtualization/interfaces/668/",
            "name": "GigabitEthernet0/0",
            "enabled": True,
            "mtu": None,
            "mac_address": None,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "tags": [],
        },
        {
            "id": 669,
            "url": "https://netbox.example.com/api/virtualization/interfaces/669/",
            "name": "GigabitEthernet0/1",
            "enabled": True,
            "mtu": None,
            "mac_address": None,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "tags": [],
        },
    ]
@pytest.fixture
def device_ip_results():
    return {
        "dict": {
            "count": 2,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 1146,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "192.0.2.1/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "assigned_object_type": "dcim.interface",
                    "assigned_object_id": 8158,
                    "assigned_object": {
                        "id": 8158,
                        "url": "https://netbox.example.com/api/dcim/interfaces/8158/",
                        "device": {
                            "id": 511,
                            "url": "https://netbox.example.com/api/dcim/devices/511/",
                            "name": "minion1",
                            "display_name": "minion1",
                        },
                        "name": "GigabitEthernet0/0",
                        "cable": None,
                    },
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:04.153386Z",
                },
                {
                    "id": 1147,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1147/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "198.51.100.1/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "assigned_object_type": "dcim.interface",
                    "assigned_object_id": 8159,
                    "assigned_object": {
                        "id": 8159,
                        "url": "https://netbox.example.com/api/dcim/interfaces/8159/",
                        "device": {
                            "id": 511,
                            "url": "https://netbox.example.com/api/dcim/devices/511/",
                            "name": "minion1",
                            "display_name": "minion1",
                        },
                        "name": "GigabitEthernet0/1",
                        "cable": None,
                    },
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:40.508154Z",
                },
            ],
        }
    }
@pytest.fixture
def virtual_machine_ip_results():
    return {
        "dict": {
            "count": 2,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 1148,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1148/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "192.0.2.2/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "assigned_object_type": "virtualization.vminterface",
                    "assigned_object_id": 668,
                    "assigned_object": {
                        "id": 668,
                        "url": "https://netbox.example.com/api/virtualization/interfaces/668/",
                        "virtual_machine": {
                            "id": 222,
                            "url": "https://netbox.example.com/api/virtualization/virtual-machines/222/",
                            "name": "minion1",
                        },
                        "name": "GigabitEthernet0/0",
                    },
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:05.784281Z",
                },
                {
                    "id": 1149,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1149/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "198.51.100.2/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "assigned_object_type": "virtualization.vminterface",
                    "assigned_object_id": 669,
                    "assigned_object": {
                        "id": 669,
                        "url": "https://netbox.example.com/api/virtualization/interfaces/669/",
                        "virtual_machine": {
                            "id": 222,
                            "url": "https://netbox.example.com/api/virtualization/virtual-machines/222/",
                            "name": "minion1",
                        },
                        "name": "GigabitEthernet0/1",
                    },
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:29.607428Z",
                },
            ],
        }
    }
@pytest.fixture
def device_interfaces_ip_list():
    return [
        {
            "id": 8158,
            "ip_addresses": [
                {
                    "id": 1146,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "192.0.2.1/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:04.153386Z",
                },
            ],
            "url": "https://netbox.example.com/api/dcim/interfaces/8158/",
            "name": "GigabitEthernet0/0",
            "label": "",
            "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
            "enabled": True,
            "lag": None,
            "mtu": None,
            "mac_address": None,
            "mgmt_only": False,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "cable": None,
            "cable_peer": None,
            "cable_peer_type": None,
            "connected_endpoint": None,
            "connected_endpoint_type": None,
            "connected_endpoint_reachable": None,
            "tags": [],
            "count_ipaddresses": 1,
        },
        {
            "id": 8159,
            "ip_addresses": [
                {
                    "id": 1147,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1147/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "198.51.100.1/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:12:40.508154Z",
                },
            ],
            "url": "https://netbox.example.com/api/dcim/interfaces/8159/",
            "name": "GigabitEthernet0/1",
            "label": "",
            "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
            "enabled": True,
            "lag": None,
            "mtu": None,
            "mac_address": None,
            "mgmt_only": False,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "cable": None,
            "cable_peer": None,
            "cable_peer_type": None,
            "connected_endpoint": None,
            "connected_endpoint_type": None,
            "connected_endpoint_reachable": None,
            "tags": [],
            "count_ipaddresses": 1,
        },
    ]
@pytest.fixture
def virtual_machine_interfaces_ip_list():
    return [
        {
            "id": 668,
            "ip_addresses": [
                {
                    "id": 1148,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1148/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "192.0.2.2/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:05.784281Z",
                },
            ],
            "url": "https://netbox.example.com/api/virtualization/interfaces/668/",
            "name": "GigabitEthernet0/0",
            "enabled": True,
            "mtu": None,
            "mac_address": None,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "tags": [],
        },
        {
            "id": 669,
            "ip_addresses": [
                {
                    "id": 1149,
                    "url": "https://netbox.example.com/api/ipam/ip-addresses/1149/",
                    "family": {"value": 4, "label": "IPv4"},
                    "address": "198.51.100.2/24",
                    "vrf": None,
                    "tenant": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "nat_inside": None,
                    "nat_outside": None,
                    "dns_name": "",
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-19",
                    "last_updated": "2021-02-19T06:23:29.607428Z",
                },
            ],
            "url": "https://netbox.example.com/api/virtualization/interfaces/669/",
            "name": "GigabitEthernet0/1",
            "enabled": True,
            "mtu": None,
            "mac_address": None,
            "description": "",
            "mode": None,
            "untagged_vlan": None,
            "tagged_vlans": [],
            "tags": [],
        },
    ]
@pytest.fixture
def site_results():
    return {
        "dict": {
            "id": 18,
            "url": "https://netbox.example.com/api/dcim/sites/18/",
            "name": "Site 1",
            "slug": "site1",
            "status": {"value": "active", "label": "Active"},
            "region": None,
            "tenant": None,
            "facility": "",
            "asn": None,
            "time_zone": None,
            "description": "",
            "physical_address": "",
            "shipping_address": "",
            "latitude": None,
            "longitude": None,
            "contact_name": "",
            "contact_phone": "",
            "contact_email": "",
            "comments": "",
            "tags": [],
            "custom_fields": {},
            "created": "2021-02-25",
            "last_updated": "2021-02-25T14:21:07.898957Z",
            "circuit_count": 0,
            "device_count": 1,
            "prefix_count": 2,
            "rack_count": 0,
            "virtualmachine_count": 1,
            "vlan_count": 0,
        }
    }
@pytest.fixture
def site_prefixes_results():
    return {
        "dict": {
            "count": 2,
            "next": None,
            "previous": None,
            "results": [
                {
                    "id": 284,
                    "url": "https://netbox.example.com/api/ipam/prefixes/284/",
                    "family": {"value": 4, "label": "IPv4"},
                    "prefix": "192.0.2.0/24",
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "vrf": None,
                    "tenant": None,
                    "vlan": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "is_pool": False,
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-25",
                    "last_updated": "2021-02-25T15:08:27.136305Z",
                },
                {
                    "id": 285,
                    "url": "https://netbox.example.com/api/ipam/prefixes/285/",
                    "family": {"value": 4, "label": "IPv4"},
                    "prefix": "198.51.100.0/24",
                    "site": {
                        "id": 18,
                        "url": "https://netbox.example.com/api/dcim/sites/18/",
                        "name": "Site 1",
                        "slug": "site1",
                    },
                    "vrf": None,
                    "tenant": None,
                    "vlan": None,
                    "status": {"value": "active", "label": "Active"},
                    "role": None,
                    "is_pool": False,
                    "description": "",
                    "tags": [],
                    "custom_fields": {},
                    "created": "2021-02-25",
                    "last_updated": "2021-02-25T15:08:59.880440Z",
                },
            ],
        }
    }
@pytest.fixture
def site_prefixes():
    return [
        {
            "id": 284,
            "url": "https://netbox.example.com/api/ipam/prefixes/284/",
            "family": {"value": 4, "label": "IPv4"},
            "prefix": "192.0.2.0/24",
            "vrf": None,
            "tenant": None,
            "vlan": None,
            "status": {"value": "active", "label": "Active"},
            "role": None,
            "is_pool": False,
            "description": "",
            "tags": [],
            "custom_fields": {},
            "created": "2021-02-25",
            "last_updated": "2021-02-25T15:08:27.136305Z",
        },
        {
            "id": 285,
            "url": "https://netbox.example.com/api/ipam/prefixes/285/",
            "family": {"value": 4, "label": "IPv4"},
            "prefix": "198.51.100.0/24",
            "vrf": None,
            "tenant": None,
            "vlan": None,
            "status": {"value": "active", "label": "Active"},
            "role": None,
            "is_pool": False,
            "description": "",
            "tags": [],
            "custom_fields": {},
            "created": "2021-02-25",
            "last_updated": "2021-02-25T15:08:59.880440Z",
        },
    ]
@pytest.fixture
def proxy_details_results():
    return {
        "dict": {
            "id": 1,
            "url": "https://netbox.example.com/api/dcim/platforms/1/",
            "name": "Cisco IOS",
            "slug": "ios",
            "manufacturer": {
                "id": 1,
                "url": "https://netbox.example.com/api/dcim/manufacturers/1/",
                "name": "Cisco",
                "slug": "cisco",
            },
            "napalm_driver": "ios",
            "napalm_args": None,
            "description": "",
            "device_count": 152,
            "virtualmachine_count": 1,
        }
    }
@pytest.fixture
def proxy_details():
    return {
        "host": "192.0.2.1",
        "driver": "ios",
        "proxytype": "napalm",
    }
@pytest.fixture
def pillar_results():
    return {
        "netbox": {
            "id": 511,
            "url": "https://netbox.example.com/api/dcim/devices/511/",
            "name": "minion1",
            "node_type": "device",
            "display_name": "minion1",
            "device_type": {
                "id": 4,
                "url": "https://netbox.example.com/api/dcim/device-types/4/",
                "manufacturer": {
                    "id": 1,
                    "url": "https://netbox.example.com/api/dcim/manufacturers/1/",
                    "name": "Cisco",
                    "slug": "cisco",
                },
                "model": "ISR2901",
                "slug": "isr2901",
                "display_name": "Cisco ISR2901",
            },
            "device_role": {
                "id": 45,
                "url": "https://netbox.example.com/api/dcim/device-roles/45/",
                "name": "Network",
                "slug": "network",
            },
            "interfaces": [
                {
                    "id": 8158,
                    "ip_addresses": [
                        {
                            "id": 1146,
                            "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                            "family": {"value": 4, "label": "IPv4"},
                            "address": "192.0.2.1/24",
                            "vrf": None,
                            "tenant": None,
                            "status": {"value": "active", "label": "Active"},
                            "role": None,
                            "nat_inside": None,
                            "nat_outside": None,
                            "dns_name": "",
                            "description": "",
                            "tags": [],
                            "custom_fields": {},
                            "created": "2021-02-19",
                            "last_updated": "2021-02-19T06:12:04.153386Z",
                        },
                    ],
                    "url": "https://netbox.example.com/api/dcim/interfaces/8158/",
                    "name": "GigabitEthernet0/0",
                    "label": "",
                    "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
                    "enabled": True,
                    "lag": None,
                    "mtu": None,
                    "mac_address": None,
                    "mgmt_only": False,
                    "description": "",
                    "mode": None,
                    "untagged_vlan": None,
                    "tagged_vlans": [],
                    "cable": None,
                    "cable_peer": None,
                    "cable_peer_type": None,
                    "connected_endpoint": None,
                    "connected_endpoint_type": None,
                    "connected_endpoint_reachable": None,
                    "tags": [],
                    "count_ipaddresses": 1,
                },
                {
                    "id": 8159,
                    "ip_addresses": [
                        {
                            "id": 1147,
                            "url": "https://netbox.example.com/api/ipam/ip-addresses/1147/",
                            "family": {"value": 4, "label": "IPv4"},
                            "address": "198.51.100.1/24",
                            "vrf": None,
                            "tenant": None,
                            "status": {"value": "active", "label": "Active"},
                            "role": None,
                            "nat_inside": None,
                            "nat_outside": None,
                            "dns_name": "",
                            "description": "",
                            "tags": [],
                            "custom_fields": {},
                            "created": "2021-02-19",
                            "last_updated": "2021-02-19T06:12:40.508154Z",
                        },
                    ],
                    "url": "https://netbox.example.com/api/dcim/interfaces/8159/",
                    "name": "GigabitEthernet0/1",
                    "label": "",
                    "type": {"value": "1000base-t", "label": "1000BASE-T (1GE)"},
                    "enabled": True,
                    "lag": None,
                    "mtu": None,
                    "mac_address": None,
                    "mgmt_only": False,
                    "description": "",
                    "mode": None,
                    "untagged_vlan": None,
                    "tagged_vlans": [],
                    "cable": None,
                    "cable_peer": None,
                    "cable_peer_type": None,
                    "connected_endpoint": None,
                    "connected_endpoint_type": None,
                    "connected_endpoint_reachable": None,
                    "tags": [],
                    "count_ipaddresses": 1,
                },
            ],
            "tenant": None,
            "platform": {
                "id": 1,
                "url": "https://netbox.example.com/api/dcim/platforms/1/",
                "name": "Cisco IOS",
                "slug": "ios",
            },
            "serial": "",
            "asset_tag": None,
            "site": {
                "id": 18,
                "url": "https://netbox.example.com/api/dcim/sites/18/",
                "name": "Site 1",
                "slug": "site1",
                "status": {"value": "active", "label": "Active"},
                "region": None,
                "tenant": None,
                "facility": "",
                "asn": None,
                "time_zone": None,
                "description": "",
                "physical_address": "",
                "shipping_address": "",
                "latitude": None,
                "longitude": None,
                "contact_name": "",
                "contact_phone": "",
                "contact_email": "",
                "comments": "",
                "tags": [],
                "custom_fields": {},
                "created": "2021-02-25",
                "last_updated": "2021-02-25T14:21:07.898957Z",
                "circuit_count": 0,
                "device_count": 1,
                "prefix_count": 2,
                "rack_count": 0,
                "virtualmachine_count": 1,
                "vlan_count": 0,
                "prefixes": [
                    {
                        "id": 284,
                        "url": "https://netbox.example.com/api/ipam/prefixes/284/",
                        "family": {"value": 4, "label": "IPv4"},
                        "prefix": "192.0.2.0/24",
                        "vrf": None,
                        "tenant": None,
                        "vlan": None,
                        "status": {"value": "active", "label": "Active"},
                        "role": None,
                        "is_pool": False,
                        "description": "",
                        "tags": [],
                        "custom_fields": {},
                        "created": "2021-02-25",
                        "last_updated": "2021-02-25T15:08:27.136305Z",
                    },
                    {
                        "id": 285,
                        "url": "https://netbox.example.com/api/ipam/prefixes/285/",
                        "family": {"value": 4, "label": "IPv4"},
                        "prefix": "198.51.100.0/24",
                        "vrf": None,
                        "tenant": None,
                        "vlan": None,
                        "status": {"value": "active", "label": "Active"},
                        "role": None,
                        "is_pool": False,
                        "description": "",
                        "tags": [],
                        "custom_fields": {},
                        "created": "2021-02-25",
                        "last_updated": "2021-02-25T15:08:59.880440Z",
                    },
                ],
            },
            "rack": None,
            "position": None,
            "face": None,
            "parent_device": None,
            "status": {"value": "active", "label": "Active"},
            "primary_ip": {
                "id": 1146,
                "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                "family": 4,
                "address": "192.0.2.1/24",
            },
            "primary_ip4": {
                "id": 1146,
                "url": "https://netbox.example.com/api/ipam/ip-addresses/1146/",
                "family": 4,
                "address": "192.0.2.1/24",
            },
            "primary_ip6": None,
            "cluster": None,
            "virtual_chassis": None,
            "vc_position": None,
            "vc_priority": None,
            "comments": "",
            "local_context_data": None,
            "tags": [],
            "custom_fields": {},
            "config_context": {},
            "created": "2021-02-19",
            "last_updated": "2021-02-19T06:12:04.171105Z",
        },
        "proxy": {"host": "192.0.2.1", "driver": "ios", "proxytype": "napalm"},
    }
def test_when_minion_id_is_star_then_result_should_be_empty_dict(default_kwargs):
    expected_result = {}
    default_kwargs["minion_id"] = "*"
    actual_result = netbox.ext_pillar(**default_kwargs)
    assert actual_result == expected_result
def test_when_api_url_is_not_http_or_https_then_error_message_should_be_logged(
    default_kwargs,
):
    default_kwargs["api_url"] = "ftp://netbox.example.com"
    with patch("salt.pillar.netbox.log.error", autospec=True) as fake_error:
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'Provided URL for api_url "%s" is malformed or is not an http/https URL',
            "ftp://netbox.example.com",
        )
def test_when_neither_devices_or_virtual_machines_requested_then_error_message_should_be_logged(
    default_kwargs,
):
    default_kwargs["devices"] = default_kwargs["virtual_machines"] = False
    with patch("salt.pillar.netbox.log.error", autospec=True) as fake_error:
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            "At least one of devices or virtual_machines must be True"
        )
def test_when_interface_ips_requested_but_not_interfaces_then_error_message_should_be_logged(
    default_kwargs,
):
    default_kwargs["interfaces"] = False
    default_kwargs["interface_ips"] = True
    with patch("salt.pillar.netbox.log.error", autospec=True) as fake_error:
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            "The value for interfaces must be True if interface_ips is True"
        )
def test_when_api_query_result_limit_set_but_not_a_positive_integer_then_error_message_should_be_logged(
    default_kwargs,
):
    default_kwargs["api_query_result_limit"] = -1
    with patch("salt.pillar.netbox.log.error", autospec=True) as fake_error:
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            "The value for api_query_result_limit must be a postive integer if set"
        )
def test_when_api_token_not_set_then_error_message_should_be_logged(
    default_kwargs,
):
    default_kwargs["api_token"] = ""
    with patch("salt.pillar.netbox.log.error", autospec=True) as fake_error:
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with("The value for api_token is not set")
def test_when_we_retrieve_a_single_device_then_return_list(
    default_kwargs, headers, device_results
):
    expected_result = device_results["dict"]["results"]
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = device_results
        actual_result = netbox._get_devices(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_a_device_and_get_http_error_then_return_empty_list(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_devices(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_a_single_virtual_machine_then_return_list(
    default_kwargs, headers, virtual_machine_results
):
    expected_result = virtual_machine_results["dict"]["results"]
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = virtual_machine_results
        actual_result = netbox._get_virtual_machines(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_a_virtual_machine_and_get_http_error_then_return_empty_dict(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_virtual_machines(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_device_interfaces_then_return_dict(
    default_kwargs, headers, device_interface_results, device_interfaces_list
):
    expected_result = device_interfaces_list
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = device_interface_results
        actual_result = netbox._get_interfaces(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            511,
            "device",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_device_interfaces_and_get_http_error_then_return_empty_list(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_interfaces(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            511,
            "device",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_virtual_machine_interfaces_then_return_list(
    default_kwargs,
    headers,
    virtual_machine_interface_results,
    virtual_machine_interfaces_list,
):
    expected_result = virtual_machine_interfaces_list
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = virtual_machine_interface_results
        actual_result = netbox._get_interfaces(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            222,
            "virtual-machine",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_virtual_machine_interfaces_and_get_http_error_then_return_empty_list(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_interfaces(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            222,
            "virtual-machine",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_device_interface_ips_then_return_list(
    default_kwargs, headers, device_ip_results
):
    expected_result = device_ip_results["dict"]["results"]
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = device_ip_results
        actual_result = netbox._get_interface_ips(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            511,
            "device",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_device_interface_ips_and_get_http_error_then_return_empty_list(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_interface_ips(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            511,
            "device",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_virtual_machine_interface_ips_then_return_list(
    default_kwargs, headers, virtual_machine_ip_results
):
    expected_result = virtual_machine_ip_results["dict"]["results"]
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = virtual_machine_ip_results
        actual_result = netbox._get_interface_ips(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            222,
            "virtual-machine",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_virtual_machine_interface_ips_and_get_http_error_then_return_empty_list(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_interface_ips(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            222,
            "virtual-machine",
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_associate_ips_to_interfaces_then_return_list(
    default_kwargs, device_interfaces_list, device_ip_results, device_interfaces_ip_list
):
    expected_result = device_interfaces_ip_list
    interfaces_list = device_interfaces_list
    interface_ips_list = device_ip_results["dict"]["results"]
    actual_result = netbox._associate_ips_to_interfaces(
        interfaces_list, interface_ips_list
    )
    assert actual_result == expected_result
def test_associate_empty_ip_list_to_interfaces_then_return_list(
    default_kwargs, device_interfaces_list, device_ip_results
):
    expected_result = device_interfaces_list
    interfaces_list = device_interfaces_list
    interface_ips_list = []
    actual_result = netbox._associate_ips_to_interfaces(
        interfaces_list, interface_ips_list
    )
    assert actual_result == expected_result
def test_when_we_retrieve_site_details_then_return_dict(
    default_kwargs, headers, site_results
):
    expected_result = site_results["dict"]
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = site_results
        actual_result = netbox._get_site_details(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            "Site 1",
            18,
            headers,
        )
        assert actual_result == expected_result
def test_when_we_retrieve_site_details_and_get_http_error_then_return_empty_dict(
    default_kwargs, headers, http_error
):
    expected_result = {}
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_site_details(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            "Site 1",
            18,
            headers,
        )
        assert actual_result == expected_result
def test_when_we_retrieve_site_prefixes_then_return_list(
    default_kwargs, headers, site_prefixes_results, site_prefixes
):
    expected_result = site_prefixes
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = site_prefixes_results
        actual_result = netbox._get_site_prefixes(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            "Site 1",
            18,
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_site_prefixes_and_get_http_error_then_return_empty_list(
    default_kwargs, headers, http_error
):
    expected_result = []
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_site_prefixes(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            "Site 1",
            18,
            headers,
            default_kwargs["api_query_result_limit"],
        )
        assert actual_result == expected_result
def test_when_we_retrieve_proxy_details_then_return_dict(
    default_kwargs, headers, proxy_details_results, proxy_details
):
    expected_result = proxy_details
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = proxy_details_results
        actual_result = netbox._get_proxy_details(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            "192.0.2.1/24",
            1,
            headers,
        )
        assert actual_result == expected_result
def test_when_we_retrieve_proxy_details_and_get_http_error_then_dont_return(
    default_kwargs, headers, http_error
):
    expected_result = None
    with patch("salt.utils.http.query", autospec=True) as query:
        query.return_value = http_error
        actual_result = netbox._get_proxy_details(
            default_kwargs["api_url"],
            default_kwargs["minion_id"],
            "192.0.2.1/24",
            1,
            headers,
        )
        assert actual_result == expected_result
def test_when_we_retrieve_multiple_devices_then_error_message_should_be_logged(
    default_kwargs, multiple_device_results
):
    with patch(
        "salt.pillar.netbox._get_devices", autospec=True
    ) as multiple_devices, patch(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        multiple_devices.return_value = multiple_device_results["dict"]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'More than one node found for "%s"',
            "minion1",
        )
def test_when_we_retrieve_multiple_virtual_machines_then_error_message_should_be_logged(
    default_kwargs, multiple_virtual_machine_results
):
    default_kwargs["devices"] = False
    default_kwargs["virtual_machines"] = True
    with patch(
        "salt.pillar.netbox._get_virtual_machines", autospec=True
    ) as multiple_virtual_machines, patch(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        multiple_virtual_machines.return_value = multiple_virtual_machine_results[
            "dict"
        ]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'More than one node found for "%s"',
            "minion1",
        )
def test_when_we_retrieve_a_device_and_a_virtual_machine_then_error_message_should_be_logged(
    default_kwargs, device_results, virtual_machine_results
):
    default_kwargs["virtual_machines"] = True
<a name="0"></a>
    with patch("salt.pillar.netbox._get_devices", autospec=True) as device, patch(
        "salt.pillar.netbox._get_virtual_machines", autospec=True
    ) as virtual_machine, patch<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        device.return_value = device_results["dict"]["results"]
        virtual_machine.return_value = virtual_machine_results["dict"]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.</b></font>assert_called_with(
            'More than one node found for "%s"',
            "minion1",
        )
def test_when_we_retrieve_no_devices_then_error_message_should_be_logged(
    default_kwargs, no_results
):
    with patch("salt.pillar.netbox._get_devices", autospec=True) as devices, patch(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        devices.return_value = no_results["dict"]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'Unable to pull NetBox data for "%s"',
            "minion1",
        )
def test_when_we_retrieve_no_virtual_machines_then_error_message_should_be_logged(
    default_kwargs, no_results
):
    default_kwargs["devices"] = False
    default_kwargs["virtual_machines"] = True
    with patch(
        "salt.pillar.netbox._get_virtual_machines", autospec=True
    ) as virtual_machines, patch(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        virtual_machines.return_value = no_results["dict"]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'Unable to pull NetBox data for "%s"',
            "minion1",
        )
def test_when_we_retrieve_everything_successfully_then_return_dict(
    default_kwargs,
    device_results,
    no_results,
    device_interfaces_list,
    device_ip_results,
    site_results,
    site_prefixes,
    proxy_details,
    pillar_results,
):
    expected_result = pillar_results
    default_kwargs["virtual_machines"] = False
    default_kwargs["interfaces"] = True
    default_kwargs["interface_ips"] = True
    default_kwargs["site_details"] = True
    default_kwargs["site_prefixes"] = True
    default_kwargs["proxy_return"] = True
    with patch("salt.pillar.netbox._get_devices", autospec=True) as get_devices, patch(
        "salt.pillar.netbox._get_virtual_machines", autospec=True
    ) as get_virtual_machines, patch(
        "salt.pillar.netbox._get_interfaces", autospec=True
    ) as get_interfaces, patch(
        "salt.pillar.netbox._get_interface_ips", autospec=True
    ) as get_interface_ips, patch(
        "salt.pillar.netbox._get_site_details", autospec=True
    ) as get_site_details, patch(
        "salt.pillar.netbox._get_site_prefixes", autospec=True
    ) as get_site_prefixes, patch(
        "salt.pillar.netbox._get_proxy_details", autospec=True
    ) as get_proxy_details:
        get_devices.return_value = device_results["dict"]["results"]
        get_virtual_machines.return_value = no_results["dict"]["results"]
        get_interfaces.return_value = device_interfaces_list
        get_interface_ips.return_value = device_ip_results["dict"]["results"]
        get_site_details.return_value = site_results["dict"]
        get_site_prefixes.return_value = site_prefixes
        get_proxy_details.return_value = proxy_details
        actual_result = netbox.ext_pillar(**default_kwargs)
        assert actual_result == expected_result
def test_when_we_set_proxy_return_but_get_no_value_for_platform_then_error_message_should_be_logged(
    default_kwargs, headers, device_results
):
    default_kwargs["site_details"] = False
    default_kwargs["site_prefixes"] = False
    default_kwargs["proxy_return"] = True
    device_results["dict"]["results"][0]["platform"] = None
    with patch("salt.pillar.netbox._get_devices", autospec=True) as devices, patch(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        devices.return_value = device_results["dict"]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'You have set "proxy_return" to "True" but you have not set the platform in NetBox for "%s"',
            "minion1",
        )
def test_when_we_set_proxy_return_but_get_no_value_for_primary_ip_then_error_message_should_be_logged(
    default_kwargs, headers, device_results
):
    default_kwargs["site_details"] = False
    default_kwargs["site_prefixes"] = False
    default_kwargs["proxy_return"] = True
    device_results["dict"]["results"][0]["primary_ip"] = None
    with patch("salt.pillar.netbox._get_devices", autospec=True) as devices, patch(
        "salt.pillar.netbox.log.error", autospec=True
    ) as fake_error:
        devices.return_value = device_results["dict"]["results"]
        netbox.ext_pillar(**default_kwargs)
        fake_error.assert_called_with(
            'You have set "proxy_return" to "True" but you have not set the primary IPv4 or IPv6 address in NetBox for "%s"',
            "minion1",
        )
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>schedule.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
import copy
import datetime
import errno
import itertools
import logging
import os
import random
import signal
import sys
import threading
import time
import weakref
import salt.config
import salt.defaults.exitcodes
import salt.exceptions
import salt.loader
import salt.minion
import salt.payload
import salt.syspaths
import salt.utils.args
import salt.utils.error
import salt.utils.event
import salt.utils.files
import salt.utils.jid
import salt.utils.master
import salt.utils.minion
import salt.utils.platform
import salt.utils.process
import salt.utils.stringutils
import salt.utils.user
import salt.utils.yaml
from salt.exceptions import SaltInvocationError
from salt.utils.odict import OrderedDict
try:
    import dateutil.parser as dateutil_parser
    _WHEN_SUPPORTED = True
    _RANGE_SUPPORTED = True
except ImportError:
    _WHEN_SUPPORTED = False
    _RANGE_SUPPORTED = False
try:
    import croniter
    _CRON_SUPPORTED = True
except ImportError:
    _CRON_SUPPORTED = False
log = logging.getLogger(__name__)
class Schedule:
    instance = None
    def __new__(
        cls,
        opts,
        functions,
        returners=None,
        intervals=None,
        cleanup=None,
        proxy=None,
        standalone=False,
        new_instance=False,
        utils=None,
        _subprocess_list=None,
    ):
        if cls.instance is None or new_instance is True:
            log.debug("Initializing new Schedule")
            instance = object.__new__(cls)
            instance.__singleton_init__(
                opts,
                functions,
                returners=returners,
                intervals=intervals,
                cleanup=cleanup,
                proxy=proxy,
                standalone=standalone,
                utils=utils,
                _subprocess_list=_subprocess_list,
            )
            if new_instance is True:
                return instance
            cls.instance = instance
        else:
            log.debug("Re-using Schedule")
        return cls.instance
    def __init__(
        self,
        opts,
        functions,
        returners=None,
        intervals=None,
        cleanup=None,
        proxy=None,
        standalone=False,
        new_instance=False,
        utils=None,
        _subprocess_list=None,
    ):
        pass
    def __singleton_init__(
        self,
        opts,
        functions,
        returners=None,
        intervals=None,
        cleanup=None,
        proxy=None,
        standalone=False,
        utils=None,
        _subprocess_list=None,
    ):
        self.opts = opts
        self.proxy = proxy
        self.functions = functions
        self.utils = utils or salt.loader.utils(opts)
        self.standalone = standalone
        self.skip_function = None
        self.skip_during_range = None
        self.splay = None
        self.enabled = True
        if isinstance(intervals, dict):
            self.intervals = intervals
        else:
            self.intervals = {}
        if not self.standalone:
            if hasattr(returners, "__getitem__"):
                self.returners = returners
            else:
                self.returners = returners.loader.gen_functions()
        try:
            self.time_offset = self.functions.get(
                "timezone.get_offset", lambda: "0000"
            )()
        except Exception:  # pylint: disable=W0703
            log.warning(
                "Unable to obtain correct timezone offset, defaulting to 0000",
                exc_info_on_loglevel=logging.DEBUG,
            )
            self.time_offset = "0000"
        self.schedule_returner = self.option("schedule_returner")
        self.loop_interval = sys.maxsize
        if not self.standalone:
            clean_proc_dir(opts)
        if cleanup:
            for prefix in cleanup:
                self.delete_job_prefix(prefix)
        if _subprocess_list is None:
            self._subprocess_list = salt.utils.process.SubprocessList()
        else:
            self._subprocess_list = _subprocess_list
    def __getnewargs__(self):
        return self.opts, self.functions, self.returners, self.intervals, None
    def option(self, opt):
        if "config.merge" in self.functions:
            return self.functions["config.merge"](opt, {}, omit_master=True)
        return self.opts.get(opt, {})
    def _get_schedule(
        self, include_opts=True, include_pillar=True, remove_hidden=False
    ):
        schedule = {}
        if include_pillar:
            pillar_schedule = self.opts.get("pillar", {}).get("schedule", {})
            if not isinstance(pillar_schedule, dict):
                raise ValueError("Schedule must be of type dict.")
            schedule.update(pillar_schedule)
        if include_opts:
            opts_schedule = self.opts.get("schedule", {})
            if not isinstance(opts_schedule, dict):
                raise ValueError("Schedule must be of type dict.")
            schedule.update(opts_schedule)
        if remove_hidden:
            _schedule = copy.deepcopy(schedule)
            for job in _schedule:
                if isinstance(_schedule[job], dict):
                    for item in _schedule[job]:
                        if item.startswith("_"):
                            del schedule[job][item]
        return schedule
    def _check_max_running(self, func, data, opts, now):
        if "run" not in data or not data["run"]:
            return data
        if "jid_include" not in data or data["jid_include"]:
            jobcount = 0
            if self.opts["__role"] == "master":
                current_jobs = salt.utils.master.get_running_jobs(self.opts)
            else:
                current_jobs = salt.utils.minion.running(self.opts)
            for job in current_jobs:
                if "schedule" in job:
                    log.debug(
                        "schedule.handle_func: Checking job against fun %s: %s",
                        func,
                        job,
                    )
                    if data["name"] == job[
                        "schedule"
                    ] and salt.utils.process.os_is_running(job["pid"]):
                        jobcount += 1
                        log.debug(
                            "schedule.handle_func: Incrementing jobcount, "
                            "now %s, maxrunning is %s",
                            jobcount,
                            data["maxrunning"],
                        )
                        if jobcount &gt;= data["maxrunning"]:
                            log.debug(
                                "schedule.handle_func: The scheduled job "
                                "%s was not started, %s already running",
                                data["name"],
                                data["maxrunning"],
                            )
                            data["_skip_reason"] = "maxrunning"
                            data["_skipped"] = True
                            data["_skipped_time"] = now
                            data["run"] = False
                            return data
        return data
    def persist(self):
        config_dir = self.opts.get("conf_dir", None)
        if config_dir is None and "conf_file" in self.opts:
            config_dir = os.path.dirname(self.opts["conf_file"])
        if config_dir is None:
            config_dir = salt.syspaths.CONFIG_DIR
        minion_d_dir = os.path.join(
            config_dir,
            os.path.dirname(
                self.opts.get(
                    "default_include",
                    salt.config.DEFAULT_MINION_OPTS["default_include"],
                )
            ),
        )
        if not os.path.isdir(minion_d_dir):
            os.makedirs(minion_d_dir)
        schedule_conf = os.path.join(minion_d_dir, "_schedule.conf")
        log.debug("Persisting schedule")
        schedule_data = self._get_schedule(include_pillar=False, remove_hidden=True)
        try:
            with salt.utils.files.fopen(schedule_conf, "wb+") as fp_:
                fp_.write(
                    salt.utils.stringutils.to_bytes(
                        salt.utils.yaml.safe_dump({"schedule": schedule_data})
                    )
                )
        except OSError:
            log.error(
                "Failed to persist the updated schedule",
                exc_info_on_loglevel=logging.DEBUG,
            )
    def delete_job(self, name, persist=True):
        if name in self.opts["schedule"]:
            del self.opts["schedule"][name]
        elif name in self._get_schedule(include_opts=False):
            log.warning("Cannot delete job %s, it's in the pillar!", name)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_delete_complete",
            )
        if name in self.intervals:
            del self.intervals[name]
        if persist:
            self.persist()
    def reset(self):
        self.skip_function = None
        self.skip_during_range = None
        self.enabled = True
        self.splay = None
        self.opts["schedule"] = {}
    def delete_job_prefix(self, name, persist=True):
        for job in list(self.opts["schedule"].keys()):
            if job.startswith(name):
                del self.opts["schedule"][job]
        for job in self._get_schedule(include_opts=False):
            if job.startswith(name):
                log.warning("Cannot delete job %s, it's in the pillar!", job)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_delete_complete",
            )
        for job in list(self.intervals.keys()):
            if job.startswith(name):
                del self.intervals[job]
        if persist:
            self.persist()
    def add_job(self, data, persist=True):
        if not isinstance(data, dict):
            raise ValueError("Scheduled jobs have to be of type dict.")
        if not len(data) == 1:
            raise ValueError("You can only schedule one new job at a time.")
        for job in data:
            if "enabled" not in data[job]:
                data[job]["enabled"] = True
        new_job = next(iter(data.keys()))
        if new_job in self._get_schedule(include_opts=False):
            log.warning("Cannot update job %s, it's in the pillar!", new_job)
        elif new_job in self.opts["schedule"]:
            log.info("Updating job settings for scheduled job: %s", new_job)
            self.opts["schedule"].update(data)
        else:
            log.info("Added new job %s to scheduler", new_job)
            self.opts["schedule"].update(data)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_add_complete",
            )
        if persist:
            self.persist()
    def enable_job(self, name, persist=True):
        if name in self.opts["schedule"]:
            self.opts["schedule"][name]["enabled"] = True
            log.info("Enabling job %s in scheduler", name)
        elif name in self._get_schedule(include_opts=False):
            log.warning("Cannot modify job %s, it's in the pillar!", name)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_enabled_job_complete",
            )
        if persist:
            self.persist()
    def disable_job(self, name, persist=True):
        if name in self.opts["schedule"]:
            self.opts["schedule"][name]["enabled"] = False
            log.info("Disabling job %s in scheduler", name)
        elif name in self._get_schedule(include_opts=False):
            log.warning("Cannot modify job %s, it's in the pillar!", name)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_disabled_job_complete",
            )
        if persist:
            self.persist()
    def modify_job(self, name, schedule, persist=True):
        if name in self.opts["schedule"]:
            self.delete_job(name, persist)
        elif name in self._get_schedule(include_opts=False):
            log.warning("Cannot modify job %s, it's in the pillar!", name)
            return
        self.opts["schedule"][name] = schedule
        if persist:
            self.persist()
    def run_job(self, name):
        data = self._get_schedule().get(name, {})
        if "function" in data:
            func = data["function"]
        elif "func" in data:
            func = data["func"]
        elif "fun" in data:
            func = data["fun"]
        else:
            func = None
        if func not in self.functions:
            log.info("Invalid function: %s in scheduled job %s.", func, name)
        if "name" not in data:
            data["name"] = name
        if "run" not in data:
            data["run"] = True
        if not self.standalone:
            data = self._check_max_running(
                func, data, self.opts, datetime.datetime.now()
            )
        if data.get("run"):
            log.info("Running Job: %s", name)
            self._run_job(func, data)
    def enable_schedule(self, persist=True):
        self.opts["schedule"]["enabled"] = True
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_enabled_complete",
            )
        if persist:
            self.persist()
    def disable_schedule(self, persist=True):
        self.opts["schedule"]["enabled"] = False
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_disabled_complete",
            )
        if persist:
            self.persist()
    def reload(self, schedule):
        self.intervals = {}
        if "schedule" in schedule:
            schedule = schedule["schedule"]
        self.opts.setdefault("schedule", {}).update(schedule)
    def list(self, where):
        if where == "pillar":
            schedule = self._get_schedule(include_opts=False)
        elif where == "opts":
            schedule = self._get_schedule(include_pillar=False)
        else:
            schedule = self._get_schedule()
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": schedule},
                tag="/salt/minion/minion_schedule_list_complete",
            )
    def save_schedule(self):
        self.persist()
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event({"complete": True}, tag="/salt/minion/minion_schedule_saved")
    def postpone_job(self, name, data):
        time = data["time"]
        new_time = data["new_time"]
        time_fmt = data.get("time_fmt", "%Y-%m-%dT%H:%M:%S")
        if name in self.opts["schedule"]:
            if "skip_explicit" not in self.opts["schedule"][name]:
                self.opts["schedule"][name]["skip_explicit"] = []
            self.opts["schedule"][name]["skip_explicit"].append(
                {"time": time, "time_fmt": time_fmt}
            )
            if "run_explicit" not in self.opts["schedule"][name]:
                self.opts["schedule"][name]["run_explicit"] = []
            self.opts["schedule"][name]["run_explicit"].append(
                {"time": new_time, "time_fmt": time_fmt}
            )
        elif name in self._get_schedule(include_opts=False):
            log.warning("Cannot modify job %s, it's in the pillar!", name)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_postpone_job_complete",
            )
    def skip_job(self, name, data):
        time = data["time"]
        time_fmt = data.get("time_fmt", "%Y-%m-%dT%H:%M:%S")
        if name in self.opts["schedule"]:
            if "skip_explicit" not in self.opts["schedule"][name]:
                self.opts["schedule"][name]["skip_explicit"] = []
            self.opts["schedule"][name]["skip_explicit"].append(
                {"time": time, "time_fmt": time_fmt}
            )
        elif name in self._get_schedule(include_opts=False):
            log.warning("Cannot modify job %s, it's in the pillar!", name)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "schedule": self._get_schedule()},
                tag="/salt/minion/minion_schedule_skip_job_complete",
            )
    def get_next_fire_time(self, name, fmt="%Y-%m-%dT%H:%M:%S"):
        schedule = self._get_schedule()
        _next_fire_time = None
        if schedule:
            _next_fire_time = schedule.get(name, {}).get("_next_fire_time", None)
            if _next_fire_time:
                _next_fire_time = _next_fire_time.strftime(fmt)
        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
            evt.fire_event(
                {"complete": True, "next_fire_time": _next_fire_time},
                tag="/salt/minion/minion_schedule_next_fire_time_complete",
            )
    def job_status(self, name, fire_event=False):
        if fire_event:
            schedule = self._get_schedule()
            data = schedule.get(name, {})
            with salt.utils.event.get_event(
                "minion", opts=self.opts, listen=False
            ) as evt:
                evt.fire_event(
                    {"complete": True, "data": data},
                    tag="/salt/minion/minion_schedule_job_status_complete",
                )
        else:
            schedule = self._get_schedule()
            return schedule.get(name, {})
    def handle_func(self, multiprocessing_enabled, func, data, jid=None):
        if salt.utils.platform.is_windows() or self.opts.get("transport") == "zeromq":
            self.utils = salt.loader.utils(self.opts)
            if self.opts["__role"] == "master":
                self.functions = salt.loader.runner(self.opts, utils=self.utils)
            else:
                self.functions = salt.loader.minion_mods(
                    self.opts, proxy=self.proxy, utils=self.utils
                )
            self.returners = salt.loader.returners(
                self.opts, self.functions, proxy=self.proxy
            )
        if jid is None:
            jid = salt.utils.jid.gen_jid(self.opts)
        ret = {
            "id": self.opts.get("id", "master"),
            "fun": func,
            "fun_args": [],
            "schedule": data["name"],
            "jid": jid,
        }
        if "metadata" in data:
            if isinstance(data["metadata"], dict):
                ret["metadata"] = data["metadata"]
                ret["metadata"]["_TOS"] = self.time_offset
                ret["metadata"]["_TS"] = time.ctime()
                ret["metadata"]["_TT"] = time.strftime(
                    "%Y %B %d %a %H %m", time.gmtime()
                )
            else:
                log.warning(
                    "schedule: The metadata parameter must be "
                    "specified as a dictionary.  Ignoring."
                )
        data_returner = data.get("returner", None)
        if not self.standalone:
            proc_fn = os.path.join(
                salt.minion.get_proc_dir(self.opts["cachedir"]), ret["jid"]
            )
        try:
            minion_blackout_violation = False
            if self.opts.get("pillar", {}).get("minion_blackout", False):
                whitelist = self.opts.get("pillar", {}).get(
                    "minion_blackout_whitelist", []
                )
                if func != "saltutil.refresh_pillar" and func not in whitelist:
                    minion_blackout_violation = True
            elif self.opts.get("grains", {}).get("minion_blackout", False):
                whitelist = self.opts.get("grains", {}).get(
                    "minion_blackout_whitelist", []
                )
                if func != "saltutil.refresh_pillar" and func not in whitelist:
                    minion_blackout_violation = True
            if minion_blackout_violation:
                raise SaltInvocationError(
                    "Minion in blackout mode. Set 'minion_blackout' "
                    "to False in pillar or grains to resume operations. Only "
                    "saltutil.refresh_pillar allowed in blackout mode."
                )
            ret["pid"] = os.getpid()
            args = tuple()
            if "args" in data:
                args = copy.deepcopy(data["args"])
                ret["fun_args"].extend(data["args"])
            kwargs = {}
            if "kwargs" in data:
                kwargs = copy.deepcopy(data["kwargs"])
                ret["fun_args"].append(copy.deepcopy(kwargs))
            if func not in self.functions:
                ret["return"] = self.functions.missing_fun_string(func)
                salt.utils.error.raise_error(
                    message=self.functions.missing_fun_string(func)
                )
            if not self.standalone:
                if "jid_include" not in data or data["jid_include"]:
                    log.debug(
                        "schedule.handle_func: adding this job to the "
                        "jobcache with data %s",
                        ret,
                    )
                    with salt.utils.files.fopen(proc_fn, "w+b") as fp_:
                        fp_.write(salt.payload.dumps(ret))
            argspec = salt.utils.args.get_function_argspec(self.functions[func])
            if argspec.keywords:
                for key, val in ret.items():
                    if key != "kwargs":
                        kwargs["__pub_{}".format(key)] = copy.deepcopy(val)
            if self.opts["__role"] == "master":
                jid = salt.utils.jid.gen_jid(self.opts)
                tag = salt.utils.event.tagify(jid, prefix="salt/scheduler/")
                namespaced_event = salt.utils.event.NamespacedEvent(
                    salt.utils.event.get_event(
                        self.opts["__role"],
                        self.opts["sock_dir"],
                        opts=self.opts,
                        listen=False,
                    ),
                    tag,
                    print_func=None,
                )
                func_globals = {
                    "__jid__": jid,
                    "__user__": salt.utils.user.get_user(),
                    "__tag__": tag,
                    "__jid_event__": weakref.proxy(namespaced_event),
                }
                self_functions = copy.copy(self.functions)
                salt.utils.lazy.verify_fun(self_functions, func)
                completed_funcs = []
                for mod_name in self_functions.keys():
                    if "." not in mod_name:
                        continue
                    mod, _ = mod_name.split(".", 1)
                    if mod in completed_funcs:
                        continue
                    completed_funcs.append(mod)
                    for global_key, value in func_globals.items():
                        self.functions[mod_name].__globals__[global_key] = value
            self.functions.pack["__context__"]["retcode"] = 0
            ret["return"] = self.functions[func](*args, **kwargs)
            if not self.standalone:
                if "retcode" in self.functions.pack["__context__"]:
                    ret["retcode"] = self.functions.pack["__context__"]["retcode"]
                ret["success"] = True
                if data_returner or self.schedule_returner:
                    if "return_config" in data:
                        ret["ret_config"] = data["return_config"]
                    if "return_kwargs" in data:
                        ret["ret_kwargs"] = data["return_kwargs"]
                    rets = []
                    for returner in [data_returner, self.schedule_returner]:
                        if isinstance(returner, str):
                            rets.append(returner)
                        elif isinstance(returner, list):
                            rets.extend(returner)
                    for returner in OrderedDict.fromkeys(rets):
                        ret_str = "{}.returner".format(returner)
                        if ret_str in self.returners:
                            self.returners[ret_str](ret)
                        else:
                            log.info(
                                "Job %s using invalid returner: %s. Ignoring.",
                                func,
                                returner,
                            )
        except Exception:  # pylint: disable=broad-except
            log.exception("Unhandled exception running %s", ret["fun"])
            if "return" not in ret:
                ret["return"] = "Unhandled exception running {}".format(ret["fun"])
            ret["success"] = False
            ret["retcode"] = 254
        finally:
            if "__role" in self.opts and self.opts["__role"] in ("master", "minion"):
                if "return_job" in data and not data["return_job"]:
                    pass
                else:
                    mret = ret.copy()
                    if not data_returner and not self.schedule_returner:
                        mret["jid"] = "req"
                        if data.get("return_job") == "nocache":
                            mret["jid"] = "nocache"
                    load = {"cmd": "_return", "id": self.opts["id"]}
                    for key, value in mret.items():
<a name="0"></a>                        load[key] = value
                    if "__role" in self.opts and self.opts["__role"] == "minion":
                        event = salt.utils.event.get_event<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(
                            "minion", opts=self.opts, listen=False
                        )
                    elif "__role" in self.opts and self.opts["__role"] == "master":
                        event = salt.utils.event.get_master_event(
                            self.</b></font>opts, self.opts["sock_dir"]
                        )
                    try:
                        event.fire_event(load, "__schedule_return")
                    except Exception as exc:  # pylint: disable=broad-except
                        log.exception(
                            "Unhandled exception firing __schedule_return event"
                        )
                    finally:
                        event.destroy()
            if self.opts["__role"] == "master":
                namespaced_event.destroy()
            if not self.standalone:
                log.debug("schedule.handle_func: Removing %s", proc_fn)
                try:
                    os.unlink(proc_fn)
                except OSError as exc:
                    if exc.errno == errno.EEXIST or exc.errno == errno.ENOENT:
                        pass
                    else:
                        log.error("Failed to delete '%s': %s", proc_fn, exc.errno)
                        raise
                finally:
                    if multiprocessing_enabled:
                        sys.exit(salt.defaults.exitcodes.EX_GENERIC)
    def eval(self, now=None):
        log.trace("==== evaluating schedule now %s =====", now)
        jids = []
        loop_interval = self.opts["loop_interval"]
        if not isinstance(loop_interval, datetime.timedelta):
            loop_interval = datetime.timedelta(seconds=loop_interval)
        def _splay(splaytime):
            splay_ = None
            if isinstance(splaytime, dict):
                if splaytime["end"] &gt;= splaytime["start"]:
                    splay_ = random.randint(splaytime["start"], splaytime["end"])
                else:
                    log.error(
                        "schedule.handle_func: Invalid Splay, "
                        "end must be larger than start. Ignoring splay."
                    )
            else:
                splay_ = random.randint(1, splaytime)
            return splay_
        def _handle_time_elements(data):
            if "_seconds" not in data:
                interval = int(data.get("seconds", 0))
                interval += int(data.get("minutes", 0)) * 60
                interval += int(data.get("hours", 0)) * 3600
                interval += int(data.get("days", 0)) * 86400
                data["_seconds"] = interval
                if not data["_next_fire_time"]:
                    data["_next_fire_time"] = now + datetime.timedelta(
                        seconds=data["_seconds"]
                    )
                if interval &lt; self.loop_interval:
                    self.loop_interval = interval
                data["_next_scheduled_fire_time"] = now + datetime.timedelta(
                    seconds=data["_seconds"]
                )
        def _handle_once(data, loop_interval):
            if data["_next_fire_time"]:
                if (
                    data["_next_fire_time"] &lt; now - loop_interval
                    or data["_next_fire_time"] &gt; now
                    and not data["_splay"]
                ):
                    data["_continue"] = True
            if not data["_next_fire_time"] and not data["_splay"]:
                once = data["once"]
                if not isinstance(once, datetime.datetime):
                    once_fmt = data.get("once_fmt", "%Y-%m-%dT%H:%M:%S")
                    try:
                        once = datetime.datetime.strptime(data["once"], once_fmt)
                    except (TypeError, ValueError):
                        data["_error"] = (
                            "Date string could not "
                            "be parsed: {}, {}. "
                            "Ignoring job {}.".format(
                                data["once"], once_fmt, data["name"]
                            )
                        )
                        log.error(data["_error"])
                        return
                data["_next_fire_time"] = once
                data["_next_scheduled_fire_time"] = once
                if once &lt; now - loop_interval:
                    data["_continue"] = True
        def _handle_when(data, loop_interval):
            if not _WHEN_SUPPORTED:
                data["_error"] = "Missing python-dateutil. Ignoring job {}.".format(
                    data["name"]
                )
                log.error(data["_error"])
                return
            if not isinstance(data["when"], list):
                _when_data = [data["when"]]
            else:
                _when_data = data["when"]
            _when = []
            for i in _when_data:
                if (
                    "pillar" in self.opts
                    and "whens" in self.opts["pillar"]
                    and i in self.opts["pillar"]["whens"]
                ):
                    if not isinstance(self.opts["pillar"]["whens"], dict):
                        data["_error"] = (
                            'Pillar item "whens" '
                            "must be a dict. "
                            "Ignoring job {}.".format(data["name"])
                        )
                        log.error(data["_error"])
                        return
                    when_ = self.opts["pillar"]["whens"][i]
                elif (
                    "whens" in self.opts["grains"] and i in self.opts["grains"]["whens"]
                ):
                    if not isinstance(self.opts["grains"]["whens"], dict):
                        data[
                            "_error"
                        ] = 'Grain "whens" must be a dict. Ignoring job {}.'.format(
                            data["name"]
                        )
                        log.error(data["_error"])
                        return
                    when_ = self.opts["grains"]["whens"][i]
                else:
                    when_ = i
                if not isinstance(when_, datetime.datetime):
                    try:
                        when_ = dateutil_parser.parse(when_)
                    except ValueError:
                        data[
                            "_error"
                        ] = "Invalid date string {}. Ignoring job {}.".format(
                            i, data["name"]
                        )
                        log.error(data["_error"])
                        return
                _when.append(when_)
            if data["_splay"]:
                _when.append(data["_splay"])
            _when.sort()
            for i in copy.deepcopy(_when):
                if len(_when) &gt; 1:
                    if i &lt; now - loop_interval:
                        _when.remove(i)
            if _when:
                when = _when[0]
                if (
                    when &lt; now - loop_interval
                    and not data.get("_run", False)
                    and not run
                    and not data["_splay"]
                ):
                    data["_next_fire_time"] = None
                    data["_continue"] = True
                    return
                if "_run" not in data:
                    data["_run"] = bool(when &gt;= now - loop_interval)
                if not data["_next_fire_time"]:
                    data["_next_fire_time"] = when
                data["_next_scheduled_fire_time"] = when
                if data["_next_fire_time"] &lt; when and not run and not data["_run"]:
                    data["_next_fire_time"] = when
                    data["_run"] = True
            elif not data.get("_run", False):
                data["_next_fire_time"] = None
                data["_continue"] = True
        def _handle_cron(data, loop_interval):
            if not _CRON_SUPPORTED:
                data["_error"] = "Missing python-croniter. Ignoring job {}.".format(
                    data["name"]
                )
                log.error(data["_error"])
                return
            if data["_next_fire_time"] is None:
                try:
                    data["_next_fire_time"] = croniter.croniter(
                        data["cron"], now
                    ).get_next(datetime.datetime)
                    data["_next_scheduled_fire_time"] = croniter.croniter(
                        data["cron"], now
                    ).get_next(datetime.datetime)
                except (ValueError, KeyError):
                    data["_error"] = "Invalid cron string. Ignoring job {}.".format(
                        data["name"]
                    )
                    log.error(data["_error"])
                    return
                interval = (now - data["_next_fire_time"]).total_seconds()
                if interval &gt;= 60 and interval &lt; self.loop_interval:
                    self.loop_interval = interval
        def _handle_run_explicit(data, loop_interval):
            _run_explicit = []
            for _run_time in data["run_explicit"]:
                if isinstance(_run_time, datetime.datetime):
                    _run_explicit.append(_run_time)
                else:
                    _run_explicit.append(
                        datetime.datetime.strptime(
                            _run_time["time"], _run_time["time_fmt"]
                        )
                    )
            data["run"] = False
            for i in copy.deepcopy(_run_explicit):
                if len(_run_explicit) &gt; 1:
                    if i &lt; now - loop_interval:
                        _run_explicit.remove(i)
            if _run_explicit:
                if _run_explicit[0] &lt;= now &lt; _run_explicit[0] + loop_interval:
                    data["run"] = True
                    data["_next_fire_time"] = _run_explicit[0]
        def _handle_skip_explicit(data, loop_interval):
            data["run"] = False
            _skip_explicit = []
            for _skip_time in data["skip_explicit"]:
                if isinstance(_skip_time, datetime.datetime):
                    _skip_explicit.append(_skip_time)
                else:
                    _skip_explicit.append(
                        datetime.datetime.strptime(
                            _skip_time["time"], _skip_time["time_fmt"]
                        )
                    )
            for i in copy.deepcopy(_skip_explicit):
                if i &lt; now - loop_interval:
                    _skip_explicit.remove(i)
            if _skip_explicit:
                if _skip_explicit[0] &lt;= now &lt;= (_skip_explicit[0] + loop_interval):
                    if self.skip_function:
                        data["run"] = True
                        data["func"] = self.skip_function
                    else:
                        data["_skip_reason"] = "skip_explicit"
                        data["_skipped_time"] = now
                        data["_skipped"] = True
                        data["run"] = False
            else:
                data["run"] = True
        def _handle_skip_during_range(data, loop_interval):
            if not _RANGE_SUPPORTED:
                data["_error"] = "Missing python-dateutil. Ignoring job {}.".format(
                    data["name"]
                )
                log.error(data["_error"])
                return
            if not isinstance(data["skip_during_range"], dict):
                data["_error"] = (
                    "schedule.handle_func: Invalid, range "
                    "must be specified as a dictionary. "
                    "Ignoring job {}.".format(data["name"])
                )
                log.error(data["_error"])
                return
            start = data["skip_during_range"]["start"]
            end = data["skip_during_range"]["end"]
            if not isinstance(start, datetime.datetime):
                try:
                    start = dateutil_parser.parse(start)
                except ValueError:
                    data["_error"] = (
                        "Invalid date string for start in "
                        "skip_during_range. Ignoring "
                        "job {}.".format(data["name"])
                    )
                    log.error(data["_error"])
                    return
            if not isinstance(end, datetime.datetime):
                try:
                    end = dateutil_parser.parse(end)
                except ValueError:
                    data["_error"] = (
                        "Invalid date string for end in "
                        "skip_during_range. Ignoring "
                        "job {}.".format(data["name"])
                    )
                    log.error(data["_error"])
                    return
            if "run_after_skip_range" in data and data["run_after_skip_range"]:
                if "run_explicit" not in data:
                    data["run_explicit"] = []
                _run_immediate = (end + loop_interval).strftime("%Y-%m-%dT%H:%M:%S")
                if _run_immediate not in data["run_explicit"]:
                    data["run_explicit"].append(
                        {"time": _run_immediate, "time_fmt": "%Y-%m-%dT%H:%M:%S"}
                    )
            if end &gt; start:
                if start &lt;= now &lt;= end:
                    if self.skip_function:
                        data["run"] = True
                        data["func"] = self.skip_function
                    else:
                        data["_skip_reason"] = "in_skip_range"
                        data["_skipped_time"] = now
                        data["_skipped"] = True
                        data["run"] = False
                else:
                    data["run"] = True
            else:
                data["_error"] = (
                    "schedule.handle_func: Invalid "
                    "range, end must be larger than "
                    "start. Ignoring job {}.".format(data["name"])
                )
                log.error(data["_error"])
        def _handle_range(data):
            if not _RANGE_SUPPORTED:
                data["_error"] = "Missing python-dateutil. Ignoring job {}".format(
                    data["name"]
                )
                log.error(data["_error"])
                return
            if not isinstance(data["range"], dict):
                data["_error"] = (
                    "schedule.handle_func: Invalid, range "
                    "must be specified as a dictionary."
                    "Ignoring job {}.".format(data["name"])
                )
                log.error(data["_error"])
                return
            start = data["range"]["start"]
            end = data["range"]["end"]
            if not isinstance(start, datetime.datetime):
                try:
                    start = dateutil_parser.parse(start)
                except ValueError:
                    data[
                        "_error"
                    ] = "Invalid date string for start. Ignoring job {}.".format(
                        data["name"]
                    )
                    log.error(data["_error"])
                    return
            if not isinstance(end, datetime.datetime):
                try:
                    end = dateutil_parser.parse(end)
                except ValueError:
                    data[
                        "_error"
                    ] = "Invalid date string for end. Ignoring job {}.".format(
                        data["name"]
                    )
                    log.error(data["_error"])
                    return
            if end &gt; start:
                if "invert" in data["range"] and data["range"]["invert"]:
                    if now &lt;= start or now &gt;= end:
                        data["run"] = True
                    else:
                        data["_skip_reason"] = "in_skip_range"
                        data["run"] = False
                else:
                    if start &lt;= now &lt;= end:
                        data["run"] = True
                    else:
                        if self.skip_function:
                            data["run"] = True
                            data["func"] = self.skip_function
                        else:
                            data["_skip_reason"] = "not_in_range"
                            data["run"] = False
            else:
                data["_error"] = (
                    "schedule.handle_func: Invalid "
                    "range, end must be larger "
                    "than start. Ignoring job {}.".format(data["name"])
                )
                log.error(data["_error"])
        def _handle_after(data):
            if not _WHEN_SUPPORTED:
                data["_error"] = "Missing python-dateutil. Ignoring job {}".format(
                    data["name"]
                )
                log.error(data["_error"])
                return
            after = data["after"]
            if not isinstance(after, datetime.datetime):
                after = dateutil_parser.parse(after)
            if after &gt;= now:
                log.debug("After time has not passed skipping job: %s.", data["name"])
                data["_skip_reason"] = "after_not_passed"
                data["_skipped_time"] = now
                data["_skipped"] = True
                data["run"] = False
            else:
                data["run"] = True
        def _handle_until(data):
            if not _WHEN_SUPPORTED:
                data["_error"] = "Missing python-dateutil. Ignoring job {}".format(
                    data["name"]
                )
                log.error(data["_error"])
                return
            until = data["until"]
            if not isinstance(until, datetime.datetime):
                until = dateutil_parser.parse(until)
            if until &lt;= now:
                log.debug("Until time has passed skipping job: %s.", data["name"])
                data["_skip_reason"] = "until_passed"
                data["_skipped_time"] = now
                data["_skipped"] = True
                data["run"] = False
            else:
                data["run"] = True
        def _chop_ms(dt):
            return dt - datetime.timedelta(microseconds=dt.microsecond)
        schedule = self._get_schedule()
        if not isinstance(schedule, dict):
            raise ValueError("Schedule must be of type dict.")
        if "skip_function" in schedule:
            self.skip_function = schedule["skip_function"]
        if "skip_during_range" in schedule:
            self.skip_during_range = schedule["skip_during_range"]
        if "enabled" in schedule:
            self.enabled = schedule["enabled"]
        if "splay" in schedule:
            self.splay = schedule["splay"]
        _hidden = ["enabled", "skip_function", "skip_during_range", "splay"]
        for job, data in schedule.items():
            if job in _hidden:
                continue
            for item in [
                "_continue",
                "_error",
                "_enabled",
                "_skipped",
                "_skip_reason",
                "_skipped_time",
            ]:
                if item in data:
                    del data[item]
            run = False
            if "name" in data:
                job_name = data["name"]
            else:
                job_name = data["name"] = job
            if not isinstance(data, dict):
                log.error(
                    'Scheduled job "%s" should have a dict value, not %s',
                    job_name,
                    type(data),
                )
                continue
            if "function" in data:
                func = data["function"]
            elif "func" in data:
                func = data["func"]
            elif "fun" in data:
                func = data["fun"]
            else:
                func = None
            if func not in self.functions:
                log.info("Invalid function: %s in scheduled job %s.", func, job_name)
            if "_next_fire_time" not in data:
                data["_next_fire_time"] = None
            if "_splay" not in data:
                data["_splay"] = None
            if (
                "run_on_start" in data
                and data["run_on_start"]
                and "_run_on_start" not in data
            ):
                data["_run_on_start"] = True
            if not now:
                now = datetime.datetime.now()
            schedule_keys = set(data.keys())
            time_elements = ("seconds", "minutes", "hours", "days")
            scheduling_elements = ("when", "cron", "once")
            invalid_sched_combos = [
                set(i) for i in itertools.combinations(scheduling_elements, 2)
            ]
            if any(i &lt;= schedule_keys for i in invalid_sched_combos):
                log.error(
                    'Unable to use "%s" options together. Ignoring.',
                    '", "'.join(scheduling_elements),
                )
                continue
            invalid_time_combos = []
            for item in scheduling_elements:
                all_items = itertools.chain([item], time_elements)
                invalid_time_combos.append(set(itertools.combinations(all_items, 2)))
            if any(set(x) &lt;= schedule_keys for x in invalid_time_combos):
                log.error(
                    'Unable to use "%s" with "%s" options. Ignoring',
                    '", "'.join(time_elements),
                    '", "'.join(scheduling_elements),
                )
                continue
            if "run_explicit" in data:
                _handle_run_explicit(data, loop_interval)
                run = data["run"]
            if True in [True for item in time_elements if item in data]:
                _handle_time_elements(data)
            elif "once" in data:
                _handle_once(data, loop_interval)
            elif "when" in data:
                _handle_when(data, loop_interval)
            elif "cron" in data:
                _handle_cron(data, loop_interval)
            else:
                continue
            if "_continue" in data and data["_continue"]:
                continue
            if "_error" in data and data["_error"]:
                continue
            seconds = int(
                (_chop_ms(data["_next_fire_time"]) - _chop_ms(now)).total_seconds()
            )
            if "splay" not in data:
                data["splay"] = self.splay
            if "splay" in data and data["splay"]:
                if not data["_splay"]:
                    splay = _splay(data["splay"])
                    if now &lt; data["_next_fire_time"] + datetime.timedelta(
                        seconds=splay
                    ):
                        log.debug(
                            "schedule.handle_func: Adding splay of "
                            "%s seconds to next run.",
                            splay,
                        )
                        data["_splay"] = data["_next_fire_time"] + datetime.timedelta(
                            seconds=splay
                        )
                        if "when" in data:
                            data["_run"] = True
                    else:
                        run = True
                if data["_splay"]:
                    seconds = (data["_splay"] - now).total_seconds()
                    if "when" in data:
                        data["_next_fire_time"] = data["_splay"]
            if "_seconds" in data:
                if seconds &lt;= 0:
                    run = True
            elif "when" in data and data["_run"]:
                if (
                    data["_next_fire_time"]
                    &lt;= now
                    &lt;= (data["_next_fire_time"] + loop_interval)
                ):
                    data["_run"] = False
                    run = True
            elif "cron" in data:
                if seconds &lt;= 0:
                    data["_next_fire_time"] = None
                    run = True
            elif "once" in data:
                if (
                    data["_next_fire_time"]
                    &lt;= now
                    &lt;= (data["_next_fire_time"] + loop_interval)
                ):
                    run = True
            elif seconds == 0:
                run = True
            if "_run_on_start" in data and data["_run_on_start"]:
                run = True
                data["_run_on_start"] = False
            elif run:
                if "range" in data:
                    _handle_range(data)
                    if "_error" in data and data["_error"]:
                        continue
                    run = data["run"]
                    if "func" in data:
                        func = data["func"]
                if "skip_during_range" not in data and self.skip_during_range:
                    data["skip_during_range"] = self.skip_during_range
                if "skip_during_range" in data and data["skip_during_range"]:
                    _handle_skip_during_range(data, loop_interval)
                    if "_error" in data and data["_error"]:
                        continue
                    run = data["run"]
                    if "func" in data:
                        func = data["func"]
                if "skip_explicit" in data:
                    _handle_skip_explicit(data, loop_interval)
                    if "_error" in data and data["_error"]:
                        continue
                    run = data["run"]
                    if "func" in data:
                        func = data["func"]
                if "until" in data:
                    _handle_until(data)
                    if "_error" in data and data["_error"]:
                        continue
                    run = data["run"]
                if "after" in data:
                    _handle_after(data)
                    if "_error" in data and data["_error"]:
                        continue
                    run = data["run"]
            if "_continue" in data and data["_continue"]:
                run = False
            if not self.enabled or not data.get("enabled", True):
                log.trace("Job: %s is disabled", job_name)
                data["_skip_reason"] = "disabled"
                data["_skipped_time"] = now
                data["_skipped"] = True
                run = False
            miss_msg = ""
            if seconds &lt; 0:
                miss_msg = " (runtime missed by {} seconds)".format(abs(seconds))
            try:
                if run:
                    if "jid_include" not in data or data["jid_include"]:
                        data["jid_include"] = True
                        log.debug(
                            "schedule: Job %s was scheduled with jid_include, "
                            "adding to cache (jid_include defaults to True)",
                            job_name,
                        )
                        if "maxrunning" in data:
                            log.debug(
                                "schedule: Job %s was scheduled with a max "
                                "number of %s",
                                job_name,
                                data["maxrunning"],
                            )
                        else:
                            log.info(
                                "schedule: maxrunning parameter was not specified for "
                                "job %s, defaulting to 1.",
                                job_name,
                            )
                            data["maxrunning"] = 1
                    if not self.standalone:
                        data["run"] = run
                        data = self._check_max_running(func, data, self.opts, now)
                        run = data["run"]
                if run:
                    jid = salt.utils.jid.gen_jid(self.opts)
                    jids.append(jid)
                    log.info(
                        "Running scheduled job: %s%s with jid %s",
                        job_name,
                        miss_msg,
                        jid,
                    )
                    self._run_job(func, data, jid=jid)
            finally:
                if run:
                    data["_last_run"] = now
                    data["_splay"] = None
                if "_seconds" in data:
                    if self.standalone:
                        data["_next_fire_time"] = now + datetime.timedelta(
                            seconds=data["_seconds"]
                        )
                    elif "_skipped" in data and data["_skipped"]:
                        data["_next_fire_time"] = now + datetime.timedelta(
                            seconds=data["_seconds"]
                        )
                    elif run:
                        data["_next_fire_time"] = now + datetime.timedelta(
                            seconds=data["_seconds"]
                        )
        return jids
    def _run_job(self, func, data, jid=None):
        job_dry_run = data.get("dry_run", False)
        if job_dry_run:
            log.debug("Job %s has 'dry_run' set to True. Not running it.", data["name"])
            return
        multiprocessing_enabled = self.opts.get("multiprocessing", True)
        run_schedule_jobs_in_background = self.opts.get(
            "run_schedule_jobs_in_background", True
        )
        if run_schedule_jobs_in_background is False:
            self.handle_func(False, func, data, jid)
            return
        if multiprocessing_enabled and salt.utils.platform.is_windows():
            functions = self.functions
            self.functions = {}
            returners = self.returners
            self.returners = {}
            utils = self.utils
            self.utils = {}
        try:
            if multiprocessing_enabled:
                thread_cls = salt.utils.process.SignalHandlingProcess
            else:
                thread_cls = threading.Thread
            name = "Schedule(name={}, jid={})".format(data["name"], jid)
            if multiprocessing_enabled:
                with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):
                    proc = thread_cls(
                        target=self.handle_func,
                        args=(multiprocessing_enabled, func, data, jid),
                        name=name,
                    )
                    proc.start()
                    self._subprocess_list.add(proc)
            else:
                proc = thread_cls(
                    target=self.handle_func,
                    args=(multiprocessing_enabled, func, data, jid),
                    name=name,
                )
                proc.start()
                self._subprocess_list.add(proc)
        finally:
            if multiprocessing_enabled and salt.utils.platform.is_windows():
                self.functions = functions
                self.returners = returners
                self.utils = utils
    def cleanup_subprocesses(self):
        self._subprocess_list.cleanup()
def clean_proc_dir(opts):
    for basefilename in os.listdir(salt.minion.get_proc_dir(opts["cachedir"])):
        fn_ = os.path.join(salt.minion.get_proc_dir(opts["cachedir"]), basefilename)
        with salt.utils.files.fopen(fn_, "rb") as fp_:
            job = None
            try:
                job = salt.payload.load(fp_)
            except Exception:  # pylint: disable=broad-except
                if salt.utils.platform.is_windows():
                    fp_.close()
                try:
                    os.unlink(fn_)
                    continue
                except OSError:
                    continue
            log.debug(
                "schedule.clean_proc_dir: checking job %s for process existence", job
            )
            if job is not None and "pid" in job:
                if salt.utils.process.os_is_running(job["pid"]):
                    log.debug(
                        "schedule.clean_proc_dir: Cleaning proc dir, pid %s "
                        "still exists.",
                        job["pid"],
                    )
                else:
                    if salt.utils.platform.is_windows():
                        fp_.close()
                    try:
                        os.unlink(fn_)
                    except OSError:
                        pass
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
