
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 121, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-vqvae.py</h3>
            <pre><code>1  from sonnet.src import base
2  from sonnet.src import initializers
3  from sonnet.src import moving_averages
4  from sonnet.src import types
5  import tensorflow as tf
6  class VectorQuantizer(base.Module):
7    def __init__(self,
8                 embedding_dim: int,
9                 num_embeddings: int,
10                 commitment_cost: types.FloatLike,
11                 dtype: tf.DType = tf.float32,
12                 name: str = 'vector_quantizer'):
13      super().__init__(name=name)
14      self.embedding_dim = embedding_dim
15      self.num_embeddings = num_embeddings
16      self.commitment_cost = commitment_cost
17      embedding_shape = [embedding_dim, num_embeddings]
18      initializer = initializers.VarianceScaling(distribution='uniform')
19      self.embeddings = tf.Variable(
20          initializer(embedding_shape, dtype), name='embeddings')
21    def __call__(self, inputs, is_training):
22      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
23      distances = (
<span onclick='openModal()' class='match'>24          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
25          2 * tf.matmul(flat_inputs, self.embeddings) +
26          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
27      encoding_indices = tf.argmax(-distances, 1)
28      encodings = tf.one_hot(encoding_indices,
29                             self.num_embeddings,
30                             dtype=distances.dtype)
31      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
32      quantized = self.quantize(encoding_indices)
33      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
34      q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs))**2)
</span>35      loss = q_latent_loss + self.commitment_cost * e_latent_loss
36      quantized = inputs + tf.stop_gradient(quantized - inputs)
37      avg_probs = tf.reduce_mean(encodings, 0)
38      perplexity = tf.exp(-tf.reduce_sum(avg_probs *
39                                         tf.math.log(avg_probs + 1e-10)))
40      return {
41          'quantize': quantized,
42          'loss': loss,
43          'perplexity': perplexity,
44          'encodings': encodings,
45          'encoding_indices': encoding_indices,
46          'distances': distances,
47      }
48    def quantize(self, encoding_indices):
49      w = tf.transpose(self.embeddings, [1, 0])
50      return tf.nn.embedding_lookup(w, encoding_indices)
51  class VectorQuantizerEMA(base.Module):
52    def __init__(self,
53                 embedding_dim,
54                 num_embeddings,
55                 commitment_cost,
56                 decay,
57                 epsilon=1e-5,
58                 dtype=tf.float32,
59                 name='vector_quantizer_ema'):
60      super().__init__(name=name)
61      self.embedding_dim = embedding_dim
62      self.num_embeddings = num_embeddings
63      if not 0 <= decay <= 1:
64        raise ValueError('decay must be in range [0, 1]')
65      self.decay = decay
66      self.commitment_cost = commitment_cost
67      self.epsilon = epsilon
68      embedding_shape = [embedding_dim, num_embeddings]
69      initializer = initializers.VarianceScaling(distribution='uniform')
70      self.embeddings = tf.Variable(
71          initializer(embedding_shape, dtype), trainable=False, name='embeddings')
72      self.ema_cluster_size = moving_averages.ExponentialMovingAverage(
73          decay=self.decay, name='ema_cluster_size')
74      self.ema_cluster_size.initialize(tf.zeros([num_embeddings], dtype=dtype))
75      self.ema_dw = moving_averages.ExponentialMovingAverage(
76          decay=self.decay, name='ema_dw')
77      self.ema_dw.initialize(self.embeddings)
78    def __call__(self, inputs, is_training):
79      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
80      distances = (
81          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
82          2 * tf.matmul(flat_inputs, self.embeddings) +
83          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
84      encoding_indices = tf.argmax(-distances, 1)
85      encodings = tf.one_hot(encoding_indices,
86                             self.num_embeddings,
87                             dtype=distances.dtype)
88      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
89      quantized = self.quantize(encoding_indices)
90      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
91      if is_training:
92        updated_ema_cluster_size = self.ema_cluster_size(
93            tf.reduce_sum(encodings, axis=0))
94        dw = tf.matmul(flat_inputs, encodings, transpose_a=True)
95        updated_ema_dw = self.ema_dw(dw)
96        n = tf.reduce_sum(updated_ema_cluster_size)
97        updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /
98                                    (n + self.num_embeddings * self.epsilon) * n)
99        normalised_updated_ema_w = (
100            updated_ema_dw / tf.reshape(updated_ema_cluster_size, [1, -1]))
101        self.embeddings.assign(normalised_updated_ema_w)
102        loss = self.commitment_cost * e_latent_loss
103      else:
104        loss = self.commitment_cost * e_latent_loss
105      quantized = inputs + tf.stop_gradient(quantized - inputs)
106      avg_probs = tf.reduce_mean(encodings, 0)
107      perplexity = tf.exp(-tf.reduce_sum(avg_probs *
108                                         tf.math.log(avg_probs + 1e-10)))
109      return {
110          'quantize': quantized,
111          'loss': loss,
112          'perplexity': perplexity,
113          'encodings': encodings,
114          'encoding_indices': encoding_indices,
115          'distances': distances,
116      }
117    def quantize(self, encoding_indices):
118      w = tf.transpose(self.embeddings, [1, 0])
119      return tf.nn.embedding_lookup(w, encoding_indices)
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-vqvae.py</h3>
            <pre><code>1  from sonnet.src import base
2  from sonnet.src import initializers
3  from sonnet.src import moving_averages
4  from sonnet.src import types
5  import tensorflow as tf
6  class VectorQuantizer(base.Module):
7    def __init__(self,
8                 embedding_dim: int,
9                 num_embeddings: int,
10                 commitment_cost: types.FloatLike,
11                 dtype: tf.DType = tf.float32,
12                 name: str = 'vector_quantizer'):
13      super().__init__(name=name)
14      self.embedding_dim = embedding_dim
15      self.num_embeddings = num_embeddings
16      self.commitment_cost = commitment_cost
17      embedding_shape = [embedding_dim, num_embeddings]
18      initializer = initializers.VarianceScaling(distribution='uniform')
19      self.embeddings = tf.Variable(
20          initializer(embedding_shape, dtype), name='embeddings')
21    def __call__(self, inputs, is_training):
22      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
23      distances = (
24          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
25          2 * tf.matmul(flat_inputs, self.embeddings) +
26          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
27      encoding_indices = tf.argmax(-distances, 1)
28      encodings = tf.one_hot(encoding_indices,
29                             self.num_embeddings,
30                             dtype=distances.dtype)
31      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
32      quantized = self.quantize(encoding_indices)
33      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
34      q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs))**2)
35      loss = q_latent_loss + self.commitment_cost * e_latent_loss
36      quantized = inputs + tf.stop_gradient(quantized - inputs)
37      avg_probs = tf.reduce_mean(encodings, 0)
38      perplexity = tf.exp(-tf.reduce_sum(avg_probs *
39                                         tf.math.log(avg_probs + 1e-10)))
40      return {
41          'quantize': quantized,
42          'loss': loss,
43          'perplexity': perplexity,
44          'encodings': encodings,
45          'encoding_indices': encoding_indices,
46          'distances': distances,
47      }
48    def quantize(self, encoding_indices):
49      w = tf.transpose(self.embeddings, [1, 0])
50      return tf.nn.embedding_lookup(w, encoding_indices)
51  class VectorQuantizerEMA(base.Module):
52    def __init__(self,
53                 embedding_dim,
54                 num_embeddings,
55                 commitment_cost,
56                 decay,
57                 epsilon=1e-5,
58                 dtype=tf.float32,
59                 name='vector_quantizer_ema'):
60      super().__init__(name=name)
61      self.embedding_dim = embedding_dim
62      self.num_embeddings = num_embeddings
63      if not 0 <= decay <= 1:
64        raise ValueError('decay must be in range [0, 1]')
65      self.decay = decay
66      self.commitment_cost = commitment_cost
67      self.epsilon = epsilon
68      embedding_shape = [embedding_dim, num_embeddings]
69      initializer = initializers.VarianceScaling(distribution='uniform')
70      self.embeddings = tf.Variable(
71          initializer(embedding_shape, dtype), trainable=False, name='embeddings')
72      self.ema_cluster_size = moving_averages.ExponentialMovingAverage(
73          decay=self.decay, name='ema_cluster_size')
74      self.ema_cluster_size.initialize(tf.zeros([num_embeddings], dtype=dtype))
75      self.ema_dw = moving_averages.ExponentialMovingAverage(
76          decay=self.decay, name='ema_dw')
77      self.ema_dw.initialize(self.embeddings)
78    def __call__(self, inputs, is_training):
79      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
80      distances = (
<span onclick='openModal()' class='match'>81          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
82          2 * tf.matmul(flat_inputs, self.embeddings) +
83          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
84      encoding_indices = tf.argmax(-distances, 1)
85      encodings = tf.one_hot(encoding_indices,
86                             self.num_embeddings,
87                             dtype=distances.dtype)
88      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
89      quantized = self.quantize(encoding_indices)
90      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
91      if is_training:
</span>92        updated_ema_cluster_size = self.ema_cluster_size(
93            tf.reduce_sum(encodings, axis=0))
94        dw = tf.matmul(flat_inputs, encodings, transpose_a=True)
95        updated_ema_dw = self.ema_dw(dw)
96        n = tf.reduce_sum(updated_ema_cluster_size)
97        updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /
98                                    (n + self.num_embeddings * self.epsilon) * n)
99        normalised_updated_ema_w = (
100            updated_ema_dw / tf.reshape(updated_ema_cluster_size, [1, -1]))
101        self.embeddings.assign(normalised_updated_ema_w)
102        loss = self.commitment_cost * e_latent_loss
103      else:
104        loss = self.commitment_cost * e_latent_loss
105      quantized = inputs + tf.stop_gradient(quantized - inputs)
106      avg_probs = tf.reduce_mean(encodings, 0)
107      perplexity = tf.exp(-tf.reduce_sum(avg_probs *
108                                         tf.math.log(avg_probs + 1e-10)))
109      return {
110          'quantize': quantized,
111          'loss': loss,
112          'perplexity': perplexity,
113          'encodings': encodings,
114          'encoding_indices': encoding_indices,
115          'distances': distances,
116      }
117    def quantize(self, encoding_indices):
118      w = tf.transpose(self.embeddings, [1, 0])
119      return tf.nn.embedding_lookup(w, encoding_indices)
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-vqvae.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-vqvae.py</div>
                </div>
                <div class="column column_space"><pre><code>24          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
25          2 * tf.matmul(flat_inputs, self.embeddings) +
26          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
27      encoding_indices = tf.argmax(-distances, 1)
28      encodings = tf.one_hot(encoding_indices,
29                             self.num_embeddings,
30                             dtype=distances.dtype)
31      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
32      quantized = self.quantize(encoding_indices)
33      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
34      q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs))**2)
</pre></code></div>
                <div class="column column_space"><pre><code>81          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
82          2 * tf.matmul(flat_inputs, self.embeddings) +
83          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
84      encoding_indices = tf.argmax(-distances, 1)
85      encodings = tf.one_hot(encoding_indices,
86                             self.num_embeddings,
87                             dtype=distances.dtype)
88      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
89      quantized = self.quantize(encoding_indices)
90      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
91      if is_training:
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    