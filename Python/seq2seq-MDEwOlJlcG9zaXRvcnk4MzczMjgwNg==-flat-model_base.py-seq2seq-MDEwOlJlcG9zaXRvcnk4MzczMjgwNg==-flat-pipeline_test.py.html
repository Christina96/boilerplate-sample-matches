
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 12, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-model_base.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import collections
6  import tensorflow as tf
7  from seq2seq.configurable import Configurable
8  from seq2seq.training import utils as training_utils
9  from seq2seq import global_vars
10  def _flatten_dict(dict_, parent_key="", sep="."):
11    items = []
12    for key, value in dict_.items():
13      new_key = parent_key + sep + key if parent_key else key
14      if isinstance(value, collections.MutableMapping):
15        items.extend(_flatten_dict(value, new_key, sep=sep).items())
16      elif isinstance(value, tuple) and hasattr(value, "_asdict"):
17        dict_items = collections.OrderedDict(zip(value._fields, value))
18        items.extend(_flatten_dict(dict_items, new_key, sep=sep).items())
19      else:
20        items.append((new_key, value))
21    return dict(items)
22  class ModelBase(Configurable):
23    def __init__(self, params, mode, name):
24      self.name = name
25      Configurable.__init__(self, params, mode)
26    def _clip_gradients(self, grads_and_vars):
27      gradients, variables = zip(*grads_and_vars)
28      clipped_gradients, _ = tf.clip_by_global_norm(
29          gradients, self.params["optimizer.clip_gradients"])
30      return list(zip(clipped_gradients, variables))
31    def _create_optimizer(self):
32      name = self.params["optimizer.name"]
33      optimizer = tf.contrib.layers.OPTIMIZER_CLS_NAMES[name](
34          learning_rate=self.params["optimizer.learning_rate"],
35          **self.params["optimizer.params"])
36      if self.params["optimizer.sync_replicas"] > 0:
37        optimizer = tf.train.SyncReplicasOptimizer(
38            opt=optimizer,
39            replicas_to_aggregate=self.params[
40                "optimizer.sync_replicas_to_aggregate"],
41            total_num_replicas=self.params["optimizer.sync_replicas"])
42        global_vars.SYNC_REPLICAS_OPTIMIZER = optimizer
43      return optimizer
44    def _build_train_op(self, loss):
45      learning_rate_decay_fn = training_utils.create_learning_rate_decay_fn(
46          decay_type=self.params["optimizer.lr_decay_type"] or None,
47          decay_steps=self.params["optimizer.lr_decay_steps"],
48          decay_rate=self.params["optimizer.lr_decay_rate"],
49          start_decay_at=self.params["optimizer.lr_start_decay_at"],
50          stop_decay_at=self.params["optimizer.lr_stop_decay_at"],
51          min_learning_rate=self.params["optimizer.lr_min_learning_rate"],
52          staircase=self.params["optimizer.lr_staircase"])
53      optimizer = self._create_optimizer()
54      train_op = tf.contrib.layers.optimize_loss(
55          loss=loss,
56          global_step=tf.contrib.framework.get_global_step(),
57          learning_rate=self.params["optimizer.learning_rate"],
58          learning_rate_decay_fn=learning_rate_decay_fn,
59          clip_gradients=self._clip_gradients,
60          optimizer=optimizer,
<span onclick='openModal()' class='match'>61          summaries=["learning_rate", "loss", "gradients", "gradient_norm"])
62      return train_op
</span>63    @staticmethod
64    def default_params():
65      return {
66          "optimizer.name": "Adam",
67          "optimizer.learning_rate": 1e-4,
68          "optimizer.params": {}, # Arbitrary parameters for the optimizer
69          "optimizer.lr_decay_type": "",
70          "optimizer.lr_decay_steps": 100,
71          "optimizer.lr_decay_rate": 0.99,
72          "optimizer.lr_start_decay_at": 0,
73          "optimizer.lr_stop_decay_at": tf.int32.max,
74          "optimizer.lr_min_learning_rate": 1e-12,
75          "optimizer.lr_staircase": False,
76          "optimizer.clip_gradients": 5.0,
77          "optimizer.sync_replicas": 0,
78          "optimizer.sync_replicas_to_aggregate": 0,
79      }
80    def batch_size(self, features, labels):
81      raise NotImplementedError()
82    def __call__(self, features, labels, params):
83      with tf.variable_scope("model"):
84        with tf.variable_scope(self.name):
85          return self._build(features, labels, params)
86    def _build(self, features, labels, params):
87      raise NotImplementedError
</code></pre>
        </div>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-pipeline_test.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import argparse
6  import imp
7  import os
8  import shutil
9  import tempfile
10  import yaml
11  import numpy as np
12  import tensorflow as tf
13  from tensorflow import gfile
14  from seq2seq.test import utils as test_utils
15  BIN_FOLDER = os.path.abspath(
16      os.path.join(os.path.dirname(__file__), "../../bin"))
17  def _clear_flags():
18    tf.app.flags.FLAGS = tf.app.flags._FlagValues()
19    tf.app.flags._global_parser = argparse.ArgumentParser()
20  class PipelineTest(tf.test.TestCase):
21    def setUp(self):
22      super(PipelineTest, self).setUp()
23      self.output_dir = tempfile.mkdtemp()
24      self.bin_folder = os.path.abspath(
25          os.path.join(os.path.dirname(__file__), "../../bin"))
26      tf.contrib.framework.get_or_create_global_step()
27    def tearDown(self):
28      shutil.rmtree(self.output_dir, ignore_errors=True)
29      super(PipelineTest, self).tearDown()
30    def test_train_infer(self):
31      sources_train, targets_train = test_utils.create_temp_parallel_data(
32          sources=["a a a a", "b b b b", "c c c c", "笑 笑 笑 笑"],
<span onclick='openModal()' class='match'>33          targets=["b b b b", "a a a a", "c c c c", "泣 泣 泣 泣"])
34      sources_dev, targets_dev = test_utils.create_temp_parallel_data(
</span>35          sources=["a a", "b b", "c c c", "笑 笑 笑"],
36          targets=["b b", "a a", "c c c", "泣 泣 泣"])
37      vocab_source = test_utils.create_temporary_vocab_file(["a", "b", "c", "笑"])
38      vocab_target = test_utils.create_temporary_vocab_file(["a", "b", "c", "泣"])
39      _clear_flags()
40      tf.reset_default_graph()
41      train_script = imp.load_source("seq2seq.test.train_bin",
42                                     os.path.join(BIN_FOLDER, "train.py"))
43      tf.app.flags.FLAGS.output_dir = self.output_dir
44      tf.app.flags.FLAGS.hooks = 
45      tf.app.flags.FLAGS.metrics = 
46      tf.app.flags.FLAGS.model = "AttentionSeq2Seq"
47      tf.app.flags.FLAGS.model_params = .format(vocab_source.name, vocab_target.name)
48      tf.app.flags.FLAGS.batch_size = 2
49      config_path = os.path.join(self.output_dir, "train_config.yml")
50      with gfile.GFile(config_path, "w") as config_file:
51        yaml.dump({
52            "input_pipeline_train": {
53                "class": "ParallelTextInputPipeline",
54                "params": {
55                    "source_files": [sources_train.name],
56                    "target_files": [targets_train.name],
57                }
58            },
59            "input_pipeline_dev": {
60                "class": "ParallelTextInputPipeline",
61                "params": {
62                    "source_files": [sources_dev.name],
63                    "target_files": [targets_dev.name],
64                }
65            },
66            "train_steps": 50,
67            "model_params": {
68                "embedding.dim": 10,
69                "decoder.params": {
70                    "rnn_cell": {
71                        "cell_class": "GRUCell",
72                        "cell_params": {
73                            "num_units": 8
74                        }
75                    }
76                },
77                "encoder.params": {
78                    "rnn_cell": {
79                        "cell_class": "GRUCell",
80                        "cell_params": {
81                            "num_units": 8
82                        }
83                    }
84                }
85            }
86        }, config_file)
87      tf.app.flags.FLAGS.config_paths = config_path
88      tf.logging.set_verbosity(tf.logging.INFO)
89      train_script.main([])
90      expected_checkpoint = os.path.join(self.output_dir,
91                                         "model.ckpt-50.data-00000-of-00001")
92      self.assertTrue(os.path.exists(expected_checkpoint))
93      _clear_flags()
94      tf.reset_default_graph()
95      infer_script = imp.load_source("seq2seq.test.infer_bin",
96                                     os.path.join(BIN_FOLDER, "infer.py"))
97      attention_dir = os.path.join(self.output_dir, "att")
98      tf.app.flags.FLAGS.model_dir = self.output_dir
99      tf.app.flags.FLAGS.input_pipeline = .format(sources_dev.name, targets_dev.name)
100      tf.app.flags.FLAGS.batch_size = 2
101      tf.app.flags.FLAGS.checkpoint_path = os.path.join(self.output_dir,
102                                                        "model.ckpt-50")
103      tf.app.flags.FLAGS.tasks = .format(attention_dir)
104      infer_script.main([])
105      self.assertTrue(
106          os.path.exists(os.path.join(attention_dir, "attention_scores.npz")))
107      self.assertTrue(os.path.exists(os.path.join(attention_dir, "00002.png")))
108      scores = np.load(os.path.join(attention_dir, "attention_scores.npz"))
109      self.assertIn("arr_0", scores)
110      self.assertEqual(scores["arr_0"].shape[1], 3)
111      self.assertIn("arr_1", scores)
112      self.assertEqual(scores["arr_1"].shape[1], 3)
113      self.assertIn("arr_2", scores)
114      self.assertEqual(scores["arr_2"].shape[1], 4)
115      self.assertIn("arr_3", scores)
116      self.assertEqual(scores["arr_3"].shape[1], 4)
117      _clear_flags()
118      tf.reset_default_graph()
119      infer_script = imp.load_source("seq2seq.test.infer_bin",
120                                     os.path.join(BIN_FOLDER, "infer.py"))
121      tf.app.flags.FLAGS.model_dir = self.output_dir
122      tf.app.flags.FLAGS.input_pipeline = .format(sources_dev.name, targets_dev.name)
123      tf.app.flags.FLAGS.batch_size = 2
124      tf.app.flags.FLAGS.checkpoint_path = os.path.join(self.output_dir,
125                                                        "model.ckpt-50")
126      tf.app.flags.FLAGS.model_params = 
127      tf.app.flags.FLAGS.tasks = .format(os.path.join(self.output_dir, "beams.npz"))
128      infer_script.main([])
129      self.assertTrue(os.path.exists(os.path.join(self.output_dir, "beams.npz")))
130  if __name__ == "__main__":
131    tf.test.main()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-model_base.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-pipeline_test.py</div>
                </div>
                <div class="column column_space"><pre><code>61          summaries=["learning_rate", "loss", "gradients", "gradient_norm"])
62      return train_op
</pre></code></div>
                <div class="column column_space"><pre><code>33          targets=["b b b b", "a a a a", "c c c c", "泣 泣 泣 泣"])
34      sources_dev, targets_dev = test_utils.create_temp_parallel_data(
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    