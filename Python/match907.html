<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_rop.py &amp; multinomial.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_rop.py &amp; multinomial.py
      </h3>
<h1 align="center">
        5.6%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_rop.py (4.0149393%)<th>multinomial.py (9.598214%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(264-274)<td><a href="#" name="0">(349-362)</a><td align="center"><font color="#ff0000">30</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(15-37)<td><a href="#" name="1">(10-25)</a><td align="center"><font color="#6e0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_rop.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 """
2 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>from theano.tests import unittest_tools as utt
3 from theano import function
4 import theano
5 from theano import tensor
6 import itertools
7 import numpy as np
8 from theano.gof import Op, Apply
9 from theano.gradient import grad_undefined
10 from theano.tests.unittest_tools import SkipTest
11 from theano.tensor.signal.pool import Pool
12 from theano.tensor.nnet import conv, conv2d
13 '''
14 Special Op created to test what happens when you have one op that is not
15 differentiable in the computational graph
16 '''
17 class BreakRop(Op):
18     """
19     @note: Non-differentiable.
20     """
21     __props__ =</b></font> ()
22     def make_node(self, x):
23         return Apply(self, [x], [x.type()])
24     def perform(self, node, inp, out_):
25         x, = inp
26         out, = out_
27         out[0] = x
28     def grad(self, inp, grads):
29         return [grad_undefined(self, 0, inp[0])]
30     def R_op(self, inputs, eval_points):
31         return [None]
32 break_op = BreakRop()
33 class RopLop_checker(unittest.TestCase):
34     """
35     Don't peform any test, but provide the function to test the
36     Rop to class that inherit from it.
37     """
38     def setUp(self):
39         utt.seed_rng()
40         self.x = tensor.vector('x')
41         self.v = tensor.vector('v')
42         self.rng = np.random.RandomState(utt.fetch_seed())
43         self.in_shape = (5 + self.rng.randint(3),)
44         self.mx = tensor.matrix('mx')
45         self.mv = tensor.matrix('mv')
46         self.mat_in_shape = (5 + self.rng.randint(3),
47                              5 + self.rng.randint(3))
48     def check_nondiff_rop(self, y):
49         """
50         If your op is not differentiable(so you can't define Rop)
51         test that an error is raised.
52         """
53         raised = False
54         try:
55             tensor.Rop(y, self.x, self.v)
56         except ValueError:
57             raised = True
58         if not raised:
59             self.fail((
60                 'Op did not raise an error even though the function'
61                 ' is not differentiable'))
62     def check_mat_rop_lop(self, y, out_shape):
63         """
64         Test the Rop/Lop when input is a matrix and the output is a vector
65         :param y: the output variable of the op applied to self.mx
66         :param out_shape: Used to generate a random tensor
67                           corresponding to the evaluation point of the Rop
68                           (i.e. the tensor with which you multiply the
69                           Jacobian). It should be a tuple of ints.
70         If the Op has more than 1 input, one of them must be mx, while
71         others must be shared variables / constants. We will test only
72         against the input self.mx, so you must call
73         check_mat_rop_lop/check_rop_lop for the other inputs.
74         We expect all inputs/outputs have dtype floatX.
75         If you want to test an Op with an output matrix, add a sum
76         after the Op you want to test.
77         """
78         vx = np.asarray(self.rng.uniform(size=self.mat_in_shape),
79                         theano.config.floatX)
80         vv = np.asarray(self.rng.uniform(size=self.mat_in_shape),
81                         theano.config.floatX)
82         yv = tensor.Rop(y, self.mx, self.mv)
83         rop_f = function([self.mx, self.mv], yv, on_unused_input='ignore')
84         sy, _ = theano.scan(lambda i, y, x, v:
85                             (tensor.grad(y[i], x) * v).sum(),
86                             sequences=tensor.arange(y.shape[0]),
87                             non_sequences=[y, self.mx, self.mv])
88         scan_f = function([self.mx, self.mv], sy, on_unused_input='ignore')
89         v1 = rop_f(vx, vv)
90         v2 = scan_f(vx, vv)
91         assert np.allclose(v1, v2), ('ROP mismatch: %s %s' % (v1, v2))
92         self.check_nondiff_rop(theano.clone(y, replace={self.mx: break_op(self.mx)}))
93         vv = np.asarray(self.rng.uniform(size=out_shape), theano.config.floatX)
94         yv = tensor.Lop(y, self.mx, self.v)
95         lop_f = function([self.mx, self.v], yv)
96         sy = tensor.grad((self.v * y).sum(), self.mx)
97         scan_f = function([self.mx, self.v], sy)
98         v1 = lop_f(vx, vv)
99         v2 = scan_f(vx, vv)
100         assert np.allclose(v1, v2), ('LOP mismatch: %s %s' % (v1, v2))
101     def check_rop_lop(self, y, out_shape):
102         """
103         As check_mat_rop_lop, except the input is self.x which is a
104         vector. The output is still a vector.
105         """
106         vx = np.asarray(self.rng.uniform(size=self.in_shape),
107                         theano.config.floatX)
108         vv = np.asarray(self.rng.uniform(size=self.in_shape),
109                         theano.config.floatX)
110         yv = tensor.Rop(y, self.x, self.v)
111         rop_f = function([self.x, self.v], yv, on_unused_input='ignore')
112         J, _ = theano.scan(lambda i, y, x: tensor.grad(y[i], x),
113                            sequences=tensor.arange(y.shape[0]),
114                            non_sequences=[y, self.x])
115         sy = tensor.dot(J, self.v)
116         scan_f = function([self.x, self.v], sy, on_unused_input='ignore')
117         v1 = rop_f(vx, vv)
118         v2 = scan_f(vx, vv)
119         assert np.allclose(v1, v2), ('ROP mismatch: %s %s' % (v1, v2))
120         known_fail = False
121         try:
122             self.check_nondiff_rop(theano.clone(y, replace={self.x: break_op(self.x)}))
123         except AssertionError:
124             known_fail = True
125         vx = np.asarray(self.rng.uniform(size=self.in_shape),
126                         theano.config.floatX)
127         vv = np.asarray(self.rng.uniform(size=out_shape),
128                         theano.config.floatX)
129         yv = tensor.Lop(y, self.x, self.v)
130         lop_f = function([self.x, self.v], yv, on_unused_input='ignore')
131         J, _ = theano.scan(lambda i, y, x: tensor.grad(y[i], x),
132                            sequences=tensor.arange(y.shape[0]),
133                            non_sequences=[y, self.x])
134         sy = tensor.dot(self.v, J)
135         scan_f = function([self.x, self.v], sy)
136         v1 = lop_f(vx, vv)
137         v2 = scan_f(vx, vv)
138         assert np.allclose(v1, v2), ('LOP mismatch: %s %s' % (v1, v2))
139         if known_fail:
140             raise SkipTest('Rop does not handle non-differentiable inputs '
141                            'correctly. Bug exposed by fixing Add.grad method.')
142 class test_RopLop(RopLop_checker):
143     def test_shape(self):
144         self.check_nondiff_rop(self.x.shape[0])
145     def test_specifyshape(self):
146         self.check_rop_lop(tensor.specify_shape(self.x, self.in_shape),
147                            self.in_shape)
148     def test_max(self):
149         self.check_mat_rop_lop(tensor.max(self.mx, axis=0),
150                                (self.mat_in_shape[1],))
151         self.check_mat_rop_lop(tensor.max(self.mx, axis=1),
152                                (self.mat_in_shape[0],))
153     def test_argmax(self):
154         self.check_nondiff_rop(tensor.argmax(self.mx, axis=1))
155     def test_subtensor(self):
156         self.check_rop_lop(self.x[:4], (4,))
157     def test_incsubtensor1(self):
158         tv = np.asarray(self.rng.uniform(size=(3,)),
159                         theano.config.floatX)
160         t = theano.shared(tv)
161         out = tensor.inc_subtensor(self.x[:3], t)
162         self.check_rop_lop(out, self.in_shape)
163     def test_incsubtensor2(self):
164         tv = np.asarray(self.rng.uniform(size=(10,)),
165                         theano.config.floatX)
166         t = theano.shared(tv)
167         out = tensor.inc_subtensor(t[:4], self.x[:4])
168         self.check_rop_lop(out, (10,))
169     def test_setsubtensor1(self):
170         tv = np.asarray(self.rng.uniform(size=(3,)),
171                         theano.config.floatX)
172         t = theano.shared(tv)
173         out = tensor.set_subtensor(self.x[:3], t)
174         self.check_rop_lop(out, self.in_shape)
175     def test_print(self):
176         out = theano.printing.Print('x', attrs=('shape',))(self.x)
177         self.check_rop_lop(out, self.in_shape)
178     def test_setsubtensor2(self):
179         tv = np.asarray(self.rng.uniform(size=(10,)),
180                         theano.config.floatX)
181         t = theano.shared(tv)
182         out = tensor.set_subtensor(t[:4], self.x[:4])
183         self.check_rop_lop(out, (10,))
184     def test_dimshuffle(self):
185         self.check_rop_lop(self.x[:4].dimshuffle('x', 0).sum(axis=0),
186                            (4,))
187     def test_rebroadcast(self):
188         self.check_rop_lop(tensor.unbroadcast(
189             self.x[:4].dimshuffle('x', 0), 0).sum(axis=1),
190             (1,))
191         rng = np.random.RandomState(utt.fetch_seed())
192         examples <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (
193             ((2,), (16,)),
194             ((2,), (4, 16,)),
195             ((2,), (4, 2, 16,)),
196             ((1, 1), (4, 2, 16, 16)),
197             ((2, 2), (4, 2, 16, 16)),
198             ((3, 3), (4, 2, 16, 16)),
199             ((3, 2), (4, 2, 16, 16)),
200             ((3, 2, 2), (3, 2, 16, 16, 16)),
201             ((2, 3, 2), (3, 2, 16, 16, 16)),
202             ((</b></font>2, 2, 3), (3, 2, 16, 16, 16)),
203             ((2, 2, 3, 2), (3, 2, 6, 6, 6, 5)),
204         )
205         for example, ignore_border in itertools.product(examples, [True, False]):
206             (ws, shp) = example
207             vx = rng.rand(*shp)
208             vex = rng.rand(*shp)
209             x = theano.shared(vx)
210             ex = theano.shared(vex)
211             maxpool_op = Pool(ignore_border, ndim=len(ws))
212             a_pooled = maxpool_op(x, ws).flatten()
213             yv = tensor.Rop(a_pooled, x, ex)
214             mode = None
215             if theano.config.mode == "FAST_COMPILE":
216                 mode = "FAST_RUN"
217             rop_f = function([], yv, on_unused_input='ignore', mode=mode)
218             sy, _ = theano.scan(lambda i, y, x, v:
219                                 (tensor.grad(y[i], x) * v).sum(),
220                                 sequences=tensor.arange(a_pooled.shape[0]),
221                                 non_sequences=[a_pooled, x, ex],
222                                 mode=mode)
223             scan_f = function([], sy, on_unused_input='ignore', mode=mode)
224             v1 = rop_f()
225             v2 = scan_f()
226             assert np.allclose(v1, v2), ("Rop mismatch: %s %s" % (v1, v2))
227     def test_conv(self):
228         for conv_op in [conv.conv2d, conv2d]:
229             for border_mode in ['valid', 'full']:
230                 image_shape = (2, 2, 4, 5)
231                 filter_shape = (2, 2, 2, 3)
232                 image_dim = len(image_shape)
233                 filter_dim = len(filter_shape)
234                 input = tensor.TensorType(
235                     theano.config.floatX,
236                     [False] * image_dim)(name='input')
237                 filters = tensor.TensorType(
238                     theano.config.floatX,
239                     [False] * filter_dim)(name='filter')
240                 ev_input = tensor.TensorType(
241                     theano.config.floatX,
242                     [False] * image_dim)(name='ev_input')
243                 ev_filters = tensor.TensorType(
244                     theano.config.floatX,
245                     [False] * filter_dim)(name='ev_filters')
246                 def sym_conv2d(input, filters):
247                     return conv_op(input, filters, border_mode=border_mode)
248                 output = sym_conv2d(input, filters).flatten()
249                 yv = tensor.Rop(output, [input, filters], [ev_input, ev_filters])
250                 mode = None
251                 if theano.config.mode == "FAST_COMPILE":
252                     mode = "FAST_RUN"
253                 rop_f = function([input, filters, ev_input, ev_filters],
254                                  yv, on_unused_input='ignore', mode=mode)
255                 sy, _ = theano.scan(lambda i, y, x1, x2, v1, v2:
256                                     (tensor.grad(y[i], x1) * v1).sum() +
257                                     (tensor.grad(y[i], x2) * v2).sum(),
258                                     sequences=tensor.arange(output.shape[0]),
259                                     non_sequences=[output, input, filters,
260                                                    ev_input, ev_filters],
261                                     mode=mode)
262                 scan_f = function([input, filters, ev_input, ev_filters], sy,
263                                   on_unused_input='ignore', mode=mode)
264                 dtype = theano.config.floatX
265                 image_data = np.random.random(image_shape).astype(dtype)
266                 filter_data = np.random.random(filter_shape).astype(dtype)
267                 ev_image_data = np.random.random(image_shape).astype(dtype)
268                 ev_filter_data = np.random.random(filter_shape).astype(dtype)
269                 v1 = rop_f(image_data, filter_data, ev_image_data, ev_filter_data)
270                 v2 = scan_f(image_data, filter_data, ev_image_data, ev_filter_data)
271                 assert np.allclose(v1, v2), ("Rop mismatch: %s %s" % (v1, v2))
272     def test_join(self):
273         tv = np.asarray(self.rng.uniform(size=(10,)),
274                         theano.config.floatX)
275         t = theano.shared(tv)
276         out = tensor.join(0, self.x, t)
277         self.check_rop_lop(out, (self.in_shape[0] + 10,))
278     def test_dot(self):
279         insh = self.in_shape[0]
280         vW = np.asarray(self.rng.uniform(size=(insh, insh)),
281                         theano.config.floatX)
282         W = theano.shared(vW)
283         self.check_rop_lop(tensor.dot(self.x, W), self.in_shape)
284     def test_elemwise0(self):
285         self.check_rop_lop((self.x + 1) ** 2, self.in_shape)
286     def test_elemwise1(self):
287         self.check_rop_lop(self.x + tensor.cast(self.x, 'int32'),
288                            self.in_shape)
289     def test_reshape(self):
290         new_shape = tensor.constant(np.asarray([
291             self.mat_in_shape[0] * self.mat_in_shape[1]],
292             dtype='int64'))
293         self.check_mat_rop_lop(self.mx.reshape(new_shape),
294                                (self.mat_in_shape[0] * self.mat_in_shape[1],))
295     def test_flatten(self):
296         self.check_mat_rop_lop(self.mx.flatten(),
297                                (self.mat_in_shape[0] * self.mat_in_shape[1],))
298     def test_sum(self):
299         self.check_mat_rop_lop(self.mx.sum(axis=1), (self.mat_in_shape[0],))
300     def test_softmax(self):
301         self.check_rop_lop(tensor.nnet.softmax(self.x)[0], self.in_shape[0])
302     def test_alloc(self):
303         out1d = tensor.alloc(self.x.sum(), self.in_shape[0])
304         self.check_rop_lop(out1d, self.in_shape[0])
305         out3d = tensor.alloc(self.x, self.mat_in_shape[0], self.mat_in_shape[1], self.in_shape[0])
306         self.check_rop_lop(out3d.flatten(), self.mat_in_shape[0] * self.mat_in_shape[1] * self.in_shape[0])
307     def test_invalid_input(self):
308         success = False
309         try:
310             tensor.Rop(0., [tensor.matrix()], [tensor.vector()])
311             success = True
312         except ValueError:
313             pass
314         assert not success
315     def test_multiple_outputs(self):
316         m = tensor.matrix('m')
317         v = tensor.vector('v')
318         m_ = tensor.matrix('m_')
319         v_ = tensor.vector('v_')
320         mval = self.rng.uniform(size=(3, 7)).astype(theano.config.floatX)
321         vval = self.rng.uniform(size=(7,)).astype(theano.config.floatX)
322         m_val = self.rng.uniform(size=(3, 7)).astype(theano.config.floatX)
323         v_val = self.rng.uniform(size=(7,)).astype(theano.config.floatX)
324         rop_out1 = tensor.Rop([m, v, m + v], [m, v], [m_, v_])
325         assert isinstance(rop_out1, list)
326         assert len(rop_out1) == 3
327         rop_out2 = tensor.Rop((m, v, m + v), [m, v], [m_, v_])
328         assert isinstance(rop_out2, tuple)
329         assert len(rop_out2) == 3
330         all_outs = []
331         for o in rop_out1, rop_out2:
332             all_outs.extend(o)
333         f = theano.function([m, v, m_, v_], all_outs)
334         f(mval, vval, m_val, v_val)
335     def test_Rop_dot_bug_18Oct2013_Jeremiah(self):
336         x = tensor.arange(20.0).reshape([1, 20])
337         v = theano.shared(np.ones([20]))
338         d = tensor.dot(x, v).sum()
339         tensor.Rop(tensor.grad(d, v), v, v)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>multinomial.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import warnings
3 try:
4     import pygpu
5     pass
6 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import theano
7 import theano.sandbox.multinomial
8 from theano import Apply
9 from theano.gof import Op
10 from theano.tensor import NotScalarConstantError, get_scalar_constant_value
11 from .basic_ops import as_gpuarray_variable, infer_context_name, GpuKernelBase, Kernel, gpuarray_helper_inc_dir
12 from .opt import register_opt, op_lifter, register_opt2
13 from .type import GpuArrayType
14 from .elemwise import GpuDimShuffle
15 from theano.scalar import as_scalar
16 from .fp16_help import write_w, load_w, work_dtype
17 class GPUAMultinomialFromUniform(GpuKernelBase, Op):
18     __props__ =</b></font> ("odtype",)
19     _f16_ok = True
20     def __init__(self, odtype):
21         Op.__init__(self)
22         self.odtype = odtype
23     def get_params(self, node):
24         return node.outputs[0].type.context
25     def c_headers(self):
26         return ['&lt;numpy_compat.h&gt;', 'gpuarray_helper.h']
27     def c_header_dirs(self):
28         return [gpuarray_helper_inc_dir()]
29     def make_node(self, pvals, unis):
30         ctx_name = infer_context_name(pvals, unis)
31         pvals = as_gpuarray_variable(pvals, ctx_name)
32         unis = as_gpuarray_variable(unis, ctx_name)
33         assert pvals.dtype in ['float32', 'float16', 'float64']
34         assert unis.dtype in ['float32', 'float16', 'float64']
35         if pvals.ndim != 2:
36             raise NotImplementedError('pvals ndim should be 2', pvals.ndim)
37         if unis.ndim != 1:
38             raise NotImplementedError('unis ndim should be 1', unis.ndim)
39         if self.odtype == 'auto':
40             odtype = pvals.dtype
41         else:
42             odtype = self.odtype
43         br = (pvals.broadcastable[1], pvals.broadcastable[0])
44         out = GpuArrayType(broadcastable=br,
45                            dtype=odtype,
46                            context_name=ctx_name)()
47         return Apply(self, [pvals, unis], [out])
48     def gpu_kernels(self, node, name):
49         out_ctype = pygpu.gpuarray.dtype_to_ctype(node.outputs[0].dtype)
50         pvals_ctype = pygpu.gpuarray.dtype_to_ctype(node.inputs[0].dtype)
51         unis_ctype = pygpu.gpuarray.dtype_to_ctype(node.inputs[1].dtype)
52         work_ctype = pygpu.gpuarray.dtype_to_ctype(work_dtype(node.inputs[0].dtype))
53         write_out_ctype = write_w(node.outputs[0].dtype)
54         load_in_ctype = load_w(node.inputs[0].dtype)
55         code = """#include "cluda.h"
56 KERNEL void k_multi_warp_multinomial(
57     const ga_size nb_multi,
58     const ga_size nb_outcomes,
59     GLOBAL_MEM %(pvals_ctype)s *global_pvals,
60     const ga_size global_pvals_offset,
61     const ga_ssize pvals_row_stride,
62     const ga_ssize pvals_col_stride,
63     GLOBAL_MEM %(unis_ctype)s *global_unis,
64     const ga_size global_unis_offset,
65     const ga_ssize unis_stride,
66     GLOBAL_MEM %(out_ctype)s *global_outs,
67     const ga_size global_outs_offset,
68     const ga_ssize outs_row_stride,
69     const ga_ssize outs_col_stride
70 )
71 {
72     global_pvals = (GLOBAL_MEM %(pvals_ctype)s *)(((GLOBAL_MEM char *)global_pvals) + global_pvals_offset);
73     global_unis = (GLOBAL_MEM %(unis_ctype)s *)(((GLOBAL_MEM char *)global_unis) + global_unis_offset);
74     global_outs = (GLOBAL_MEM %(out_ctype)s *)(((GLOBAL_MEM char *)global_outs) + global_outs_offset);
75     // each thread takes care of one multinomial draw
76     int n = LDIM_0*GID_0 + LID_0;
77     if (n &lt; nb_multi)
78     {
79         %(work_ctype)s cummul = 0.;
80         bool done = false;
81         const %(work_ctype)s unis_n = %(load_in_ctype)s(global_unis[n*unis_stride]);
82         for (ga_size m = 0; m &lt; nb_outcomes; ++m)
83         {
84             %(work_ctype)s current_out = 0;
85             if (!done)
86             {
87                 cummul += %(load_in_ctype)s(global_pvals[m * pvals_col_stride + n * pvals_row_stride]);
88                 if (unis_n &lt; cummul)
89                 {
90                     current_out = 1;
91                     done = true;
92                 }
93             }
94             //write out transposed for speed.
95             global_outs[n * outs_col_stride +
96                         m * outs_row_stride] = %(write_out_ctype)s(current_out);
97         }
98     }
99 }
100 """ % dict(out_ctype=out_ctype, write_out_ctype=write_out_ctype,
101            work_ctype=work_ctype, pvals_ctype=pvals_ctype,
102            unis_ctype=unis_ctype, load_in_ctype=load_in_ctype)
103         return [Kernel(
104             code=code, name="k_multi_warp_multinomial",
105             params=[pygpu.gpuarray.SIZE,
106                     pygpu.gpuarray.SIZE,
107                     pygpu.gpuarray.GpuArray,
108                     pygpu.gpuarray.SIZE,
109                     pygpu.gpuarray.SSIZE,
110                     pygpu.gpuarray.SSIZE,
111                     pygpu.gpuarray.GpuArray,
112                     pygpu.gpuarray.SIZE,
113                     pygpu.gpuarray.SSIZE,
114                     pygpu.gpuarray.GpuArray,
115                     pygpu.gpuarray.SIZE,
116                     pygpu.gpuarray.SSIZE,
117                     pygpu.gpuarray.SSIZE],
118             flags=Kernel.get_flags(node.outputs[0].dtype),
119             objvar='k_multi_warp_multinomial_' + name)]
120     def c_code(self, node, name, inp, outputs, sub):
121         pvals, unis = inp
122         out, = outputs
123         fail = sub['fail']
124         ctx = sub['params']
125         kname = self.gpu_kernels(node, name)[0].objvar
126         out_typecode = pygpu.gpuarray.dtype_to_typecode(node.outputs[0].dtype)
127         pvals_typecode = pygpu.gpuarray.dtype_to_typecode(node.inputs[0].dtype)
128         unis_typecode = pygpu.gpuarray.dtype_to_typecode(node.inputs[1].dtype)
129         s = """
130         PyGpuArrayObject * pvals = %(pvals)s;
131         PyGpuArrayObject * unis = %(unis)s;
132         PyGpuArrayObject * out = %(out)s;
133     size_t dims[2];
134     if (PyGpuArray_NDIM(pvals) != 2)
135     {
136         PyErr_Format(PyExc_TypeError, "pvals wrong rank");
137         %(fail)s
138     }
139     if (PyGpuArray_NDIM(unis) != 1)
140     {
141         PyErr_Format(PyExc_TypeError, "unis wrong rank");
142         %(fail)s
143     }
144     if (PyGpuArray_DIMS(unis)[0] != PyGpuArray_DIMS(pvals)[0])
145     {
146         PyErr_Format(PyExc_ValueError, "unis.shape[0] != pvals.shape[0]");
147         %(fail)s
148     }
149     dims[0] = PyGpuArray_DIMS(pvals)[1];
150     dims[1] = PyGpuArray_DIMS(pvals)[0];
151     if (theano_prep_output(&amp;out, 2, dims, %(out_typecode)s,
152                            GA_C_ORDER, %(ctx)s) != 0){
153       %(fail)s
154     }
155     %(out)s = out;
156     GpuArray_memset(&amp;(out-&gt;ga), 0);
157     { // NESTED SCOPE
158         int nb_multi = PyGpuArray_DIMS(pvals)[0];
159         int nb_outcomes = PyGpuArray_DIMS(pvals)[1];
160         //TODO : change this for a beautiful constant
161         int max_nb_blocks = 2&lt;&lt;15 - 1;
162         size_t nb_blocks = max_nb_blocks + 1;
163         size_t nb_threads=16; // so it really starts at 32, because of the *2
164         do
165         {
166             nb_threads*=2;
167             if (nb_multi %% nb_threads == 0)
168                 nb_blocks = nb_multi/nb_threads;
169             else
170                 nb_blocks = (int)((float)nb_multi/(float)nb_threads + 1.);
171         } while (nb_blocks &gt; max_nb_blocks);
172         //printf("\\nN=%%i b=%%i t=%%i t*b=%%i",
173         //         nb_multi, nb_blocks, nb_threads, nb_blocks*nb_threads);
174         // TODO : next line is a bit hardcoded...
175         if (nb_threads &gt; 512)
176         {
177             PyErr_Format(
178                 PyExc_ValueError,
179                 "Multinomial is not implemented for so many rows in the matrix (%%i)",
180                 nb_multi);
181             %(fail)s
182         }
183         assert(nb_blocks*nb_threads &gt;= nb_multi);
184         int err = k_multi_warp_multinomial_call(
185           1, &amp;nb_blocks, &amp;nb_threads, 0,
186           PyGpuArray_DIMS(out)[1], PyGpuArray_DIMS(out)[0], pvals-&gt;ga.data, pvals-&gt;ga.offset,
187           PyGpuArray_STRIDES(pvals)[0]/gpuarray_get_elsize(%(pvals_typecode)s),
188           PyGpuArray_STRIDES(pvals)[1]/gpuarray_get_elsize(%(pvals_typecode)s),
189           unis-&gt;ga.data, unis-&gt;ga.offset,
190           PyGpuArray_STRIDES(unis)[0]/gpuarray_get_elsize(%(unis_typecode)s), out-&gt;ga.data,
191           out-&gt;ga.offset, PyGpuArray_STRIDES(out)[0]/gpuarray_get_elsize(%(out_typecode)s),
192           PyGpuArray_STRIDES(out)[1]/gpuarray_get_elsize(%(out_typecode)s));
193         if (err != GA_NO_ERROR) {
194            PyErr_Format(
195                 PyExc_RuntimeError,
196                 "gpuarray error: %%s: %%s.\\n",
197                 "k_multi_warp_%(name)s",
198                 GpuKernel_error(&amp;%(kname)s, err));
199             %(fail)s;
200         }
201     } // END NESTED SCOPE
202         """ % locals()
203         return s
204     def c_code_cache_version(self):
205         return (7,)
206 class GPUAChoiceFromUniform(GpuKernelBase, Op):
207     """
208     The output is transposed compared to MultinomialWOReplacementFromUniform.
209     We must insert a Transpose op after it.
210     The optimization that moves it to the gpu does it.
211     """
212     __props__ = ("odtype", "replace")
213     def __init__(self, odtype, replace=False):
214         Op.__init__(self)
215         self.odtype = odtype
216         self.replace = replace
217     def __setstate__(self, state):
218         self.__dict__.update(state)
219         if "replace" not in state:
220             self.replace = False
221     def get_params(self, node):
222         return node.outputs[0].type.context
223     def c_headers(self):
224         return ['&lt;numpy_compat.h&gt;', 'gpuarray_helper.h']
225     def c_header_dirs(self):
226         return [gpuarray_helper_inc_dir()]
227     def make_node(self, pvals, unis, n):
228         assert pvals.dtype == 'float32'
229         assert unis.dtype == 'float32'
230         ctx_name = infer_context_name(pvals, unis)
231         pvals = as_gpuarray_variable(pvals, ctx_name)
232         unis = as_gpuarray_variable(unis, ctx_name)
233         if pvals.ndim != 2:
234             raise NotImplementedError('pvals ndim should be 2', pvals.ndim)
235         if unis.ndim != 1:
236             raise NotImplementedError('unis ndim should be 1', unis.ndim)
237         if self.odtype == 'auto':
238             odtype = 'int64'
239         else:
240             odtype = self.odtype
241         assert odtype == 'int64', odtype
242         br = (pvals.broadcastable[1], pvals.broadcastable[0])
243         out = GpuArrayType(broadcastable=br,
244                            dtype=odtype,
245                            context_name=ctx_name)()
246         return Apply(self, [pvals, unis, as_scalar(n)], [out])
247     def gpu_kernels(self, node, name):
248         replace = int(self.replace)
249         code = """#include "cluda.h"
250 KERNEL void k_multi_warp_multinomial_wor(
251     const ga_size nb_multi,
252     const ga_size nb_outcomes,
253     const ga_size n_samples,
254     GLOBAL_MEM float * global_pvals_copy,
255     const ga_size global_pvals_offset,
256     const ga_ssize pvals_row_stride,
257     const ga_ssize pvals_col_stride,
258     GLOBAL_MEM float * global_unis,
259     const ga_size global_unis_offset,
260     const ga_ssize unis_stride,
261     GLOBAL_MEM ga_long * global_outs,
262     const ga_size global_outs_offset,
263     const ga_ssize outs_row_stride,
264     const ga_ssize outs_col_stride
265 )
266 {
267     global_pvals_copy = (GLOBAL_MEM float *)(((GLOBAL_MEM char *)global_pvals_copy) + global_pvals_offset);
268     global_unis = (GLOBAL_MEM float *)(((GLOBAL_MEM char *)global_unis) + global_unis_offset);
269     global_outs = (GLOBAL_MEM ga_long *)(((GLOBAL_MEM char *)global_outs) + global_outs_offset);
270     // each thread takes care of one multinomial-wor n_samples-draw
271     int n = LDIM_0*GID_0 + LID_0;
272     if (n &lt; nb_multi)
273     {
274         // Sum of the remaining p_vals in global_pvals_copy[n]
275         float pvals_sum = 1.;
276         for (int c = 0; c &lt; n_samples; ++c)
277         {
278             float cummul = 0.;
279             const float unis_n = global_unis[(c * nb_multi + n)*unis_stride] * pvals_sum;
280             for (ga_size m = 0; m &lt; nb_outcomes; ++m)
281             {
282                 float pvals_nm = global_pvals_copy[m * pvals_col_stride + n * pvals_row_stride];
283                 cummul += pvals_nm;
284                 if (unis_n &lt; cummul)
285                 {
286                     // write out transposed for speed.
287                     global_outs[n * outs_col_stride +
288                                 c * outs_row_stride] = m;
289                     if (! %(replace)s )
290                     {
291                         global_pvals_copy[m * pvals_col_stride + n * pvals_row_stride] = 0.0;
292                         pvals_sum -= pvals_nm;
293                     }
294                     break;
295                 }
296             }
297         }
298     }
299 }
300         return [Kernel(
301             code=code, name="k_multi_warp_multinomial_wor",
302             params<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=[pygpu.gpuarray.SIZE,
303                     pygpu.gpuarray.SIZE,
304                     pygpu.gpuarray.SIZE,
305                     pygpu.gpuarray.GpuArray,
306                     pygpu.gpuarray.SIZE,
307                     pygpu.gpuarray.SSIZE,
308                     pygpu.gpuarray.SSIZE,
309                     pygpu.gpuarray.GpuArray,
310                     pygpu.gpuarray.SIZE,
311                     pygpu.gpuarray.SSIZE,
312                     pygpu.gpuarray.GpuArray,
313                     pygpu.gpuarray.SIZE,
314                     pygpu.gpuarray.SSIZE,
315                     pygpu.gpuarray.</b></font>SSIZE
316                     ],
317             flags=Kernel.get_flags(node.outputs[0].dtype),
318             objvar='k_multi_warp_multinomial_wor_' + name)]
319     def c_code(self, node, name, inp, outputs, sub):
320         pvals, unis, n = inp
321         out, = outputs
322         replace = int(self.replace)
323         fail = sub['fail']
324         ctx = sub['params']
325         kname = self.gpu_kernels(node, name)[0].objvar
326         s = """
327     PyGpuArrayObject * pvals = %(pvals)s;
328     PyGpuArrayObject * unis = %(unis)s;
329     const size_t n_samples = %(n)s;
330     PyGpuArrayObject * out = %(out)s;
331     // create a copy of pvals matrix
332     PyGpuArrayObject * pvals_copy = NULL;
333     size_t dims[2];
334     if (PyGpuArray_NDIM(pvals) != 2)
335     {
336         PyErr_Format(PyExc_TypeError, "pvals wrong rank");
337         %(fail)s
338     }
339     if (PyGpuArray_NDIM(unis) != 1)
340     {
341         PyErr_Format(PyExc_TypeError, "unis wrong rank");
342         %(fail)s
343     }
344     if ( n_samples &gt; (PyGpuArray_DIMS(pvals)[1]) )
345     {
346         PyErr_Format(PyExc_ValueError, "Cannot sample without replacement n samples bigger than the size of the distribution.");
347         %(fail)s;
348     }
349     if (PyGpuArray_DIMS(unis)[0] != PyGpuArray_DIMS(pvals)[0] * n_samples)
350     {
351         PyErr_Format(PyExc_ValueError, "unis.shape[0] != pvals.shape[0] * n");
352         %(fail)s
353     }
354     if (! %(replace)s) {
355         pvals_copy = pygpu_copy(pvals, GA_C_ORDER);
356     } else {
357         pvals_copy = pvals;
358         Py_INCREF(pvals_copy);
359     }
360     dims[0] = n_samples;
361     dims[1] = PyGpuArray_DIMS(pvals)[0];
362     if (theano_prep_output(&amp;out, 2, dims, GA_LONG,
363                            GA_C_ORDER, %(ctx)s) != 0){
364         Py_DECREF(pvals_copy);
365         %(fail)s
366     }
367     %(out)s = out;
368     { // NESTED SCOPE
369         int nb_multi = PyGpuArray_DIMS(pvals)[0];
370         int nb_outcomes = PyGpuArray_DIMS(pvals)[1];
371         //TODO : change this for a beautiful constant
372         int max_nb_blocks = 2&lt;&lt;15 - 1;
373         size_t nb_blocks = max_nb_blocks + 1;
374         size_t nb_threads=16; // so it really starts at 32, because of the *2
375         do
376         {
377             nb_threads*=2;
378             if (nb_multi %% nb_threads == 0)
379                 nb_blocks = nb_multi/nb_threads;
380             else
381                 nb_blocks = (int)((float)nb_multi/(float)nb_threads + 1.);
382         } while (nb_blocks &gt; max_nb_blocks);
383         // TODO : next line is a bit hardcoded...
384         if (nb_threads &gt; 512)
385         {
386             PyErr_Format(
387                 PyExc_ValueError,
388                 "Multinomial is not implemented for so many rows in the matrix (%%i)",
389                 nb_multi);
390             Py_DECREF(pvals_copy);
391             %(fail)s
392         }
393         assert(nb_blocks*nb_threads &gt;= nb_multi);
394         int err = k_multi_warp_multinomial_wor_call(1, &amp;nb_blocks, &amp;nb_threads, 0, PyGpuArray_DIMS(pvals)[0], PyGpuArray_DIMS(pvals)[1], n_samples, pvals_copy-&gt;ga.data, pvals_copy-&gt;ga.offset, PyGpuArray_STRIDES(pvals)[0]/sizeof(float), PyGpuArray_STRIDES(pvals)[1]/sizeof(float), unis-&gt;ga.data, unis-&gt;ga.offset, PyGpuArray_STRIDES(unis)[0]/sizeof(float), out-&gt;ga.data, out-&gt;ga.offset, PyGpuArray_STRIDES(out)[0]/8, PyGpuArray_STRIDES(out)[1]/8);
395         if (err != GA_NO_ERROR) {
396            PyErr_Format(
397                 PyExc_RuntimeError,
398                 "gpuarray error: %%s: %%s.\\n",
399                 "k_multi_warp_%(name)s",
400                 GpuKernel_error(&amp;%(kname)s, err));
401            Py_DECREF(pvals_copy);
402            %(fail)s;
403         }
404         Py_DECREF(pvals_copy);
405     } // END NESTED SCOPE
406         """ % locals()
407         return s
408     def c_code_cache_version(self):
409         return (10,)
410 @register_opt('fast_compile')
411 @op_lifter([theano.sandbox.multinomial.MultinomialFromUniform])
412 @register_opt2([theano.sandbox.multinomial.MultinomialFromUniform], 'fast_compile')
413 def local_gpua_multinomial(op, context_name, inputs, outputs):
414     if len(inputs) == 2:
415         p, u = inputs
416         n_samples = 1
417     else:
418         p, u, n_samples = inputs
419     try:
420         if get_scalar_constant_value(n_samples) != 1:
421             return None
422     except NotScalarConstantError:
423         return None
424     m, = outputs
425     gpu_op = GPUAMultinomialFromUniform(op.odtype)
426     return GpuDimShuffle([False, False], [1, 0])(
427         gpu_op(p, u))
428 @register_opt('fast_compile')
429 @op_lifter([theano.sandbox.multinomial.ChoiceFromUniform])
430 @register_opt2([theano.sandbox.multinomial.ChoiceFromUniform], 'fast_compile')
431 def local_gpua_multinomial_wor(op, context_name, inputs, outputs):
432     p, u, n = inputs
433     m, = outputs
434     if ((p.dtype == u.dtype == 'float32') and (m.dtype == 'int64')):
435         gpu_op = GPUAChoiceFromUniform(**op._props_dict())
436         return GpuDimShuffle([False, False], [1, 0])(
437             gpu_op(p, u, n))
438 class GPUAMultinomialWOReplacementFromUniform(GPUAChoiceFromUniform):
439     def __init__(self, *args, **kwargs):
440         warnings.warn("GPUAMultinomialWOReplacementFromUniform is deprecated, "
441                       "use GPUAChoiceFromUniform instead.",
442                       DeprecationWarning,
443                       stacklevel=2)
444         super(GPUAMultinomialWOReplacementFromUniform, self).__init__(*args, **kwargs)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
