<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_rop.py &amp; multinomial.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_rop.py &amp; multinomial.py
      </h3>
<h1 align="center">
        5.6%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_rop.py (4.0149393%)<th>multinomial.py (9.598214%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(264-274)<td><a href="#" name="0">(349-362)</a><td align="center"><font color="#ff0000">30</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(15-37)<td><a href="#" name="1">(10-25)</a><td align="center"><font color="#6e0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_rop.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>from theano.tests import unittest_tools as utt
2 from theano import function
3 import theano
4 from theano import tensor
5 import itertools
6 import numpy as np
7 from theano.gof import Op, Apply
8 from theano.gradient import grad_undefined
9 from theano.tests.unittest_tools import SkipTest
10 from theano.tensor.signal.pool import Pool
11 from theano.tensor.nnet import conv, conv2d
12 class BreakRop(Op):
13     __props__ =</b></font> ()
14     def make_node(self, x):
15         return Apply(self, [x], [x.type()])
16     def perform(self, node, inp, out_):
17         x, = inp
18         out, = out_
19         out[0] = x
20     def grad(self, inp, grads):
21         return [grad_undefined(self, 0, inp[0])]
22     def R_op(self, inputs, eval_points):
23         return [None]
24 break_op = BreakRop()
25 class RopLop_checker(unittest.TestCase):
26     def setUp(self):
27         utt.seed_rng()
28         self.x = tensor.vector('x')
29         self.v = tensor.vector('v')
30         self.rng = np.random.RandomState(utt.fetch_seed())
31         self.in_shape = (5 + self.rng.randint(3),)
32         self.mx = tensor.matrix('mx')
33         self.mv = tensor.matrix('mv')
34         self.mat_in_shape = (5 + self.rng.randint(3),
35                              5 + self.rng.randint(3))
36     def check_nondiff_rop(self, y):
37         raised = False
38         try:
39             tensor.Rop(y, self.x, self.v)
40         except ValueError:
41             raised = True
42         if not raised:
43             self.fail((
44                 'Op did not raise an error even though the function'
45                 ' is not differentiable'))
46     def check_mat_rop_lop(self, y, out_shape):
47         vx = np.asarray(self.rng.uniform(size=self.mat_in_shape),
48                         theano.config.floatX)
49         vv = np.asarray(self.rng.uniform(size=self.mat_in_shape),
50                         theano.config.floatX)
51         yv = tensor.Rop(y, self.mx, self.mv)
52         rop_f = function([self.mx, self.mv], yv, on_unused_input='ignore')
53         sy, _ = theano.scan(lambda i, y, x, v:
54                             (tensor.grad(y[i], x) * v).sum(),
55                             sequences=tensor.arange(y.shape[0]),
56                             non_sequences=[y, self.mx, self.mv])
57         scan_f = function([self.mx, self.mv], sy, on_unused_input='ignore')
58         v1 = rop_f(vx, vv)
59         v2 = scan_f(vx, vv)
60         assert np.allclose(v1, v2), ('ROP mismatch: %s %s' % (v1, v2))
61         self.check_nondiff_rop(theano.clone(y, replace={self.mx: break_op(self.mx)}))
62         vv = np.asarray(self.rng.uniform(size=out_shape), theano.config.floatX)
63         yv = tensor.Lop(y, self.mx, self.v)
64         lop_f = function([self.mx, self.v], yv)
65         sy = tensor.grad((self.v * y).sum(), self.mx)
66         scan_f = function([self.mx, self.v], sy)
67         v1 = lop_f(vx, vv)
68         v2 = scan_f(vx, vv)
69         assert np.allclose(v1, v2), ('LOP mismatch: %s %s' % (v1, v2))
70     def check_rop_lop(self, y, out_shape):
71         vx = np.asarray(self.rng.uniform(size=self.in_shape),
72                         theano.config.floatX)
73         vv = np.asarray(self.rng.uniform(size=self.in_shape),
74                         theano.config.floatX)
75         yv = tensor.Rop(y, self.x, self.v)
76         rop_f = function([self.x, self.v], yv, on_unused_input='ignore')
77         J, _ = theano.scan(lambda i, y, x: tensor.grad(y[i], x),
78                            sequences=tensor.arange(y.shape[0]),
79                            non_sequences=[y, self.x])
80         sy = tensor.dot(J, self.v)
81         scan_f = function([self.x, self.v], sy, on_unused_input='ignore')
82         v1 = rop_f(vx, vv)
83         v2 = scan_f(vx, vv)
84         assert np.allclose(v1, v2), ('ROP mismatch: %s %s' % (v1, v2))
85         known_fail = False
86         try:
87             self.check_nondiff_rop(theano.clone(y, replace={self.x: break_op(self.x)}))
88         except AssertionError:
89             known_fail = True
90         vx = np.asarray(self.rng.uniform(size=self.in_shape),
91                         theano.config.floatX)
92         vv = np.asarray(self.rng.uniform(size=out_shape),
93                         theano.config.floatX)
94         yv = tensor.Lop(y, self.x, self.v)
95         lop_f = function([self.x, self.v], yv, on_unused_input='ignore')
96         J, _ = theano.scan(lambda i, y, x: tensor.grad(y[i], x),
97                            sequences=tensor.arange(y.shape[0]),
98                            non_sequences=[y, self.x])
99         sy = tensor.dot(self.v, J)
100         scan_f = function([self.x, self.v], sy)
101         v1 = lop_f(vx, vv)
102         v2 = scan_f(vx, vv)
103         assert np.allclose(v1, v2), ('LOP mismatch: %s %s' % (v1, v2))
104         if known_fail:
105             raise SkipTest('Rop does not handle non-differentiable inputs '
106                            'correctly. Bug exposed by fixing Add.grad method.')
107 class test_RopLop(RopLop_checker):
108     def test_shape(self):
109         self.check_nondiff_rop(self.x.shape[0])
110     def test_specifyshape(self):
111         self.check_rop_lop(tensor.specify_shape(self.x, self.in_shape),
112                            self.in_shape)
113     def test_max(self):
114         self.check_mat_rop_lop(tensor.max(self.mx, axis=0),
115                                (self.mat_in_shape[1],))
116         self.check_mat_rop_lop(tensor.max(self.mx, axis=1),
117                                (self.mat_in_shape[0],))
118     def test_argmax(self):
119         self.check_nondiff_rop(tensor.argmax(self.mx, axis=1))
120     def test_subtensor(self):
121         self.check_rop_lop(self.x[:4], (4,))
122     def test_incsubtensor1(self):
123         tv = np.asarray(self.rng.uniform(size=(3,)),
124                         theano.config.floatX)
125         t = theano.shared(tv)
126         out = tensor.inc_subtensor(self.x[:3], t)
127         self.check_rop_lop(out, self.in_shape)
128     def test_incsubtensor2(self):
129         tv = np.asarray(self.rng.uniform(size=(10,)),
130                         theano.config.floatX)
131         t = theano.shared(tv)
132         out = tensor.inc_subtensor(t[:4], self.x[:4])
133         self.check_rop_lop(out, (10,))
134     def test_setsubtensor1(self):
135         tv = np.asarray(self.rng.uniform(size=(3,)),
136                         theano.config.floatX)
137         t = theano.shared(tv)
138         out = tensor.set_subtensor(self.x[:3], t)
139         self.check_rop_lop(out, self.in_shape)
140     def test_print(self):
141         out = theano.printing.Print('x', attrs=('shape',))(self.x)
142         self.check_rop_lop(out, self.in_shape)
143     def test_setsubtensor2(self):
144         tv = np.asarray(self.rng.uniform(size=(10,)),
145                         theano.config.floatX)
146         t = theano.shared(tv)
147         out = tensor.set_subtensor(t[:4], self.x[:4])
148         self.check_rop_lop(out, (10,))
149     def test_dimshuffle(self):
150         self.check_rop_lop(self.x[:4].dimshuffle('x', 0).sum(axis=0),
151                            (4,))
152     def test_rebroadcast(self):
153         self.check_rop_lop(tensor.unbroadcast(
154             self.x[:4].dimshuffle('x', 0), 0).sum(axis=1),
155             (1,))
156 <a name="0"></a>    def test_downsample(self):
157         rng = np.random.RandomState(utt.fetch_seed())
158         examples <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (
159             ((2,), (16,)),
160             ((2,), (4, 16,)),
161             ((2,), (4, 2, 16,)),
162             ((1, 1), (4, 2, 16, 16)),
163             ((2, 2), (4, 2, 16, 16)),
164             ((3, 3), (4, 2, 16, 16)),
165             ((3, 2), (4, 2, 16, 16)),
166             ((3, 2, 2), (3, 2, 16, 16, 16)),
167             ((2, 3, 2), (3, 2, 16, 16, 16)),
168             ((</b></font>2, 2, 3), (3, 2, 16, 16, 16)),
169             ((2, 2, 3, 2), (3, 2, 6, 6, 6, 5)),
170         )
171         for example, ignore_border in itertools.product(examples, [True, False]):
172             (ws, shp) = example
173             vx = rng.rand(*shp)
174             vex = rng.rand(*shp)
175             x = theano.shared(vx)
176             ex = theano.shared(vex)
177             maxpool_op = Pool(ignore_border, ndim=len(ws))
178             a_pooled = maxpool_op(x, ws).flatten()
179             yv = tensor.Rop(a_pooled, x, ex)
180             mode = None
181             if theano.config.mode == "FAST_COMPILE":
182                 mode = "FAST_RUN"
183             rop_f = function([], yv, on_unused_input='ignore', mode=mode)
184             sy, _ = theano.scan(lambda i, y, x, v:
185                                 (tensor.grad(y[i], x) * v).sum(),
186                                 sequences=tensor.arange(a_pooled.shape[0]),
187                                 non_sequences=[a_pooled, x, ex],
188                                 mode=mode)
189             scan_f = function([], sy, on_unused_input='ignore', mode=mode)
190             v1 = rop_f()
191             v2 = scan_f()
192             assert np.allclose(v1, v2), ("Rop mismatch: %s %s" % (v1, v2))
193     def test_conv(self):
194         for conv_op in [conv.conv2d, conv2d]:
195             for border_mode in ['valid', 'full']:
196                 image_shape = (2, 2, 4, 5)
197                 filter_shape = (2, 2, 2, 3)
198                 image_dim = len(image_shape)
199                 filter_dim = len(filter_shape)
200                 input = tensor.TensorType(
201                     theano.config.floatX,
202                     [False] * image_dim)(name='input')
203                 filters = tensor.TensorType(
204                     theano.config.floatX,
205                     [False] * filter_dim)(name='filter')
206                 ev_input = tensor.TensorType(
207                     theano.config.floatX,
208                     [False] * image_dim)(name='ev_input')
209                 ev_filters = tensor.TensorType(
210                     theano.config.floatX,
211                     [False] * filter_dim)(name='ev_filters')
212                 def sym_conv2d(input, filters):
213                     return conv_op(input, filters, border_mode=border_mode)
214                 output = sym_conv2d(input, filters).flatten()
215                 yv = tensor.Rop(output, [input, filters], [ev_input, ev_filters])
216                 mode = None
217                 if theano.config.mode == "FAST_COMPILE":
218                     mode = "FAST_RUN"
219                 rop_f = function([input, filters, ev_input, ev_filters],
220                                  yv, on_unused_input='ignore', mode=mode)
221                 sy, _ = theano.scan(lambda i, y, x1, x2, v1, v2:
222                                     (tensor.grad(y[i], x1) * v1).sum() +
223                                     (tensor.grad(y[i], x2) * v2).sum(),
224                                     sequences=tensor.arange(output.shape[0]),
225                                     non_sequences=[output, input, filters,
226                                                    ev_input, ev_filters],
227                                     mode=mode)
228                 scan_f = function([input, filters, ev_input, ev_filters], sy,
229                                   on_unused_input='ignore', mode=mode)
230                 dtype = theano.config.floatX
231                 image_data = np.random.random(image_shape).astype(dtype)
232                 filter_data = np.random.random(filter_shape).astype(dtype)
233                 ev_image_data = np.random.random(image_shape).astype(dtype)
234                 ev_filter_data = np.random.random(filter_shape).astype(dtype)
235                 v1 = rop_f(image_data, filter_data, ev_image_data, ev_filter_data)
236                 v2 = scan_f(image_data, filter_data, ev_image_data, ev_filter_data)
237                 assert np.allclose(v1, v2), ("Rop mismatch: %s %s" % (v1, v2))
238     def test_join(self):
239         tv = np.asarray(self.rng.uniform(size=(10,)),
240                         theano.config.floatX)
241         t = theano.shared(tv)
242         out = tensor.join(0, self.x, t)
243         self.check_rop_lop(out, (self.in_shape[0] + 10,))
244     def test_dot(self):
245         insh = self.in_shape[0]
246         vW = np.asarray(self.rng.uniform(size=(insh, insh)),
247                         theano.config.floatX)
248         W = theano.shared(vW)
249         self.check_rop_lop(tensor.dot(self.x, W), self.in_shape)
250     def test_elemwise0(self):
251         self.check_rop_lop((self.x + 1) ** 2, self.in_shape)
252     def test_elemwise1(self):
253         self.check_rop_lop(self.x + tensor.cast(self.x, 'int32'),
254                            self.in_shape)
255     def test_reshape(self):
256         new_shape = tensor.constant(np.asarray([
257             self.mat_in_shape[0] * self.mat_in_shape[1]],
258             dtype='int64'))
259         self.check_mat_rop_lop(self.mx.reshape(new_shape),
260                                (self.mat_in_shape[0] * self.mat_in_shape[1],))
261     def test_flatten(self):
262         self.check_mat_rop_lop(self.mx.flatten(),
263                                (self.mat_in_shape[0] * self.mat_in_shape[1],))
264     def test_sum(self):
265         self.check_mat_rop_lop(self.mx.sum(axis=1), (self.mat_in_shape[0],))
266     def test_softmax(self):
267         self.check_rop_lop(tensor.nnet.softmax(self.x)[0], self.in_shape[0])
268     def test_alloc(self):
269         out1d = tensor.alloc(self.x.sum(), self.in_shape[0])
270         self.check_rop_lop(out1d, self.in_shape[0])
271         out3d = tensor.alloc(self.x, self.mat_in_shape[0], self.mat_in_shape[1], self.in_shape[0])
272         self.check_rop_lop(out3d.flatten(), self.mat_in_shape[0] * self.mat_in_shape[1] * self.in_shape[0])
273     def test_invalid_input(self):
274         success = False
275         try:
276             tensor.Rop(0., [tensor.matrix()], [tensor.vector()])
277             success = True
278         except ValueError:
279             pass
280         assert not success
281     def test_multiple_outputs(self):
282         m = tensor.matrix('m')
283         v = tensor.vector('v')
284         m_ = tensor.matrix('m_')
285         v_ = tensor.vector('v_')
286         mval = self.rng.uniform(size=(3, 7)).astype(theano.config.floatX)
287         vval = self.rng.uniform(size=(7,)).astype(theano.config.floatX)
288         m_val = self.rng.uniform(size=(3, 7)).astype(theano.config.floatX)
289         v_val = self.rng.uniform(size=(7,)).astype(theano.config.floatX)
290         rop_out1 = tensor.Rop([m, v, m + v], [m, v], [m_, v_])
291         assert isinstance(rop_out1, list)
292         assert len(rop_out1) == 3
293         rop_out2 = tensor.Rop((m, v, m + v), [m, v], [m_, v_])
294         assert isinstance(rop_out2, tuple)
295         assert len(rop_out2) == 3
296         all_outs = []
297         for o in rop_out1, rop_out2:
298             all_outs.extend(o)
299         f = theano.function([m, v, m_, v_], all_outs)
300         f(mval, vval, m_val, v_val)
301     def test_Rop_dot_bug_18Oct2013_Jeremiah(self):
302         x = tensor.arange(20.0).reshape([1, 20])
303         v = theano.shared(np.ones([20]))
304         d = tensor.dot(x, v).sum()
305         tensor.Rop(tensor.grad(d, v), v, v)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>multinomial.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import warnings
3 try:
4     import pygpu
5 <a name="1"></a>except ImportError:
6     pass
7 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import theano
8 import theano.sandbox.multinomial
9 from theano import Apply
10 from theano.gof import Op
11 from theano.tensor import NotScalarConstantError, get_scalar_constant_value
12 from .basic_ops import as_gpuarray_variable, infer_context_name, GpuKernelBase, Kernel, gpuarray_helper_inc_dir
13 from .opt import register_opt, op_lifter, register_opt2
14 from .type import GpuArrayType
15 from .elemwise import GpuDimShuffle
16 from theano.scalar import as_scalar
17 from .fp16_help import write_w, load_w, work_dtype
18 class GPUAMultinomialFromUniform(GpuKernelBase, Op):
19     __props__ =</b></font> ("odtype",)
20     _f16_ok = True
21     def __init__(self, odtype):
22         Op.__init__(self)
23         self.odtype = odtype
24     def get_params(self, node):
25         return node.outputs[0].type.context
26     def c_headers(self):
27         return ['&lt;numpy_compat.h&gt;', 'gpuarray_helper.h']
28     def c_header_dirs(self):
29         return [gpuarray_helper_inc_dir()]
30     def make_node(self, pvals, unis):
31         ctx_name = infer_context_name(pvals, unis)
32         pvals = as_gpuarray_variable(pvals, ctx_name)
33         unis = as_gpuarray_variable(unis, ctx_name)
34         assert pvals.dtype in ['float32', 'float16', 'float64']
35         assert unis.dtype in ['float32', 'float16', 'float64']
36         if pvals.ndim != 2:
37             raise NotImplementedError('pvals ndim should be 2', pvals.ndim)
38         if unis.ndim != 1:
39             raise NotImplementedError('unis ndim should be 1', unis.ndim)
40         if self.odtype == 'auto':
41             odtype = pvals.dtype
42         else:
43             odtype = self.odtype
44         br = (pvals.broadcastable[1], pvals.broadcastable[0])
45         out = GpuArrayType(broadcastable=br,
46                            dtype=odtype,
47                            context_name=ctx_name)()
48         return Apply(self, [pvals, unis], [out])
49     def gpu_kernels(self, node, name):
50         out_ctype = pygpu.gpuarray.dtype_to_ctype(node.outputs[0].dtype)
51         pvals_ctype = pygpu.gpuarray.dtype_to_ctype(node.inputs[0].dtype)
52         unis_ctype = pygpu.gpuarray.dtype_to_ctype(node.inputs[1].dtype)
53         work_ctype = pygpu.gpuarray.dtype_to_ctype(work_dtype(node.inputs[0].dtype))
54         write_out_ctype = write_w(node.outputs[0].dtype)
55         load_in_ctype = load_w(node.inputs[0].dtype)
56         code = """#include "cluda.h"
57 KERNEL void k_multi_warp_multinomial(
58     const ga_size nb_multi,
59     const ga_size nb_outcomes,
60     GLOBAL_MEM %(pvals_ctype)s *global_pvals,
61     const ga_size global_pvals_offset,
62     const ga_ssize pvals_row_stride,
63     const ga_ssize pvals_col_stride,
64     GLOBAL_MEM %(unis_ctype)s *global_unis,
65     const ga_size global_unis_offset,
66     const ga_ssize unis_stride,
67     GLOBAL_MEM %(out_ctype)s *global_outs,
68     const ga_size global_outs_offset,
69     const ga_ssize outs_row_stride,
70     const ga_ssize outs_col_stride
71 )
72 {
73     global_pvals = (GLOBAL_MEM %(pvals_ctype)s *)(((GLOBAL_MEM char *)global_pvals) + global_pvals_offset);
74     global_unis = (GLOBAL_MEM %(unis_ctype)s *)(((GLOBAL_MEM char *)global_unis) + global_unis_offset);
75     global_outs = (GLOBAL_MEM %(out_ctype)s *)(((GLOBAL_MEM char *)global_outs) + global_outs_offset);
76     // each thread takes care of one multinomial draw
77     int n = LDIM_0*GID_0 + LID_0;
78     if (n &lt; nb_multi)
79     {
80         %(work_ctype)s cummul = 0.;
81         bool done = false;
82         const %(work_ctype)s unis_n = %(load_in_ctype)s(global_unis[n*unis_stride]);
83         for (ga_size m = 0; m &lt; nb_outcomes; ++m)
84         {
85             %(work_ctype)s current_out = 0;
86             if (!done)
87             {
88                 cummul += %(load_in_ctype)s(global_pvals[m * pvals_col_stride + n * pvals_row_stride]);
89                 if (unis_n &lt; cummul)
90                 {
91                     current_out = 1;
92                     done = true;
93                 }
94             }
95             //write out transposed for speed.
96             global_outs[n * outs_col_stride +
97                         m * outs_row_stride] = %(write_out_ctype)s(current_out);
98         }
99     }
100 }
101         return s
102     def c_code_cache_version(self):
103         return (7,)
104 class GPUAChoiceFromUniform(GpuKernelBase, Op):
105     __props__ = ("odtype", "replace")
106     def __init__(self, odtype, replace=False):
107         Op.__init__(self)
108         self.odtype = odtype
109         self.replace = replace
110     def __setstate__(self, state):
111         self.__dict__.update(state)
112         if "replace" not in state:
113             self.replace = False
114     def get_params(self, node):
115         return node.outputs[0].type.context
116     def c_headers(self):
117         return ['&lt;numpy_compat.h&gt;', 'gpuarray_helper.h']
118     def c_header_dirs(self):
119         return [gpuarray_helper_inc_dir()]
120     def make_node(self, pvals, unis, n):
121         assert pvals.dtype == 'float32'
122         assert unis.dtype == 'float32'
123         ctx_name = infer_context_name(pvals, unis)
124         pvals = as_gpuarray_variable(pvals, ctx_name)
125         unis = as_gpuarray_variable(unis, ctx_name)
126         if pvals.ndim != 2:
127             raise NotImplementedError('pvals ndim should be 2', pvals.ndim)
128         if unis.ndim != 1:
129             raise NotImplementedError('unis ndim should be 1', unis.ndim)
130         if self.odtype == 'auto':
131             odtype = 'int64'
132         else:
133             odtype = self.odtype
134         assert odtype == 'int64', odtype
135         br = (pvals.broadcastable[1], pvals.broadcastable[0])
136         out = GpuArrayType(broadcastable=br,
137                            dtype=odtype,
138                            context_name=ctx_name)()
139         return Apply(self, [pvals, unis, as_scalar(n)], [out])
140     def gpu_kernels(self, node, name):
141         replace = int(self.replace)
142         code = """#include "cluda.h"
143 KERNEL void k_multi_warp_multinomial_wor(
144     const ga_size nb_multi,
145     const ga_size nb_outcomes,
146     const ga_size n_samples,
147     GLOBAL_MEM float * global_pvals_copy,
148     const ga_size global_pvals_offset,
149     const ga_ssize pvals_row_stride,
150     const ga_ssize pvals_col_stride,
151     GLOBAL_MEM float * global_unis,
152     const ga_size global_unis_offset,
153     const ga_ssize unis_stride,
154     GLOBAL_MEM ga_long * global_outs,
155     const ga_size global_outs_offset,
156     const ga_ssize outs_row_stride,
157     const ga_ssize outs_col_stride
158 )
159 {
160     global_pvals_copy = (GLOBAL_MEM float *)(((GLOBAL_MEM char *)global_pvals_copy) + global_pvals_offset);
161     global_unis = (GLOBAL_MEM float *)(((GLOBAL_MEM char *)global_unis) + global_unis_offset);
162     global_outs = (GLOBAL_MEM ga_long *)(((GLOBAL_MEM char *)global_outs) + global_outs_offset);
163     // each thread takes care of one multinomial-wor n_samples-draw
164     int n = LDIM_0*GID_0 + LID_0;
165     if (n &lt; nb_multi)
166     {
167         // Sum of the remaining p_vals in global_pvals_copy[n]
168         float pvals_sum = 1.;
169         for (int c = 0; c &lt; n_samples; ++c)
170         {
171             float cummul = 0.;
172             const float unis_n = global_unis[(c * nb_multi + n)*unis_stride] * pvals_sum;
173             for (ga_size m = 0; m &lt; nb_outcomes; ++m)
174             {
175                 float pvals_nm = global_pvals_copy[m * pvals_col_stride + n * pvals_row_stride];
176                 cummul += pvals_nm;
177                 if (unis_n &lt; cummul)
178                 {
179                     // write out transposed for speed.
180                     global_outs[n * outs_col_stride +
181                                 c * outs_row_stride] = m;
182                     if (! %(replace)s )
183                     {
184                         global_pvals_copy[m * pvals_col_stride + n * pvals_row_stride] = 0.0;
185                         pvals_sum -= pvals_nm;
186                     }
187                     break;
188                 }
189             }
190         }
191     }
192 }
193 <a name="0"></a>""" % {"replace": replace}
194         return [Kernel(
195             code=code, name="k_multi_warp_multinomial_wor",
196             params<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=[pygpu.gpuarray.SIZE,
197                     pygpu.gpuarray.SIZE,
198                     pygpu.gpuarray.SIZE,
199                     pygpu.gpuarray.GpuArray,
200                     pygpu.gpuarray.SIZE,
201                     pygpu.gpuarray.SSIZE,
202                     pygpu.gpuarray.SSIZE,
203                     pygpu.gpuarray.GpuArray,
204                     pygpu.gpuarray.SIZE,
205                     pygpu.gpuarray.SSIZE,
206                     pygpu.gpuarray.GpuArray,
207                     pygpu.gpuarray.SIZE,
208                     pygpu.gpuarray.SSIZE,
209                     pygpu.gpuarray.</b></font>SSIZE
210                     ],
211             flags=Kernel.get_flags(node.outputs[0].dtype),
212             objvar='k_multi_warp_multinomial_wor_' + name)]
213     def c_code(self, node, name, inp, outputs, sub):
214         pvals, unis, n = inp
215         out, = outputs
216         replace = int(self.replace)
217         fail = sub['fail']
218         ctx = sub['params']
219         kname = self.gpu_kernels(node, name)[0].objvar
220         s = """
221     PyGpuArrayObject * pvals = %(pvals)s;
222     PyGpuArrayObject * unis = %(unis)s;
223     const size_t n_samples = %(n)s;
224     PyGpuArrayObject * out = %(out)s;
225     // create a copy of pvals matrix
226     PyGpuArrayObject * pvals_copy = NULL;
227     size_t dims[2];
228     if (PyGpuArray_NDIM(pvals) != 2)
229     {
230         PyErr_Format(PyExc_TypeError, "pvals wrong rank");
231         %(fail)s
232     }
233     if (PyGpuArray_NDIM(unis) != 1)
234     {
235         PyErr_Format(PyExc_TypeError, "unis wrong rank");
236         %(fail)s
237     }
238     if ( n_samples &gt; (PyGpuArray_DIMS(pvals)[1]) )
239     {
240         PyErr_Format(PyExc_ValueError, "Cannot sample without replacement n samples bigger than the size of the distribution.");
241         %(fail)s;
242     }
243     if (PyGpuArray_DIMS(unis)[0] != PyGpuArray_DIMS(pvals)[0] * n_samples)
244     {
245         PyErr_Format(PyExc_ValueError, "unis.shape[0] != pvals.shape[0] * n");
246         %(fail)s
247     }
248     if (! %(replace)s) {
249         pvals_copy = pygpu_copy(pvals, GA_C_ORDER);
250     } else {
251         pvals_copy = pvals;
252         Py_INCREF(pvals_copy);
253     }
254     dims[0] = n_samples;
255     dims[1] = PyGpuArray_DIMS(pvals)[0];
256     if (theano_prep_output(&amp;out, 2, dims, GA_LONG,
257                            GA_C_ORDER, %(ctx)s) != 0){
258         Py_DECREF(pvals_copy);
259         %(fail)s
260     }
261     %(out)s = out;
262     { // NESTED SCOPE
263         int nb_multi = PyGpuArray_DIMS(pvals)[0];
264         int nb_outcomes = PyGpuArray_DIMS(pvals)[1];
265         //TODO : change this for a beautiful constant
266         int max_nb_blocks = 2&lt;&lt;15 - 1;
267         size_t nb_blocks = max_nb_blocks + 1;
268         size_t nb_threads=16; // so it really starts at 32, because of the *2
269         do
270         {
271             nb_threads*=2;
272             if (nb_multi %% nb_threads == 0)
273                 nb_blocks = nb_multi/nb_threads;
274             else
275                 nb_blocks = (int)((float)nb_multi/(float)nb_threads + 1.);
276         } while (nb_blocks &gt; max_nb_blocks);
277         // TODO : next line is a bit hardcoded...
278         if (nb_threads &gt; 512)
279         {
280             PyErr_Format(
281                 PyExc_ValueError,
282                 "Multinomial is not implemented for so many rows in the matrix (%%i)",
283                 nb_multi);
284             Py_DECREF(pvals_copy);
285             %(fail)s
286         }
287         assert(nb_blocks*nb_threads &gt;= nb_multi);
288         int err = k_multi_warp_multinomial_wor_call(1, &amp;nb_blocks, &amp;nb_threads, 0, PyGpuArray_DIMS(pvals)[0], PyGpuArray_DIMS(pvals)[1], n_samples, pvals_copy-&gt;ga.data, pvals_copy-&gt;ga.offset, PyGpuArray_STRIDES(pvals)[0]/sizeof(float), PyGpuArray_STRIDES(pvals)[1]/sizeof(float), unis-&gt;ga.data, unis-&gt;ga.offset, PyGpuArray_STRIDES(unis)[0]/sizeof(float), out-&gt;ga.data, out-&gt;ga.offset, PyGpuArray_STRIDES(out)[0]/8, PyGpuArray_STRIDES(out)[1]/8);
289         if (err != GA_NO_ERROR) {
290            PyErr_Format(
291                 PyExc_RuntimeError,
292                 "gpuarray error: %%s: %%s.\\n",
293                 "k_multi_warp_%(name)s",
294                 GpuKernel_error(&amp;%(kname)s, err));
295            Py_DECREF(pvals_copy);
296            %(fail)s;
297         }
298         Py_DECREF(pvals_copy);
299     } // END NESTED SCOPE
300         """ % locals()
301         return s
302     def c_code_cache_version(self):
303         return (10,)
304 @register_opt('fast_compile')
305 @op_lifter([theano.sandbox.multinomial.MultinomialFromUniform])
306 @register_opt2([theano.sandbox.multinomial.MultinomialFromUniform], 'fast_compile')
307 def local_gpua_multinomial(op, context_name, inputs, outputs):
308     if len(inputs) == 2:
309         p, u = inputs
310         n_samples = 1
311     else:
312         p, u, n_samples = inputs
313     try:
314         if get_scalar_constant_value(n_samples) != 1:
315             return None
316     except NotScalarConstantError:
317         return None
318     m, = outputs
319     gpu_op = GPUAMultinomialFromUniform(op.odtype)
320     return GpuDimShuffle([False, False], [1, 0])(
321         gpu_op(p, u))
322 @register_opt('fast_compile')
323 @op_lifter([theano.sandbox.multinomial.ChoiceFromUniform])
324 @register_opt2([theano.sandbox.multinomial.ChoiceFromUniform], 'fast_compile')
325 def local_gpua_multinomial_wor(op, context_name, inputs, outputs):
326     p, u, n = inputs
327     m, = outputs
328     if ((p.dtype == u.dtype == 'float32') and (m.dtype == 'int64')):
329         gpu_op = GPUAChoiceFromUniform(**op._props_dict())
330         return GpuDimShuffle([False, False], [1, 0])(
331             gpu_op(p, u, n))
332 class GPUAMultinomialWOReplacementFromUniform(GPUAChoiceFromUniform):
333     def __init__(self, *args, **kwargs):
334         warnings.warn("GPUAMultinomialWOReplacementFromUniform is deprecated, "
335                       "use GPUAChoiceFromUniform instead.",
336                       DeprecationWarning,
337                       stacklevel=2)
338         super(GPUAMultinomialWOReplacementFromUniform, self).__init__(*args, **kwargs)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
