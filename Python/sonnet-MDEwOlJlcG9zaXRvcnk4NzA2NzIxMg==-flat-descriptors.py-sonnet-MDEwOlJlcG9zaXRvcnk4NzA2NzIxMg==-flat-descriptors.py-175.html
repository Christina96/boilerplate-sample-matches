
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 16, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-descriptors.py</h3>
            <pre><code>1  import collections
2  from typing import Callable, Union
3  import sonnet as snt
4  import tensorflow as tf
5  class Wrapped(snt.Module):
6    @snt.no_name_scope
7    def __init__(self, wrapped: snt.Module):
8      super().__init__()
9      self.wrapped = wrapped
10  class Training(Wrapped):
11    @snt.no_name_scope
12    def __call__(self, x: tf.Tensor):
13      return self.wrapped(x, is_training=True)
14  class Recurrent(Wrapped):
15    def __init__(self,
16                 module: Union[snt.RNNCore, snt.UnrolledRNN],
17                 unroller=None):
18      super().__init__(module)
19      self.unroller = unroller
20    @snt.no_name_scope
21    def __call__(self, x: tf.Tensor):
22      initial_state = self.wrapped.initial_state(batch_size=tf.shape(x)[0])
23      if isinstance(self.wrapped, snt.UnrolledRNN):
24        assert self.unroller is None
25        x = tf.transpose(x, [1, 0] + list(range(2, x.shape.rank)))
26        return self.wrapped(x, initial_state)
27      else:
28        x = tf.expand_dims(x, axis=0)
29        return self.unroller(self.wrapped, x, initial_state)
30  def unwrap(module: snt.Module) -&gt; snt.Module:
31    while isinstance(module, Wrapped):
32      module = module.wrapped
33    return module
34  ModuleDescriptor = collections.namedtuple(&quot;ModuleDescriptor&quot;,
35                                            [&quot;name&quot;, &quot;create&quot;, &quot;shape&quot;, &quot;dtype&quot;])
36  ModuleDescriptor.__new__.__defaults__ = (None, None, None, tf.float32)
37  BATCH_SIZE = 8
38  BATCH_MODULES = (
39      ModuleDescriptor(
40          name=&quot;BatchNorm&quot;,
41          create=lambda: Training(snt.BatchNorm(True, True)),
42          shape=(BATCH_SIZE, 2, 2, 3)),
43      ModuleDescriptor(
44          name=&quot;Bias&quot;, create=lambda: snt.Bias(), shape=(BATCH_SIZE, 3, 3, 3)),
45      ModuleDescriptor(
46          name=&quot;Conv1D&quot;,
47          create=lambda: snt.Conv1D(3, 3),
48          shape=(BATCH_SIZE, 2, 2)),
49      ModuleDescriptor(
50          name=&quot;Conv1DTranspose&quot;,
51          create=lambda: snt.Conv1DTranspose(3, 3),
52          shape=(BATCH_SIZE, 2, 2)),
53      ModuleDescriptor(
54          name=&quot;Conv2D&quot;,
55          create=lambda: snt.Conv2D(3, 3),
56          shape=(BATCH_SIZE, 2, 2, 2)),
57      ModuleDescriptor(
58          name=&quot;Conv2DTranspose&quot;,
59          create=lambda: snt.Conv2DTranspose(3, 3),
60          shape=(BATCH_SIZE, 2, 2, 2)),
61      ModuleDescriptor(
62          name=&quot;Conv3D&quot;,
63          create=lambda: snt.Conv3D(3, 3),
64          shape=(BATCH_SIZE, 2, 2, 2, 2)),
65      ModuleDescriptor(
66          name=&quot;Conv3DTranspose&quot;,
67          create=lambda: snt.Conv3DTranspose(3, 3),
68          shape=(BATCH_SIZE, 2, 2, 2, 2)),
69      ModuleDescriptor(
70          name=&quot;CrossReplicaBatchNorm&quot;,
71          create=lambda: Training(snt.distribute.CrossReplicaBatchNorm(  # pylint: disable=g-long-lambda
72              True, True,
73              snt.ExponentialMovingAverage(0.9),
74              snt.ExponentialMovingAverage(0.9))),
75          shape=(BATCH_SIZE, 2, 2, 3)),
76      ModuleDescriptor(
77          name=&quot;DepthwiseConv2D&quot;,
78          create=lambda: snt.DepthwiseConv2D(3),
79          shape=(BATCH_SIZE, 2, 2, 2)),
80      ModuleDescriptor(
81          name=&quot;Dropout&quot;,
82          create=lambda: Training(snt.Dropout(0.5)),
83          shape=(BATCH_SIZE, 3, 3)),
84      ModuleDescriptor(
85          name=&quot;Embed&quot;,
86          create=lambda: snt.Embed(10),
87          shape=(BATCH_SIZE,),
88          dtype=tf.int32),
89      ModuleDescriptor(
90          name=&quot;Flatten&quot;,
91          create=lambda: snt.Flatten(),
92          shape=(BATCH_SIZE, 3, 3, 3)),
93      ModuleDescriptor(
94          name=&quot;GroupNorm&quot;,
95          create=lambda: snt.GroupNorm(2, True, True),
96          shape=(BATCH_SIZE, 3, 4)),
97      ModuleDescriptor(
98          name=&quot;InstanceNorm&quot;,
99          create=lambda: snt.InstanceNorm(True, True),
100          shape=(BATCH_SIZE, 3, 2)),
101      ModuleDescriptor(
102          name=&quot;LayerNorm&quot;,
103          create=lambda: snt.LayerNorm(1, True, True),
104          shape=(BATCH_SIZE, 3, 2)),
105      ModuleDescriptor(
106          name=&quot;Linear&quot;, create=lambda: snt.Linear(10), shape=(BATCH_SIZE, 1)),
107      ModuleDescriptor(
108          name=&quot;Sequential&quot;,
109          create=lambda: snt.Sequential([lambda x: x]),
110          shape=(BATCH_SIZE, 2, 2)),
111      ModuleDescriptor(
112          name=&quot;nets.VectorQuantizer&quot;,
113          create=lambda: Training(snt.nets.VectorQuantizer(4, 6, 0.25)),
114          shape=(BATCH_SIZE, 3, 4)),
115      ModuleDescriptor(
116          name=&quot;nets.VectorQuantizerEMA&quot;,
117          create=lambda: Training(snt.nets.VectorQuantizerEMA(5, 7, 0.5, 0.9)),
118          shape=(BATCH_SIZE, 5)),
119      ModuleDescriptor(
120          name=&quot;nets.Cifar10ConvNet&quot;,
121          create=lambda: Training(snt.nets.Cifar10ConvNet()),
122          shape=(BATCH_SIZE, 3, 3, 2)),
123      ModuleDescriptor(
124          name=&quot;nets.ResNet50&quot;,
125          create=lambda: Training(snt.nets.ResNet([1, 1, 1, 1], 4)),
126          shape=(BATCH_SIZE, 3, 3, 2)),
127      ModuleDescriptor(
128          name=&quot;nets.MLP&quot;,
129          create=lambda: snt.nets.MLP([3, 4, 5]),
130          shape=(BATCH_SIZE, 3)),
131  )
132  RNN_CORES = (
133      ModuleDescriptor(
134          name=&quot;Conv1DLSTM&quot;,
135          create=lambda: snt.Conv1DLSTM((2, 2), 3, 3),
136          shape=(BATCH_SIZE, 2, 2)),
137      ModuleDescriptor(
138          name=&quot;Conv2DLSTM&quot;,
139          create=lambda: snt.Conv2DLSTM((2, 2, 2), 3, 3),
140          shape=(BATCH_SIZE, 2, 2, 2)),
141      ModuleDescriptor(
142          name=&quot;Conv3DLSTM&quot;,
143          create=lambda: snt.Conv3DLSTM((2, 2, 2, 2), 3, 3),
144          shape=(BATCH_SIZE, 2, 2, 2, 2)),
145      ModuleDescriptor(
146          name=&quot;GRU&quot;,
147          create=lambda: snt.GRU(1),
148          shape=(BATCH_SIZE, 128)),
149      ModuleDescriptor(
150          name=&quot;LSTM&quot;,
151          create=lambda: snt.LSTM(1),
152          shape=(BATCH_SIZE, 128)),
153      ModuleDescriptor(
154          name=&quot;VanillaRNN&quot;,
155          create=lambda: snt.VanillaRNN(8),
156          shape=(BATCH_SIZE, 128)),
157  )
158  UNROLLED_RNN_CORES = (
159      ModuleDescriptor(
160          name=&quot;UnrolledLSTM&quot;,
161          create=lambda: snt.UnrolledLSTM(1),
162          shape=(BATCH_SIZE, 1, 128)),
163  )
164  def recurrent_factory(
165      create_core: Callable[[], snt.RNNCore],
166      unroller,
167  ) -&gt; Callable[[], Recurrent]:
168    return lambda: Recurrent(create_core(), unroller)
169  def unroll_descriptors(descriptors, unroller=None):
170    out = []
171    for name, create, shape, dtype in descriptors:
172      if unroller is None:
173        name = &quot;Recurrent({})&quot;.format(name)
174      else:
175        name = &quot;Recurrent({}, {})&quot;.format(name, unroller.__name__)
176      out.append(
177          ModuleDescriptor(name=name,
178                           create=recurrent_factory(create, unroller),
179                           shape=shape,
180                           dtype=dtype))
181    return tuple(out)
182  RECURRENT_MODULES = (
183      unroll_descriptors(RNN_CORES, snt.dynamic_unroll) +
184      unroll_descriptors(RNN_CORES, snt.static_unroll) +
185      unroll_descriptors(UNROLLED_RNN_CORES))
186  OPTIMIZER_MODULES = (
187      ModuleDescriptor(
188          name=&quot;optimizers.Adam&quot;,
189          create=lambda: snt.optimizers.Adam(learning_rate=0.1)),
190      ModuleDescriptor(
191          name=&quot;optimizers.Momentum&quot;,
192          create=lambda: snt.optimizers.Momentum(learning_rate=0.1, momentum=.9)),
193      ModuleDescriptor(
194          name=&quot;optimizers.RMSProp&quot;,
<span onclick='openModal()' class='match'>195          create=lambda: snt.optimizers.RMSProp(learning_rate=0.1)),
196      ModuleDescriptor(
</span>197          name=&quot;optimizers.SGD&quot;,
198          create=lambda: snt.optimizers.SGD(learning_rate=0.1)),
199  )
200  IGNORED_MODULES = {
201      snt.BatchApply,
202      snt.Deferred,
203      snt.Module,
204      snt.Optimizer,
205      snt.Reshape,
206      snt.ExponentialMovingAverage,
207      snt.Mean,
208      snt.Metric,
209      snt.Sum,
210      snt.BaseBatchNorm,  # Tested via `snt.BatchNorm`.
211      snt.DeepRNN,
212      snt.RNNCore,
213      snt.TrainableState,
214      snt.UnrolledRNN,
215      snt.nets.ResNet50,
216      snt.nets.resnet.BottleNeckBlockV1,
217      snt.nets.resnet.BottleNeckBlockV2,
218      snt.nets.resnet.BlockGroup,
219  }
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-descriptors.py</h3>
            <pre><code>1  import collections
2  from typing import Callable, Union
3  import sonnet as snt
4  import tensorflow as tf
5  class Wrapped(snt.Module):
6    @snt.no_name_scope
7    def __init__(self, wrapped: snt.Module):
8      super().__init__()
9      self.wrapped = wrapped
10  class Training(Wrapped):
11    @snt.no_name_scope
12    def __call__(self, x: tf.Tensor):
13      return self.wrapped(x, is_training=True)
14  class Recurrent(Wrapped):
15    def __init__(self,
16                 module: Union[snt.RNNCore, snt.UnrolledRNN],
17                 unroller=None):
18      super().__init__(module)
19      self.unroller = unroller
20    @snt.no_name_scope
21    def __call__(self, x: tf.Tensor):
22      initial_state = self.wrapped.initial_state(batch_size=tf.shape(x)[0])
23      if isinstance(self.wrapped, snt.UnrolledRNN):
24        assert self.unroller is None
25        x = tf.transpose(x, [1, 0] + list(range(2, x.shape.rank)))
26        return self.wrapped(x, initial_state)
27      else:
28        x = tf.expand_dims(x, axis=0)
29        return self.unroller(self.wrapped, x, initial_state)
30  def unwrap(module: snt.Module) -&gt; snt.Module:
31    while isinstance(module, Wrapped):
32      module = module.wrapped
33    return module
34  ModuleDescriptor = collections.namedtuple(&quot;ModuleDescriptor&quot;,
35                                            [&quot;name&quot;, &quot;create&quot;, &quot;shape&quot;, &quot;dtype&quot;])
36  ModuleDescriptor.__new__.__defaults__ = (None, None, None, tf.float32)
37  BATCH_SIZE = 8
38  BATCH_MODULES = (
39      ModuleDescriptor(
40          name=&quot;BatchNorm&quot;,
41          create=lambda: Training(snt.BatchNorm(True, True)),
42          shape=(BATCH_SIZE, 2, 2, 3)),
43      ModuleDescriptor(
44          name=&quot;Bias&quot;, create=lambda: snt.Bias(), shape=(BATCH_SIZE, 3, 3, 3)),
45      ModuleDescriptor(
46          name=&quot;Conv1D&quot;,
47          create=lambda: snt.Conv1D(3, 3),
48          shape=(BATCH_SIZE, 2, 2)),
49      ModuleDescriptor(
50          name=&quot;Conv1DTranspose&quot;,
51          create=lambda: snt.Conv1DTranspose(3, 3),
52          shape=(BATCH_SIZE, 2, 2)),
53      ModuleDescriptor(
54          name=&quot;Conv2D&quot;,
55          create=lambda: snt.Conv2D(3, 3),
56          shape=(BATCH_SIZE, 2, 2, 2)),
57      ModuleDescriptor(
58          name=&quot;Conv2DTranspose&quot;,
59          create=lambda: snt.Conv2DTranspose(3, 3),
60          shape=(BATCH_SIZE, 2, 2, 2)),
61      ModuleDescriptor(
62          name=&quot;Conv3D&quot;,
63          create=lambda: snt.Conv3D(3, 3),
64          shape=(BATCH_SIZE, 2, 2, 2, 2)),
65      ModuleDescriptor(
66          name=&quot;Conv3DTranspose&quot;,
67          create=lambda: snt.Conv3DTranspose(3, 3),
68          shape=(BATCH_SIZE, 2, 2, 2, 2)),
69      ModuleDescriptor(
70          name=&quot;CrossReplicaBatchNorm&quot;,
71          create=lambda: Training(snt.distribute.CrossReplicaBatchNorm(  # pylint: disable=g-long-lambda
72              True, True,
73              snt.ExponentialMovingAverage(0.9),
74              snt.ExponentialMovingAverage(0.9))),
75          shape=(BATCH_SIZE, 2, 2, 3)),
76      ModuleDescriptor(
77          name=&quot;DepthwiseConv2D&quot;,
78          create=lambda: snt.DepthwiseConv2D(3),
79          shape=(BATCH_SIZE, 2, 2, 2)),
80      ModuleDescriptor(
81          name=&quot;Dropout&quot;,
82          create=lambda: Training(snt.Dropout(0.5)),
83          shape=(BATCH_SIZE, 3, 3)),
84      ModuleDescriptor(
85          name=&quot;Embed&quot;,
86          create=lambda: snt.Embed(10),
87          shape=(BATCH_SIZE,),
88          dtype=tf.int32),
89      ModuleDescriptor(
90          name=&quot;Flatten&quot;,
91          create=lambda: snt.Flatten(),
92          shape=(BATCH_SIZE, 3, 3, 3)),
93      ModuleDescriptor(
94          name=&quot;GroupNorm&quot;,
95          create=lambda: snt.GroupNorm(2, True, True),
96          shape=(BATCH_SIZE, 3, 4)),
97      ModuleDescriptor(
98          name=&quot;InstanceNorm&quot;,
99          create=lambda: snt.InstanceNorm(True, True),
100          shape=(BATCH_SIZE, 3, 2)),
101      ModuleDescriptor(
102          name=&quot;LayerNorm&quot;,
103          create=lambda: snt.LayerNorm(1, True, True),
104          shape=(BATCH_SIZE, 3, 2)),
105      ModuleDescriptor(
106          name=&quot;Linear&quot;, create=lambda: snt.Linear(10), shape=(BATCH_SIZE, 1)),
107      ModuleDescriptor(
108          name=&quot;Sequential&quot;,
109          create=lambda: snt.Sequential([lambda x: x]),
110          shape=(BATCH_SIZE, 2, 2)),
111      ModuleDescriptor(
112          name=&quot;nets.VectorQuantizer&quot;,
113          create=lambda: Training(snt.nets.VectorQuantizer(4, 6, 0.25)),
114          shape=(BATCH_SIZE, 3, 4)),
115      ModuleDescriptor(
116          name=&quot;nets.VectorQuantizerEMA&quot;,
117          create=lambda: Training(snt.nets.VectorQuantizerEMA(5, 7, 0.5, 0.9)),
118          shape=(BATCH_SIZE, 5)),
119      ModuleDescriptor(
120          name=&quot;nets.Cifar10ConvNet&quot;,
121          create=lambda: Training(snt.nets.Cifar10ConvNet()),
122          shape=(BATCH_SIZE, 3, 3, 2)),
123      ModuleDescriptor(
124          name=&quot;nets.ResNet50&quot;,
125          create=lambda: Training(snt.nets.ResNet([1, 1, 1, 1], 4)),
126          shape=(BATCH_SIZE, 3, 3, 2)),
127      ModuleDescriptor(
128          name=&quot;nets.MLP&quot;,
129          create=lambda: snt.nets.MLP([3, 4, 5]),
130          shape=(BATCH_SIZE, 3)),
131  )
132  RNN_CORES = (
133      ModuleDescriptor(
134          name=&quot;Conv1DLSTM&quot;,
135          create=lambda: snt.Conv1DLSTM((2, 2), 3, 3),
136          shape=(BATCH_SIZE, 2, 2)),
137      ModuleDescriptor(
138          name=&quot;Conv2DLSTM&quot;,
139          create=lambda: snt.Conv2DLSTM((2, 2, 2), 3, 3),
140          shape=(BATCH_SIZE, 2, 2, 2)),
141      ModuleDescriptor(
142          name=&quot;Conv3DLSTM&quot;,
143          create=lambda: snt.Conv3DLSTM((2, 2, 2, 2), 3, 3),
144          shape=(BATCH_SIZE, 2, 2, 2, 2)),
145      ModuleDescriptor(
146          name=&quot;GRU&quot;,
147          create=lambda: snt.GRU(1),
148          shape=(BATCH_SIZE, 128)),
149      ModuleDescriptor(
150          name=&quot;LSTM&quot;,
151          create=lambda: snt.LSTM(1),
152          shape=(BATCH_SIZE, 128)),
153      ModuleDescriptor(
154          name=&quot;VanillaRNN&quot;,
155          create=lambda: snt.VanillaRNN(8),
156          shape=(BATCH_SIZE, 128)),
157  )
158  UNROLLED_RNN_CORES = (
159      ModuleDescriptor(
160          name=&quot;UnrolledLSTM&quot;,
161          create=lambda: snt.UnrolledLSTM(1),
162          shape=(BATCH_SIZE, 1, 128)),
163  )
164  def recurrent_factory(
165      create_core: Callable[[], snt.RNNCore],
166      unroller,
167  ) -&gt; Callable[[], Recurrent]:
168    return lambda: Recurrent(create_core(), unroller)
169  def unroll_descriptors(descriptors, unroller=None):
170    out = []
171    for name, create, shape, dtype in descriptors:
172      if unroller is None:
173        name = &quot;Recurrent({})&quot;.format(name)
174      else:
175        name = &quot;Recurrent({}, {})&quot;.format(name, unroller.__name__)
176      out.append(
177          ModuleDescriptor(name=name,
178                           create=recurrent_factory(create, unroller),
179                           shape=shape,
180                           dtype=dtype))
181    return tuple(out)
182  RECURRENT_MODULES = (
183      unroll_descriptors(RNN_CORES, snt.dynamic_unroll) +
184      unroll_descriptors(RNN_CORES, snt.static_unroll) +
185      unroll_descriptors(UNROLLED_RNN_CORES))
186  OPTIMIZER_MODULES = (
187      ModuleDescriptor(
188          name=&quot;optimizers.Adam&quot;,
189          create=lambda: snt.optimizers.Adam(learning_rate=0.1)),
190      ModuleDescriptor(
191          name=&quot;optimizers.Momentum&quot;,
192          create=lambda: snt.optimizers.Momentum(learning_rate=0.1, momentum=.9)),
193      ModuleDescriptor(
194          name=&quot;optimizers.RMSProp&quot;,
195          create=lambda: snt.optimizers.RMSProp(learning_rate=0.1)),
196      ModuleDescriptor(
197          name=&quot;optimizers.SGD&quot;,
<span onclick='openModal()' class='match'>198          create=lambda: snt.optimizers.SGD(learning_rate=0.1)),
199  )
</span>200  IGNORED_MODULES = {
201      snt.BatchApply,
202      snt.Deferred,
203      snt.Module,
204      snt.Optimizer,
205      snt.Reshape,
206      snt.ExponentialMovingAverage,
207      snt.Mean,
208      snt.Metric,
209      snt.Sum,
210      snt.BaseBatchNorm,  # Tested via `snt.BatchNorm`.
211      snt.DeepRNN,
212      snt.RNNCore,
213      snt.TrainableState,
214      snt.UnrolledRNN,
215      snt.nets.ResNet50,
216      snt.nets.resnet.BottleNeckBlockV1,
217      snt.nets.resnet.BottleNeckBlockV2,
218      snt.nets.resnet.BlockGroup,
219  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-descriptors.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-descriptors.py</div>
                </div>
                <div class="column column_space"><pre><code>195          create=lambda: snt.optimizers.RMSProp(learning_rate=0.1)),
196      ModuleDescriptor(
</pre></code></div>
                <div class="column column_space"><pre><code>198          create=lambda: snt.optimizers.SGD(learning_rate=0.1)),
199  )
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    