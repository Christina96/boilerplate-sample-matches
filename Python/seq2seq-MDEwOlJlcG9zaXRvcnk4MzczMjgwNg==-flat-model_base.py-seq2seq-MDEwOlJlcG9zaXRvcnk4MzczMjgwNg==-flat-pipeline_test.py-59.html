
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 12, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-model_base.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import collections
6  import tensorflow as tf
7  from seq2seq.configurable import Configurable
8  from seq2seq.training import utils as training_utils
9  from seq2seq import global_vars
10  def _flatten_dict(dict_, parent_key=&quot;&quot;, sep=&quot;.&quot;):
11    items = []
12    for key, value in dict_.items():
13      new_key = parent_key + sep + key if parent_key else key
14      if isinstance(value, collections.MutableMapping):
15        items.extend(_flatten_dict(value, new_key, sep=sep).items())
16      elif isinstance(value, tuple) and hasattr(value, &quot;_asdict&quot;):
17        dict_items = collections.OrderedDict(zip(value._fields, value))
18        items.extend(_flatten_dict(dict_items, new_key, sep=sep).items())
19      else:
20        items.append((new_key, value))
21    return dict(items)
22  class ModelBase(Configurable):
23    def __init__(self, params, mode, name):
24      self.name = name
25      Configurable.__init__(self, params, mode)
26    def _clip_gradients(self, grads_and_vars):
27      gradients, variables = zip(*grads_and_vars)
28      clipped_gradients, _ = tf.clip_by_global_norm(
29          gradients, self.params[&quot;optimizer.clip_gradients&quot;])
30      return list(zip(clipped_gradients, variables))
31    def _create_optimizer(self):
32      name = self.params[&quot;optimizer.name&quot;]
33      optimizer = tf.contrib.layers.OPTIMIZER_CLS_NAMES[name](
34          learning_rate=self.params[&quot;optimizer.learning_rate&quot;],
35          **self.params[&quot;optimizer.params&quot;])
36      if self.params[&quot;optimizer.sync_replicas&quot;] &gt; 0:
37        optimizer = tf.train.SyncReplicasOptimizer(
38            opt=optimizer,
39            replicas_to_aggregate=self.params[
40                &quot;optimizer.sync_replicas_to_aggregate&quot;],
41            total_num_replicas=self.params[&quot;optimizer.sync_replicas&quot;])
42        global_vars.SYNC_REPLICAS_OPTIMIZER = optimizer
43      return optimizer
44    def _build_train_op(self, loss):
45      learning_rate_decay_fn = training_utils.create_learning_rate_decay_fn(
46          decay_type=self.params[&quot;optimizer.lr_decay_type&quot;] or None,
47          decay_steps=self.params[&quot;optimizer.lr_decay_steps&quot;],
48          decay_rate=self.params[&quot;optimizer.lr_decay_rate&quot;],
49          start_decay_at=self.params[&quot;optimizer.lr_start_decay_at&quot;],
50          stop_decay_at=self.params[&quot;optimizer.lr_stop_decay_at&quot;],
51          min_learning_rate=self.params[&quot;optimizer.lr_min_learning_rate&quot;],
52          staircase=self.params[&quot;optimizer.lr_staircase&quot;])
53      optimizer = self._create_optimizer()
54      train_op = tf.contrib.layers.optimize_loss(
55          loss=loss,
56          global_step=tf.contrib.framework.get_global_step(),
57          learning_rate=self.params[&quot;optimizer.learning_rate&quot;],
58          learning_rate_decay_fn=learning_rate_decay_fn,
59          clip_gradients=self._clip_gradients,
60          optimizer=optimizer,
<span onclick='openModal()' class='match'>61          summaries=[&quot;learning_rate&quot;, &quot;loss&quot;, &quot;gradients&quot;, &quot;gradient_norm&quot;])
62      return train_op
</span>63    @staticmethod
64    def default_params():
65      return {
66          &quot;optimizer.name&quot;: &quot;Adam&quot;,
67          &quot;optimizer.learning_rate&quot;: 1e-4,
68          &quot;optimizer.params&quot;: {}, # Arbitrary parameters for the optimizer
69          &quot;optimizer.lr_decay_type&quot;: &quot;&quot;,
70          &quot;optimizer.lr_decay_steps&quot;: 100,
71          &quot;optimizer.lr_decay_rate&quot;: 0.99,
72          &quot;optimizer.lr_start_decay_at&quot;: 0,
73          &quot;optimizer.lr_stop_decay_at&quot;: tf.int32.max,
74          &quot;optimizer.lr_min_learning_rate&quot;: 1e-12,
75          &quot;optimizer.lr_staircase&quot;: False,
76          &quot;optimizer.clip_gradients&quot;: 5.0,
77          &quot;optimizer.sync_replicas&quot;: 0,
78          &quot;optimizer.sync_replicas_to_aggregate&quot;: 0,
79      }
80    def batch_size(self, features, labels):
81      raise NotImplementedError()
82    def __call__(self, features, labels, params):
83      with tf.variable_scope(&quot;model&quot;):
84        with tf.variable_scope(self.name):
85          return self._build(features, labels, params)
86    def _build(self, features, labels, params):
87      raise NotImplementedError
</code></pre>
        </div>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-pipeline_test.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import argparse
6  import imp
7  import os
8  import shutil
9  import tempfile
10  import yaml
11  import numpy as np
12  import tensorflow as tf
13  from tensorflow import gfile
14  from seq2seq.test import utils as test_utils
15  BIN_FOLDER = os.path.abspath(
16      os.path.join(os.path.dirname(__file__), &quot;../../bin&quot;))
17  def _clear_flags():
18    tf.app.flags.FLAGS = tf.app.flags._FlagValues()
19    tf.app.flags._global_parser = argparse.ArgumentParser()
20  class PipelineTest(tf.test.TestCase):
21    def setUp(self):
22      super(PipelineTest, self).setUp()
23      self.output_dir = tempfile.mkdtemp()
24      self.bin_folder = os.path.abspath(
25          os.path.join(os.path.dirname(__file__), &quot;../../bin&quot;))
26      tf.contrib.framework.get_or_create_global_step()
27    def tearDown(self):
28      shutil.rmtree(self.output_dir, ignore_errors=True)
29      super(PipelineTest, self).tearDown()
30    def test_train_infer(self):
31      sources_train, targets_train = test_utils.create_temp_parallel_data(
32          sources=[&quot;a a a a&quot;, &quot;b b b b&quot;, &quot;c c c c&quot;, &quot;笑 笑 笑 笑&quot;],
<span onclick='openModal()' class='match'>33          targets=[&quot;b b b b&quot;, &quot;a a a a&quot;, &quot;c c c c&quot;, &quot;泣 泣 泣 泣&quot;])
34      sources_dev, targets_dev = test_utils.create_temp_parallel_data(
</span>35          sources=[&quot;a a&quot;, &quot;b b&quot;, &quot;c c c&quot;, &quot;笑 笑 笑&quot;],
36          targets=[&quot;b b&quot;, &quot;a a&quot;, &quot;c c c&quot;, &quot;泣 泣 泣&quot;])
37      vocab_source = test_utils.create_temporary_vocab_file([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;笑&quot;])
38      vocab_target = test_utils.create_temporary_vocab_file([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;泣&quot;])
39      _clear_flags()
40      tf.reset_default_graph()
41      train_script = imp.load_source(&quot;seq2seq.test.train_bin&quot;,
42                                     os.path.join(BIN_FOLDER, &quot;train.py&quot;))
43      tf.app.flags.FLAGS.output_dir = self.output_dir
44      tf.app.flags.FLAGS.hooks = 
45      tf.app.flags.FLAGS.metrics = 
46      tf.app.flags.FLAGS.model = &quot;AttentionSeq2Seq&quot;
47      tf.app.flags.FLAGS.model_params = .format(vocab_source.name, vocab_target.name)
48      tf.app.flags.FLAGS.batch_size = 2
49      config_path = os.path.join(self.output_dir, &quot;train_config.yml&quot;)
50      with gfile.GFile(config_path, &quot;w&quot;) as config_file:
51        yaml.dump({
52            &quot;input_pipeline_train&quot;: {
53                &quot;class&quot;: &quot;ParallelTextInputPipeline&quot;,
54                &quot;params&quot;: {
55                    &quot;source_files&quot;: [sources_train.name],
56                    &quot;target_files&quot;: [targets_train.name],
57                }
58            },
59            &quot;input_pipeline_dev&quot;: {
60                &quot;class&quot;: &quot;ParallelTextInputPipeline&quot;,
61                &quot;params&quot;: {
62                    &quot;source_files&quot;: [sources_dev.name],
63                    &quot;target_files&quot;: [targets_dev.name],
64                }
65            },
66            &quot;train_steps&quot;: 50,
67            &quot;model_params&quot;: {
68                &quot;embedding.dim&quot;: 10,
69                &quot;decoder.params&quot;: {
70                    &quot;rnn_cell&quot;: {
71                        &quot;cell_class&quot;: &quot;GRUCell&quot;,
72                        &quot;cell_params&quot;: {
73                            &quot;num_units&quot;: 8
74                        }
75                    }
76                },
77                &quot;encoder.params&quot;: {
78                    &quot;rnn_cell&quot;: {
79                        &quot;cell_class&quot;: &quot;GRUCell&quot;,
80                        &quot;cell_params&quot;: {
81                            &quot;num_units&quot;: 8
82                        }
83                    }
84                }
85            }
86        }, config_file)
87      tf.app.flags.FLAGS.config_paths = config_path
88      tf.logging.set_verbosity(tf.logging.INFO)
89      train_script.main([])
90      expected_checkpoint = os.path.join(self.output_dir,
91                                         &quot;model.ckpt-50.data-00000-of-00001&quot;)
92      self.assertTrue(os.path.exists(expected_checkpoint))
93      _clear_flags()
94      tf.reset_default_graph()
95      infer_script = imp.load_source(&quot;seq2seq.test.infer_bin&quot;,
96                                     os.path.join(BIN_FOLDER, &quot;infer.py&quot;))
97      attention_dir = os.path.join(self.output_dir, &quot;att&quot;)
98      tf.app.flags.FLAGS.model_dir = self.output_dir
99      tf.app.flags.FLAGS.input_pipeline = .format(sources_dev.name, targets_dev.name)
100      tf.app.flags.FLAGS.batch_size = 2
101      tf.app.flags.FLAGS.checkpoint_path = os.path.join(self.output_dir,
102                                                        &quot;model.ckpt-50&quot;)
103      tf.app.flags.FLAGS.tasks = .format(attention_dir)
104      infer_script.main([])
105      self.assertTrue(
106          os.path.exists(os.path.join(attention_dir, &quot;attention_scores.npz&quot;)))
107      self.assertTrue(os.path.exists(os.path.join(attention_dir, &quot;00002.png&quot;)))
108      scores = np.load(os.path.join(attention_dir, &quot;attention_scores.npz&quot;))
109      self.assertIn(&quot;arr_0&quot;, scores)
110      self.assertEqual(scores[&quot;arr_0&quot;].shape[1], 3)
111      self.assertIn(&quot;arr_1&quot;, scores)
112      self.assertEqual(scores[&quot;arr_1&quot;].shape[1], 3)
113      self.assertIn(&quot;arr_2&quot;, scores)
114      self.assertEqual(scores[&quot;arr_2&quot;].shape[1], 4)
115      self.assertIn(&quot;arr_3&quot;, scores)
116      self.assertEqual(scores[&quot;arr_3&quot;].shape[1], 4)
117      _clear_flags()
118      tf.reset_default_graph()
119      infer_script = imp.load_source(&quot;seq2seq.test.infer_bin&quot;,
120                                     os.path.join(BIN_FOLDER, &quot;infer.py&quot;))
121      tf.app.flags.FLAGS.model_dir = self.output_dir
122      tf.app.flags.FLAGS.input_pipeline = .format(sources_dev.name, targets_dev.name)
123      tf.app.flags.FLAGS.batch_size = 2
124      tf.app.flags.FLAGS.checkpoint_path = os.path.join(self.output_dir,
125                                                        &quot;model.ckpt-50&quot;)
126      tf.app.flags.FLAGS.model_params = 
127      tf.app.flags.FLAGS.tasks = .format(os.path.join(self.output_dir, &quot;beams.npz&quot;))
128      infer_script.main([])
129      self.assertTrue(os.path.exists(os.path.join(self.output_dir, &quot;beams.npz&quot;)))
130  if __name__ == &quot;__main__&quot;:
131    tf.test.main()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-model_base.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-pipeline_test.py</div>
                </div>
                <div class="column column_space"><pre><code>61          summaries=[&quot;learning_rate&quot;, &quot;loss&quot;, &quot;gradients&quot;, &quot;gradient_norm&quot;])
62      return train_op
</pre></code></div>
                <div class="column column_space"><pre><code>33          targets=[&quot;b b b b&quot;, &quot;a a a a&quot;, &quot;c c c c&quot;, &quot;泣 泣 泣 泣&quot;])
34      sources_dev, targets_dev = test_utils.create_temp_parallel_data(
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    