
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 6.12736660929432%, Tokens: 10, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-backbone.py</h3>
            <pre><code>1  import torch
2  import torch.nn as nn
3  import pickle
4  from collections import OrderedDict
5  try:
6      from dcn_v2 import DCN
7  except ImportError:
8      def DCN(*args, **kwdargs):
9          raise Exception('DCN could not be imported. If you want to use YOLACT++ models, compile DCN. Check the README for instructions.')
10  class Bottleneck(nn.Module):
11      expansion = 4
12      def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d, dilation=1, use_dcn=False):
13          super(Bottleneck, self).__init__()
14          self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, dilation=dilation)
15          self.bn1 = norm_layer(planes)
16          if use_dcn:
17              self.conv2 = DCN(planes, planes, kernel_size=3, stride=stride,
18                                  padding=dilation, dilation=dilation, deformable_groups=1)
19              self.conv2.bias.data.zero_()
20              self.conv2.conv_offset_mask.weight.data.zero_()
21              self.conv2.conv_offset_mask.bias.data.zero_()
22          else:
23              self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
24                                  padding=dilation, bias=False, dilation=dilation)
25          self.bn2 = norm_layer(planes)
26          self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, dilation=dilation)
27          self.bn3 = norm_layer(planes * 4)
28          self.relu = nn.ReLU(inplace=True)
29          self.downsample = downsample
30          self.stride = stride
31      def forward(self, x):
32          residual = x
33          out = self.conv1(x)
34          out = self.bn1(out)
35          out = self.relu(out)
36          out = self.conv2(out)
37          out = self.bn2(out)
38          out = self.relu(out)
39          out = self.conv3(out)
40          out = self.bn3(out)
41          if self.downsample is not None:
42              residual = self.downsample(x)
43          out += residual
44          out = self.relu(out)
45          return out
46  class ResNetBackbone(nn.Module):
47      def __init__(self, layers, dcn_layers=[0, 0, 0, 0], dcn_interval=1, atrous_layers=[], block=Bottleneck, norm_layer=nn.BatchNorm2d):
48          super().__init__()
49          self.num_base_layers = len(layers)
50          self.layers = nn.ModuleList()
51          self.channels = []
52          self.norm_layer = norm_layer
53          self.dilation = 1
54          self.atrous_layers = atrous_layers
55          self.inplanes = 64
56          self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
57          self.bn1 = norm_layer(64)
58          self.relu = nn.ReLU(inplace=True)
59          self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
60          self._make_layer(block, 64, layers[0], dcn_layers=dcn_layers[0], dcn_interval=dcn_interval)
61          self._make_layer(block, 128, layers[1], stride=2, dcn_layers=dcn_layers[1], dcn_interval=dcn_interval)
62          self._make_layer(block, 256, layers[2], stride=2, dcn_layers=dcn_layers[2], dcn_interval=dcn_interval)
63          self._make_layer(block, 512, layers[3], stride=2, dcn_layers=dcn_layers[3], dcn_interval=dcn_interval)
64          self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]
65      def _make_layer(self, block, planes, blocks, stride=1, dcn_layers=0, dcn_interval=1):
66          downsample = None
67          if stride != 1 or self.inplanes != planes * block.expansion:
68              if len(self.layers) in self.atrous_layers:
69                  self.dilation += 1
70                  stride = 1
71              downsample = nn.Sequential(
72                  nn.Conv2d(self.inplanes, planes * block.expansion,
73                            kernel_size=1, stride=stride, bias=False,
74                            dilation=self.dilation),
75                  self.norm_layer(planes * block.expansion),
76              )
77          layers = []
78          use_dcn = (dcn_layers >= blocks)
79          layers.append(block(self.inplanes, planes, stride, downsample, self.norm_layer, self.dilation, use_dcn=use_dcn))
80          self.inplanes = planes * block.expansion
81          for i in range(1, blocks):
82              use_dcn = ((i+dcn_layers) >= blocks) and (i % dcn_interval == 0)
83              layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer, use_dcn=use_dcn))
84          layer = nn.Sequential(*layers)
85          self.channels.append(planes * block.expansion)
86          self.layers.append(layer)
87          return layer
88      def forward(self, x):
89          x = self.conv1(x)
90          x = self.bn1(x)
91          x = self.relu(x)
92          x = self.maxpool(x)
93          outs = []
94          for layer in self.layers:
95              x = layer(x)
96              outs.append(x)
97          return tuple(outs)
98      def init_backbone(self, path):
99          state_dict = torch.load(path)
100          keys = list(state_dict)
101          for key in keys:
102              if key.startswith('layer'):
103                  idx = int(key[5])
104                  new_key = 'layers.' + str(idx-1) + key[6:]
105                  state_dict[new_key] = state_dict.pop(key)
106          self.load_state_dict(state_dict, strict=False)
107      def add_layer(self, conv_channels=1024, downsample=2, depth=1, block=Bottleneck):
108          self._make_layer(block, conv_channels // block.expansion, blocks=depth, stride=downsample)
109  class ResNetBackboneGN(ResNetBackbone):
110      def __init__(self, layers, num_groups=32):
111          super().__init__(layers, norm_layer=lambda x: nn.GroupNorm(num_groups, x))
112      def init_backbone(self, path):
113          with open(path, 'rb') as f:
114              state_dict = pickle.load(f, encoding='latin1') # From the detectron source
115              state_dict = state_dict['blobs']
116          our_state_dict_keys = list(self.state_dict().keys())
117          new_state_dict = {}
118          gn_trans     = lambda x: ('gn_s' if x == 'weight' else 'gn_b')
119          layeridx2res = lambda x: 'res' + str(int(x)+2)
120          block2branch = lambda x: 'branch2' + ('a', 'b', 'c')[int(x[-1:])-1]
121          for key in our_state_dict_keys:
122              parts = key.split('.')
123              transcribed_key = ''
124              if (parts[0] == 'conv1'):
125                  transcribed_key = 'conv1_w'
126              elif (parts[0] == 'bn1'):
127                  transcribed_key = 'conv1_' + gn_trans(parts[1])
128              elif (parts[0] == 'layers'):
129                  if int(parts[1]) >= self.num_base_layers: continue
130                  transcribed_key = layeridx2res(parts[1])
131                  transcribed_key += '_' + parts[2] + '_'
132                  if parts[3] == 'downsample':
133                      transcribed_key += 'branch1_'
134                      if parts[4] == '0':
135                          transcribed_key += 'w'
136                      else:
137                          transcribed_key += gn_trans(parts[5])
138                  else:
139                      transcribed_key += block2branch(parts[3]) + '_'
140                      if 'conv' in parts[3]:
141                          transcribed_key += 'w'
142                      else:
143                          transcribed_key += gn_trans(parts[4])
144              new_state_dict[key] = torch.Tensor(state_dict[transcribed_key])
145          self.load_state_dict(new_state_dict, strict=False)
146  def darknetconvlayer(in_channels, out_channels, *args, **kwdargs):
147      return nn.Sequential(
148          nn.Conv2d(in_channels, out_channels, *args, **kwdargs, bias=False),
149          nn.BatchNorm2d(out_channels),
150          nn.LeakyReLU(0.1, inplace=True)
151      )
152  class DarkNetBlock(nn.Module):
153      expansion = 2
154      def __init__(self, in_channels, channels):
155          super().__init__()
156          self.conv1 = darknetconvlayer(in_channels, channels,                  kernel_size=1)
157          self.conv2 = darknetconvlayer(channels,    channels * self.expansion, kernel_size=3, padding=1)
158      def forward(self, x):
159          return self.conv2(self.conv1(x)) + x
160  class DarkNetBackbone(nn.Module):
161      def __init__(self, layers=[1, 2, 8, 8, 4], block=DarkNetBlock):
162          super().__init__()
163          self.num_base_layers = len(layers)
164          self.layers = nn.ModuleList()
165          self.channels = []
166          self._preconv = darknetconvlayer(3, 32, kernel_size=3, padding=1)
167          self.in_channels = 32
168          self._make_layer(block, 32,  layers[0])
169          self._make_layer(block, 64,  layers[1])
170          self._make_layer(block, 128, layers[2])
171          self._make_layer(block, 256, layers[3])
172          self._make_layer(block, 512, layers[4])
173          self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]
174      def _make_layer(self, block, channels, num_blocks, stride=2):
175          layer_list = []
176          layer_list.append(
177              darknetconvlayer(self.in_channels, channels * block.expansion,
178                               kernel_size=3, padding=1, stride=stride))
179          self.in_channels = channels * block.expansion
180          layer_list += [block(self.in_channels, channels) for _ in range(num_blocks)]
181          self.channels.append(self.in_channels)
182          self.layers.append(nn.Sequential(*layer_list))
183      def forward(self, x):
184          x = self._preconv(x)
185          outs = []
186          for layer in self.layers:
187              x = layer(x)
188              outs.append(x)
189          return tuple(outs)
190      def add_layer(self, conv_channels=1024, stride=2, depth=1, block=DarkNetBlock):
191          self._make_layer(block, conv_channels // block.expansion, num_blocks=depth, stride=stride)
192      def init_backbone(self, path):
193          self.load_state_dict(torch.load(path), strict=False)
194  class VGGBackbone(nn.Module):
195      def __init__(self, cfg, extra_args=[], norm_layers=[]):
196          super().__init__()
197          self.channels = []
198          self.layers = nn.ModuleList()
199          self.in_channels = 3
200          self.extra_args = list(reversed(extra_args)) # So I can use it as a stack
201          self.total_layer_count = 0
202          self.state_dict_lookup = {}
203          for idx, layer_cfg in enumerate(cfg):
204              self._make_layer(layer_cfg)
205          self.norms = nn.ModuleList([nn.BatchNorm2d(self.channels[l]) for l in norm_layers])
206          self.norm_lookup = {l: idx for idx, l in enumerate(norm_layers)}
207          self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]
208      def _make_layer(self, cfg):
209          layers = []
210          for v in cfg:
211              args = None
212              if isinstance(v, tuple):
213                  args = v[1]
214                  v = v[0]
215              if v == 'M':
216                  if args is None:
217                      args = {'kernel_size': 2, 'stride': 2}
218                  layers.append(nn.MaxPool2d(**args))
219              else:
<span onclick='openModal()' class='match'>220                  cur_layer_idx = self.total_layer_count + len(layers)
221                  self.state_dict_lookup[cur_layer_idx] = '%d.%d' % (len(self.layers), len(layers))
</span>222                  if args is None:
223                      args = {'kernel_size': 3, 'padding': 1}
224                  layers.append(nn.Conv2d(self.in_channels, v, **args))
225                  layers.append(nn.ReLU(inplace=True))
226                  self.in_channels = v
227          self.total_layer_count += len(layers)
228          self.channels.append(self.in_channels)
229          self.layers.append(nn.Sequential(*layers))
230      def forward(self, x):
231          outs = []
232          for idx, layer in enumerate(self.layers):
233              x = layer(x)
234              if idx in self.norm_lookup:
235                  x = self.norms[self.norm_lookup[idx]](x)
236              outs.append(x)
237          return tuple(outs)
238      def transform_key(self, k):
239          vals = k.split('.')
240          layerIdx = self.state_dict_lookup[int(vals[0])]
241          return 'layers.%s.%s' % (layerIdx, vals[1])
242      def init_backbone(self, path):
243          state_dict = torch.load(path)
244          state_dict = OrderedDict([(self.transform_key(k), v) for k,v in state_dict.items()])
245          self.load_state_dict(state_dict, strict=False)
246      def add_layer(self, conv_channels=128, downsample=2):
247          if len(self.extra_args) > 0:
248              conv_channels, downsample = self.extra_args.pop()
249          padding = 1 if downsample > 1 else 0
250          layer = nn.Sequential(
251              nn.Conv2d(self.in_channels, conv_channels, kernel_size=1),
252              nn.ReLU(inplace=True),
253              nn.Conv2d(conv_channels, conv_channels*2, kernel_size=3, stride=downsample, padding=padding),
254              nn.ReLU(inplace=True)
255          )
256          self.in_channels = conv_channels*2
257          self.channels.append(self.in_channels)
258          self.layers.append(layer)
259  def construct_backbone(cfg):
260      backbone = cfg.type(*cfg.args)
261      num_layers = max(cfg.selected_layers) + 1
262      while len(backbone.layers) < num_layers:
263          backbone.add_layer()
264      return backbone
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-eval.py</h3>
            <pre><code>1  from data import COCODetection, get_label_map, MEANS, COLORS
2  from yolact import Yolact
3  from utils.augmentations import BaseTransform, FastBaseTransform, Resize
4  from utils.functions import MovingAverage, ProgressBar
5  from layers.box_utils import jaccard, center_size, mask_iou
6  from utils import timer
7  from utils.functions import SavePath
8  from layers.output_utils import postprocess, undo_image_transformation
9  import pycocotools
10  from data import cfg, set_cfg, set_dataset
11  import numpy as np
12  import torch
13  import torch.backends.cudnn as cudnn
14  from torch.autograd import Variable
15  import argparse
16  import time
17  import random
18  import cProfile
19  import pickle
20  import json
21  import os
22  from collections import defaultdict
23  from pathlib import Path
24  from collections import OrderedDict
25  from PIL import Image
26  import matplotlib.pyplot as plt
27  import cv2
28  def str2bool(v):
29      if v.lower() in ('yes', 'true', 't', 'y', '1'):
30          return True
31      elif v.lower() in ('no', 'false', 'f', 'n', '0'):
32          return False
33      else:
34          raise argparse.ArgumentTypeError('Boolean value expected.')
35  def parse_args(argv=None):
36      parser = argparse.ArgumentParser(
37          description='YOLACT COCO Evaluation')
38      parser.add_argument('--trained_model',
39                          default='weights/ssd300_mAP_77.43_v2.pth', type=str,
40                          help='Trained state_dict file path to open. If "interrupt", this will open the interrupt file.')
41      parser.add_argument('--top_k', default=5, type=int,
42                          help='Further restrict the number of predictions to parse')
43      parser.add_argument('--cuda', default=True, type=str2bool,
44                          help='Use cuda to evaulate model')
45      parser.add_argument('--fast_nms', default=True, type=str2bool,
46                          help='Whether to use a faster, but not entirely correct version of NMS.')
47      parser.add_argument('--cross_class_nms', default=False, type=str2bool,
48                          help='Whether compute NMS cross-class or per-class.')
49      parser.add_argument('--display_masks', default=True, type=str2bool,
50                          help='Whether or not to display masks over bounding boxes')
51      parser.add_argument('--display_bboxes', default=True, type=str2bool,
52                          help='Whether or not to display bboxes around masks')
53      parser.add_argument('--display_text', default=True, type=str2bool,
54                          help='Whether or not to display text (class [score])')
55      parser.add_argument('--display_scores', default=True, type=str2bool,
56                          help='Whether or not to display scores in addition to classes')
57      parser.add_argument('--display', dest='display', action='store_true',
58                          help='Display qualitative results instead of quantitative ones.')
59      parser.add_argument('--shuffle', dest='shuffle', action='store_true',
60                          help='Shuffles the images when displaying them. Doesn\'t have much of an effect when display is off though.')
61      parser.add_argument('--ap_data_file', default='results/ap_data.pkl', type=str,
62                          help='In quantitative mode, the file to save detections before calculating mAP.')
63      parser.add_argument('--resume', dest='resume', action='store_true',
64                          help='If display not set, this resumes mAP calculations from the ap_data_file.')
65      parser.add_argument('--max_images', default=-1, type=int,
66                          help='The maximum number of images from the dataset to consider. Use -1 for all.')
67      parser.add_argument('--output_coco_json', dest='output_coco_json', action='store_true',
68                          help='If display is not set, instead of processing IoU values, this just dumps detections into the coco json file.')
69      parser.add_argument('--bbox_det_file', default='results/bbox_detections.json', type=str,
70                          help='The output file for coco bbox results if --coco_results is set.')
71      parser.add_argument('--mask_det_file', default='results/mask_detections.json', type=str,
72                          help='The output file for coco mask results if --coco_results is set.')
73      parser.add_argument('--config', default=None,
74                          help='The config object to use.')
75      parser.add_argument('--output_web_json', dest='output_web_json', action='store_true',
76                          help='If display is not set, instead of processing IoU values, this dumps detections for usage with the detections viewer web thingy.')
77      parser.add_argument('--web_det_path', default='web/dets/', type=str,
78                          help='If output_web_json is set, this is the path to dump detections into.')
79      parser.add_argument('--no_bar', dest='no_bar', action='store_true',
80                          help='Do not output the status bar. This is useful for when piping to a file.')
81      parser.add_argument('--display_lincomb', default=False, type=str2bool,
82                          help='If the config uses lincomb masks, output a visualization of how those masks are created.')
83      parser.add_argument('--benchmark', default=False, dest='benchmark', action='store_true',
84                          help='Equivalent to running display mode but without displaying an image.')
85      parser.add_argument('--no_sort', default=False, dest='no_sort', action='store_true',
86                          help='Do not sort images by hashed image ID.')
87      parser.add_argument('--seed', default=None, type=int,
88                          help='The seed to pass into random.seed. Note: this is only really for the shuffle and does not (I think) affect cuda stuff.')
89      parser.add_argument('--mask_proto_debug', default=False, dest='mask_proto_debug', action='store_true',
90                          help='Outputs stuff for scripts/compute_mask.py.')
91      parser.add_argument('--no_crop', default=False, dest='crop', action='store_false',
92                          help='Do not crop output masks with the predicted bounding box.')
93      parser.add_argument('--image', default=None, type=str,
94                          help='A path to an image to use for display.')
95      parser.add_argument('--images', default=None, type=str,
96                          help='An input folder of images and output folder to save detected images. Should be in the format input->output.')
97      parser.add_argument('--video', default=None, type=str,
98                          help='A path to a video to evaluate on. Passing in a number will use that index webcam.')
99      parser.add_argument('--video_multiframe', default=1, type=int,
100                          help='The number of frames to evaluate in parallel to make videos play at higher fps.')
101      parser.add_argument('--score_threshold', default=0, type=float,
102                          help='Detections with a score under this threshold will not be considered. This currently only works in display mode.')
103      parser.add_argument('--dataset', default=None, type=str,
104                          help='If specified, override the dataset specified in the config with this one (example: coco2017_dataset).')
105      parser.add_argument('--detect', default=False, dest='detect', action='store_true',
106                          help='Don\'t evauluate the mask branch at all and only do object detection. This only works for --display and --benchmark.')
107      parser.add_argument('--display_fps', default=False, dest='display_fps', action='store_true',
108                          help='When displaying / saving video, draw the FPS on the frame')
109      parser.add_argument('--emulate_playback', default=False, dest='emulate_playback', action='store_true',
110                          help='When saving a video, emulate the framerate that you\'d get running in real-time mode.')
111      parser.set_defaults(no_bar=False, display=False, resume=False, output_coco_json=False, output_web_json=False, shuffle=False,
112                          benchmark=False, no_sort=False, no_hash=False, mask_proto_debug=False, crop=True, detect=False, display_fps=False,
113                          emulate_playback=False)
114      global args
115      args = parser.parse_args(argv)
116      if args.output_web_json:
117          args.output_coco_json = True
118      if args.seed is not None:
119          random.seed(args.seed)
120  iou_thresholds = [x / 100 for x in range(50, 100, 5)]
121  coco_cats = {} # Call prep_coco_cats to fill this
122  coco_cats_inv = {}
123  color_cache = defaultdict(lambda: {})
124  def prep_display(dets_out, img, h, w, undo_transform=True, class_color=False, mask_alpha=0.45, fps_str=''):
125      if undo_transform:
126          img_numpy = undo_image_transformation(img, w, h)
127          img_gpu = torch.Tensor(img_numpy).cuda()
128      else:
129          img_gpu = img / 255.0
130          h, w, _ = img.shape
131      with timer.env('Postprocess'):
132          save = cfg.rescore_bbox
133          cfg.rescore_bbox = True
134          t = postprocess(dets_out, w, h, visualize_lincomb = args.display_lincomb,
135                                          crop_masks        = args.crop,
136                                          score_threshold   = args.score_threshold)
137          cfg.rescore_bbox = save
138      with timer.env('Copy'):
139          idx = t[1].argsort(0, descending=True)[:args.top_k]
140          if cfg.eval_mask_branch:
141              masks = t[3][idx]
142          classes, scores, boxes = [x[idx].cpu().numpy() for x in t[:3]]
143      num_dets_to_consider = min(args.top_k, classes.shape[0])
144      for j in range(num_dets_to_consider):
145          if scores[j] < args.score_threshold:
146              num_dets_to_consider = j
147              break
148      def get_color(j, on_gpu=None):
149          global color_cache
150          color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS)
151          if on_gpu is not None and color_idx in color_cache[on_gpu]:
152              return color_cache[on_gpu][color_idx]
153          else:
154              color = COLORS[color_idx]
155              if not undo_transform:
156                  color = (color[2], color[1], color[0])
157              if on_gpu is not None:
158                  color = torch.Tensor(color).to(on_gpu).float() / 255.
159                  color_cache[on_gpu][color_idx] = color
160              return color
161      if args.display_masks and cfg.eval_mask_branch and num_dets_to_consider > 0:
162          masks = masks[:num_dets_to_consider, :, :, None]
163          colors = torch.cat([get_color(j, on_gpu=img_gpu.device.index).view(1, 1, 1, 3) for j in range(num_dets_to_consider)], dim=0)
164          masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha
165          inv_alph_masks = masks * (-mask_alpha) + 1
166          masks_color_summand = masks_color[0]
167          if num_dets_to_consider > 1:
168              inv_alph_cumul = inv_alph_masks[:(num_dets_to_consider-1)].cumprod(dim=0)
169              masks_color_cumul = masks_color[1:] * inv_alph_cumul
170              masks_color_summand += masks_color_cumul.sum(dim=0)
171          img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand
172      if args.display_fps:
173          font_face = cv2.FONT_HERSHEY_DUPLEX
174          font_scale = 0.6
175          font_thickness = 1
176          text_w, text_h = cv2.getTextSize(fps_str, font_face, font_scale, font_thickness)[0]
177          img_gpu[0:text_h+8, 0:text_w+8] *= 0.6 # 1 - Box alpha
178      img_numpy = (img_gpu * 255).byte().cpu().numpy()
179      if args.display_fps:
180          text_pt = (4, text_h + 2)
181          text_color = [255, 255, 255]
182          cv2.putText(img_numpy, fps_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)
183      if num_dets_to_consider == 0:
184          return img_numpy
185      if args.display_text or args.display_bboxes:
186          for j in reversed(range(num_dets_to_consider)):
187              x1, y1, x2, y2 = boxes[j, :]
188              color = get_color(j)
189              score = scores[j]
190              if args.display_bboxes:
191                  cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1)
192              if args.display_text:
193                  _class = cfg.dataset.class_names[classes[j]]
194                  text_str = '%s: %.2f' % (_class, score) if args.display_scores else _class
195                  font_face = cv2.FONT_HERSHEY_DUPLEX
196                  font_scale = 0.6
197                  font_thickness = 1
198                  text_w, text_h = cv2.getTextSize(text_str, font_face, font_scale, font_thickness)[0]
199                  text_pt = (x1, y1 - 3)
200                  text_color = [255, 255, 255]
201                  cv2.rectangle(img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1)
202                  cv2.putText(img_numpy, text_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)
203      return img_numpy
204  def prep_benchmark(dets_out, h, w):
205      with timer.env('Postprocess'):
206          t = postprocess(dets_out, w, h, crop_masks=args.crop, score_threshold=args.score_threshold)
207      with timer.env('Copy'):
208          classes, scores, boxes, masks = [x[:args.top_k] for x in t]
209          if isinstance(scores, list):
210              box_scores = scores[0].cpu().numpy()
211              mask_scores = scores[1].cpu().numpy()
212          else:
213              scores = scores.cpu().numpy()
214          classes = classes.cpu().numpy()
215          boxes = boxes.cpu().numpy()
216          masks = masks.cpu().numpy()
217      with timer.env('Sync'):
218          torch.cuda.synchronize()
219  def prep_coco_cats():
220      for coco_cat_id, transformed_cat_id_p1 in get_label_map().items():
221          transformed_cat_id = transformed_cat_id_p1 - 1
222          coco_cats[transformed_cat_id] = coco_cat_id
223          coco_cats_inv[coco_cat_id] = transformed_cat_id
224  def get_coco_cat(transformed_cat_id):
225      return coco_cats[transformed_cat_id]
226  def get_transformed_cat(coco_cat_id):
227      return coco_cats_inv[coco_cat_id]
228  class Detections:
229      def __init__(self):
230          self.bbox_data = []
231          self.mask_data = []
232      def add_bbox(self, image_id:int, category_id:int, bbox:list, score:float):
233          bbox = [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]]
234          bbox = [round(float(x)*10)/10 for x in bbox]
235          self.bbox_data.append({
236              'image_id': int(image_id),
237              'category_id': get_coco_cat(int(category_id)),
238              'bbox': bbox,
239              'score': float(score)
240          })
241      def add_mask(self, image_id:int, category_id:int, segmentation:np.ndarray, score:float):
242          rle = pycocotools.mask.encode(np.asfortranarray(segmentation.astype(np.uint8)))
243          rle['counts'] = rle['counts'].decode('ascii') # json.dump doesn't like bytes strings
244          self.mask_data.append({
245              'image_id': int(image_id),
246              'category_id': get_coco_cat(int(category_id)),
247              'segmentation': rle,
248              'score': float(score)
249          })
250      def dump(self):
251          dump_arguments = [
252              (self.bbox_data, args.bbox_det_file),
253              (self.mask_data, args.mask_det_file)
254          ]
255          for data, path in dump_arguments:
256              with open(path, 'w') as f:
257                  json.dump(data, f)
258      def dump_web(self):
259          config_outs = ['preserve_aspect_ratio', 'use_prediction_module',
260                          'use_yolo_regressors', 'use_prediction_matching',
261                          'train_masks']
262          output = {
263              'info' : {
264                  'Config': {key: getattr(cfg, key) for key in config_outs},
265              }
266          }
267          image_ids = list(set([x['image_id'] for x in self.bbox_data]))
268          image_ids.sort()
269          image_lookup = {_id: idx for idx, _id in enumerate(image_ids)}
270          output['images'] = [{'image_id': image_id, 'dets': []} for image_id in image_ids]
271          for bbox, mask in zip(self.bbox_data, self.mask_data):
272              image_obj = output['images'][image_lookup[bbox['image_id']]]
273              image_obj['dets'].append({
274                  'score': bbox['score'],
275                  'bbox': bbox['bbox'],
276                  'category': cfg.dataset.class_names[get_transformed_cat(bbox['category_id'])],
277                  'mask': mask['segmentation'],
278              })
279          with open(os.path.join(args.web_det_path, '%s.json' % cfg.name), 'w') as f:
280              json.dump(output, f)
281  def _mask_iou(mask1, mask2, iscrowd=False):
282      with timer.env('Mask IoU'):
283          ret = mask_iou(mask1, mask2, iscrowd)
284      return ret.cpu()
285  def _bbox_iou(bbox1, bbox2, iscrowd=False):
286      with timer.env('BBox IoU'):
287          ret = jaccard(bbox1, bbox2, iscrowd)
288      return ret.cpu()
289  def prep_metrics(ap_data, dets, img, gt, gt_masks, h, w, num_crowd, image_id, detections:Detections=None):
290      if not args.output_coco_json:
291          with timer.env('Prepare gt'):
292              gt_boxes = torch.Tensor(gt[:, :4])
293              gt_boxes[:, [0, 2]] *= w
294              gt_boxes[:, [1, 3]] *= h
295              gt_classes = list(gt[:, 4].astype(int))
296              gt_masks = torch.Tensor(gt_masks).view(-1, h*w)
297              if num_crowd > 0:
298                  split = lambda x: (x[-num_crowd:], x[:-num_crowd])
299                  crowd_boxes  , gt_boxes   = split(gt_boxes)
300                  crowd_masks  , gt_masks   = split(gt_masks)
301                  crowd_classes, gt_classes = split(gt_classes)
302      with timer.env('Postprocess'):
303          classes, scores, boxes, masks = postprocess(dets, w, h, crop_masks=args.crop, score_threshold=args.score_threshold)
304          if classes.size(0) == 0:
305              return
306          classes = list(classes.cpu().numpy().astype(int))
307          if isinstance(scores, list):
308              box_scores = list(scores[0].cpu().numpy().astype(float))
309              mask_scores = list(scores[1].cpu().numpy().astype(float))
310          else:
311              scores = list(scores.cpu().numpy().astype(float))
312              box_scores = scores
313              mask_scores = scores
314          masks = masks.view(-1, h*w).cuda()
315          boxes = boxes.cuda()
316      if args.output_coco_json:
317          with timer.env('JSON Output'):
318              boxes = boxes.cpu().numpy()
319              masks = masks.view(-1, h, w).cpu().numpy()
320              for i in range(masks.shape[0]):
321                  if (boxes[i, 3] - boxes[i, 1]) * (boxes[i, 2] - boxes[i, 0]) > 0:
322                      detections.add_bbox(image_id, classes[i], boxes[i,:],   box_scores[i])
323                      detections.add_mask(image_id, classes[i], masks[i,:,:], mask_scores[i])
324              return
325      with timer.env('Eval Setup'):
326          num_pred = len(classes)
327          num_gt   = len(gt_classes)
328          mask_iou_cache = _mask_iou(masks, gt_masks)
329          bbox_iou_cache = _bbox_iou(boxes.float(), gt_boxes.float())
330          if num_crowd > 0:
331              crowd_mask_iou_cache = _mask_iou(masks, crowd_masks, iscrowd=True)
332              crowd_bbox_iou_cache = _bbox_iou(boxes.float(), crowd_boxes.float(), iscrowd=True)
333          else:
334              crowd_mask_iou_cache = None
335              crowd_bbox_iou_cache = None
336          box_indices = sorted(range(num_pred), key=lambda i: -box_scores[i])
337          mask_indices = sorted(box_indices, key=lambda i: -mask_scores[i])
338          iou_types = [
339              ('box',  lambda i,j: bbox_iou_cache[i, j].item(),
340                       lambda i,j: crowd_bbox_iou_cache[i,j].item(),
341                       lambda i: box_scores[i], box_indices),
342              ('mask', lambda i,j: mask_iou_cache[i, j].item(),
343                       lambda i,j: crowd_mask_iou_cache[i,j].item(),
344                       lambda i: mask_scores[i], mask_indices)
345          ]
346      timer.start('Main loop')
347      for _class in set(classes + gt_classes):
348          ap_per_iou = []
349          num_gt_for_class = sum([1 for x in gt_classes if x == _class])
350          for iouIdx in range(len(iou_thresholds)):
351              iou_threshold = iou_thresholds[iouIdx]
352              for iou_type, iou_func, crowd_func, score_func, indices in iou_types:
353                  gt_used = [False] * len(gt_classes)
354                  ap_obj = ap_data[iou_type][iouIdx][_class]
355                  ap_obj.add_gt_positives(num_gt_for_class)
356                  for i in indices:
357                      if classes[i] != _class:
358                          continue
359                      max_iou_found = iou_threshold
360                      max_match_idx = -1
361                      for j in range(num_gt):
362                          if gt_used[j] or gt_classes[j] != _class:
363                              continue
364                          iou = iou_func(i, j)
365                          if iou > max_iou_found:
366                              max_iou_found = iou
367                              max_match_idx = j
368                      if max_match_idx >= 0:
369                          gt_used[max_match_idx] = True
370                          ap_obj.push(score_func(i), True)
371                      else:
372                          matched_crowd = False
373                          if num_crowd > 0:
374                              for j in range(len(crowd_classes)):
375                                  if crowd_classes[j] != _class:
376                                      continue
377                                  iou = crowd_func(i, j)
378                                  if iou > iou_threshold:
379                                      matched_crowd = True
380                                      break
381                          if not matched_crowd:
382                              ap_obj.push(score_func(i), False)
383      timer.stop('Main loop')
384  class APDataObject:
385      def __init__(self):
386          self.data_points = []
387          self.num_gt_positives = 0
388      def push(self, score:float, is_true:bool):
389          self.data_points.append((score, is_true))
390      def add_gt_positives(self, num_positives:int):
391          self.num_gt_positives += num_positives
392      def is_empty(self) -> bool:
393          return len(self.data_points) == 0 and self.num_gt_positives == 0
394      def get_ap(self) -> float:
395          if self.num_gt_positives == 0:
396              return 0
397          self.data_points.sort(key=lambda x: -x[0])
398          precisions = []
399          recalls    = []
400          num_true  = 0
401          num_false = 0
402          for datum in self.data_points:
403              if datum[1]: num_true += 1
404              else: num_false += 1
405              precision = num_true / (num_true + num_false)
406              recall    = num_true / self.num_gt_positives
407              precisions.append(precision)
408              recalls.append(recall)
409          for i in range(len(precisions)-1, 0, -1):
410              if precisions[i] > precisions[i-1]:
411                  precisions[i-1] = precisions[i]
412          y_range = [0] * 101 # idx 0 is recall == 0.0 and idx 100 is recall == 1.00
413          x_range = np.array([x / 100 for x in range(101)])
414          recalls = np.array(recalls)
415          indices = np.searchsorted(recalls, x_range, side='left')
416          for bar_idx, precision_idx in enumerate(indices):
417              if precision_idx < len(precisions):
418                  y_range[bar_idx] = precisions[precision_idx]
419          return sum(y_range) / len(y_range)
420  def badhash(x):
421      x = (((x >> 16) ^ x) * 0x045d9f3b) & 0xFFFFFFFF
422      x = (((x >> 16) ^ x) * 0x045d9f3b) & 0xFFFFFFFF
423      x =  ((x >> 16) ^ x) & 0xFFFFFFFF
424      return x
425  def evalimage(net:Yolact, path:str, save_path:str=None):
426      frame = torch.from_numpy(cv2.imread(path)).cuda().float()
427      batch = FastBaseTransform()(frame.unsqueeze(0))
428      preds = net(batch)
429      img_numpy = prep_display(preds, frame, None, None, undo_transform=False)
430      if save_path is None:
431          img_numpy = img_numpy[:, :, (2, 1, 0)]
432      if save_path is None:
433          plt.imshow(img_numpy)
434          plt.title(path)
435          plt.show()
436      else:
437          cv2.imwrite(save_path, img_numpy)
438  def evalimages(net:Yolact, input_folder:str, output_folder:str):
439      if not os.path.exists(output_folder):
440          os.mkdir(output_folder)
441      print()
442      for p in Path(input_folder).glob('*'): 
443          path = str(p)
444          name = os.path.basename(path)
445          name = '.'.join(name.split('.')[:-1]) + '.png'
446          out_path = os.path.join(output_folder, name)
447          evalimage(net, path, out_path)
448          print(path + ' -> ' + out_path)
449      print('Done.')
450  from multiprocessing.pool import ThreadPool
451  from queue import Queue
452  class CustomDataParallel(torch.nn.DataParallel):
453      def gather(self, outputs, output_device):
454          return sum(outputs, [])
455  def evalvideo(net:Yolact, path:str, out_path:str=None):
456      is_webcam = path.isdigit()
457      cudnn.benchmark = True
458      if is_webcam:
459          vid = cv2.VideoCapture(int(path))
460      else:
461          vid = cv2.VideoCapture(path)
462      if not vid.isOpened():
463          print('Could not open video "%s"' % path)
464          exit(-1)
465      target_fps   = round(vid.get(cv2.CAP_PROP_FPS))
466      frame_width  = round(vid.get(cv2.CAP_PROP_FRAME_WIDTH))
467      frame_height = round(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
468      if is_webcam:
469          num_frames = float('inf')
470      else:
471          num_frames = round(vid.get(cv2.CAP_PROP_FRAME_COUNT))
472      net = CustomDataParallel(net).cuda()
473      transform = torch.nn.DataParallel(FastBaseTransform()).cuda()
474      frame_times = MovingAverage(100)
475      fps = 0
476      frame_time_target = 1 / target_fps
477      running = True
478      fps_str = ''
479      vid_done = False
480      frames_displayed = 0
481      if out_path is not None:
482          out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*"mp4v"), target_fps, (frame_width, frame_height))
483      def cleanup_and_exit():
484          print()
485          pool.terminate()
486          vid.release()
487          if out_path is not None:
488              out.release()
489          cv2.destroyAllWindows()
490          exit()
491      def get_next_frame(vid):
492          frames = []
493          for idx in range(args.video_multiframe):
494              frame = vid.read()[1]
495              if frame is None:
496                  return frames
497              frames.append(frame)
498          return frames
499      def transform_frame(frames):
500          with torch.no_grad():
501              frames = [torch.from_numpy(frame).cuda().float() for frame in frames]
502              return frames, transform(torch.stack(frames, 0))
503      def eval_network(inp):
504          with torch.no_grad():
505              frames, imgs = inp
506              num_extra = 0
507              while imgs.size(0) < args.video_multiframe:
508                  imgs = torch.cat([imgs, imgs[0].unsqueeze(0)], dim=0)
509                  num_extra += 1
510              out = net(imgs)
511              if num_extra > 0:
512                  out = out[:-num_extra]
513              return frames, out
514      def prep_frame(inp, fps_str):
515          with torch.no_grad():
516              frame, preds = inp
517              return prep_display(preds, frame, None, None, undo_transform=False, class_color=True, fps_str=fps_str)
518      frame_buffer = Queue()
519      video_fps = 0
520      def play_video():
521          try:
522              nonlocal frame_buffer, running, video_fps, is_webcam, num_frames, frames_displayed, vid_done
523              video_frame_times = MovingAverage(100)
524              frame_time_stabilizer = frame_time_target
525              last_time = None
526              stabilizer_step = 0.0005
527              progress_bar = ProgressBar(30, num_frames)
528              while running:
529                  frame_time_start = time.time()
530                  if not frame_buffer.empty():
531                      next_time = time.time()
532                      if last_time is not None:
533                          video_frame_times.add(next_time - last_time)
534                          video_fps = 1 / video_frame_times.get_avg()
535                      if out_path is None:
536                          cv2.imshow(path, frame_buffer.get())
537                      else:
538                          out.write(frame_buffer.get())
539                      frames_displayed += 1
540                      last_time = next_time
541                      if out_path is not None:
542                          if video_frame_times.get_avg() == 0:
543                              fps = 0
544                          else:
545                              fps = 1 / video_frame_times.get_avg()
546                          progress = frames_displayed / num_frames * 100
547                          progress_bar.set_val(frames_displayed)
548                          print('\rProcessing Frames  %s %6d / %6d (%5.2f%%)    %5.2f fps        '
549                              % (repr(progress_bar), frames_displayed, num_frames, progress, fps), end='')
550                  if out_path is None and cv2.waitKey(1) == 27:
551                      running = False
552                  if not (frames_displayed < num_frames):
553                      running = False
554                  if not vid_done:
555                      buffer_size = frame_buffer.qsize()
556                      if buffer_size < args.video_multiframe:
557                          frame_time_stabilizer += stabilizer_step
558                      elif buffer_size > args.video_multiframe:
559                          frame_time_stabilizer -= stabilizer_step
560                          if frame_time_stabilizer < 0:
561                              frame_time_stabilizer = 0
562                      new_target = frame_time_stabilizer if is_webcam else max(frame_time_stabilizer, frame_time_target)
563                  else:
564                      new_target = frame_time_target
565                  next_frame_target = max(2 * new_target - video_frame_times.get_avg(), 0)
566                  target_time = frame_time_start + next_frame_target - 0.001 # Let's just subtract a millisecond to be safe
567                  if out_path is None or args.emulate_playback:
568                      while time.time() < target_time:
569                          time.sleep(0.001)
570                  else:
571                      time.sleep(0.001)
572          except:
573              import traceback
574              traceback.print_exc()
575      extract_frame = lambda x, i: (x[0][i] if x[1][i]['detection'] is None else x[0][i].to(x[1][i]['detection']['box'].device), [x[1][i]])
576      print('Initializing model... ', end='')
577      first_batch = eval_network(transform_frame(get_next_frame(vid)))
578      print('Done.')
579      sequence = [prep_frame, eval_network, transform_frame]
580      pool = ThreadPool(processes=len(sequence) + args.video_multiframe + 2)
581      pool.apply_async(play_video)
582      active_frames = [{'value': extract_frame(first_batch, i), 'idx': 0} for i in range(len(first_batch[0]))]
583      print()
584      if out_path is None: print('Press Escape to close.')
585      try:
586          while vid.isOpened() and running:
587              while frame_buffer.qsize() > 100:
588                  time.sleep(0.001)
589              start_time = time.time()
590              if not vid_done:
591                  next_frames = pool.apply_async(get_next_frame, args=(vid,))
592              else:
593                  next_frames = None
594              if not (vid_done and len(active_frames) == 0):
595                  for frame in active_frames:
596                      _args =  [frame['value']]
597                      if frame['idx'] == 0:
598                          _args.append(fps_str)
599                      frame['value'] = pool.apply_async(sequence[frame['idx']], args=_args)
600                  for frame in active_frames:
601                      if frame['idx'] == 0:
602                          frame_buffer.put(frame['value'].get())
603                  active_frames = [x for x in active_frames if x['idx'] > 0]
604                  for frame in list(reversed(active_frames)):
605                      frame['value'] = frame['value'].get()
606                      frame['idx'] -= 1
607                      if frame['idx'] == 0:
608                          active_frames += [{'value': extract_frame(frame['value'], i), 'idx': 0} for i in range(1, len(frame['value'][0]))]
609                          frame['value'] = extract_frame(frame['value'], 0)
610                  if next_frames is not None:
611                      frames = next_frames.get()
612                      if len(frames) == 0:
613                          vid_done = True
614                      else:
615                          active_frames.append({'value': frames, 'idx': len(sequence)-1})
616                  frame_times.add(time.time() - start_time)
617                  fps = args.video_multiframe / frame_times.get_avg()
618              else:
619                  fps = 0
620              fps_str = 'Processing FPS: %.2f | Video Playback FPS: %.2f | Frames in Buffer: %d' % (fps, video_fps, frame_buffer.qsize())
621              if not args.display_fps:
622                  print('\r' + fps_str + '    ', end='')
623      except KeyboardInterrupt:
624          print('\nStopping...')
625      cleanup_and_exit()
626  def evaluate(net:Yolact, dataset, train_mode=False):
627      net.detect.use_fast_nms = args.fast_nms
628      net.detect.use_cross_class_nms = args.cross_class_nms
629      cfg.mask_proto_debug = args.mask_proto_debug
630      if args.image is not None:
631          if ':' in args.image:
632              inp, out = args.image.split(':')
633              evalimage(net, inp, out)
634          else:
635              evalimage(net, args.image)
636          return
637      elif args.images is not None:
638          inp, out = args.images.split(':')
639          evalimages(net, inp, out)
640          return
641      elif args.video is not None:
642          if ':' in args.video:
643              inp, out = args.video.split(':')
644              evalvideo(net, inp, out)
645          else:
646              evalvideo(net, args.video)
647          return
648      frame_times = MovingAverage()
649      dataset_size = len(dataset) if args.max_images < 0 else min(args.max_images, len(dataset))
650      progress_bar = ProgressBar(30, dataset_size)
651      print()
652      if not args.display and not args.benchmark:
653          ap_data = {
654              'box' : [[APDataObject() for _ in cfg.dataset.class_names] for _ in iou_thresholds],
655              'mask': [[APDataObject() for _ in cfg.dataset.class_names] for _ in iou_thresholds]
656          }
657          detections = Detections()
658      else:
659          timer.disable('Load Data')
660      dataset_indices = list(range(len(dataset)))
661      if args.shuffle:
662          random.shuffle(dataset_indices)
663      elif not args.no_sort:
664          hashed = [badhash(x) for x in dataset.ids]
665          dataset_indices.sort(key=lambda x: hashed[x])
666      dataset_indices = dataset_indices[:dataset_size]
667      try:
668          for it, image_idx in enumerate(dataset_indices):
669              timer.reset()
670              with timer.env('Load Data'):
671                  img, gt, gt_masks, h, w, num_crowd = dataset.pull_item(image_idx)
672                  if cfg.mask_proto_debug:
673                      with open('scripts/info.txt', 'w') as f:
674                          f.write(str(dataset.ids[image_idx]))
675                      np.save('scripts/gt.npy', gt_masks)
676                  batch = Variable(img.unsqueeze(0))
677                  if args.cuda:
678                      batch = batch.cuda()
679              with timer.env('Network Extra'):
680                  preds = net(batch)
681              if args.display:
682                  img_numpy = prep_display(preds, img, h, w)
683              elif args.benchmark:
684                  prep_benchmark(preds, h, w)
685              else:
686                  prep_metrics(ap_data, preds, img, gt, gt_masks, h, w, num_crowd, dataset.ids[image_idx], detections)
687              if it > 1:
688                  frame_times.add(timer.total_time())
689              if args.display:
690                  if it > 1:
691                      print('Avg FPS: %.4f' % (1 / frame_times.get_avg()))
692                  plt.imshow(img_numpy)
693                  plt.title(str(dataset.ids[image_idx]))
694                  plt.show()
695              elif not args.no_bar:
696                  if it > 1: fps = 1 / frame_times.get_avg()
697                  else: fps = 0
698                  progress = (it+1) / dataset_size * 100
699                  progress_bar.set_val(it+1)
700                  print('\rProcessing Images  %s %6d / %6d (%5.2f%%)    %5.2f fps        '
701                      % (repr(progress_bar), it+1, dataset_size, progress, fps), end='')
702          if not args.display and not args.benchmark:
703              print()
704              if args.output_coco_json:
705                  print('Dumping detections...')
706                  if args.output_web_json:
707                      detections.dump_web()
708                  else:
709                      detections.dump()
710              else:
711                  if not train_mode:
712                      print('Saving data...')
713                      with open(args.ap_data_file, 'wb') as f:
714                          pickle.dump(ap_data, f)
715                  return calc_map(ap_data)
716          elif args.benchmark:
717              print()
718              print()
719              print('Stats for the last frame:')
720              timer.print_stats()
721              avg_seconds = frame_times.get_avg()
722              print('Average: %5.2f fps, %5.2f ms' % (1 / frame_times.get_avg(), 1000*avg_seconds))
723      except KeyboardInterrupt:
724          print('Stopping...')
725  def calc_map(ap_data):
726      print('Calculating mAP...')
727      aps = [{'box': [], 'mask': []} for _ in iou_thresholds]
728      for _class in range(len(cfg.dataset.class_names)):
729          for iou_idx in range(len(iou_thresholds)):
730              for iou_type in ('box', 'mask'):
731                  ap_obj = ap_data[iou_type][iou_idx][_class]
732                  if not ap_obj.is_empty():
733                      aps[iou_idx][iou_type].append(ap_obj.get_ap())
734      all_maps = {'box': OrderedDict(), 'mask': OrderedDict()}
735      for iou_type in ('box', 'mask'):
736          all_maps[iou_type]['all'] = 0 # Make this first in the ordereddict
737          for i, threshold in enumerate(iou_thresholds):
738              mAP = sum(aps[i][iou_type]) / len(aps[i][iou_type]) * 100 if len(aps[i][iou_type]) > 0 else 0
739              all_maps[iou_type][int(threshold*100)] = mAP
740          all_maps[iou_type]['all'] = (sum(all_maps[iou_type].values()) / (len(all_maps[iou_type].values())-1))
741      print_maps(all_maps)
742      all_maps = {k: {j: round(u, 2) for j, u in v.items()} for k, v in all_maps.items()}
743      return all_maps
744  def print_maps(all_maps):
745      make_row = lambda vals: (' %5s |' * len(vals)) % tuple(vals)
746      make_sep = lambda n:  ('-------+' * n)
747      print()
748      print(make_row([''] + [('.%d ' % x if isinstance(x, int) else x + ' ') for x in all_maps['box'].keys()]))
749      print(make_sep(len(all_maps['box']) + 1))
750      for iou_type in ('box', 'mask'):
751          print(make_row([iou_type] + ['%.2f' % x if x < 100 else '%.1f' % x for x in all_maps[iou_type].values()]))
752      print(make_sep(len(all_maps['box']) + 1))
753      print()
754  if __name__ == '__main__':
755      parse_args()
756      if args.config is not None:
757          set_cfg(args.config)
758      if args.trained_model == 'interrupt':
759          args.trained_model = SavePath.get_interrupt('weights/')
760      elif args.trained_model == 'latest':
761          args.trained_model = SavePath.get_latest('weights/', cfg.name)
762      if args.config is None:
<span onclick='openModal()' class='match'>763          model_path = SavePath.from_str(args.trained_model)
764          args.config = model_path.model_name + '_config'
765          print('Config not specified. Parsed %s from the file name.\n' % args.config)
766          set_cfg(args.config)
</span>767      if args.detect:
768          cfg.eval_mask_branch = False
769      if args.dataset is not None:
770          set_dataset(args.dataset)
771      with torch.no_grad():
772          if not os.path.exists('results'):
773              os.makedirs('results')
774          if args.cuda:
775              cudnn.fastest = True
776              torch.set_default_tensor_type('torch.cuda.FloatTensor')
777          else:
778              torch.set_default_tensor_type('torch.FloatTensor')
779          if args.resume and not args.display:
780              with open(args.ap_data_file, 'rb') as f:
781                  ap_data = pickle.load(f)
782              calc_map(ap_data)
783              exit()
784          if args.image is None and args.video is None and args.images is None:
785              dataset = COCODetection(cfg.dataset.valid_images, cfg.dataset.valid_info,
786                                      transform=BaseTransform(), has_gt=cfg.dataset.has_gt)
787              prep_coco_cats()
788          else:
789              dataset = None        
790          print('Loading model...', end='')
791          net = Yolact()
792          net.load_weights(args.trained_model)
793          net.eval()
794          print(' Done.')
795          if args.cuda:
796              net = net.cuda()
797          evaluate(net, dataset)
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-backbone.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-eval.py</div>
                </div>
                <div class="column column_space"><pre><code>220                  cur_layer_idx = self.total_layer_count + len(layers)
221                  self.state_dict_lookup[cur_layer_idx] = '%d.%d' % (len(self.layers), len(layers))
</pre></code></div>
                <div class="column column_space"><pre><code>763          model_path = SavePath.from_str(args.trained_model)
764          args.config = model_path.model_name + '_config'
765          print('Config not specified. Parsed %s from the file name.\n' % args.config)
766          set_cfg(args.config)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    