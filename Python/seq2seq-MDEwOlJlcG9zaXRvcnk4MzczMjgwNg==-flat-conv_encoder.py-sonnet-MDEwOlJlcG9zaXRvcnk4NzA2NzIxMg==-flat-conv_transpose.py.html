
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 6.36042402826855%, Tokens: 9</h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-conv_encoder.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from pydoc import locate
5  import tensorflow as tf
6  from seq2seq.encoders.encoder import Encoder, EncoderOutput
7  from seq2seq.encoders.pooling_encoder import _create_position_embedding
8  class ConvEncoder(Encoder):
9    def __init__(self, params, mode, name="conv_encoder"):
10      super(ConvEncoder, self).__init__(params, mode, name)
11      self._combiner_fn = locate(self.params["position_embeddings.combiner_fn"])
12    @staticmethod
13    def default_params():
14      return {
15          "attention_cnn.units": 512,
16          "attention_cnn.kernel_size": 3,
17          "attention_cnn.layers": 15,
18          "embedding_dropout_keep_prob": 0.8,
19          "output_cnn.units": 256,
20          "output_cnn.kernel_size": 3,
21          "output_cnn.layers": 5,
22          "position_embeddings.enable": True,
23          "position_embeddings.combiner_fn": "tensorflow.multiply",
24          "position_embeddings.num_positions": 100,
25      }
26    def encode(self, inputs, sequence_length):
27      if self.params["position_embeddings.enable"]:
28        positions_embed = _create_position_embedding(
29            embedding_dim=inputs.get_shape().as_list()[-1],
30            num_positions=self.params["position_embeddings.num_positions"],
31            lengths=sequence_length,
32            maxlen=tf.shape(inputs)[1])
33        inputs = self._combiner_fn(inputs, positions_embed)
34      inputs = tf.contrib.layers.dropout(
35          inputs=inputs,
36          keep_prob=self.params["embedding_dropout_keep_prob"],
37          is_training=self.mode == tf.contrib.learn.ModeKeys.TRAIN)
38      with tf.variable_scope("cnn_a"):
39        cnn_a_output = inputs
40        for layer_idx in range(self.params["attention_cnn.layers"]):
41          next_layer = tf.contrib.layers.conv2d(
<span onclick='openModal()' class='match'>42              inputs=cnn_a_output,
43              num_outputs=self.params["attention_cnn.units"],
44              kernel_size=self.params["attention_cnn.kernel_size"],
45              padding="SAME",
46              activation_fn=None)
</span>47          if layer_idx > 0:
48            next_layer += cnn_a_output
49          cnn_a_output = tf.tanh(next_layer)
50      with tf.variable_scope("cnn_c"):
51        cnn_c_output = inputs
52        for layer_idx in range(self.params["output_cnn.layers"]):
53          next_layer = tf.contrib.layers.conv2d(
54              inputs=cnn_c_output,
55              num_outputs=self.params["output_cnn.units"],
56              kernel_size=self.params["output_cnn.kernel_size"],
57              padding="SAME",
58              activation_fn=None)
59          if layer_idx > 0:
60            next_layer += cnn_c_output
61          cnn_c_output = tf.tanh(next_layer)
62      final_state = tf.reduce_mean(cnn_c_output, 1)
63      return EncoderOutput(
64          outputs=cnn_a_output,
65          final_state=final_state,
66          attention_values=cnn_c_output,
67          attention_values_length=sequence_length)
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-conv_transpose.py</h3>
            <pre><code>1  from typing import Optional, Sequence, Union
2  import numpy as np
3  from sonnet.src import base
4  from sonnet.src import initializers
5  from sonnet.src import once
6  from sonnet.src import types
7  from sonnet.src import utils
8  import tensorflow as tf
9  def smart_concat(v1, v2):
10    if isinstance(v1, tf.Tensor) or isinstance(v2, tf.Tensor):
11      return tf.concat([v1, v2], 0)
12    else:
13      return v1 + v2
14  def smart_lambda(func, v1, v2):
15    if isinstance(v1, tf.Tensor) or isinstance(v2, tf.Tensor):
16      return func(v1, v2)
17    else:
18      return [func(x, y) for (x, y) in zip(v1, v2)]
19  class ConvNDTranspose(base.Module):
20    def __init__(self,
21                 num_spatial_dims: int,
22                 output_channels: int,
23                 kernel_shape: Union[int, Sequence[int]],
24                 output_shape: Optional[types.ShapeLike] = None,
25                 stride: Union[int, Sequence[int]] = 1,
26                 rate: Union[int, Sequence[int]] = 1,
27                 padding: str = "SAME",
28                 with_bias: bool = True,
29                 w_init: Optional[initializers.Initializer] = None,
30                 b_init: Optional[initializers.Initializer] = None,
31                 data_format: Optional[str] = None,
32                 name: Optional[str] = None):
33      super().__init__(name=name)
34      if not 1 <= num_spatial_dims <= 3:
35        raise ValueError(
36            "We only support transpose convolution operations for "
37            "num_spatial_dims=1, 2 or 3, received num_spatial_dims={}.".format(
38                num_spatial_dims))
39      self._num_spatial_dims = num_spatial_dims
40      self._output_channels = output_channels
41      self._kernel_shape = kernel_shape
42      self._output_shape = output_shape
43      self._stride = stride
44      self._rate = rate
45      if padding == "SAME" or padding == "VALID":
46        self._padding = padding
47      else:
48        raise TypeError("ConvNDTranspose only takes string padding, please "
49                        "provide either `SAME` or `VALID`.")
50      self._data_format = data_format
51      self._channel_index = utils.get_channel_index(data_format)
52      self._with_bias = with_bias
53      self._w_init = w_init
54      if with_bias:
55        self._b_init = b_init if b_init is not None else initializers.Zeros()
56      elif b_init is not None:
57        raise ValueError("When not using a bias the b_init must be None.")
58    def __call__(self, inputs):
59      self._initialize(inputs)
60      if self._output_shape is None:
61        output_shape = self._get_output_shape(inputs)
62        if self._channel_index == 1:
63          output_shape = smart_concat([self._output_channels], output_shape)
64        else:
65          output_shape = smart_concat(output_shape, [self._output_channels])
66      else:
67        output_shape = self._output_shape
68      output_shape = smart_concat([tf.shape(inputs)[0]], output_shape)
69      outputs = tf.nn.conv_transpose(
70          input=inputs,
71          filters=self.w,
72          output_shape=output_shape,
73          strides=self._stride,
74          padding=self._padding,
75          data_format=self._data_format,
76          dilations=self._rate,
77          name=None)
78      if self._with_bias:
79        outputs = tf.nn.bias_add(outputs, self.b, data_format=self._data_format)
80      return outputs
81    @once.once
82    def _initialize(self, inputs):
83      utils.assert_rank(inputs, self._num_spatial_dims + 2)
84      self.input_channels = inputs.shape[self._channel_index]
85      if self.input_channels is None:
86        raise ValueError("The number of input channels must be known")
87      self._dtype = inputs.dtype
88      if self._output_shape is not None:
89        if len(self._output_shape) != self._num_spatial_dims:
90          raise ValueError(
91              "The output_shape must be of length {} but instead was {}.".format(
92                  self._num_spatial_dims, len(self._output_shape)))
93        if self._channel_index == 1:
94          self._output_shape = [self._output_channels] + list(self._output_shape)
95        else:
96          self._output_shape = list(self._output_shape) + [self._output_channels]
97      self.w = self._make_w()
98      if self._with_bias:
99        self.b = tf.Variable(
100            self._b_init((self._output_channels,), self._dtype), name="b")
101    def _make_w(self):
102      kernel_shape = utils.replicate(self._kernel_shape, self._num_spatial_dims,
103                                     "kernel_shape")
104      weight_shape = kernel_shape + (self._output_channels, self.input_channels)
105      if self._w_init is None:
106        fan_in_shape = kernel_shape + (self.input_channels,)
107        stddev = 1 / np.sqrt(np.prod(fan_in_shape))
108        self._w_init = initializers.TruncatedNormal(stddev=stddev)
109      return tf.Variable(self._w_init(weight_shape, self._dtype), name="w")
110    def _get_output_shape(self, inputs):
111      input_shape = inputs.shape if inputs.shape.is_fully_defined() else tf.shape(
112          inputs)
113      if self._channel_index == 1:
114        input_size = input_shape[2:]
115      else:
116        input_size = input_shape[1:-1]
117      stride = utils.replicate(self._stride, self._num_spatial_dims, "stride")
118      output_shape = smart_lambda(lambda x, y: x * y, input_size, stride)
119      if self._padding == "VALID":
120        kernel_shape = utils.replicate(self._kernel_shape, self._num_spatial_dims,
121                                       "kernel_shape")
122        rate = utils.replicate(self._rate, self._num_spatial_dims, "rate")
123        effective_kernel_shape = [
124            (shape - 1) * rate + 1 for (shape, rate) in zip(kernel_shape, rate)
125        ]
126        output_shape = smart_lambda(lambda x, y: x + y - 1, output_shape,
127                                    effective_kernel_shape)
128      return output_shape
129  class Conv1DTranspose(ConvNDTranspose):
130    def __init__(self,
131                 output_channels: int,
132                 kernel_shape: Union[int, Sequence[int]],
133                 output_shape: Optional[types.ShapeLike] = None,
134                 stride: Union[int, Sequence[int]] = 1,
135                 rate: Union[int, Sequence[int]] = 1,
<span onclick='openModal()' class='match'>136                 padding: str = "SAME",
137                 with_bias: bool = True,
138                 w_init: Optional[initializers.Initializer] = None,
139                 b_init: Optional[initializers.Initializer] = None,
140                 data_format: str = "NWC",
</span>141                 name: Optional[str] = None):
142      super().__init__(
143          num_spatial_dims=1,
144          output_channels=output_channels,
145          kernel_shape=kernel_shape,
146          output_shape=output_shape,
147          stride=stride,
148          rate=rate,
149          padding=padding,
150          with_bias=with_bias,
151          w_init=w_init,
152          b_init=b_init,
153          data_format=data_format,
154          name=name)
155  class Conv2DTranspose(ConvNDTranspose):
156    def __init__(self,
157                 output_channels: int,
158                 kernel_shape: Union[int, Sequence[int]],
159                 output_shape: Optional[types.ShapeLike] = None,
160                 stride: Union[int, Sequence[int]] = 1,
161                 rate: Union[int, Sequence[int]] = 1,
162                 padding: str = "SAME",
163                 with_bias: bool = True,
164                 w_init: Optional[initializers.Initializer] = None,
165                 b_init: Optional[initializers.Initializer] = None,
166                 data_format: str = "NHWC",
167                 name: Optional[str] = None):
168      super().__init__(
169          num_spatial_dims=2,
170          output_channels=output_channels,
171          kernel_shape=kernel_shape,
172          output_shape=output_shape,
173          stride=stride,
174          rate=rate,
175          padding=padding,
176          with_bias=with_bias,
177          w_init=w_init,
178          b_init=b_init,
179          data_format=data_format,
180          name=name)
181  class Conv3DTranspose(ConvNDTranspose):
182    def __init__(self,
183                 output_channels: int,
184                 kernel_shape: Union[int, Sequence[int]],
185                 output_shape: Optional[types.ShapeLike] = None,
186                 stride: Union[int, Sequence[int]] = 1,
187                 rate: Union[int, Sequence[int]] = 1,
188                 padding: str = "SAME",
189                 with_bias: bool = True,
190                 w_init: Optional[initializers.Initializer] = None,
191                 b_init: Optional[initializers.Initializer] = None,
192                 data_format: str = "NDHWC",
193                 name: Optional[str] = None):
194      super().__init__(
195          num_spatial_dims=3,
196          output_channels=output_channels,
197          kernel_shape=kernel_shape,
198          output_shape=output_shape,
199          stride=stride,
200          rate=rate,
201          padding=padding,
202          with_bias=with_bias,
203          w_init=w_init,
204          b_init=b_init,
205          data_format=data_format,
206          name=name)
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-conv_encoder.py</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-conv_transpose.py</div>
                <div class="column column_space"><pre><code>42              inputs=cnn_a_output,
43              num_outputs=self.params["attention_cnn.units"],
44              kernel_size=self.params["attention_cnn.kernel_size"],
45              padding="SAME",
46              activation_fn=None)
</pre></code></div>
                <div class="column column_space"><pre><code>136                 padding: str = "SAME",
137                 with_bias: bool = True,
138                 w_init: Optional[initializers.Initializer] = None,
139                 b_init: Optional[initializers.Initializer] = None,
140                 data_format: str = "NWC",
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    