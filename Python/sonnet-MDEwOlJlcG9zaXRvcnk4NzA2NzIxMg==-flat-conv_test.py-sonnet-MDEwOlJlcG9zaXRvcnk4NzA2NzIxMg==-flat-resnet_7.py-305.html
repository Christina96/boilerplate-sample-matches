
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 12, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-conv_test.py</h3>
            <pre><code>1  from absl.testing import parameterized
2  import numpy as np
3  from sonnet.src import conv
4  from sonnet.src import initializers
5  from sonnet.src import test_utils
6  import tensorflow as tf
7  def create_constant_initializers(w, b, with_bias):
8    if with_bias:
9      return {
10          &quot;w_init&quot;: initializers.Constant(w),
11          &quot;b_init&quot;: initializers.Constant(b)
12      }
13    else:
14      return {&quot;w_init&quot;: initializers.Constant(w)}
15  class ConvTest(test_utils.TestCase, parameterized.TestCase):
16    def testPaddingFunctionReached(self):
17      self.reached = False
18      def padding_func(*unused_args):
19        padding_func.called = True
20        return [0, 0]
21      conv1 = conv.ConvND(
22          num_spatial_dims=2,
23          output_channels=1,
24          kernel_shape=3,
25          stride=1,
26          padding=padding_func,
27          data_format=&quot;NHWC&quot;,
28          **create_constant_initializers(1.0, 1.0, True))
29      conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
30      self.assertEqual(conv1.conv_padding, &quot;VALID&quot;)
31      self.assertEqual(conv1.padding_func, padding_func)
32      self.assertTrue(getattr(padding_func, &quot;called&quot;, False))
33    @parameterized.parameters(0, 4)
34    def testIncorrectN(self, n):
35      with self.assertRaisesRegex(
36          ValueError,
37          &quot;We only support convoltion operations for num_spatial_dims=1, 2 or 3&quot;):
38        conv.ConvND(
<span onclick='openModal()' class='match'>39            num_spatial_dims=n,
40            output_channels=1,
41            kernel_shape=3,
42            data_format=&quot;NHWC&quot;)
</span>43    def testInitializerKeysInvalidWithoutBias(self):
44      with self.assertRaisesRegex(ValueError, &quot;b_init must be None&quot;):
45        conv.ConvND(
46            num_spatial_dims=2,
47            output_channels=1,
48            kernel_shape=3,
49            data_format=&quot;NHWC&quot;,
50            with_bias=False,
51            b_init=tf.zeros_initializer())
52    def testIncorrectRankInput(self):
53      c = conv.ConvND(
54          num_spatial_dims=2,
55          output_channels=1,
56          kernel_shape=3,
57          data_format=&quot;NHWC&quot;)
58      with self.assertRaisesRegex(ValueError, &quot;Shape .* must have rank 4&quot;):
59        c(tf.ones([2, 4, 4]))
60    @parameterized.parameters(tf.float32, tf.float64)
61    def testDefaultInitializers(self, dtype):
62      if &quot;TPU&quot; in self.device_types and dtype == tf.float64:
63        self.skipTest(&quot;Double precision not supported on TPU.&quot;)
64      conv1 = conv.ConvND(
65          num_spatial_dims=2,
66          output_channels=1,
67          kernel_shape=16,
68          stride=1,
69          padding=&quot;VALID&quot;,
70          data_format=&quot;NHWC&quot;)
71      out = conv1(tf.random.normal([8, 64, 64, 1], dtype=dtype))
72      self.assertAllEqual(out.shape, [8, 49, 49, 1])
73      self.assertEqual(out.dtype, dtype)
74      err = 0.2 if self.primary_device == &quot;TPU&quot; else 0.1
75      self.assertNear(out.numpy().std(), 0.87, err=err)
76    @parameterized.named_parameters(
77        (&quot;SamePaddingUseBias&quot;, True, &quot;SAME&quot;),
78        (&quot;SamePaddingNoBias&quot;, False, &quot;SAME&quot;),
79        (&quot;samePaddingUseBias&quot;, True, &quot;same&quot;),
80        (&quot;samePaddingNoBias&quot;, False, &quot;same&quot;),
81        (&quot;ValidPaddingNoBias&quot;, False, &quot;VALID&quot;),
82        (&quot;ValidPaddingUseBias&quot;, True, &quot;VALID&quot;),
83        (&quot;validPaddingNoBias&quot;, False, &quot;valid&quot;),
84        (&quot;validPaddingUseBias&quot;, True, &quot;valid&quot;),
85    )
86    def testFunction(self, with_bias, padding):
87      conv1 = conv.ConvND(
88          num_spatial_dims=2,
89          output_channels=1,
90          kernel_shape=3,
91          stride=1,
92          padding=padding,
93          with_bias=with_bias,
94          data_format=&quot;NHWC&quot;,
95          **create_constant_initializers(1.0, 1.0, with_bias))
96      conv2 = conv.ConvND(
97          num_spatial_dims=2,
98          output_channels=1,
99          kernel_shape=3,
100          stride=1,
101          padding=padding,
102          with_bias=with_bias,
103          data_format=&quot;NHWC&quot;,
104          **create_constant_initializers(1.0, 1.0, with_bias))
105      defun_conv = tf.function(conv2)
106      iterations = 5
107      for _ in range(iterations):
108        x = tf.random.uniform([1, 5, 5, 1])
109        y1 = conv1(x)
110        y2 = defun_conv(x)
111        self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)
112    def testUnknownBatchSizeNHWC(self):
113      x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)
114      c = conv.ConvND(
115          num_spatial_dims=2,
116          output_channels=2,
117          kernel_shape=3,
118          data_format=&quot;NHWC&quot;)
119      defun_conv = tf.function(c).get_concrete_function(x)
120      out1 = defun_conv(tf.ones([3, 5, 5, 3]))
121      self.assertEqual(out1.shape, [3, 5, 5, 2])
122      out2 = defun_conv(tf.ones([5, 5, 5, 3]))
123      self.assertEqual(out2.shape, [5, 5, 5, 2])
124    def testUnknownBatchSizeNCHW(self):
125      if self.primary_device == &quot;CPU&quot;:
126        self.skipTest(&quot;NCHW not supported on CPU&quot;)
127      x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
128      c = conv.ConvND(
129          num_spatial_dims=2,
130          output_channels=2,
131          kernel_shape=3,
132          data_format=&quot;NCHW&quot;)
133      defun_conv = tf.function(c).get_concrete_function(x)
134      out1 = defun_conv(tf.ones([3, 3, 5, 5]))
135      self.assertEqual(out1.shape, [3, 2, 5, 5])
136      out2 = defun_conv(tf.ones([5, 3, 5, 5]))
137      self.assertEqual(out2.shape, [5, 2, 5, 5])
138    @parameterized.parameters(True, False)
139    def testUnknownChannels(self, autograph):
140      x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)
141      c = conv.ConvND(
142          num_spatial_dims=2,
143          output_channels=1,
144          kernel_shape=3,
145          data_format=&quot;NHWC&quot;)
146      defun_conv = tf.function(c, autograph=autograph)
147      with self.assertRaisesRegex(ValueError,
148                                  &quot;The number of input channels must be known&quot;):
149        defun_conv.get_concrete_function(x)
150    def testUnknownSpatialDims(self):
151      x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)
152      c = conv.ConvND(
153          num_spatial_dims=2,
154          output_channels=1,
155          kernel_shape=3,
156          data_format=&quot;NHWC&quot;)
157      defun_conv = tf.function(c).get_concrete_function(x)
158      out = defun_conv(tf.ones([3, 5, 5, 3]))
159      expected_out = c(tf.ones([3, 5, 5, 3]))
160      self.assertEqual(out.shape, [3, 5, 5, 1])
161      self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))
162      out = defun_conv(tf.ones([3, 4, 4, 3]))
163      expected_out = c(tf.ones([3, 4, 4, 3]))
164      self.assertEqual(out.shape, [3, 4, 4, 1])
165      self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))
166  class Conv2DTest(test_utils.TestCase, parameterized.TestCase):
167    @parameterized.parameters(True, False)
168    def testComputationPaddingSame(self, with_bias):
169      expected_out = [[4, 6, 6, 6, 4], [6, 9, 9, 9, 6], [6, 9, 9, 9, 6],
170                      [6, 9, 9, 9, 6], [4, 6, 6, 6, 4]]
171      conv1 = conv.Conv2D(
172          output_channels=1,
173          kernel_shape=3,
174          stride=1,
175          padding=&quot;SAME&quot;,
176          with_bias=with_bias,
177          **create_constant_initializers(1.0, 1.0, with_bias))
178      out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
179      self.assertEqual(out.shape, [1, 5, 5, 1])
180      out = tf.squeeze(out, axis=(0, 3))
181      expected_out = np.asarray(expected_out, dtype=np.float32)
182      if with_bias:
183        expected_out += 1
184      self.assertAllClose(self.evaluate(out), expected_out)
185    @parameterized.parameters(True, False)
186    def testComputationPaddingValid(self, with_bias):
187      expected_out = [[9, 9, 9], [9, 9, 9], [9, 9, 9]]
188      conv1 = conv.Conv2D(
189          output_channels=1,
190          kernel_shape=3,
191          stride=1,
192          padding=&quot;VALID&quot;,
193          with_bias=with_bias,
194          **create_constant_initializers(1.0, 1.0, with_bias))
195      out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))
196      self.assertEqual(out.shape, [1, 3, 3, 1])
197      out = tf.squeeze(out, axis=(0, 3))
198      expected_out = np.asarray(expected_out, dtype=np.float32)
199      if with_bias:
200        expected_out += 1
201      self.assertAllClose(self.evaluate(out), expected_out)
202  class Conv1DTest(test_utils.TestCase, parameterized.TestCase):
203    @parameterized.parameters(True, False)
204    def testComputationPaddingSame(self, with_bias):
205      expected_out = [2, 3, 3, 3, 2]
206      conv1 = conv.Conv1D(
207          output_channels=1,
208          kernel_shape=3,
209          stride=1,
210          padding=&quot;SAME&quot;,
211          with_bias=with_bias,
212          **create_constant_initializers(1.0, 1.0, with_bias))
213      out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))
214      self.assertEqual(out.shape, [1, 5, 1])
215      out = tf.squeeze(out, axis=(0, 2))
216      expected_out = np.asarray(expected_out, dtype=np.float32)
217      if with_bias:
218        expected_out += 1
219      self.assertAllClose(self.evaluate(out), expected_out)
220    @parameterized.parameters(True, False)
221    def testComputationPaddingValid(self, with_bias):
222      expected_out = [3, 3, 3]
223      expected_out = np.asarray(expected_out, dtype=np.float32)
224      if with_bias:
225        expected_out += 1
226      conv1 = conv.Conv1D(
227          output_channels=1,
228          kernel_shape=3,
229          stride=1,
230          padding=&quot;VALID&quot;,
231          with_bias=with_bias,
232          **create_constant_initializers(1.0, 1.0, with_bias))
233      out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))
234      self.assertEqual(out.shape, [1, 3, 1])
235      out = tf.squeeze(out, axis=(0, 2))
236      self.assertAllClose(self.evaluate(out), expected_out)
237  class Conv3DTest(test_utils.TestCase, parameterized.TestCase):
238    @parameterized.parameters(True, False)
239    def testComputationPaddingSame(self, with_bias):
240      expected_out = np.asarray([
241          9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 13, 19, 19,
242          19, 13, 9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19,
243          28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 13, 19, 19, 19,
244          13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19,
245          19, 19, 13, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19,
246          19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9, 13, 19, 19,
247          19, 13, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9
248      ]).reshape((5, 5, 5))
249      if not with_bias:
250        expected_out -= 1
251      conv1 = conv.Conv3D(
252          output_channels=1,
253          kernel_shape=3,
254          stride=1,
255          padding=&quot;SAME&quot;,
256          with_bias=with_bias,
257          **create_constant_initializers(1.0, 1.0, with_bias))
258      out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))
259      self.assertEqual(out.shape, [1, 5, 5, 5, 1])
260      out = tf.squeeze(out, axis=(0, 4))
261      self.assertAllClose(self.evaluate(out), expected_out)
262    @parameterized.parameters(True, False)
263    def testComputationPaddingValid(self, with_bias):
264      expected_out = np.asarray([
265          28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
266          28, 28, 28, 28, 28, 28, 28, 28, 28
267      ]).reshape((3, 3, 3))
268      if not with_bias:
269        expected_out -= 1
270      conv1 = conv.Conv3D(
271          output_channels=1,
272          kernel_shape=3,
273          stride=1,
274          padding=&quot;VALID&quot;,
275          with_bias=with_bias,
276          **create_constant_initializers(1.0, 1.0, with_bias))
277      out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))
278      self.assertEqual(out.shape, [1, 3, 3, 3, 1])
279      out = tf.squeeze(out, axis=(0, 4))
280      self.assertAllClose(self.evaluate(out), expected_out)
281  if __name__ == &quot;__main__&quot;:
282    tf.test.main()
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-resnet_7.py</h3>
            <pre><code>1  from typing import Mapping, Optional, Sequence, Union
2  from sonnet.src import base
3  from sonnet.src import batch_norm
4  from sonnet.src import conv
5  from sonnet.src import initializers
6  from sonnet.src import linear
7  from sonnet.src import pad
8  import tensorflow as tf
9  class BottleNeckBlockV1(base.Module):
10    def __init__(self,
11                 channels: int,
12                 stride: Union[int, Sequence[int]],
13                 use_projection: bool,
14                 bn_config: Mapping[str, float],
15                 name: Optional[str] = None):
16      super().__init__(name=name)
17      self._channels = channels
18      self._stride = stride
19      self._use_projection = use_projection
20      self._bn_config = bn_config
21      batchnorm_args = {&quot;create_scale&quot;: True, &quot;create_offset&quot;: True}
22      batchnorm_args.update(bn_config)
23      if self._use_projection:
24        self._proj_conv = conv.Conv2D(
25            output_channels=channels,
26            kernel_shape=1,
27            stride=stride,
28            with_bias=False,
29            padding=pad.same,
30            name=&quot;shortcut_conv&quot;)
31        self._proj_batchnorm = batch_norm.BatchNorm(
32            name=&quot;shortcut_batchnorm&quot;, **batchnorm_args)
33      self._layers = []
34      conv_0 = conv.Conv2D(
35          output_channels=channels // 4,
36          kernel_shape=1,
37          stride=1,
38          with_bias=False,
39          padding=pad.same,
40          name=&quot;conv_0&quot;)
41      self._layers.append(
42          [conv_0,
43           batch_norm.BatchNorm(name=&quot;batchnorm_0&quot;, **batchnorm_args)])
44      conv_1 = conv.Conv2D(
45          output_channels=channels // 4,
46          kernel_shape=3,
47          stride=stride,
48          with_bias=False,
49          padding=pad.same,
50          name=&quot;conv_1&quot;)
51      self._layers.append(
52          [conv_1,
53           batch_norm.BatchNorm(name=&quot;batchnorm_1&quot;, **batchnorm_args)])
54      conv_2 = conv.Conv2D(
55          output_channels=channels,
56          kernel_shape=1,
57          stride=1,
58          with_bias=False,
59          padding=pad.same,
60          name=&quot;conv_2&quot;)
61      batchnorm_2 = batch_norm.BatchNorm(
62          name=&quot;batchnorm_2&quot;, scale_init=initializers.Zeros(), **batchnorm_args)
63      self._layers.append([conv_2, batchnorm_2])
64    def __call__(self, inputs, is_training):
65      if self._use_projection:
66        shortcut = self._proj_conv(inputs)
67        shortcut = self._proj_batchnorm(shortcut, is_training=is_training)
68      else:
69        shortcut = inputs
70      net = inputs
71      for i, [conv_layer, batchnorm_layer] in enumerate(self._layers):
72        net = conv_layer(net)
73        net = batchnorm_layer(net, is_training=is_training)
74        net = tf.nn.relu(net) if i &lt; 2 else net  # Don&#x27;t apply relu on last layer
75      return tf.nn.relu(net + shortcut)
76  class BottleNeckBlockV2(base.Module):
77    def __init__(self,
78                 channels: int,
79                 stride: Union[int, Sequence[int]],
80                 use_projection: bool,
81                 bn_config: Mapping[str, float],
82                 name: Optional[str] = None):
83      super().__init__(name=name)
84      self._channels = channels
85      self._stride = stride
86      self._use_projection = use_projection
87      self._bn_config = bn_config
88      batchnorm_args = {&quot;create_scale&quot;: True, &quot;create_offset&quot;: True}
89      batchnorm_args.update(bn_config)
90      if self._use_projection:
91        self._proj_conv = conv.Conv2D(
92            output_channels=channels,
93            kernel_shape=1,
94            stride=stride,
95            with_bias=False,
96            padding=pad.same,
97            name=&quot;shortcut_conv&quot;)
98      self._conv_0 = conv.Conv2D(
99          output_channels=channels // 4,
100          kernel_shape=1,
101          stride=1,
102          with_bias=False,
103          padding=pad.same,
104          name=&quot;conv_0&quot;)
105      self._bn_0 = batch_norm.BatchNorm(name=&quot;batchnorm_0&quot;, **batchnorm_args)
106      self._conv_1 = conv.Conv2D(
107          output_channels=channels // 4,
108          kernel_shape=3,
109          stride=stride,
110          with_bias=False,
111          padding=pad.same,
112          name=&quot;conv_1&quot;)
113      self._bn_1 = batch_norm.BatchNorm(name=&quot;batchnorm_1&quot;, **batchnorm_args)
114      self._conv_2 = conv.Conv2D(
<span onclick='openModal()' class='match'>115          output_channels=channels,
116          kernel_shape=1,
117          stride=1,
118          with_bias=False,
</span>119          padding=pad.same,
120          name=&quot;conv_2&quot;)
121      self._bn_2 = batch_norm.BatchNorm(name=&quot;batchnorm_2&quot;, **batchnorm_args)
122    def __call__(self, inputs, is_training):
123      net = inputs
124      shortcut = inputs
125      for i, (conv_i, bn_i) in enumerate(((self._conv_0, self._bn_0),
126                                          (self._conv_1, self._bn_1),
127                                          (self._conv_2, self._bn_2))):
128        net = bn_i(net, is_training=is_training)
129        net = tf.nn.relu(net)
130        if i == 0 and self._use_projection:
131          shortcut = self._proj_conv(net)
132        net = conv_i(net)
133      return net + shortcut
134  class BlockGroup(base.Module):
135    def __init__(self,
136                 channels: int,
137                 num_blocks: int,
138                 stride: Union[int, Sequence[int]],
139                 bn_config: Mapping[str, float],
140                 resnet_v2: bool = False,
141                 name: Optional[str] = None):
142      super().__init__(name=name)
143      self._channels = channels
144      self._num_blocks = num_blocks
145      self._stride = stride
146      self._bn_config = bn_config
147      if resnet_v2:
148        bottle_neck_block = BottleNeckBlockV2
149      else:
150        bottle_neck_block = BottleNeckBlockV1
151      self._blocks = []
152      for id_block in range(num_blocks):
153        self._blocks.append(
154            bottle_neck_block(
155                channels=channels,
156                stride=stride if id_block == 0 else 1,
157                use_projection=(id_block == 0),
158                bn_config=bn_config,
159                name=&quot;block_%d&quot; % (id_block)))
160    def __call__(self, inputs, is_training):
161      net = inputs
162      for block in self._blocks:
163        net = block(net, is_training=is_training)
164      return net
165  class ResNet(base.Module):
166    def __init__(self,
167                 blocks_per_group_list: Sequence[int],
168                 num_classes: int,
169                 bn_config: Optional[Mapping[str, float]] = None,
170                 resnet_v2: bool = False,
171                 channels_per_group_list: Sequence[int] = (256, 512, 1024, 2048),
172                 name: Optional[str] = None):
173      super().__init__(name=name)
174      if bn_config is None:
175        bn_config = {&quot;decay_rate&quot;: 0.9, &quot;eps&quot;: 1e-5}
176      self._bn_config = bn_config
177      self._resnet_v2 = resnet_v2
178      if len(blocks_per_group_list) != 4:
179        raise ValueError(
180            &quot;`blocks_per_group_list` must be of length 4 not {}&quot;.format(
181                len(blocks_per_group_list)))
182      self._blocks_per_group_list = blocks_per_group_list
183      if len(channels_per_group_list) != 4:
184        raise ValueError(
185            &quot;`channels_per_group_list` must be of length 4 not {}&quot;.format(
186                len(channels_per_group_list)))
187      self._channels_per_group_list = channels_per_group_list
188      self._initial_conv = conv.Conv2D(
189          output_channels=64,
190          kernel_shape=7,
191          stride=2,
192          with_bias=False,
193          padding=pad.same,
194          name=&quot;initial_conv&quot;)
195      if not self._resnet_v2:
196        self._initial_batchnorm = batch_norm.BatchNorm(
197            create_scale=True,
198            create_offset=True,
199            name=&quot;initial_batchnorm&quot;,
200            **bn_config)
201      self._block_groups = []
202      strides = [1, 2, 2, 2]
203      for i in range(4):
204        self._block_groups.append(
205            BlockGroup(
206                channels=self._channels_per_group_list[i],
207                num_blocks=self._blocks_per_group_list[i],
208                stride=strides[i],
209                bn_config=bn_config,
210                resnet_v2=resnet_v2,
211                name=&quot;block_group_%d&quot; % (i)))
212      if self._resnet_v2:
213        self._final_batchnorm = batch_norm.BatchNorm(
214            create_scale=True,
215            create_offset=True,
216            name=&quot;final_batchnorm&quot;,
217            **bn_config)
218      self._logits = linear.Linear(
219          output_size=num_classes, w_init=initializers.Zeros(), name=&quot;logits&quot;)
220    def __call__(self, inputs, is_training):
221      net = inputs
222      net = self._initial_conv(net)
223      if not self._resnet_v2:
224        net = self._initial_batchnorm(net, is_training=is_training)
225        net = tf.nn.relu(net)
226      net = tf.nn.max_pool2d(
227          net, ksize=3, strides=2, padding=&quot;SAME&quot;, name=&quot;initial_max_pool&quot;)
228      for block_group in self._block_groups:
229        net = block_group(net, is_training)
230      if self._resnet_v2:
231        net = self._final_batchnorm(net, is_training=is_training)
232        net = tf.nn.relu(net)
233      net = tf.reduce_mean(net, axis=[1, 2], name=&quot;final_avg_pool&quot;)
234      return self._logits(net)
235  class ResNet50(ResNet):
236    def __init__(self,
237                 num_classes: int,
238                 bn_config: Optional[Mapping[str, float]] = None,
239                 resnet_v2: bool = False,
240                 name: Optional[str] = None):
241      super().__init__([3, 4, 6, 3],
242                       num_classes=num_classes,
243                       bn_config=bn_config,
244                       resnet_v2=resnet_v2,
245                       name=name)
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-conv_test.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-resnet_7.py</div>
                </div>
                <div class="column column_space"><pre><code>39            num_spatial_dims=n,
40            output_channels=1,
41            kernel_shape=3,
42            data_format=&quot;NHWC&quot;)
</pre></code></div>
                <div class="column column_space"><pre><code>115          output_channels=channels,
116          kernel_shape=1,
117          stride=1,
118          with_bias=False,
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    