<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for dnn.py &amp; test_basic_3.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for dnn.py &amp; test_basic_3.py
      </h3>
<h1 align="center">
        3.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>dnn.py (7.747804%)<th>test_basic_3.py (2.481913%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(2214-2222)<td><a href="#" name="0">(5195-5203)</a><td align="center"><font color="#ff0000">41</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(5-56)<td><a href="#" name="1">(1-62)</a><td align="center"><font color="#c00000">31</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(2223-2227)<td><a href="#" name="2">(5203-5207)</a><td align="center"><font color="#950000">24</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(2893-2900)<td><a href="#" name="3">(1048-1053)</a><td align="center"><font color="#820000">21</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(4015-4019)<td><a href="#" name="4">(4547-4550)</a><td align="center"><font color="#690000">17</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(3285-3290)<td><a href="#" name="5">(1724-1729)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1470-1472)<td><a href="#" name="6">(3032-3037)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(1399-1401)<td><a href="#" name="7">(878-881)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(3237-3240)<td><a href="#" name="8">(6525-6532)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(3228-3231)<td><a href="#" name="9">(6508-6515)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(1973-1976)<td><a href="#" name="10">(8761-8765)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(1943-1950)<td><a href="#" name="11">(6141-6144)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(3391-3396)<td><a href="#" name="12">(1730-1735)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1999-2005)<td><a href="#" name="13">(8089-8095)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(1231-1235)<td><a href="#" name="14">(5036-5043)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(4049-4053)<td><a href="#" name="15">(5979-5981)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(4023-4026)<td><a href="#" name="16">(8443-8445)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(3814-3815)<td><a href="#" name="17">(5279-5281)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(3421-3426)<td><a href="#" name="18">(7957-7964)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(3316-3321)<td><a href="#" name="19">(5022-5028)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(3179-3182)<td><a href="#" name="20">(7224-7229)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(4039-4042)<td><a href="#" name="21">(1116-1129)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(3858-3859)<td><a href="#" name="22">(5514-5516)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(3712-3715)<td><a href="#" name="23">(5293-5294)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(3188-3190)<td><a href="#" name="24">(3202-3204)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(3133-3135)<td><a href="#" name="25">(5016-5017)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#68818b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#68818b"><font color="#68818b">-</font><td><a href="#" name="26">(3076-3083)<td><a href="#" name="26">(6299-6306)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#e77471")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#e77471"><font color="#e77471">-</font><td><a href="#" name="27">(2798-2801)<td><a href="#" name="27">(1920-1922)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#717d7d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#717d7d"><font color="#717d7d">-</font><td><a href="#" name="28">(762-764)<td><a href="#" name="28">(2222-2225)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#af7a82")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#af7a82"><font color="#af7a82">-</font><td><a href="#" name="29">(720-727)<td><a href="#" name="29">(2085-2097)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#ae694a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ae694a"><font color="#ae694a">-</font><td><a href="#" name="30">(659-661)<td><a href="#" name="30">(687-692)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#3ea99f")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3ea99f"><font color="#3ea99f">-</font><td><a href="#" name="31">(579-586)<td><a href="#" name="31">(2017-2029)</a><td align="center"><font color="#4a0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>dnn.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
from __future__ import absolute_import, print_function, division
<a name="1"></a>import ctypes
import os
import sys
<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import warnings
import numpy as np
from six import integer_types
from six.moves import reduce
import theano
from theano import Op, Apply, tensor, config, Variable
from theano.scalar import (as_scalar, constant, Log, get_scalar_type,
                           int32 as int_t, bool as bool_t, uint32 as uint32_t)
from theano.tensor import as_tensor_variable, Argmax
from theano.tensor.extra_ops import cpu_contiguous
from theano.gradient import DisconnectedType, grad_not_implemented
from theano.gof import Optimizer, local_optimizer, COp, ParamsType, EnumList
from theano.gof.cmodule import GCC_compiler
from theano.gof.type import CDataType, Generic
from theano.gof.opt import inherit_stack_trace
from theano.tensor.opt import Assert
from theano.compile import optdb
from theano.compile.ops import shape_i, shape_i_op
from theano.tensor.nnet import LogSoftmax, SoftmaxGrad
from theano.tensor.nnet.abstract_conv import (AbstractConv2d,
                                              AbstractConv2d_gradWeights,
                                              AbstractConv2d_gradInputs,
                                              AbstractConv3d,
                                              AbstractConv3d_gradWeights,
                                              AbstractConv3d_gradInputs,
                                              get_conv_output_shape,
                                              assert_conv_shape)
from theano.tensor.signal.pool import (
    Pool, MaxPoolGrad, AveragePoolGrad)
from . import pygpu, cudnn_defs
from .type import (get_context, gpu_context_type, list_contexts,
                   GpuArraySharedVariable)
from .basic_ops import (as_gpuarray_variable, infer_context_name, gpuarray_helper_inc_dir,
                        gpu_contiguous, GpuAllocEmpty,
                        empty_like, GpuArrayType, HostFromGpu)
from .elemwise import GpuElemwise, GpuCAReduceCuda
from .reduction import GpuMaxAndArgmax
from .nnet import GpuSoftmax
from .opt import (gpu_seqopt, register_opt, pool_db, pool_db2,
                  op_lifter, register_opt2, register_inplace)
from .opt_util import alpha_merge, output_merge, inplace_allocempty, pad_dims, unpad_dims
from theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME
import theano.pathparse
DNN_CONV_ALGO_CHOOSE_ONCE =</b></font> ['guess_once', 'time_once']
DNN_CONV_ALGO_CHOOSE_TIME = ['time_once', 'time_on_shape_change']
try:
    from pygpu import gpuarray
except ImportError:
    pass
WIN32_CUDNN_NAMES = ['cudnn64_7.dll', 'cudnn64_6.dll', 'cudnn64_5.dll']
if sys.platform == 'win32':
    theano.pathparse.PathParser(theano.config.dnn.bin_path)
def _load_lib(name):
    try:
        return ctypes.cdll.LoadLibrary(name)
    except OSError:
        return None
def _dnn_lib():
    if _dnn_lib.handle is None:
        import ctypes.util
        if config.dnn.bin_path != "":
            if sys.platform == 'darwin':
                dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, 'libcudnn.dylib'))
            elif sys.platform == 'win32':
                for name in WIN32_CUDNN_NAMES:
                    dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, name))
                    if dnn_handle is not None:
                        break
            else:
                dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, 'libcudnn.so'))
        else:
            lib_name = ctypes.util.find_library('cudnn')
            if lib_name is None and sys.platform == 'win32':
                for name in WIN32_CUDNN_NAMES:
                    lib_name = ctypes.util.find_library(name)
                    if lib_name:
                        break
            if lib_name is None:
                raise RuntimeError(
                    'Could not find cudnn library (looked for v5* to v7*).'
                    ' Check your cudnn installation. Maybe using the Theano'
                    ' flag dnn.base_path can help you. Current value "%s"' %
                    config.dnn.base_path)
            else:
                dnn_handle = ctypes.cdll.LoadLibrary(lib_name)
        if dnn_handle is None:
            raise RuntimeError('Could not load cudnn library. Check your cudnn'
                               ' installation. Maybe using the Theano'
                               ' flag dnn.base_path can help you. Current value "%s"' %
                               config.dnn.base_path)
        _dnn_lib.handle = dnn_handle
        cudnn = _dnn_lib.handle
        cudnn.cudnnCreate.argtypes = [ctypes.POINTER(ctypes.c_void_p)]
        cudnn.cudnnCreate.restype = ctypes.c_int
        cudnn.cudnnDestroy.argtypes = [ctypes.c_void_p]
        cudnn.cudnnDestroy.restype = ctypes.c_int
    return _dnn_lib.handle
_dnn_lib.handle = None
def _make_handle(ctx):
    cudnn = _dnn_lib()
    handle = ctypes.c_void_p()
    with ctx:
        err = cudnn.cudnnCreate(ctypes.byref(handle))
    if err != 0:
        raise RuntimeError("Error creating cudnn handle. "
                           "This can be a sign of a too old driver.", err)
    return handle
def _dnn_check_compile():
    preambule = """
    path_wrapper = "\"" if os.name == 'nt' else ""
    params = ["-l", "cudnn"]
    params.extend(['-I%s%s%s' % (path_wrapper, gpuarray_helper_inc_dir(), path_wrapper)])
    if config.dnn.include_path:
        params.extend(['-I%s%s%s' % (path_wrapper, config.dnn.include_path, path_wrapper)])
    if config.cuda.include_path:
        params.extend(['-I%s%s%s' % (path_wrapper, config.cuda.include_path, path_wrapper)])
    if config.dnn.library_path:
        params.extend(['-L%s%s%s' % (path_wrapper, config.dnn.library_path, path_wrapper)])
    compiler_res = GCC_compiler.try_flags(
        params, preambule=preambule, body=body,
        try_run=False, output=True)
    avail, out, err = compiler_res if isinstance(compiler_res, tuple) else (compiler_res, None, None)
    if not avail:
        return False, ("cannot compile with cuDNN. "
                       "We got this error:\n" + str(err))
    return True, None
def _dnn_check_version():
    v = version()
    if v &lt; 5000:
        return False, "cuDNN version is too old. Update to v5* or higher, was %d." % v
    if v &gt;= 7200:
        warnings.warn("Your cuDNN version is more recent than "
                      "Theano. If you encounter problems, try "
                      "updating Theano or downgrading cuDNN to "
                      "a version &gt;= v5 and &lt;= v7.")
    return True, None
def dnn_present():
    if dnn_present.avail is not None:
        return dnn_present.avail
    if config.dnn.enabled == "False":
        dnn_present.msg = "Disabled by dnn.enabled flag"
        dnn_present.avail = False
        return False
    if pygpu is None:
        dnn_present.msg = "PyGPU not available"
        dnn_present.avail = False
        return False
    if config.dnn.enabled == "no_check":
        dnn_present.avail, dnn_present.msg = True, "presence check disabled by dnn.enabled flag"
    else:
        dnn_present.avail, dnn_present.msg = _dnn_check_compile()
    if dnn_present.avail:
        dnn_present.avail, dnn_present.msg = _dnn_check_version()
        if not dnn_present.avail:
            return False
    return dnn_present.avail
dnn_present.avail = None
dnn_present.msg = None
def dnn_available(context_name):
    if not dnn_present():
        dnn_available.msg = dnn_present.msg
        return False
    ctx = get_context(context_name)
    if not ctx.kind == b'cuda':
        dnn_available.msg = "Not on a CUDA device."
        return False
    if int(ctx.bin_id[-2:]) &lt; 30:
        dnn_available.msg = "Device not supported"
        return False
    if version() &lt; 7002:
        if int(ctx.bin_id[-2:]) &gt;= 70:
            dnn_available.msg = "Use cuDNN 7.0.2 or higher for Volta."
            return False
    return True
dnn_available.msg = None
def CUDNNDataType(name, freefunc=None):
    cargs = []
    if config.dnn.bin_path and sys.platform != 'win32':
        cargs.append('-Wl,-rpath,' + config.dnn.bin_path)
    return CDataType(name, freefunc,
                     headers=['cudnn.h'],
                     header_dirs=[config.dnn.include_path,
                                  config.cuda.include_path],
                     libraries=['cudnn'],
                     lib_dirs=[config.dnn.library_path],
                     compile_args=cargs,
                     version=version(raises=False))
class DnnVersion(Op):
    __props__ = ()
    def c_headers(self):
        return ['cudnn.h']
    def c_header_dirs(self):
        return [config.dnn.include_path, config.cuda.include_path]
    def c_libraries(self):
        return ['cudnn']
    def c_lib_dirs(self):
        return [config.dnn.library_path]
    def c_compile_args(self):
        if config.dnn.bin_path and sys.platform != 'win32':
            return ['-Wl,-rpath,' + config.dnn.bin_path]
        return []
    def c_support_code(self):
        return """
    def do_constant_folding(self, node):
        return False
    def c_code_cache_version(self):
        return None
def version(raises=True):
    if not dnn_present():
        if raises:
            raise RuntimeError(
                "We can't determine the cudnn version as it is not available",
                dnn_available.msg)
        else:
            return -1
    if version.v is None:
        f = theano.function([], DnnVersion()(),
                            theano.Mode(optimizer=None),
                            profile=False)
        v = f()
        if v[0] != v[1]:
            raise RuntimeError("Mixed dnn version. The header is version %s "
                               "while the library is version %s." % v)
        version.v = v[1]
    return version.v
version.v = None
handle_type = CUDNNDataType('cudnnHandle_t', 'cudnnDestroy')
cudnn = cudnn_defs.get_definitions(version(raises=False))
def get_precision(precision, inputs, for_grad=False):
    common_dtype = theano.scalar.upcast(*[i.dtype for i in inputs])
    if not common_dtype.startswith('float'):
        raise TypeError("cuDNN convolution only works on real numbers")
    if precision is None:
        precision = theano.config.dnn.conv.precision
    if precision == 'as_input' or precision == 'as_input_f32':
        if common_dtype == 'float16' and precision == 'as_input_f32':
            precision = 'float32'
        else:
            precision = common_dtype
    if for_grad and precision == 'float16':
        raise TypeError("Float16 precision is disabled for cuDNN backward convolutions due to computation errors.")
    return precision, common_dtype
class DnnBase(COp):
    check_broadcast = False
    params_type = handle_type
    def dnn_context(self, node):
        return node.outputs[0].type.context_name
    def get_params(self, node):
        ctx_name = self.dnn_context(node)
        ctx = get_context(ctx_name)
        if not hasattr(ctx, 'cudnn_handle_param'):
            ptr = ctx.cudnn_handle.value
            res = handle_type.make_value(ptr)
            ctx.cudnn_handle_param = res
        if isinstance(self.params_type, ParamsType):
            if not self.params_type.has_type(handle_type):
                raise TypeError('DnnBase: params_type must take into account the cuDNN handle type.')
            handle_field = self.params_type.get_field(handle_type)
            return self.params_type.get_params(self, **{handle_field: ctx.cudnn_handle_param})
        return ctx.cudnn_handle_param
    def __init__(self, files=None, c_func=None):
        if files is None:
            files = []
        COp.__init__(self, ["c_code/dnn_base.c"] + files, c_func)
    def c_headers(self):
        return ['gpuarray/types.h', 'gpuarray/array.h', 'gpuarray/kernel.h',
                'gpuarray/util.h', 'gpuarray/ext_cuda.h', 'gpuarray_api.h',
                'numpy_compat.h', 'cudnn.h', 'cudnn_helper.h',
                'gpuarray_helper.h']
    def c_header_dirs(self):
        return [gpuarray_helper_inc_dir(), pygpu.get_include(),
                config.dnn.include_path, config.cuda.include_path]
    def c_libraries(self):
        return ['cudnn', 'gpuarray']
    def c_lib_dirs(self):
        return [config.dnn.library_path]
    def c_compile_args(self):
        if config.dnn.bin_path and sys.platform != 'win32':
            return ['-Wl,-rpath,' + config.dnn.bin_path]
        return []
    def c_code_cache_version(self):
        return (super(DnnBase, self).c_code_cache_version(), version(), 4)
class GpuDnnConvDesc(COp):
    __props__ = ('border_mode', 'subsample', 'dilation', 'conv_mode',
                 'precision', 'num_groups')
    params_type = ParamsType(pad0=int_t, pad1=int_t, pad2=int_t,
                             sub0=int_t, sub1=int_t, sub2=int_t,
                             dil0=int_t, dil1=int_t, dil2=int_t,
                             nb_dims=int_t,
                             bmode=EnumList(('BORDER_MODE_FULL', 'full'),
                                            ('BORDER_MODE_VALID', 'valid'),
                                            ('BORDER_MODE_HALF', 'half')),
                             conv_mode=cudnn.cudnnConvolutionMode_t,
                             precision=cudnn.cudnnDataType_t,
                             num_groups=int_t)
    def c_headers(self):
        return ['cudnn.h', 'cudnn_helper.h']
    def c_header_dirs(self):
        return [gpuarray_helper_inc_dir(), config.dnn.include_path,
                config.cuda.include_path]
    def c_libraries(self):
        return ['cudnn']
    def c_lib_dirs(self):
        return [config.dnn.library_path]
    def c_compile_args(self):
        if config.dnn.bin_path and sys.platform != 'win32':
            return ['-Wl,-rpath,' + config.dnn.bin_path]
        return []
    def do_constant_folding(self, node):
        return False
    def __init__(self, border_mode, subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
                 precision="float32", num_groups=1):
        COp.__init__(self, ["c_code/conv_desc.c"], "APPLY_SPECIFIC(conv_desc)")
        if version() &lt; 6000 and any([d != 1 for d in dilation]):
            raise RuntimeError("Dilation &gt; 1 not supported for cuDNN version &lt; 6.")
        if isinstance(border_mode, integer_types):
            border_mode = (border_mode,) * len(subsample)
        if isinstance(border_mode, tuple):
            assert len(border_mode) == len(subsample)
            border_mode = tuple(map(int, border_mode))
        if not ((isinstance(border_mode, tuple) and min(border_mode) &gt;= 0) or
                border_mode in ('valid', 'full', 'half')):
            raise ValueError(
                'invalid border_mode {}, which must be either '
                '"valid", "full", "half", an integer or a pair of'
                ' integers'.format(border_mode))
        self.border_mode = border_mode
        assert len(subsample) in (2, 3)
        self.subsample = subsample
        assert cudnn.cudnnConvolutionMode_t.has_alias(conv_mode)
        self.conv_mode = conv_mode
        self.num_groups = num_groups
        assert len(dilation) == len(subsample)
        self.dilation = dilation
        assert cudnn.cudnnDataType_t.has_alias(precision)
        self.precision = precision
    def make_node(self, kern_shape):
        kern_shape = as_tensor_variable(kern_shape)
        if kern_shape.type.ndim != 1 or kern_shape.dtype not in theano.tensor.basic.int_dtypes:
            raise TypeError('kern must be an int64 1D shape tensor')
        kern_shape = theano.tensor.basic.cast(kern_shape, 'int64')
        node = Apply(self, [kern_shape],
                     [CUDNNDataType("cudnnConvolutionDescriptor_t",
                                    freefunc="cudnnDestroyConvolutionDescriptor")()])
        out = node.outputs[0]
        out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true
        return node
    bmode = property(lambda self: 'valid' if isinstance(self.border_mode, tuple) else self.border_mode)
    pad0 = property(lambda self: self.border_mode[0] if isinstance(self.border_mode, tuple) else 0)
    pad1 = property(lambda self: self.border_mode[1] if isinstance(self.border_mode, tuple) else 0)
    pad2 = property(lambda self: self.border_mode[2] if (isinstance(self.border_mode, tuple) and
                                                         len(self.border_mode) &gt; 2) else 0)
    sub0 = property(lambda self: self.subsample[0])
    sub1 = property(lambda self: self.subsample[1])
    sub2 = property(lambda self: self.subsample[2] if len(self.subsample) &gt; 2 else 0)
    dil0 = property(lambda self: self.dilation[0])
    dil1 = property(lambda self: self.dilation[1])
    dil2 = property(lambda self: self.dilation[2] if len(self.dilation) &gt; 2 else 0)
    nb_dims = property(lambda self: len(self.subsample))
    def c_code_cache_version(self):
        return (super(GpuDnnConvDesc, self).c_code_cache_version(), version())
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, "dilation"):
            self.dilation = (1,) * len(self.subsample)
        if not hasattr(self, "num_groups"):
            self.num_groups = 1
_zero = constant(np.asarray(0.0, dtype='float64'))
_one = constant(np.asarray(1.0, dtype='float64'))
def ensure_dt(val, default, name, dtype):
    if dtype == 'float16':
        dtype = 'float32'
    if val is None:
        val = default.clone()
    if not isinstance(val, Variable):
        val = constant(val)
    if hasattr(val, 'ndim') and val.ndim == 0:
        val = as_scalar(val)
    if not isinstance(val.type, theano.scalar.Scalar):
        raise TypeError("%s: expected a scalar value" % (name,))
    if not val.type.dtype == dtype:
        val = val.astype(dtype)
    return val
class GpuDnnConv(DnnBase):
<a name="31"></a>    _f16_ok = True
    __props__ = ('algo', 'inplace', 'num_groups')
    check_input <font color="#3ea99f"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= False
    params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionFwdAlgo_t,
                             choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
                             inplace=bool_t,
                             handle=handle_type,
                             num_groups=int_t)
    def</b></font> __init__(self, algo=None, inplace=False, num_groups=1):
        DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_fwd.c"],
                         "APPLY_SPECIFIC(conv_fwd)")
        if algo is None:
            algo = config.dnn.conv.algo_fwd
        self.algo = algo
        self.inplace = bool(inplace)
        if self.inplace:
            self.destroy_map = {0: [2]}
        assert cudnn.cudnnConvolutionFwdAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
        self.conv_algo = cudnn.cudnnConvolutionFwdAlgo_t.CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
        if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            self.conv_algo = self.algo
        self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
        self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
        self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
        self.num_groups = num_groups
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, 'algo'):
            if hasattr(self, 'workmem'):
                self.algo = self.workmem
            else:
                self.algo = config.dnn.conv.algo_fwd
        if not hasattr(self, 'inplace'):
            self.inplace = False
        if not hasattr(self, 'num_groups'):
            self.num_groups = 1
    def make_node(self, img, kern, output, desc, alpha=None, beta=None):
        ctx_name = infer_context_name(img, kern, output)
        img = as_gpuarray_variable(img, ctx_name)
        kern = as_gpuarray_variable(kern, ctx_name)
        output = as_gpuarray_variable(output, ctx_name)
        if img.type.ndim not in (4, 5):
            raise TypeError('img must be 4D or 5D tensor')
        if kern.type.ndim not in (4, 5):
            raise TypeError('kern must be 4D or 5D tensor')
        if output.type.ndim not in (4, 5):
            raise TypeError('output must be a 4D or 5D tensor')
        if (img.type.ndim != kern.type.ndim or
                img.type.ndim != output.type.ndim):
            raise TypeError("The number of dimensions of "
                            "img, kern and output must match")
        if img.type.ndim == 5 and self.algo not in (cudnn.conv3d_fwd_algorithms +
                                                    SUPPORTED_DNN_CONV_ALGO_RUNTIME):
            raise ValueError("convolution algo %s can't be used for "
                             "3d convolutions", (self.algo,))
        if (not isinstance(desc.type, CDataType) or
                desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
            raise TypeError('desc must be cudnnConvolutionDescriptor_t')
        alpha = ensure_dt(alpha, _one, 'alpha', img.dtype)
        beta = ensure_dt(beta, _zero, 'beta', img.dtype)
        return Apply(self, [img, kern, output, desc, alpha, beta],
                     [output.type()])
    def grad(self, inp, grads):
        img, kerns, output, desc, alpha, beta = inp
        top, = grads
<a name="30"></a>
        top = gpu_contiguous(top)
        d_img = GpuDnnConvGradI<font color="#ae694a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(num_groups=self.num_groups)(kerns, top, empty_like(img), desc)
        d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(img, top, empty_like(kerns), desc)
        d_alpha =</b></font> grad_not_implemented(self, 4, alpha)
        d_beta = grad_not_implemented(self, 5, beta)
        return [d_img * alpha, d_kerns * alpha, top * beta,
                DisconnectedType()(), d_alpha, d_beta]
    def connection_pattern(self, node):
        return [[1], [1], [1], [0], [1], [1]]
    @staticmethod
    def get_out_shape(ishape, kshape, border_mode, subsample, dilation):
        if not isinstance(ishape, (list, tuple)):
            ishape = [ishape[i] for i in range(len(subsample) + 2)]
        if not isinstance(kshape, (list, tuple)):
            kshape = [kshape[i] for i in range(len(subsample) + 2)]
        return get_conv_output_shape(
            ishape,
            kshape,
            border_mode,
            subsample,
            dilation)
    def infer_shape(self, node, shape):
        return [shape[2]]
class GpuDnnConvGradW(DnnBase):
<a name="29"></a>    _f16_ok = True
    __props__ = ('algo', 'inplace', 'num_groups')
    check_input <font color="#af7a82"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= False
    params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionBwdFilterAlgo_t,
                             choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
                             inplace=bool_t,
                             handle=handle_type,
                             num_groups=int_t)
    def</b></font> __init__(self, inplace=False, algo=None, num_groups=1):
        DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_gw.c"],
                         "APPLY_SPECIFIC(conv_gw)")
        self.inplace = bool(inplace)
        if self.inplace:
            self.destroy_map = {0: [2]}
        if algo is None:
            algo = config.dnn.conv.algo_bwd_filter
        self.algo = algo
        assert cudnn.cudnnConvolutionBwdFilterAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
        self.conv_algo = cudnn.cudnnConvolutionBwdFilterAlgo_t.CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0
        if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            self.conv_algo = self.algo
        self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
        self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
        self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
        self.num_groups = num_groups
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, 'inplace'):
            self.inplace = False
        if not hasattr(self, 'algo'):
            self.algo = config.dnn.conv.algo_bwd_filter
        if not hasattr(self, 'num_groups'):
            self.num_groups = 1
    def grad(self, inp, grads):
        img, top, output, desc, alpha, beta = inp
        kerns, = grads
<a name="28"></a>
        kerns = gpu_contiguous(kerns)
        d_img = GpuDnnConvGradI<font color="#717d7d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(num_groups=self.num_groups)(kerns, top, empty_like(img), desc)
        d_top = GpuDnnConv(num_groups=self.num_groups)(img, kerns, empty_like(top), desc)
        d_alpha =</b></font> grad_not_implemented(self, 4, alpha)
        d_beta = grad_not_implemented(self, 5, beta)
        return (d_img * alpha, d_top * alpha, kerns * beta,
                DisconnectedType()(), d_alpha, d_beta)
    def connection_pattern(self, node):
        return [[1], [1], [1], [0], [1], [1]]
    def op_may_fail_with_subsample(self, img, desc):
        return (version() &lt; 6000 and
                img.type.dtype == 'float32' and
                img.type.ndim == 5 and
                self.algo != 'none' and
                desc.owner.op.subsample != (1, 1, 1))
    def op_may_fail_with_beta(self, img, beta):
        return (version() &lt; 6000 and
                img.type.dtype == 'float32' and
                self.algo not in ('none', 'deterministic', 'fft', 'small') and
                beta is not None and
                theano.tensor.extract_constant(beta) != 1)
    def make_node(self, img, topgrad, output, desc, alpha=None, beta=None):
        if self.op_may_fail_with_subsample(img, desc):
            warnings.warn('cuDNN backward filter operation for 3D convolutions may produce bad results '
                          'with certain cuDNN algorithms depending on the compute capability of your GPU '
                          'if subsample is not (1, 1, 1). If you encounter problems, consider '
                          'setting the theano flag "dnn.conv.algo_bwd_filter" to "none".')
        if self.op_may_fail_with_beta(img, beta):
            warnings.warn('cuDNN backward filter operation for convolutions may produce bad results '
                          'with certain cuDNN algorithms depending on the compute capability of your GPU '
                          'if beta != 1. If you encounter problems, consider '
                          'setting the theano flag "dnn.conv.algo_bwd_filter" to '
                          '"none", "deterministic", "fft", or "small".')
        ctx_name = infer_context_name(img, topgrad, output)
        img = as_gpuarray_variable(img, ctx_name)
        topgrad = as_gpuarray_variable(topgrad, ctx_name)
        output = as_gpuarray_variable(output, ctx_name)
        if img.type.ndim not in (4, 5):
            raise TypeError('img must be 4D or 5D tensor')
        if topgrad.type.ndim not in (4, 5):
            raise TypeError('topgrad must be 4D or 5D tensor')
        if output.type.ndim not in (4, 5):
            raise TypeError('output must be 4D or 5D tensor')
        if (img.type.ndim != topgrad.type.ndim or
                img.type.ndim != output.type.ndim):
            raise TypeError("The number of dimensions of "
                            "img, topgrad and output must match")
        if img.type.ndim == 5 and self.algo not in (cudnn.conv3d_bwd_filter_algorithms +
                                                    SUPPORTED_DNN_CONV_ALGO_RUNTIME):
            raise ValueError("convolution algo %s can't be used for "
                             "3d convolutions", (self.algo,))
        if (not isinstance(desc.type, CDataType) or
                desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
            raise TypeError('desc must be cudnnConvolutionDescriptor_t')
        alpha = ensure_dt(alpha, _one, 'alpha', img.dtype)
        beta = ensure_dt(beta, _zero, 'beta', img.dtype)
        return Apply(self, [img, topgrad, output, desc, alpha, beta],
                     [output.type()])
    def infer_shape(self, node, shape):
        return [shape[2]]
class GpuDnnConvGradI(DnnBase):
    _f16_ok = True
    __props__ = ('algo', 'inplace', 'num_groups')
    check_input = False
    params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionBwdDataAlgo_t,
                             choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
                             inplace=bool_t,
                             handle=handle_type,
                             num_groups=int_t)
    def __init__(self, inplace=False, algo=None, num_groups=1):
        DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_gi.c"],
                         "APPLY_SPECIFIC(conv_gi)")
        self.inplace = bool(inplace)
        if self.inplace:
            self.destroy_map = {0: [2]}
        if algo is None:
            algo = config.dnn.conv.algo_bwd_data
        self.algo = algo
        assert cudnn.cudnnConvolutionBwdDataAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
        self.conv_algo = cudnn.cudnnConvolutionBwdDataAlgo_t.CUDNN_CONVOLUTION_BWD_DATA_ALGO_0
        if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            self.conv_algo = self.algo
        self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
        self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
        self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
        self.num_groups = num_groups
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, 'algo'):
            self.algo = config.dnn.conv.algo_bwd_data
        if not hasattr(self, 'inplace'):
            self.inplace = False
        if not hasattr(self, 'num_groups'):
            self.num_groups = 1
    def grad(self, inp, grads):
        kerns, top, output, desc, alpha, beta = inp
        img, = grads
        img = gpu_contiguous(img)
        d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(img, top, empty_like(kerns), desc)
        d_top = GpuDnnConv(num_groups=self.num_groups)(img, kerns, empty_like(top), desc)
        d_alpha = grad_not_implemented(self, 4, alpha)
        d_beta = grad_not_implemented(self, 5, beta)
        return (d_kerns * alpha, d_top * alpha, img * beta,
                DisconnectedType()(), d_alpha, d_beta)
    def connection_pattern(self, node):
        return [[1], [1], [1], [0], [1], [1]]
    def make_node(self, kern, topgrad, output, desc, alpha=None, beta=None):
        ctx_name = infer_context_name(kern, topgrad, output)
        kern = as_gpuarray_variable(kern, ctx_name)
        topgrad = as_gpuarray_variable(topgrad, ctx_name)
        output = as_gpuarray_variable(output, ctx_name)
        if kern.type.ndim not in (4, 5):
            raise TypeError('kern must be 4D or 5D tensor')
        if topgrad.type.ndim not in (4, 5):
            raise TypeError('topgrad must be 4D or 5D tensor')
        if output.type.ndim not in (4, 5):
            raise TypeError('output must be 4D or 5D tensor')
        if (kern.type.ndim != topgrad.type.ndim or
                kern.type.ndim != output.type.ndim):
            raise TypeError("The number of dimensions of "
                            "kern, topgrad and output must match")
        if kern.type.ndim == 5 and self.algo not in (cudnn.conv3d_bwd_data_algorithms +
                                                     SUPPORTED_DNN_CONV_ALGO_RUNTIME):
            raise ValueError("convolution algo %s can't be used for "
                             "3d convolutions", (self.algo,))
        if (not isinstance(desc.type, CDataType) or
                desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
            raise TypeError('desc must be cudnnConvolutionDescriptor_t')
        alpha = ensure_dt(alpha, _one, 'alpha', kern.dtype)
        beta = ensure_dt(beta, _zero, 'beta', kern.dtype)
        return Apply(self, [kern, topgrad, output, desc, alpha, beta],
                     [output.type()])
    def infer_shape(self, node, shape):
        return [shape[2]]
def _dnn_conv(img, kerns, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1), dilation=(1, 1),
              conv_mode='conv', algo=None, precision=None, num_groups=1):
    ctx_name = infer_context_name(img, kerns)
    img = as_gpuarray_variable(img, ctx_name)
    kerns = as_gpuarray_variable(kerns, ctx_name)
    precision, dt = get_precision(precision, [img, kerns])
    img = gpu_contiguous(img.astype(dt))
    kerns = gpu_contiguous(kerns.astype(dt))
    desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
                          conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns.shape)
    desc_op = desc.owner.op
    ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
    kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
    out_shp = get_conv_output_shape(ishape, kshape, desc_op.border_mode, desc_op.subsample, filter_dilation=dilation)
    out_shp = assert_conv_shape(out_shp)
    if beta == 0:
        real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
    else:
        assert out is not None
        out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
        check = Assert('GpuDnnConv: given output (for beta not null) does not have expected shape')
        real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, out_shp)))
    return GpuDnnConv(algo=algo, num_groups=num_groups)(img, kerns, real_out, desc, alpha, beta)
def _dnn_gradweight(img, topgrad, kerns_shp, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1),
                    dilation=(1, 1), conv_mode='conv', algo=None, precision=None, num_groups=1):
    ctx_name = infer_context_name(img, topgrad)
    img = as_gpuarray_variable(img, ctx_name)
    topgrad = as_gpuarray_variable(topgrad, ctx_name)
    kerns_shp = theano.tensor.as_tensor_variable(kerns_shp)
    precision, dt = get_precision(precision, [img, topgrad], for_grad=True)
    img = gpu_contiguous(img.astype(dt))
    topgrad = gpu_contiguous(topgrad.astype(dt))
    desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
                          conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns_shp)
    if beta == 0:
        real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*kerns_shp)
    else:
        assert out is not None
        out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
        check = Assert('GpuDnnConvGradW: given output (for beta not null) does not have expected shape')
        real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, kerns_shp)))
    return GpuDnnConvGradW(algo=algo, num_groups=num_groups)(img, topgrad, real_out, desc, alpha, beta)
def _dnn_gradinput(kerns, topgrad, img_shp, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1),
                   dilation=(1, 1), conv_mode='conv', algo=None, precision=None, num_groups=1):
    ctx_name = infer_context_name(kerns, topgrad)
    kerns = as_gpuarray_variable(kerns, ctx_name)
    topgrad = as_gpuarray_variable(topgrad, ctx_name)
    img_shp = theano.tensor.as_tensor_variable(img_shp)
    precision, dt = get_precision(precision, [kerns, topgrad], for_grad=True)
    kerns = gpu_contiguous(kerns.astype(dt))
    topgrad = gpu_contiguous(topgrad.astype(dt))
    desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
                          conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns.shape)
    if beta == 0:
        real_out = GpuAllocEmpty(dtype=kerns.dtype, context_name=ctx_name)(*img_shp)
    else:
        assert out is not None
        out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
        check = Assert('GpuDnnConvGradI: given output (for beta not null) does not have expected shape')
        real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, img_shp)))
    return GpuDnnConvGradI(algo=algo, num_groups=num_groups)(kerns, topgrad, real_out, desc, alpha, beta)
def dnn_conv(img, kerns, border_mode='valid', subsample=(1, 1), dilation=(1, 1),
             conv_mode='conv', direction_hint=None, workmem=None,
             algo=None, precision=None, num_groups=1):
    if workmem is not None:
        if algo is not None:
            raise ValueError("You can't use both algo and workmem")
        warnings.warn("workmem is deprecated, use algo instead", stacklevel=2)
        algo = workmem
    fgraph = getattr(img, 'fgraph', None) or getattr(kerns, 'fgraph', None)
    ctx_name = infer_context_name(img, kerns)
    if (border_mode == 'valid' and subsample == (1, 1) and dilation == (1, 1) and
            direction_hint == 'bprop weights' and num_groups == 1):
        img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3))
        if conv_mode == 'conv':
            kerns = kerns[:, :, ::-1, ::-1]
        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))
        out_shp = (shape_i(kerns, 1, fgraph),
                   shape_i(img, 1, fgraph),
                   shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,
                   shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1)
        out_shp = assert_conv_shape(out_shp)
        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
        precision, _ = get_precision(precision, [img, kerns], for_grad=True)
        desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1), dilation=(1, 1),
                              num_groups=num_groups,
                              conv_mode='cross', precision=precision)(out.shape)
        conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)
        return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)
    elif (border_mode == 'full' and subsample == (1, 1) and
          direction_hint != 'forward!' and num_groups == 1):
        img = gpu_contiguous(img)  # cudnn v2 rc3 need contiguous data
        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))
        conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
        out_shp = (shape_i(img, 0, fgraph),
                   shape_i(kerns, 1, fgraph),
                   shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],
                   shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1])
        out_shp = assert_conv_shape(out_shp)
        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
        precision, _ = get_precision(precision, [img, kerns], for_grad=True)
        desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1), dilation=dilation,
                              num_groups=num_groups,
                              conv_mode=conv_mode, precision=precision)(kerns.shape)
        return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)
    return _dnn_conv(img, kerns, algo=algo, border_mode=border_mode, subsample=subsample, dilation=dilation,
                     conv_mode=conv_mode, precision=precision, num_groups=num_groups)
def dnn_conv3d(img, kerns, border_mode='valid', subsample=(1, 1, 1), dilation=(1, 1, 1),
               conv_mode='conv', direction_hint=None,
               algo=None, precision=None, num_groups=1):
    fgraph = getattr(img, 'fgraph', None) or getattr(kerns, 'fgraph', None)
    ctx_name = infer_context_name(img, kerns)
    if (border_mode == 'valid' and subsample == (1, 1, 1) and dilation == (1, 1, 1) and
            direction_hint == 'bprop weights' and num_groups == 1):
        img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3, 4))
        if conv_mode == 'conv':
            kerns = kerns[:, :, ::-1, ::-1, ::-1]
        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))
        out_shp = (shape_i(kerns, 1, fgraph),
                   shape_i(img, 1, fgraph),
                   shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,
                   shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1,
                   shape_i(img, 4, fgraph) - shape_i(kerns, 4, fgraph) + 1)
        out_shp = assert_conv_shape(out_shp)
        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
        precision, _ = get_precision(precision, [img, kerns], for_grad=True)
        desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1, 1), dilation=(1, 1, 1),
                              num_groups=num_groups,
                              conv_mode='cross', precision=precision)(out.shape)
        conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)
        return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)
    elif (border_mode == 'full' and subsample == (1, 1, 1) and
          direction_hint != 'forward!' and num_groups == 1):
        kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))
        conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
        out_shp = (<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>shape_i(img, 0, fgraph),
                   shape_i(kerns, 1, fgraph),
                   shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],
                   shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1],
                   shape_i(img, 4, fgraph) + (shape_i(</b></font>kerns, 4, fgraph) - 1) * dilation[2])
        out_shp = assert_conv_shape(out_shp)
        out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
        precision, _ = get_precision(precision, [img, kerns], for_grad=True)
        desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1, 1), dilation=dilation,
                              num_groups=num_groups,
                              conv_mode=conv_mode, precision=precision)(kerns.shape)
        return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)
    return _dnn_conv(img, kerns, algo=algo, border_mode=border_mode, subsample=subsample, dilation=dilation,
                     conv_mode=conv_mode, precision=precision, num_groups=num_groups)
def dnn_gradweight(img, topgrad, kerns_shp, border_mode='valid',
                   subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
                   precision=None, algo=None, num_groups=1):
    return _dnn_gradweight(img, topgrad, kerns_shp, border_mode=border_mode, subsample=subsample, dilation=dilation,
                           conv_mode=conv_mode, algo=algo, precision=precision, num_groups=num_groups)
def dnn_gradweight3d(img, topgrad, kerns_shp, border_mode='valid',
                     subsample=(1, 1, 1), dilation=(1, 1, 1), conv_mode='conv',
                     precision=None, algo=None, num_groups=1):
    return dnn_gradweight(img, topgrad, kerns_shp, border_mode,
                          subsample, dilation, conv_mode, precision,
                          algo, num_groups)
def dnn_gradinput(kerns, topgrad, img_shp, border_mode='valid',
                  subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
                  precision=None, algo=None, num_groups=1):
    return _dnn_gradinput(kerns, topgrad, img_shp, border_mode=border_mode, subsample=subsample, dilation=dilation,
                          conv_mode=conv_mode, algo=algo, precision=precision, num_groups=num_groups)
def dnn_gradinput3d(kerns, topgrad, img_shp, border_mode='valid',
                    subsample=(1, 1, 1), dilation=(1, 1, 1), conv_mode='conv',
                    precision=None, algo=None, num_groups=1):
    return dnn_gradinput(kerns, topgrad, img_shp, border_mode, subsample,
                         dilation, conv_mode, precision, algo,
                         num_groups)
class GpuDnnPoolDesc(Op):
    __props__ = ('ws', 'stride', 'mode', 'pad')
    def c_headers(self):
        return ['cudnn.h', 'cudnn_helper.h']
    def c_header_dirs(self):
        return [gpuarray_helper_inc_dir(), config.dnn.include_path]
    def c_libraries(self):
        return ['cudnn']
    def c_lib_dirs(self):
        return [config.dnn.library_path]
    def do_constant_folding(self, node):
        return False
    def __init__(self, ws=(1, 1), stride=(1, 1), mode='max', pad=(0, 0)):
        if mode == 'average':
            mode = 'average_inc_pad'
        assert mode in ('max', 'average_inc_pad', 'average_exc_pad')
        self.mode = mode
        assert len(ws) == len(stride) and len(stride) == len(pad)
        assert len(ws) in (2, 3)
        self.ws = ws
        self.stride = stride
        self.pad = pad
    def get_ndim(self):
        return len(self.ws)
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, 'pad'):
            self.pad = (0, 0)
    def make_node(self):
        node = Apply(self, [],
                     [CUDNNDataType("cudnnPoolingDescriptor_t",
                                    freefunc="cudnnDestroyPoolingDescriptor")()])
        out = node.outputs[0]
        out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true
        return node
    def c_code(self, node, name, inputs, outputs, sub):
        desc, = outputs
        if self.mode == 'max':
            mode_flag = 'CUDNN_POOLING_MAX'
        elif self.mode == "average_inc_pad":
            mode_flag = 'CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING'
        elif self.mode == "average_exc_pad":
            mode_flag = 'CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING'
        else:
            raise NotImplementedError("Unsupported pooling model.")
        return """
{
  cudnnStatus_t err;
  if ((err = cudnnCreatePoolingDescriptor(&amp;%(desc)s)) != CUDNN_STATUS_SUCCESS) {
    PyErr_Format(PyExc_MemoryError, "could not allocate pooling "
                 "descriptor: %%s", cudnnGetErrorString(err));
    %(fail)s
  }
  static const int win[%(nd)d] = {%(win)s};
  static const int pad[%(nd)d] = {%(pad)s};
  static const int str[%(nd)d] = {%(str)s};
    err = cudnnSetPoolingNdDescriptor(%(desc)s, %(mode_flag)s, CUDNN_PROPAGATE_NAN, %(nd)d, win, pad, str);
  if (err != CUDNN_STATUS_SUCCESS) {
    PyErr_Format(PyExc_RuntimeError, "could not set op descriptor: %%s",
                 cudnnGetErrorString(err));
    %(fail)s
<a name="7"></a>  }
}
           nd=self.get_ndim<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(), win=', '.join(map(str, self.ws)),
           pad=', '.join(map(str, self.pad)),
           str=', '.join(map(str, self.</b></font>stride)))
    def c_code_cache_version(self):
        return (4, version())
class GpuDnnPoolBase(DnnBase):
    c_file = None
    c_function = None
    _f16_ok = True
    __props__ = ('mode',)
    check_input = False
    params_type = ParamsType(mode=cudnn.cudnnPoolingMode_t,
                             handle=handle_type)
    def __init__(self, mode='max'):
        DnnBase.__init__(self, [self.c_file], self.c_function)
        if mode == 'average':
            mode = 'average_inc_pad'
        assert cudnn.cudnnPoolingMode_t.has_alias(mode)
        self.mode = mode
class GpuDnnPool(GpuDnnPoolBase):
    c_file = "c_code/dnn_pool.c"
    c_function = "APPLY_SPECIFIC(dnn_pool)"
    def make_node(self, img, ws, stride, pad):
        ctx_name = infer_context_name(img)
        img = as_gpuarray_variable(img, ctx_name)
        ws = tensor.as_tensor_variable(ws)
        stride = tensor.as_tensor_variable(stride)
        pad = tensor.as_tensor_variable(pad)
        assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim
        assert ws.type.ndim == 1
        return Apply(self, [img, ws, stride, pad], [img.type()])
    def infer_shape(self, node, shape):
        w = node.inputs[1]
<a name="6"></a>        s = node.inputs[2]
        p = node.inputs[3]
        res <font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= [shape[0][0], shape[0][1],
               (shape[0][2] + 2 * p[0] - w[0]) // s[0] + 1,
               (shape[0][3] + 2 * p[1] - w[1]) // s[</b></font>1] + 1
               ]
        if node.inputs[0].ndim == 5:
            res.append((shape[0][4] + 2 * p[2] - w[2]) // s[2] + 1)
        return [res]
    def L_op(self, inp, outputs, grads):
        img, ws, stride, pad = inp
        grad, = grads
        grad = gpu_contiguous(grad)
        out, = outputs
        g_out = GpuDnnPoolGrad(mode=self.mode)(img, out, grad, ws, stride, pad)
        return g_out, theano.gradient.DisconnectedType()(), theano.gradient.DisconnectedType()(), theano.gradient.DisconnectedType()()
    def connection_pattern(self, node):
        return [[1], [0], [0], [0]]
class GpuDnnPoolGrad(GpuDnnPoolBase):
    c_file = "c_code/dnn_pool_grad.c"
    c_function = "APPLY_SPECIFIC(dnn_pool_grad)"
    def make_node(self, inp, out, out_grad, ws, stride, pad):
        ctx_name = infer_context_name(inp, out, out_grad)
        inp = as_gpuarray_variable(inp, ctx_name)
        assert (inp.ndim in [4, 5])
        out_grad = as_gpuarray_variable(out_grad, ctx_name)
        assert (out_grad.ndim in [4, 5])
        out = as_gpuarray_variable(out, ctx_name)
        assert(out.ndim in [4, 5])
        assert (out_grad.ndim == inp.ndim)
        assert (inp.ndim == out.ndim)
        ws = tensor.as_tensor_variable(ws)
        stride = tensor.as_tensor_variable(stride)
        pad = tensor.as_tensor_variable(pad)
        assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim
        assert ws.type.ndim == 1
        return Apply(self, [inp, out, out_grad, ws, stride, pad], [inp.type()])
    def infer_shape(self, node, shape):
        return [shape[0]]
def dnn_pool(img, ws, stride=None, mode='max', pad=None):
    img = gpu_contiguous(img)
    if stride is None:
        stride = (1,) * len(ws)
    if pad is None:
        pad = (0,) * len(ws)
    if mode == "sum":
        ret = GpuDnnPool(mode="average_inc_pad")(img, ws, stride, pad)
        context_name = ret.type.context_name
        window_elem = theano.tensor.prod(ws).astype(ret.dtype)
        return as_gpuarray_variable(ret * window_elem, context_name)
    return GpuDnnPool(mode=mode)(img, ws, stride, pad)
class GpuDnnSoftmaxBase(DnnBase):
    __props__ = ('mode', 'algo')
    check_input = False
    params_type = ParamsType(algo=cudnn.cudnnSoftmaxAlgorithm_t,
                             mode=cudnn.cudnnSoftmaxMode_t,
                             handle=handle_type)
    def __init__(self, algo, mode):
        DnnBase.__init__(self, [self.file], self.c_func)
        assert cudnn.cudnnSoftmaxAlgorithm_t.has_alias(algo)
        self.algo = algo
        assert cudnn.cudnnSoftmaxMode_t.has_alias(mode)
        self.mode = mode
    def infer_shape(self, node, shape):
        if self.direction == 'forward':
            return [shape[0]]
        else:
            return [shape[1]]
class GpuDnnSoftmax(GpuDnnSoftmaxBase):
    _f16_ok = True
    direction = "forward"
    file = "c_code/dnn_softmax.c"
    c_func = "APPLY_SPECIFIC(softmax)"
    def make_node(self, x):
        x = as_gpuarray_variable(x, infer_context_name(x))
        assert x.ndim == 4
        return Apply(self, [x], [x.type()])
    def L_op(self, inp, outputs, grads):
        x, = inp
        g_sm, = grads
        sm, = outputs
        return [GpuDnnSoftmaxGrad(
                self.algo,
                self.mode
                )(g_sm, sm)]
class GpuDnnSoftmaxGrad(GpuDnnSoftmaxBase):
    _f16_ok = True
    direction = 'backward'
    file = "c_code/dnn_softmax_grad.c"
    c_func = "APPLY_SPECIFIC(softmax_grad)"
    def make_node(self, dy, sm):
        ctx_name = infer_context_name(dy, sm)
        dy = as_gpuarray_variable(dy, ctx_name)
        sm = as_gpuarray_variable(sm, ctx_name)
        assert dy.ndim == 4
        assert sm.ndim == 4
        return Apply(self, [dy, sm], [sm.type()])
class GpuDnnReduction(DnnBase):
    check_input = False
    _f16_ok = True
    _cop_num_outputs = 2
    __props__ = ('red_op', 'axis', 'acc_dtype', 'dtype', 'return_indices')
    params_type = ParamsType(red_op=cudnn.cudnnReduceTensorOp_t,
                             acc_dtype=cudnn.cudnnDataType_t,
                             c_axis=uint32_t,
                             handle=handle_type)
    def __init__(self, red_op, axis, acc_dtype, dtype, return_indices):
        DnnBase.__init__(self, ['c_code/dnn_redux.c'], 'APPLY_SPECIFIC(dnn_redux)')
        assert cudnn.cudnnReduceTensorOp_t.has_alias(red_op)
        self.red_op = red_op
        assert acc_dtype in ['float16', 'float32', 'float64']
        self.acc_dtype = acc_dtype
        assert dtype in ['float16', 'float32', 'float64']
        self.dtype = dtype
        if axis is not None:
            if len(axis) &gt; 8:
                raise ValueError('Too many axes to reduce on')
            if any(a &gt;= 8 for a in axis):
                raise ValueError('Axes larger than 8 not supported')
            axis = tuple(axis)
        self.c_axis = self._convert_axis(axis)
        self.axis = axis
        if return_indices and (red_op != 'maximum' and red_op != 'minimum'):
            raise ValueError("Can't request indices for something other than"
                             " minimum or maximum")
        self.return_indices = return_indices
    def _convert_axis(self, axis):
        if axis is None:
            return np.uint32(-1)
        else:
            return reduce(lambda a, b: a | b, map(lambda a: 1 &lt;&lt; a, axis), 0)
    def make_node(self, inp):
        ctx_name = infer_context_name(inp)
        inp = as_gpuarray_variable(inp, ctx_name)
        inp = gpu_contiguous(inp)
        if inp.ndim &gt; 8:
            raise ValueError("cuDNN reduction doesn't support nd &gt; 8")
        assert inp.dtype in ['float16', 'float32', 'float64']
        if inp.dtype == 'float64':
            assert self.acc_dtype == 'float64'
        if inp.dtype == 'float32':
            assert self.acc_dtype == 'float32'
        if inp.dtype == 'float16':
            assert self.acc_dtype != 'float64'
        bcast = []
        for i in range(inp.ndim):
            if not (self.c_axis &amp; (1 &lt;&lt; i)):
                bcast.append(inp.broadcastable[i])
        outs = [inp.type.clone(dtype=self.dtype, broadcastable=bcast)()]
        if self.return_indices:
            outs.append(GpuArrayType(dtype='uint32', broadcastable=bcast,
                                     context_name=ctx_name)())
        return Apply(self, [inp], outs)
class GpuDnnBatchNorm(DnnBase):
    __props__ = ('mode', 'running_averages', 'inplace_running_mean',
                 'inplace_running_var', 'inplace_output')
    _cop_num_inputs = 7
    _cop_num_outputs = 5
    check_input = False
    params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
                             inplace_output=bool_t,
                             inplace_running_mean=bool_t,
                             inplace_running_var=bool_t,
                             handle=handle_type)
    def __init__(self, mode='per-activation', running_averages=False,
                 inplace_running_mean=False, inplace_running_var=False,
                 inplace_output=False):
        DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm.c'],
                         'dnn_batchnorm_op')
        assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
        self.mode = mode
        self.running_averages = running_averages
        self.inplace_output = inplace_output
        self.inplace_running_mean = inplace_running_mean
        self.inplace_running_var = inplace_running_var
        self.destroy_map = {}
        if self.inplace_output:
            self.destroy_map[0] = [0]
        if self.running_averages and self.inplace_running_mean:
            self.destroy_map[3] = [5]
        if self.running_averages and self.inplace_running_var:
            self.destroy_map[4] = [6]
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, 'running_average_factor'):
            self.running_average_factor = 0
        if not hasattr(self, 'running_averages'):
            self.running_averages = False
        if not (hasattr(self, 'inplace_running_mean') and
                hasattr(self, 'inplace_running_var') and
                hasattr(self, 'inplace_output')):
            self.inplace_running_mean = False
            self.inplace_running_var = False
            self.inplace_output = False
            self.destroy_map = {}
    def infer_shape(self, node, shape):
        return [shape[0]] + [shape[1]] * (len(node.outputs) - 1)
    def make_node(self, x, scale, bias, epsilon=1e-4,
                  running_average_factor=0.1,
                  running_mean=None, running_var=None):
        assert x.ndim == scale.ndim == bias.ndim
        assert x.ndim in (4, 5)
        assert self.running_averages == (running_mean is not None) == (running_var is not None)
        assert (running_mean is None or running_mean.ndim == x.ndim)
        assert (running_var is None or running_var.ndim == x.ndim)
        ctx_name = infer_context_name(x, scale, bias)
        x = as_gpuarray_variable(x, ctx_name)
        scale = as_gpuarray_variable(scale, ctx_name)
        bias = as_gpuarray_variable(bias, ctx_name)
        epsilon = as_scalar(epsilon).astype('float64')
        running_average_factor = as_scalar(running_average_factor).astype('float64')
        inputs = [x, scale, bias, epsilon, running_average_factor]
        output_types = [x.type(), scale.type(), scale.type()]
        if running_mean is not None and running_var is not None:
            inputs.append(as_gpuarray_variable(running_mean, ctx_name))
            inputs.append(as_gpuarray_variable(running_var, ctx_name))
            output_types.append(scale.type())
            output_types.append(scale.type())
        return Apply(self, inputs, output_types)
    def L_op(self, inputs, outputs, grads):
        x, scale, bias, epsilon, running_average_factor = inputs[:5]
        dy = grads[0]
        _, x_mean, x_invstd = outputs[:3]
        disconnected_outputs = [
            DisconnectedType()(),  # epsilon
            DisconnectedType()()]  # running_average_factor
        for i in range(5, len(inputs)):
            disconnected_outputs.append(DisconnectedType()())
        return GpuDnnBatchNormGrad(self.mode)(
            x, dy, scale, x_mean, x_invstd, epsilon) + disconnected_outputs
    def connection_pattern(self, node):
        patterns = [[True, True, True],     # x
                    [True, True, True],     # scale
                    [True, True, True],     # bias
                    [False, False, False],  # epsilon
                    [False, False, False]]  # running_average_factor
        for i in range(5, len(node.inputs)):
            patterns[0].append(True)
            for pattern in patterns[1:]:
                pattern.append(False)
            patterns.append([False] * (3 + i - 5) + [True])
        return patterns
class GpuDnnBatchNormInference(DnnBase):
    __props__ = ('mode', 'inplace')
    check_input = False
    params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
                             inplace=bool_t,
                             handle=handle_type)
    def __init__(self, mode='per-activation', inplace=False):
        DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm_inf.c'],
                         'dnn_batchnorm_op')
        assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
        self.mode = mode
        self.inplace = bool(inplace)
        if self.inplace:
            self.destroy_map = {0: [0]}
    def __setstate__(self, d):
        self.__dict__.update(d)
        if not hasattr(self, 'inplace'):
            self.inplace = False
    def infer_shape(self, node, shape):
<a name="11"></a>        return [shape[0]]
    def make_node(self, x, scale, bias, estimated_mean, estimated_variance, epsilon=1e-4):
        ctx_name <font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= infer_context_name(x, scale, bias, estimated_mean,
                                      estimated_variance)
        x = as_gpuarray_variable(x, ctx_name)
        scale = as_gpuarray_variable(scale, ctx_name)
        bias = as_gpuarray_variable(bias, ctx_name)
        estimated_mean = as_gpuarray_variable(estimated_mean, ctx_name)
        estimated_variance = as_gpuarray_variable(estimated_variance, ctx_name)
        epsilon = as_scalar(epsilon).</b></font>astype('float64')
        assert x.ndim == scale.ndim == bias.ndim == estimated_mean.ndim == estimated_variance.ndim
        assert x.ndim in (4, 5)
        return Apply(self, [x, scale, bias, estimated_mean, estimated_variance, epsilon], [x.type()])
    def grad(self, inputs, grads):
        x, scale, bias, est_mean, est_var, epsilon = inputs
        dy = grads[0]
        if self.mode == "per-activation":
            axes = (0,)
        elif self.mode == "spatial":
            axes = (0,) + tuple(range(2, x.ndim))
        scale, bias, est_mean, est_var = (theano.tensor.addbroadcast(t, *axes)
                                          for t in (scale, bias, est_mean, est_var))
        est_var_eps = est_var + epsilon
        est_std = theano.tensor.sqrt(est_var_eps)
        two = theano.tensor.constant(2.)
<a name="10"></a>
        dx = dy * (scale / est_std)
        dscale <font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (dy * (x - est_mean)).sum(axes, keepdims=True) / est_std
        dbias = dy.sum(axes, keepdims=True)
        dmean = -dy.sum(axes, keepdims=True) * (scale / est_std)
        dvar = -(dy * (x - est_mean)).sum(</b></font>axes, keepdims=True) * (scale / (two * est_var_eps * est_std))
        return [dx, dscale, dbias, dmean, dvar, DisconnectedType()()]
    def connection_pattern(self, node):
        return [[True], [True], [True], [True], [True], [False]]
class GpuDnnBatchNormGrad(DnnBase):
    __props__ = ('mode',)
    check_input = False
    params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
                             handle=handle_type)
    def __init__(self, mode='per-activation'):
        DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm_grad.c'],
                         'dnn_batchnorm_grad')
        assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
<a name="13"></a>        self.mode = mode
    def make_node(self, x, dy, scale, x_mean, x_invstd, epsilon=1e-4):
        ctx_name <font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= infer_context_name(x, dy, scale, x_mean, x_invstd)
        x = as_gpuarray_variable(x, ctx_name)
        dy = as_gpuarray_variable(dy, ctx_name)
        scale = as_gpuarray_variable(scale, ctx_name)
        x_mean = as_gpuarray_variable(x_mean, ctx_name)
        x_invstd = as_gpuarray_variable(x_invstd, ctx_name)
        epsilon = as_scalar(</b></font>epsilon).astype('float64')
        assert x.ndim == dy.ndim == scale.ndim == x_mean.ndim == x_invstd.ndim
        assert x.ndim in (4, 5)
        return Apply(self, [x, dy, scale, x_mean, x_invstd, epsilon], [x.type(), scale.type(), scale.type()])
    def infer_shape(self, node, shape):
        return [shape[0], shape[2], shape[2]]
gpudata_type = CDataType('gpudata *', 'gpudata_release')
dropoutdesc_type = CUDNNDataType('cudnnDropoutDescriptor_t',
                                 'cudnnDestroyDropoutDescriptor')
class GpuDnnDropoutOp(DnnBase):
    __props__ = ('inplace',)
    def __init__(self, inplace=False):
        DnnBase.__init__(self, ["c_code/dnn_dropout_fwd.c"], "dnn_dropout_fwd")
        self.inplace = inplace
        if self.inplace:
            self.destroy_map = {1: [2]}
    def make_node(self, inp, descriptor, state):
        ctx_name = infer_context_name(inp)
        inp = as_gpuarray_variable(inp, ctx_name)
        return Apply(self, [inp, descriptor, state],
                     [inp.type(), state.type(), gpudata_type()])
    def prepare_node(self, node, storage_map, compute_map, impl):
        assert self.inplace, "GpuDnnDropoutOp not inplace"
class _DropoutDescriptor(DnnBase):
    __props__ = ('context_name',)
    def __init__(self, context_name):
        DnnBase.__init__(self, ["c_code/dnn_dropout_desc.c"], "dnn_dropout_desc")
        self.context_name = context_name
    def dnn_context(self, node):
        return self.context_name
    def do_constant_folding(self, node):
        return False
    def make_node(self, dropout, seed, context_name):
        dropout = as_scalar(dropout).astype('float32')
        seed = as_scalar(seed).astype('uint64')
        assert context_name == self.context_name
        context = gpu_context_type.make_constant(get_context(context_name))
        return Apply(self, [dropout, seed, context],
                     [dropoutdesc_type(),
                      GpuArrayType('uint8', (False,),
                                   context_name=context_name)()])
    def c_code_cache_version_apply(self, node):
        return None
def _make_dropout_desc(dropout, seed, context_name):
    desc, states = theano.function(
        [],
        _DropoutDescriptor(context_name)(dropout, seed, context_name),
        theano.Mode(optimizer=None),
        profile=False)()
    return desc, states
def dropout(x, dropout=0.0, seed=4242):
    desc, states = _make_dropout_desc(dropout, seed, x.type.context_name)
    y, odesc = GpuDnnDropoutOp()(x, desc)
    return y, desc, odesc, states
rnndesc_type = CUDNNDataType('cudnnRNNDescriptor_t',
                             'cudnnDestroyRNNDescriptor')
def as_i32(v):
    return as_scalar(v).astype('int32')
class _RNNDescriptor(DnnBase):
    __props__ = ('context_name',)
    def __init__(self, context_name):
        if version() &lt; 5005:
            raise RuntimeError("cudnn RNN require cudnn v5 final or higher.")
        DnnBase.__init__(self, ["c_code/dnn_rnn_desc.c"], "dnn_rnn_desc")
        self.context_name = context_name
    def dnn_context(self, node):
        return self.context_name
    def do_constant_folding(self, node):
        return False
    def make_node(self, hidden_size, num_layers, ddesc, input_mode,
                  direction_mode, rnn_mode, dtype):
        hidden_size = as_i32(hidden_size)
        num_layers = as_i32(num_layers)
        if version() &lt; 5005:
            raise RuntimeError("cudnn RNN require cudnn v5 final or higher.")
        if input_mode == 'linear':
            input_mode = as_i32(0)
        elif input_mode == 'skip':
            input_mode = as_i32(1)
        else:
            raise ValueError("input_mode")
        if direction_mode == 'unidirectional':
            direction_mode = as_i32(0)
        elif direction_mode == 'bidirectional':
            direction_mode = as_i32(1)
        else:
            raise ValueError("direction_mode")
        if rnn_mode == 'rnn_relu':
            rnn_mode = as_i32(0)
        elif rnn_mode == 'rnn_tanh':
            rnn_mode = as_i32(1)
        elif rnn_mode == 'lstm':
            rnn_mode = as_i32(2)
        elif rnn_mode == 'gru':
            rnn_mode = as_i32(3)
        else:
            raise ValueError("rnn_mode")
        dtype = as_i32(gpuarray.dtype_to_typecode(dtype))
        return Apply(self, [hidden_size, num_layers,
                            dropoutdesc_type.make_constant(ddesc),
                            input_mode, direction_mode, rnn_mode, dtype],
                     [rnndesc_type()])
def _make_rnn_desc(hidden_size, num_layers, ddesc, rnn_mode,
                   input_mode, direction_mode, dtype, context_name):
    desc = theano.function(
        [],
        _RNNDescriptor(context_name)(hidden_size, num_layers, ddesc,
                                     input_mode, direction_mode,
                                     rnn_mode, dtype),
        theano.Mode(optimizer=None),
        profile=False)()
    return desc
class _RNNParamSize(DnnBase):
    __props__ = ('context_name',)
    def __init__(self, context_name):
        DnnBase.__init__(self, ["c_code/dnn_rnn_paramsize.c"],
                         "dnn_rnn_paramsize")
        self.context_name = context_name
    def dnn_context(self, node):
        return self.context_name
    def do_constant_folding(self, node):
        return False
    def make_node(self, desc, input_size, typecode):
        input_size = as_tensor_variable(input_size).astype('uint64')
        typecode = as_i32(typecode)
        return Apply(self, [rnndesc_type.make_constant(desc), input_size,
                            typecode],
                     [get_scalar_type('uint64')()])
def _get_param_size(desc, input_size, dtype, context_name):
    typecode = gpuarray.dtype_to_typecode(dtype)
    return theano.function(
        [],
        _RNNParamSize(context_name)(desc, input_size, typecode),
        theano.Mode(optimizer=None),
        profile=False)()
class _RNNSplitParams(DnnBase):
    __props__ = ('rnn_mode',)
    def __init__(self, rnn_mode):
        DnnBase.__init__(self)
        self.rnn_mode = rnn_mode
    def make_node(self, w, desc, layer, isize, typecode):
        w = as_gpuarray_variable(w, infer_context_name(w))
        assert w.ndim == 1
        layer = as_scalar(layer).astype('int32')
        isize = as_tensor_variable(isize).astype('uint64')
        assert isize.ndim == 1
        typecode = as_scalar(typecode).astype('int32')
        _1d = GpuArrayType(w.type.dtype, [False],
                           context_name=w.type.context_name)
        _2d = GpuArrayType(w.type.dtype, [False, False],
                           context_name=w.type.context_name)
        outputs = []
        if self.rnn_mode == 'rnn_relu' or self.rnn_mode == 'rnn_tanh':
            outputs.extend([_2d(), _1d()])  # recurrent
        elif self.rnn_mode == 'lstm':
            outputs<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.extend([_2d(), _1d()])  # input input
            outputs.extend([_2d(), _1d()])  # input forget
            outputs.extend([_2d(), _1d()])  # input newmem
            outputs.extend([_2d(), _1d()])  # input output
            outputs.extend([_2d(), _1d()])  # recur input
            outputs.extend([_2d(), _1d()])  # recur forget
            outputs.extend([_2d(), _1d()])  # recur output
        elif self.</b></font>rnn_mode == 'gru':
            outputs.extend<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>([_2d(), _1d()])  # input reset
            outputs.extend([_2d(), _1d()])  # input update
            outputs.extend([_2d(), _1d()])  # input newmem
            outputs.extend([_2d(), _1d()])  # recur reset
            outputs.extend([_2d(), _1d(</b></font>)])  # recur update
            outputs.extend([_2d(), _1d()])  # recur newmem
        return Apply(self, [w, layer, rnndesc_type.make_constant(desc),
                            isize, typecode], outputs)
    def c_code(self, node, name, inputs, outputs, sub):
        kw = dict(fail=sub['fail'], w=inputs[0], layer=inputs[1],
                  desc=inputs[2], isize=inputs[3], typecode=inputs[4],
                  handle=sub['params'])
        code = """
  cudnnTensorDescriptor_t xdesc;
  cudnnFilterDescriptor_t wdesc;
  cudnnFilterDescriptor_t odesc;
  size_t nshp[2];
  void *w;
  void *o;
  ptrdiff_t off;
  size_t bshp;
  cudnnStatus_t err;
  cudnnDataType_t dt;
  cudnnTensorFormat_t tf;
  int nd;
  int dims[3];
  int strs[3];
  if (PyArray_DIM(%(isize)s, 0) != 2) {
    PyErr_SetString(PyExc_ValueError, "input_size should be of length two");
    %(fail)s;
  }
  switch (%(typecode)s) {
  case GA_FLOAT:
    dt = CUDNN_DATA_FLOAT;
    break;
  case GA_DOUBLE:
    dt = CUDNN_DATA_DOUBLE;
    break;
  case GA_HALF:
    dt = CUDNN_DATA_HALF;
    break;
  default:
    PyErr_SetString(PyExc_ValueError, "Unsupported data type");
    %(fail)s;
  }
  err = cudnnCreateTensorDescriptor(&amp;xdesc);
  if (err != CUDNN_STATUS_SUCCESS) {
    PyErr_SetString(PyExc_RuntimeError, "Could not create xdesc");
    %(fail)s;
  }
  dims[0] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 0);
  dims[1] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 1);
  dims[2] = 1;
  strs[0] = dims[2] * dims[1];
  strs[1] = dims[2];
  strs[2] = 1;
  err = cudnnSetTensorNdDescriptor(xdesc, dt, 3, dims, strs);
  if (err != CUDNN_STATUS_SUCCESS) {
    cudnnDestroyTensorDescriptor(xdesc);
    PyErr_Format(PyExc_RuntimeError, "Could not set xdesc: %%s",
                 cudnnGetErrorString(err));
    %(fail)s;
  }
  if (c_make_filter(%(w)s, &amp;wdesc)) {
    cudnnDestroyTensorDescriptor(xdesc);
    %(fail)s
  }
  err = cudnnCreateFilterDescriptor(&amp;odesc);
  if (err != CUDNN_STATUS_SUCCESS) {
    PyErr_SetString(PyExc_RuntimeError, "could not create odesc");
    cudnnDestroyTensorDescriptor(xdesc);
    cudnnDestroyFilterDescriptor(wdesc);
    %(fail)s
  }
  w = PyGpuArray_DEV_DATA(%(w)s);
  nshp[0] = PyGpuArray_DIM(%(w)s, 0);
  nshp[1] = 1;
        for i in range(len(outputs) // 2):
            code += get_params(i, outputs[2 * i], outputs[(2 * i) + 1])
        code += """
  cudnnDestroyTensorDescriptor(xdesc);
  cudnnDestroyFilterDescriptor(wdesc);
  cudnnDestroyFilterDescriptor(odesc);
    An object that allow us to use CuDNN RNN implementation.
    TODO: make an example how to use. You can check Theano tests
    test_dnn_rnn_gru() and test_dnn_rnn_lstm() in the file
    theano/gpuarray/tests/test_dnn.py for now.
    Parameters
    ----------
    dtype : data type of computation
    hidden_size : int
        hidden layer dimension.
    num_layers : int
        number of the recurrent layer you want to set.
    rnn_mode : {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'}
        rnn_relu: A single-gate recurrent neural network with a ReLU activation function.
        .. math::
        h_t=ReLU(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})
        rnn_tanh: A single-gate recurrent neural network with a tanh activation function.
        .. math::
        h_t=tanh(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})
        lstm: A four-gate Long Short-Term Memory network with no peephole connections.
        gru: A three-gate network consisting of Gated Recurrent Units.
    input_mode : {'linear', 'skip'}
        linear: input will be multiplied by a biased matrix
        skip: No operation is performed on the input.  The size must match the hidden size.
    direction_mode : {'unidirectional', 'bidirectional'}
        unidirectional: The network operates recurrently from the first input to the last.
        bidirectional: The network operates from first to last then from last to first and concatenates the results at each layer.
        Get the size of the shared variable for the parameters of the RNN.
        This will return a size (in items) necessary to store all the
        parameters for the RNN.  You should allocate a variable of
        that size to store those parameters.  The order and layout of
        the parameters is opaque.
        Parameters
        ----------
        input_size: (int, int)
            Size of the input blocks
        Split the opaque parameter block into components.
        Parameters
        ----------
        w: GpuArraySharedVariable
            opaque parameter block
        layer: int
            ID of the layer
        input_size: (int, int)
            Size of the input blocks
        Apply the RNN to some data
        Parameters
        ----------
        w:
            opaque parameter block
        x:
            input
        hx:
            initial hidden state
        cx:
            initial cell state (for LSTM)
    Performs batch normalization of the given inputs, using the mean and
    variance of the inputs.
    Parameters
    ----------
    mode : {'per-activation', 'spatial'}
        Whether to normalize per activation or share normalization factors
        across spatial dimensions (i.e., all dimensions past the second).
    gamma : tensor
        Learnable scale factors. Must match the dimensionality of `inputs`,
        but have sizes of `1` for all axes normalized over (i.e., in the first
        dimension for ``mode='per-activation'`, and additionally in all
        dimensions past the second for ``mode='spatial'``).
    beta : tensor
        Learnable biases. Must match the tensor layout of `gamma`.
    epsilon : float
        Epsilon value used in the batch normalization formula. Minimum allowed
        value is 1e-5 (imposed by cuDNN).
    running_average_factor : float
        Factor for updating the values or `running_mean` and `running_var`.
        If the factor is close to one, the running averages will update quickly,
        if the factor is close to zero it will update slowly.
    running_mean : tensor or None
        Previous value of the running mean. If this is given, the new value
        ``running_mean * (1 - r_a_factor) + batch mean * r_a_factor``
        will be returned as one of the outputs of this function.
        `running_mean` and `running_var` should either both be given or
        both be None.
    running_var : tensor or None
        Previous value of the running variance. If this is given, the new value
        ``running_var * (1 - r_a_factor) + (m / (m - 1)) * batch var * r_a_factor``
        will be returned as one of the outputs of this function,
        where `m` is the product of lengths of the averaged-over dimensions.
        `running_mean` and `running_var` should either both be given or
        both be None.
    Returns
    -------
    out : tensor
        Batch-normalized inputs.
    mean : tensor
        Means of `inputs` across the normalization axes.
    invstd : tensor
        Inverse standard deviations of `inputs` across the normalization axes.
    new_running_mean : tensor
        New value of the running mean (only if both `running_mean` and
        `running_var` were given).
    new_running_var : tensor
        New value of the running variance (only if both `running_var` and
        `running_mean` were given).
    Notes
    -----
    Requires cuDNN 5 and Theano 0.9dev2 or more recent.
    For 4d tensors, returned values are equivalent to:
    .. code-block:: python
        axes = 0 if mode == 'per-activation' else (0, 2, 3)
        mean = inputs.mean(axes, keepdims=True)
        var = inputs.var(axes, keepdims=True)
        invstd = T.inv(T.sqrt(var + epsilon))
        out = (inputs - mean) * gamma * invstd + beta
        m = T.cast(T.prod(inputs.shape) / T.prod(mean.shape), 'float32')
        running_mean = running_mean * (1 - running_average_factor) + \\
                       mean * running_average_factor
        running_var = running_var * (1 - running_average_factor) + \\
                      (m / (m - 1)) * var * running_average_factor
    For 5d tensors, the axes are (0, 2, 3, 4).
        params_shape <font color="#e77471"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= gamma.shape
        inputs = theano.tensor.flatten(inputs, 5)
        gamma = theano.tensor.flatten(gamma, 5)
        beta = theano.</b></font>tensor.flatten(beta, 5)
        if running_averages:
            running_mean = theano.tensor.flatten(running_mean, 5)
            running_var = theano.tensor.flatten(running_var, 5)
    batchnorm_op = GpuDnnBatchNorm(mode=mode, running_averages=running_averages)
    if running_averages:
        out, mean, invstd, new_running_mean, new_running_var = batchnorm_op(
            gpu_contiguous(inputs), gpu_contiguous(gamma),
            gpu_contiguous(beta), epsilon=epsilon,
            running_average_factor=running_average_factor,
            running_mean=gpu_contiguous(running_mean),
            running_var=gpu_contiguous(running_var))
        if new_running_mean.broadcastable != running_mean.broadcastable:
            new_running_mean = tensor.patternbroadcast(new_running_mean, running_mean.broadcastable)
        if new_running_var.broadcastable != running_var.broadcastable:
            new_running_var = tensor.patternbroadcast(new_running_var, running_var.broadcastable)
        result = (out, mean, invstd, new_running_mean, new_running_var)
    else:
        result = batchnorm_op(gpu_contiguous(inputs), gpu_contiguous(gamma),
                              gpu_contiguous(beta), epsilon=epsilon)
    if ndim &lt; 4:
        result = tuple(theano.tensor.flatten(r, ndim) for r in result)
    elif ndim &gt; 5:
        result = (theano.tensor.reshape(result[0], inputs_shape),) + tuple(
            theano.tensor.reshape(r, params_shape) for r in result[1:])
    return result
def dnn_batch_normalization_test(inputs, gamma, beta, mean, var,
                                 mode='per-activation', epsilon=1e-4):
    ndim = inputs.ndim
    if gamma.ndim != ndim or beta.ndim != ndim:
        raise ValueError("gamma and beta must be of the same dimensionality "
                         "as inputs; got %d and %d instead of %d" %
                         (gamma.ndim, beta.ndim, ndim))
    if mean.ndim != ndim or var.ndim != ndim:
        raise ValueError("mean and var must be of the same dimensionality "
                         "as inputs; got %d and %d instead of %d" %
                         (mean.ndim, var.ndim, ndim))
    if epsilon &lt; 1e-5:
        raise ValueError("epsilon must be at least 1e-5, got %f" % epsilon)
<a name="3"></a>    if ndim &lt; 4:
        inputs = theano.tensor.shape_padright(inputs, 4 - ndim)
        gamma = theano.tensor.shape_padright(gamma, 4 - ndim)
        beta = theano.tensor.shape_padright<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(beta, 4 - ndim)
        mean = theano.tensor.shape_padright(mean, 4 - ndim)
        var = theano.tensor.shape_padright(var, 4 - ndim)
    elif ndim &gt; 5:
        inputs_shape = inputs.shape
        inputs = theano.tensor.flatten(inputs, 5)
        gamma = theano.tensor.flatten(gamma, 5)
        beta = theano.</b></font>tensor.flatten(beta, 5)
        mean = theano.tensor.flatten(mean, 5)
        var = theano.tensor.flatten(var, 5)
    batchnorm_op = GpuDnnBatchNormInference(mode=mode)
    result = batchnorm_op(gpu_contiguous(inputs), gpu_contiguous(gamma),
                          gpu_contiguous(beta), gpu_contiguous(mean),
                          gpu_contiguous(var), epsilon=epsilon)
    if ndim &lt; 4:
        result = theano.tensor.flatten(result, ndim)
    elif ndim &gt; 5:
        result = theano.tensor.reshape(result, inputs_shape)
    return result
class GpuDnnTransformerGrid(DnnBase):
    __props__ = ()
    _cop_num_inputs = 2
    _cop_num_outputs = 1
    _f16_ok = True
    check_input = False
    def __init__(self):
        DnnBase.__init__(self, ["c_code/dnn_sptf_grid.c"], "APPLY_SPECIFIC(dnn_sptf_grid)")
    def make_node(self, theta, out_dims):
        context_name = infer_context_name(theta)
        theta = gpu_contiguous(as_gpuarray_variable(theta, context_name))
        assert theta.dtype in ('float16', 'float32', 'float64')
        assert theta.ndim == 3
        out_dims = cpu_contiguous(as_tensor_variable(out_dims))
        assert out_dims.dtype in theano.tensor.basic.integer_dtypes
        assert out_dims.ndim == 1
        out_dims = theano.tensor.basic.cast(out_dims, 'int64')
        grid = GpuArrayType(dtype=theta.dtype,
                            broadcastable=(theta.type.ndim + 1) * (False,),
                            context_name=context_name)()
        inputs = [theta, out_dims]
        outputs = [grid]
        return Apply(self, inputs, outputs)
    def grad(self, inputs, grads):
        theta, out_dims = inputs
        dgrid = grads[0]
        dtheta = GpuDnnTransformerGradT()(dgrid)
        return [dtheta, grad_not_implemented(self, 1, out_dims)]
class GpuDnnTransformerSampler(DnnBase):
    __props__ = ()
    _cop_num_inputs = 2
    _cop_num_outputs = 1
    _f16_ok = True
    check_input = False
    def __init__(self):
        DnnBase.__init__(self, ["c_code/dnn_sptf_sampler.c"], "APPLY_SPECIFIC(dnn_sptf_sampler)")
    def make_node(self, img, grid):
        context_name = infer_context_name(img, grid)
        img = gpu_contiguous(as_gpuarray_variable(img, context_name))
        if img.type.ndim != 4:
            raise TypeError('img must be a 4D tensor')
        elif img.dtype not in ('float16', 'float32', 'float64'):
            raise TypeError('img type must be floating-point')
        grid = gpu_contiguous(as_gpuarray_variable(grid, context_name))
        if grid.type.ndim != 4:
            raise TypeError('grid must be a 4D tensor')
        elif grid.dtype not in ('float16', 'float32', 'float64'):
            raise TypeError('grid type must be floating-point')
        out = GpuArrayType(dtype=img.dtype,
                           broadcastable=img.type.ndim * (False,),
                           context_name=context_name)()
        inputs = [img, grid]
        outputs = [out]
        return Apply(self, inputs, outputs)
    def grad(self, inputs, grads):
        img, grid = inputs
        dy = grads[0]
        dimg, dgrid = GpuDnnTransformerGradI()(img, grid, dy)
        return [dimg, dgrid]
class GpuDnnTransformerGradI(DnnBase):
    __props__ = ()
    _cop_num_inputs = 3
    _cop_num_outputs = 2
    _f16_ok = True
    check_input = False
    def __init__(self):
        DnnBase.__init__(self, ["c_code/dnn_sptf_gi.c"], "APPLY_SPECIFIC(dnn_sptf_gi)")
    def make_node(self, img, grid, dy):
        context_name = infer_context_name(img, grid, dy)
        img = as_gpuarray_variable(gpu_contiguous(img), context_name)
        if img.ndim != 4:
            raise TypeError('img must have 4 dimensions.')
        grid = as_gpuarray_variable(gpu_contiguous(grid), context_name)
        if img.ndim != grid.ndim:
            raise TypeError('grid should have the same number of dimensions as img')
        dy = as_gpuarray_variable(dy, context_name)
        if dy.ndim != 4:
            raise TypeError('dy must have 4 dimensions.')
        dimg = img.type()
        dgrid = grid.type()
        inputs = [img, grid, dy]
        outputs = [dimg, dgrid]
        return Apply(self, inputs, outputs)
class GpuDnnTransformerGradT(DnnBase):
    __props__ = ()
    _cop_num_inputs = 1
    _cop_num_outputs = 1
<a name="26"></a>    _f16_ok = True
    check_input = False
    <font color="#68818b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def __init__(self):
        DnnBase.__init__(self, ["c_code/dnn_sptf_gt.c"], "APPLY_SPECIFIC(dnn_sptf_gt)")
    def make_node(self, dgrid):
        context_name = infer_context_name(dgrid)
        dgrid = as_gpuarray_variable(dgrid, context_name)
        assert dgrid.dtype in ('float16'</b></font>, 'float32', 'float64')
        assert dgrid.ndim == 4
        dtheta = GpuArrayType(dtype=dgrid.dtype,
                              broadcastable=(dgrid.type.ndim - 1) * (False,),
                              context_name=context_name)()
        inputs = [dgrid]
        outputs = [dtheta]
        return Apply(self, inputs, outputs)
def dnn_spatialtf(img, theta, scale_width=1, scale_height=1):
    out_dims = (<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>img.shape[0], img.shape[1],
                theano.tensor.ceil(img.shape[2] * scale_height),
                theano.tensor.</b></font>ceil(img.shape[3] * scale_width))
    out_dims = tuple([as_scalar(v).astype('int64') for v in out_dims])
    grid = GpuDnnTransformerGrid()(theta, out_dims)
    sampler = GpuDnnTransformerSampler()(img, grid)
    return sampler
def local_abstractconv_cudnn_graph(op, context_name, inputs, outputs):
    if (not isinstance(op, (AbstractConv2d,
                            AbstractConv2d_gradWeights,
                            AbstractConv2d_gradInputs))):
        return
    if version(raises=False) &lt; 6000 and op.filter_dilation != (1, 1):
        return None
    if op.unshared:
        return None
    if isinstance(op.border_mode, tuple) and any(isinstance(p, tuple) for p in op.border_mode):
        return None
    inp1 = inputs[0]
    inp2 = inputs[1]
    if not dnn_available(inp1.type.context_name):
        return
    if op.filter_flip:
        conv_mode = 'conv'
    else:
        conv_mode = 'cross'
    if isinstance(op, AbstractConv2d):
        rval = dnn_conv(inp1, inp2,
                        border_mode=op.border_mode,
                        subsample=op.subsample,
                        dilation=op.filter_dilation,
                        direction_hint='forward!',
<a name="20"></a>                        conv_mode=conv_mode,
                        num_groups=op.num_groups)
    elif isinstance(op, AbstractConv2d_gradWeights):
        shape = (<font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[1], inp1.shape[1] // op.num_groups,
                 inputs[2][0], inputs[2][1])
        rval = dnn_gradweight(inp1, inp2, shape,
                              border_mode=</b></font>op.border_mode,
                              subsample=op.subsample,
                              dilation=op.filter_dilation,
<a name="24"></a>                              conv_mode=conv_mode,
                              num_groups=op.num_groups)
    elif isinstance(op, AbstractConv2d_gradInputs):
        shape = (<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[0], inp1.shape[1] * op.num_groups,
                 inputs[2][0], inputs[2][1])
        rval = dnn_gradinput(</b></font>inp1, inp2, shape,
                             border_mode=op.border_mode,
                             subsample=op.subsample,
                             dilation=op.filter_dilation,
                             conv_mode=conv_mode,
                             num_groups=op.num_groups)
    return [rval]
def local_abstractconv3d_cudnn_graph(op, context_name, inputs, outputs):
    if (not isinstance(op, (AbstractConv3d,
                            AbstractConv3d_gradWeights,
                            AbstractConv3d_gradInputs))):
        return
    if version(raises=False) &lt; 6000 and op.filter_dilation != (1, 1, 1):
        return None
    inp1 = inputs[0]
    inp2 = inputs[1]
    if not dnn_available(inp1.type.context_name):
        return
    if op.filter_flip:
        conv_mode = 'conv'
    else:
        conv_mode = 'cross'
    if isinstance(op, AbstractConv3d):
        rval = dnn_conv3d(inp1, inp2,
                          border_mode=op.border_mode,
                          subsample=op.subsample,
                          dilation=op.filter_dilation,
                          direction_hint='forward!',
<a name="9"></a>                          conv_mode=conv_mode,
                          num_groups=op.num_groups)
    elif isinstance(op, AbstractConv3d_gradWeights):
        shape = (<font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[1], inp1.shape[1] // op.num_groups,
                 inputs[2][0], inputs[2][1], inputs[2][2])
        rval = dnn_gradweight3d(inp1, inp2, shape,
                                border_mode=</b></font>op.border_mode,
                                subsample=op.subsample,
                                dilation=op.filter_dilation,
<a name="8"></a>                                conv_mode=conv_mode,
                                num_groups=op.num_groups)
    elif isinstance(op, AbstractConv3d_gradInputs):
        shape = (<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[0], inp1.shape[1] * op.num_groups,
                 inputs[2][0], inputs[2][1], inputs[2][2])
        rval = dnn_gradinput3d(inp1, inp2, shape,
                               border_mode=</b></font>op.border_mode,
                               subsample=op.subsample,
                               dilation=op.filter_dilation,
                               conv_mode=conv_mode,
                               num_groups=op.num_groups)
    return [rval]
@local_optimizer([AbstractConv2d, AbstractConv3d])
def local_abstractconv_cudnn(node):
    ctx = infer_context_name(*node.inputs)
    if not isinstance(node.inputs[0].type, GpuArrayType):
        return
    if node.op.unshared:
        return None
    if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
        return None
    if isinstance(node.op, AbstractConv2d):
        with inherit_stack_trace(node.outputs):
            return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
    elif isinstance(node.op, AbstractConv3d):
        with inherit_stack_trace(node.outputs):
            return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
@local_optimizer([AbstractConv2d, AbstractConv2d_gradWeights, AbstractConv2d_gradInputs])
def local_abstractconv_cudnn_alt(node):
    if(not isinstance(node.op, (AbstractConv2d, AbstractConv2d_gradWeights,
       AbstractConv2d_gradInputs))):
        return
    if version(raises=False) &lt; 6000 and node.op.filter_dilation != (1, 1):
        return None
    if node.op.unshared:
        return None
    if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
        return None
    inp1 = node.inputs[0]
    inp2 = node.inputs[1]
<a name="5"></a>    if not dnn_available(inp1.type.context_name):
        return
    op = node<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op
    border_mode = node.op.border_mode
    subsample = node.op.subsample
    filter_dilation = node.op.filter_dilation
    num_groups = node.op.num_groups
    precision, _ = get_precision(None, [inp1</b></font>, inp2])
    if node.op.filter_flip:
        conv_mode = 'conv'
    else:
        conv_mode = 'cross'
    if isinstance(op, AbstractConv2d):
        if border_mode == 'half' or subsample != (1, 1) or num_groups != 1:
            return None
        if border_mode == 'full':
            direction_hint = 'bprop inputs'
        elif border_mode == 'valid' and filter_dilation == (1, 1):
            direction_hint = 'bprop weights'
        else:
            return None
        rval = dnn_conv(inp1, inp2,
                        border_mode=border_mode,
                        subsample=subsample,
                        dilation=filter_dilation,
                        direction_hint=direction_hint,
                        conv_mode=conv_mode,
<a name="19"></a>                        num_groups=num_groups)
    elif isinstance(op, AbstractConv2d_gradWeights):
        if(<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>border_mode == 'valid' and subsample == (1, 1) and
           filter_dilation == (1, 1) and num_groups == 1):
            img = gpu_contiguous(inp1)
            topgrad = gpu_contiguous(inp2)
            ctx_name = infer_context_name(img, topgrad)
            img = gpu_contiguous(img.dimshuffle(</b></font>1, 0, 2, 3))
            topgrad = gpu_contiguous(topgrad.dimshuffle(1, 0, 2, 3))
            ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
            tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
            out_shp = get_conv_output_shape(ishape,
                                            tshape,
                                            border_mode=border_mode,
                                            subsample=subsample,
                                            filter_dilation=filter_dilation)
            out_shp = assert_conv_shape(out_shp)
            out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
            desc = GpuDnnConvDesc(border_mode=border_mode,
                                  subsample=subsample,
                                  dilation=filter_dilation,
                                  conv_mode='cross',
                                  precision=precision)(out.shape)
            conv = GpuDnnConv(algo=None, num_groups=num_groups)(img, topgrad, out, desc)
            if conv_mode == 'conv':
                conv = conv[:, :, ::-1, ::-1]
            rval = as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)
        else:
            return None
    elif isinstance(op, AbstractConv2d_gradInputs):
        if border_mode == 'valid' and subsample == (1, 1) and num_groups == 1:
            kerns = gpu_contiguous(inp1.dimshuffle(1, 0, 2, 3))
            topgrad = gpu_contiguous(inp2)
            ctx_name = infer_context_name(kerns, topgrad)
            conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
            desc = GpuDnnConvDesc(border_mode='full',
                                  subsample=subsample,
                                  dilation=filter_dilation,
                                  conv_mode=conv_mode,
                                  precision=precision)(kerns.shape)
            tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
            kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
            shape = get_conv_output_shape(tshape,
                                          kshape,
                                          border_mode='full',
                                          subsample=subsample,
                                          filter_dilation=filter_dilation)
            shape = assert_conv_shape(shape)
            out = GpuAllocEmpty(dtype=topgrad.dtype, context_name=ctx_name)(*shape)
            rval = GpuDnnConv(algo=None, num_groups=num_groups)(topgrad, kerns, out, desc)
        else:
            return None
    return [rval]
@local_optimizer([AbstractConv3d, AbstractConv3d_gradWeights, AbstractConv3d_gradInputs])
def local_abstractconv3d_cudnn_alt(node):
    if(not isinstance(node.op, (AbstractConv3d,
                                AbstractConv3d_gradWeights,
                                AbstractConv3d_gradInputs))):
        return
    if version(raises=False) &lt; 6000 and node.op.filter_dilation != (1, 1, 1):
        return None
    inp1 = node.inputs[0]
    inp2 = node.inputs[1]
<a name="12"></a>    if not dnn_available(inp1.type.context_name):
        return
    op = node<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op
    border_mode = node.op.border_mode
    subsample = node.op.subsample
    filter_dilation = node.op.filter_dilation
    num_groups = node.op.num_groups
    precision, _ =</b></font> get_precision(None, [inp1, inp2])
    if node.op.filter_flip:
        conv_mode = 'conv'
    else:
        conv_mode = 'cross'
    if isinstance(op, AbstractConv3d):
        if border_mode == 'half' or subsample != (1, 1, 1) or num_groups &gt; 1:
            return None
        if border_mode == 'full':
            direction_hint = 'bprop inputs'
        elif border_mode == 'valid' and filter_dilation == (1, 1, 1):
            direction_hint = 'bprop weights'
        else:
            return None
        rval = dnn_conv3d(inp1, inp2,
                          border_mode=border_mode,
                          subsample=subsample,
                          dilation=filter_dilation,
                          direction_hint=direction_hint,
<a name="18"></a>                          conv_mode=conv_mode)
    elif isinstance(op, AbstractConv3d_gradWeights):
        if(<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>border_mode == 'valid' and subsample == (1, 1, 1) and
           filter_dilation == (1, 1, 1) and num_groups == 1):
            img = gpu_contiguous(inp1)
            topgrad = gpu_contiguous(inp2)
            ctx_name = infer_context_name(img, topgrad)
            img = gpu_contiguous(img.dimshuffle(</b></font>1, 0, 2, 3, 4))
            topgrad = gpu_contiguous(topgrad.dimshuffle(1, 0, 2, 3, 4))
            ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
            tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
            out_shp = get_conv_output_shape(ishape,
                                            tshape,
                                            border_mode=border_mode,
                                            subsample=subsample,
                                            filter_dilation=filter_dilation)
            out_shp = assert_conv_shape(out_shp)
            out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
            desc = GpuDnnConvDesc(border_mode=border_mode,
                                  subsample=subsample,
                                  dilation=filter_dilation,
                                  conv_mode='cross',
                                  num_groups=num_groups,
                                  precision=precision)(out.shape)
            conv = GpuDnnConv(algo=None, num_groups=num_groups)(
                img, topgrad, out, desc)
            if conv_mode == 'conv':
                conv = conv[:, :, ::-1, ::-1, ::-1]
            rval = as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)
        else:
            return None
    elif isinstance(op, AbstractConv3d_gradInputs):
        if border_mode == 'valid' and subsample == (1, 1, 1) and num_groups == 1:
            kerns = gpu_contiguous(inp1.dimshuffle(1, 0, 2, 3, 4))
            topgrad = gpu_contiguous(inp2)
            ctx_name = infer_context_name(kerns, topgrad)
            conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
            desc = GpuDnnConvDesc(border_mode='full',
                                  subsample=subsample,
                                  dilation=filter_dilation,
                                  conv_mode=conv_mode,
                                  num_groups=num_groups,
                                  precision=precision)(kerns.shape)
            tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
            kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
            shape = get_conv_output_shape(tshape,
                                          kshape,
                                          border_mode='full',
                                          subsample=subsample,
                                          filter_dilation=filter_dilation)
            shape = assert_conv_shape(shape)
            out = GpuAllocEmpty(dtype=topgrad.dtype, context_name=ctx_name)(*shape)
            rval = GpuDnnConv(algo=None, num_groups=num_groups)(
                topgrad, kerns, out, desc)
        else:
            return None
    return [rval]
@local_optimizer([AbstractConv2d_gradWeights, AbstractConv3d_gradWeights])
def local_abstractconv_gw_cudnn(node):
    ctx = infer_context_name(*node.inputs)
    if not isinstance(node.inputs[0].type, GpuArrayType):
        return
    if node.op.unshared:
        return None
    if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
        return None
    if isinstance(node.op, AbstractConv2d_gradWeights):
        with inherit_stack_trace(node.outputs):
            return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
    elif isinstance(node.op, AbstractConv3d_gradWeights):
        with inherit_stack_trace(node.outputs):
            return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
@local_optimizer([AbstractConv2d_gradInputs, AbstractConv3d_gradInputs])
def local_abstractconv_gi_cudnn(node):
    ctx = infer_context_name(*node.inputs)
    if not isinstance(node.inputs[0].type, GpuArrayType):
        return
    if node.op.unshared:
        return None
    if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
        return None
    if isinstance(node.op, AbstractConv2d_gradInputs):
        with inherit_stack_trace(node.outputs):
            return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
    elif isinstance(node.op, AbstractConv3d_gradInputs):
        with inherit_stack_trace(node.outputs):
            return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
@inplace_allocempty(GpuDnnConv, 2)
def local_dnn_conv_inplace(node, inputs):
    return [GpuDnnConv(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
@inplace_allocempty(GpuDnnConvGradW, 2)
def local_dnn_convgw_inplace(node, inputs):
    return [GpuDnnConvGradW(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
@inplace_allocempty(GpuDnnConvGradI, 2)
def local_dnn_convgi_inplace(node, inputs):
    return [GpuDnnConvGradI(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
optdb.register('local_dnna_conv_inplace',
               tensor.opt.in2out(local_dnn_conv_inplace,
                                 local_dnn_convgw_inplace,
                                 local_dnn_convgi_inplace,
                                 name="local_dnna_conv_inplace"),
               70.0, 'fast_run', 'inplace', 'gpuarray', 'cudnn')
@register_opt('cudnn')
@alpha_merge(GpuDnnConv, alpha_in=4, beta_in=5)
def local_dnn_conv_alpha_merge(node, *inputs):
    return [GpuDnnConv(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
@register_opt('cudnn')
@alpha_merge(GpuDnnConvGradW, alpha_in=4, beta_in=5)
def local_dnn_convw_alpha_merge(node, *inputs):
    return [GpuDnnConvGradW(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
@register_opt('cudnn')
@alpha_merge(GpuDnnConvGradI, alpha_in=4, beta_in=5)
def local_dnn_convi_alpha_merge(node, *inputs):
    return [GpuDnnConvGradI(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
@register_opt('cudnn')
@output_merge(GpuDnnConv, alpha_in=4, beta_in=5, out_in=2)
def local_dnn_conv_output_merge(node, *inputs):
    inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
    return [GpuDnnConv(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
@register_opt('cudnn')
@output_merge(GpuDnnConvGradW, alpha_in=4, beta_in=5, out_in=2)
def local_dnn_convw_output_merge(node, *inputs):
    inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
    return [GpuDnnConvGradW(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
@register_opt('cudnn')
@output_merge(GpuDnnConvGradI, alpha_in=4, beta_in=5, out_in=2)
def local_dnn_convi_output_merge(node, *inputs):
    inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
    return [GpuDnnConvGradI(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
def local_gpua_pool_dnn_alternative(op, ctx_name, inputs, outputs):
    if not dnn_available(ctx_name):
        return
    if not op.ignore_border:
        return
    img, ws, stride, pad = inputs
    nd = op.ndim
    if nd not in (2, 3):
        return
    img = gpu_contiguous(as_gpuarray_variable(img, ctx_name))
    mode = op.mode
    if img.ndim == nd + 2:
        return dnn_pool(img, ws, stride=stride, pad=pad, mode=mode)
    else:
        img_padded = pad_dims(img, 2, nd)
        ret_padded = dnn_pool(img_padded, ws, stride=stride, pad=pad, mode=mode)
        return unpad_dims(ret_padded, img, 2, nd)
pool_db.register("local_gpua_pool_dnn_alternative",
                 op_lifter([Pool])(local_gpua_pool_dnn_alternative),
                 'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
                 position=0)
pool_db2.register("local_gpua_pool_dnn_alternative",
                  local_optimizer([Pool])(local_gpua_pool_dnn_alternative),
                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
                  position=0)
def local_gpua_pool_dnn_grad_stride(op, ctx_name, inputs, outputs):
    if not dnn_available(ctx_name):
        return
    if not op.ignore_border:
        return
    inp, out, out_grad, ws, stride, pad = inputs
    nd = op.ndim
    if nd not in (2, 3):
        return
    inp = gpu_contiguous(as_gpuarray_variable(inp, ctx_name))
    out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
    out_grad = gpu_contiguous(as_gpuarray_variable(out_grad, ctx_name))
    mode = op.mode
    if inp.ndim == nd + 2:
        return GpuDnnPoolGrad(mode=mode)(inp,
                                         out,
                                         out_grad,
                                         ws,
                                         stride,
                                         pad)
    else:
        inp_padded = pad_dims(inp, 2, nd)
        out_padded = pad_dims(out, 2, nd)
        out_grad_padded = pad_dims(out_grad, 2, nd)
        ret_padded = GpuDnnPoolGrad(mode=mode)(inp_padded,
                                               out_padded,
                                               out_grad_padded,
                                               ws,
                                               stride,
                                               pad)
        return unpad_dims(ret_padded, inp, 2, nd)
pool_db.register("local_gpua_pool_dnn_grad_stride",
                 op_lifter([MaxPoolGrad])(local_gpua_pool_dnn_grad_stride),
                 'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
                 position=0)
pool_db2.register("local_gpua_pool_dnn_grad_stride",
                  local_optimizer([MaxPoolGrad])(local_gpua_pool_dnn_grad_stride),
                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
                  position=0)
def local_gpua_avg_pool_dnn_grad_stride(op, ctx_name, inputs, outputs):
    if not dnn_available(ctx_name):
        return
    if not op.ignore_border:
        return
    inp, out_grad, ws, stride, pad = inputs
    nd = op.ndim
    if nd not in (2, 3):
        return
    inp = gpu_contiguous(as_gpuarray_variable(inp, ctx_name))
    out_grad = gpu_contiguous(as_gpuarray_variable(out_grad, ctx_name))
    mode = op.mode
    if inp.ndim == nd + 2:
        return GpuDnnPoolGrad(mode=mode)(inp, out_grad, out_grad, ws, stride, pad)
    else:
        inp_padded = pad_dims(inp, 2, nd)
        out_grad_padded = pad_dims(out_grad, 2, nd)
        ret_padded = GpuDnnPoolGrad(mode=mode)(inp_padded,
                                               out_grad_padded,
                                               out_grad_padded,
                                               ws,
                                               stride,
                                               pad)
        return unpad_dims(ret_padded, inp, 2, nd)
pool_db.register("local_gpua_avg_pool_dnn_grad_stride",
                 op_lifter([AveragePoolGrad])(local_gpua_avg_pool_dnn_grad_stride),
                 'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
                 position=0)
pool_db2.register("local_gpua_avg_pool_dnn_grad_stride",
                  local_optimizer([AveragePoolGrad])(local_gpua_avg_pool_dnn_grad_stride),
                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
                  position=0)
@register_opt('cudnn', 'fast_compile')
@local_optimizer([GpuSoftmax])
def local_softmax_dnn(node):
    if isinstance(node.op, GpuSoftmax):
        if not dnn_available(node.outputs[0].type.context_name):
            return
        ins = node.inputs[0].dimshuffle(0, 1, 'x', 'x')
        ins = gpu_contiguous(ins)
        out = GpuDnnSoftmax('accurate', 'channel')(ins)
        out = as_gpuarray_variable(out.dimshuffle(0, 1), out.type.context_name)
        return [out]
@register_opt('cudnn', 'stabilize')
<a name="23"></a>@local_optimizer([GpuElemwise])
def local_log_softmax_dnn(node):
    if (isinstance(node<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op, GpuElemwise) and
            isinstance(node.op.scalar_op, Log) and
            node.inputs[0].owner and
            isinstance(node.inputs[0].owner.</b></font>op, GpuDnnSoftmax) and
            len(node.inputs[0].clients) == 1):
        softmax_node = node.inputs[0].owner
        new_softmax = GpuDnnSoftmax('log', softmax_node.op.mode)
        return [new_softmax(softmax_node.inputs[0])]
@register_opt('cudnn', 'fast_compile')
@op_lifter([LogSoftmax])
@register_opt2([LogSoftmax], 'fast_compile', 'cudnn')
def local_gpua_logsoftmax_to_dnn(op, ctx_name, inputs, outputs):
    inp = inputs[0]
    if inp.ndim != 2:
        return
    if not dnn_available(ctx_name):
        return
    inp = inp.dimshuffle(0, 1, 'x', 'x')
    inp.tag.context_name = ctx_name
    out = GpuDnnSoftmax('log', 'channel')(gpu_contiguous(inp))
    return [out.dimshuffle(0, 1)]
@register_opt('cudnn', 'fast_compile')
@op_lifter([SoftmaxGrad])
@register_opt2([SoftmaxGrad], 'cudnn', 'fast_compile')
def local_gpua_softmax_dnn_grad(op, ctx_name, inputs, outputs):
    if not dnn_available(ctx_name):
        return
    ins = []
    for n in inputs:
        n = as_gpuarray_variable(n, ctx_name)
        if n.ndim != 2:
            return
        ins.append(n.dimshuffle(0, 'x', 1, 'x'))
    out = GpuDnnSoftmaxGrad('accurate', 'instance')(
        gpu_contiguous(ins[0]), gpu_contiguous(ins[1]))
    return [out.dimshuffle(0, 2)]
@register_opt('cudnn')
@local_optimizer([GpuCAReduceCuda])
def local_dnn_reduction(node):
    if not isinstance(node.op, GpuCAReduceCuda):
        return
    if not dnn_available(node.inputs[0].type.context_name):
        return
    if version(raises=False) &lt; 6000:
        return
    if node.inputs[0].ndim &gt; 8:
        return
    acc_dtype = node.op._acc_dtype(node.inputs[0].dtype)
    if node.inputs[0].dtype != node.outputs[0].dtype:
        if (node.inputs[0].dtype == 'float64' or
                node.outputs[0].dtype == 'float64'):
            return
        if acc_dtype != 'float32':
            return
    if node.inputs[0].dtype not in ['float16', 'float32', 'float64']:
        return
    if (node.inputs[0].dtype == 'float64' and acc_dtype != 'float64'):
        return
    if (node.inputs[0].dtype == 'float32' and acc_dtype != 'float32'):
        return
    if (node.inputs[0].dtype == 'float16' and acc_dtype == 'float64'):
        return
    def _identity(a):
        return a
    def _square(a):
        return GpuElemwise(theano.scalar.basic.sqr)(a)
    scal = node.op.scalar_op.name
    post = _identity
    if node.op.pre_scalar_op is not None:
        if isinstance(node.op.scalar_op, theano.scalar.basic.Add):
            if isinstance(node.op.pre_scalar_op, theano.scalar.basic.Sqr):
                scal = 'norm2'
                post = _square
            elif isinstance(node.op.pre_scalar_op, theano.scalar.basic.Abs):
<a name="17"></a>                scal = 'norm1'
            else:
                return
        elif (<font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>isinstance(node.op.scalar_op, theano.scalar.basic.Maximum) and
                isinstance(node.op.pre_scalar_op, theano.scalar.basic.</b></font>Abs)):
            scal = 'absmax'
        else:
            return
    if not cudnn.cudnnReduceTensorOp_t.has_alias(scal):
        return
    with inherit_stack_trace(node.outputs):
        ret = GpuDnnReduction(scal,
                              node.op.axis,
                              acc_dtype,
                              node.op.dtype,
                              False)(node.inputs[0])
        return [post(ret)]
@register_opt('cudnn')
@local_optimizer([GpuMaxAndArgmax])
def local_cudnn_maxandargmax(node):
    if not isinstance(node.op, GpuMaxAndArgmax):
        return
    if not dnn_available(node.inputs[0].type.context_name):
        return
    if version(raises=False) &lt; 6000:
        return
    if node.inputs[0].ndim &gt; 8:
        return
    if node.inputs[0].dtype != node.outputs[0].dtype:
        return
    if node.inputs[0].dtype not in ['float16', 'float32', 'float64']:
        return
    if (node.op.axis is not None and
<a name="22"></a>            tuple(sorted(node.op.axis)) != node.op.axis):
        return
    max, arg = GpuDnnReduction<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('maximum', node.op.axis, node.outputs[0].dtype,
                               node.outputs[0].dtype, True)(node.inputs[</b></font>0])
    return (max, as_gpuarray_variable(arg.astype('int64'),
                                      node.outputs[1].type.context_name))
@register_opt('cudnn', 'fast_compile')
@op_lifter([Argmax])
@register_opt2([Argmax], 'fast_compile', 'cudnn')
def local_dnn_argmax(op, ctx_name, inputs, outputs):
    if not dnn_available(ctx_name):
        return
    if version(raises=False) &lt; 6000:
        return
    if inputs[0].ndim &gt; 8:
        return
    if inputs[0].dtype not in ['float16', 'float32', 'float64']:
        return
    if op.axis is not None and tuple(sorted(op.axis)) != op.axis:
        return
    max, arg = GpuDnnReduction('maximum', op.axis, inputs[0].dtype,
                               inputs[0].dtype, True)(*inputs)
    return [as_gpuarray_variable(arg.astype('int64'), ctx_name)]
class NoCuDNNRaise(Optimizer):
    def apply(self, fgraph):
        for c in list_contexts():
            if not dnn_available(c):
                raise AssertionError(
                    "cuDNN optimization was enabled, but Theano was not able "
                    "to use it for context " + str(c) + ". We got this error: \n" +
                    dnn_available.msg)
gpu_seqopt.register("NoCuDNNRaise", NoCuDNNRaise(), 0, 'cudnn')
def local_abstract_batch_norm_train_cudnn(op, ctx_name, inputs, outputs):
    x, scale, bias, epsilon, running_average_factor = inputs[:5]
    running_mean = inputs[5] if len(inputs) &gt; 5 else None
    running_var = inputs[6] if len(inputs) &gt; 6 else None
    axes = tuple(op.axes)
    if axes == (0,):
        mode = 'per-activation'
    elif axes == (0,) + tuple(range(2, x.ndim)):
        mode = 'spatial'
    else:
        return None
    try:
        eps = theano.tensor.get_scalar_constant_value(epsilon)
    except theano.tensor.NotScalarConstantError:
        return None
    if eps &lt; 1e-5:
        return None
    try:
        running_average_factor = theano.tensor.get_scalar_constant_value(running_average_factor)
    except theano.tensor.NotScalarConstantError:
        return None
    ctx = infer_context_name(*inputs)
    if not dnn_available(ctx):
        return
    x = as_gpuarray_variable(x, context_name=ctx)
    scale = as_gpuarray_variable(scale, context_name=ctx)
    bias = as_gpuarray_variable(bias, context_name=ctx)
    inputs = [x, scale, bias, mode, eps, running_average_factor]
    if running_mean is not None and running_var is not None:
        inputs.append(running_mean)
        inputs.append(running_var)
    results = list(dnn_batch_normalization_train(*inputs))
    return results
@register_inplace()
@local_optimizer([GpuDnnBatchNorm], inplace=True)
def local_batch_norm_inplace_output(node):
    if isinstance(node.op, GpuDnnBatchNorm) and not node.op.inplace_output:
        return GpuDnnBatchNorm(mode=node.op.mode,
                               running_averages=node.op.running_averages,
                               inplace_running_mean=node.op.inplace_running_mean,
                               inplace_running_var=node.op.inplace_running_var,
                               inplace_output=True)(*node.inputs)
@register_inplace()
@local_optimizer([GpuDnnBatchNorm], inplace=True)
def local_batch_norm_inplace_running_mean(node):
    if isinstance(node.op, GpuDnnBatchNorm) and node.op.running_averages and not node.op.inplace_running_mean:
        return GpuDnnBatchNorm(mode=node.op.mode,
                               running_averages=node.op.running_averages,
                               inplace_running_mean=True,
                               inplace_running_var=node.op.inplace_running_var,
                               inplace_output=node.op.inplace_output)(*node.inputs)
@register_inplace()
@local_optimizer([GpuDnnBatchNorm], inplace=True)
def local_batch_norm_inplace_running_var(node):
    if isinstance(node.op, GpuDnnBatchNorm) and node.op.running_averages and not node.op.inplace_running_var:
        return GpuDnnBatchNorm(mode=node.op.mode,
                               running_averages=node.op.running_averages,
                               inplace_running_mean=node.op.inplace_running_mean,
                               inplace_running_var=True,
                               inplace_output=node.op.inplace_output)(*node.inputs)
@register_inplace()
@local_optimizer([GpuDnnBatchNormInference], inplace=True)
def local_batch_norm_inference_inplace(node):
    if isinstance(node.op, GpuDnnBatchNormInference) and not node.op.inplace:
        return [GpuDnnBatchNormInference(mode=node.op.mode, inplace=True)(*node.inputs)]
def local_abstract_batch_norm_train_grad_cudnn(op, ctx_name, inputs, outputs):
    x, dy, scale, x_mean, x_invstd, epsilon = inputs
    x_on_gpu = (isinstance(x.type, GpuArrayType) or
                (x.owner and isinstance(x.owner.op, HostFromGpu)))
    dy_on_gpu = (isinstance(dy.type, GpuArrayType) or
                 (dy.owner and isinstance(dy.owner.op, HostFromGpu)))
    if not (x_on_gpu or dy_on_gpu):
        return None
    axes = tuple(op.axes)
    if axes == (0,):
        mode = 'per-activation'
    elif axes == (0,) + tuple(range(2, x.ndim)):
        mode = 'spatial'
    else:
        return None
<a name="4"></a>
    ndim = x.ndim
    if ndim &lt; 4:
        x <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.shape_padright(x, 4 - ndim)
        dy = theano.tensor.shape_padright(dy, 4 - ndim)
        scale = theano.tensor.shape_padright(scale, 4 - ndim)
        x_mean = theano.tensor.shape_padright(x_mean, 4 - ndim)
        x_invstd =</b></font> theano.tensor.shape_padright(x_invstd, 4 - ndim)
<a name="16"></a>    elif ndim &gt; 5:
        x_shape = x.shape
        params_shape = scale.shape
        x <font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.flatten(x, 5)
        dy = theano.tensor.flatten(dy, 5)
        scale = theano.tensor.flatten(scale, 5)
        x_mean =</b></font> theano.tensor.flatten(x_mean, 5)
        x_invstd = theano.tensor.flatten(x_invstd, 5)
    try:
        eps = theano.tensor.get_scalar_constant_value(epsilon)
    except theano.tensor.NotScalarConstantError:
        return None
    if eps &lt; 1e-5:
        return None
<a name="21"></a>    ctx = infer_context_name(*inputs)
    if not dnn_available(ctx):
        return
    x <font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= as_gpuarray_variable(x, context_name=ctx)
    dy = as_gpuarray_variable(dy, context_name=ctx)
    scale = as_gpuarray_variable(scale, context_name=ctx)
    x_mean = as_gpuarray_variable(x_mean, context_name=</b></font>ctx)
    x_invstd = as_gpuarray_variable(x_invstd, context_name=ctx)
    g_wrt_inputs, g_wrt_scale, g_wrt_bias = \
<a name="15"></a>        GpuDnnBatchNormGrad(mode)(x, dy, scale, x_mean, x_invstd, eps)
    if ndim &lt; 4:
        g_wrt_inputs = theano.tensor<font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.flatten(g_wrt_inputs, ndim)
        g_wrt_scale = theano.tensor.flatten(g_wrt_scale, ndim)
        g_wrt_bias = theano.tensor.flatten(g_wrt_bias, ndim)
    elif ndim &gt; 5:
        g_wrt_inputs = theano.tensor.</b></font>reshape(g_wrt_inputs, x_shape)
        g_wrt_scale = theano.tensor.reshape(g_wrt_scale, params_shape)
        g_wrt_bias = theano.tensor.reshape(g_wrt_bias, params_shape)
    return [g_wrt_inputs, g_wrt_scale, g_wrt_bias]
def local_abstract_batch_norm_inference_cudnn(op, ctx_name, inputs, outputs):
    x, scale, bias, estimated_mean, estimated_variance, epsilon = inputs
    axes = tuple(op.axes)
    if axes == (0,):
        mode = 'per-activation'
    elif axes == (0,) + tuple(range(2, x.ndim)):
        mode = 'spatial'
    else:
        return None
    try:
        eps = theano.tensor.get_scalar_constant_value(epsilon)
    except theano.tensor.NotScalarConstantError:
        return None
    if eps &lt; 1e-5:
        return None
    ctx = infer_context_name(*inputs)
    if not dnn_available(ctx):
        return
    x = as_gpuarray_variable(x, context_name=ctx)
    scale = as_gpuarray_variable(scale, context_name=ctx)
    bias = as_gpuarray_variable(bias, context_name=ctx)
    estimated_mean = as_gpuarray_variable(estimated_mean, context_name=ctx)
    estimated_variance = as_gpuarray_variable(estimated_variance, context_name=ctx)
    out = dnn_batch_normalization_test(x, scale, bias, estimated_mean, estimated_variance,
                                       mode, eps)
    return [out]
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_basic_3.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
<a name="1"></a><font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
from copy import copy, deepcopy
from functools import partial
import itertools
import logging
from nose.plugins.skip import SkipTest
from nose.tools import assert_raises
import operator
import os
import sys
from tempfile import mkstemp
import unittest
import warnings
from six import iteritems
from six.moves import StringIO, reduce
from six.moves import xrange
from six.moves.builtins import min as builtin_min
import numpy as np
from numpy.testing import dec, assert_array_equal, assert_allclose
import theano
from theano.compat import izip
from theano.compat import PY3, exc_message, operator_div
from theano import compile, config, function, gof, tensor, shared
from theano.compile import DeepCopyOp
from theano.compile.mode import get_default_mode
from theano.scalar import autocast_float_as, autocast_float
from theano.tensor import (
    wvector, bvector,
    argmin, max_and_argmax, cscalar, join,
    horizontal_stack, vertical_stack, argmax, get_vector_length,
    fscalar, sum, tensor3, vector, add, addbroadcast,
    alloc, as_tensor_variable, tensor_from_scalar, ARange,
    clip, constant, default, diag, dot, batched_dot,
    dmatrix, dscalar, dvector, eq, eye, fill, flatten, inverse_permutation,
    tensor4, permute_row_elements, fmatrix, fscalars, grad,
    inplace, iscalar, matrix, minimum, matrices, maximum, mul, neq,
    Reshape, row, scalar, scalars, second, smallest, stack, sub, Tensor,
    tensor_copy, tensordot, TensorType, Tri, tri, tril, triu, unbroadcast,
    var, Argmax, Join, shape, MaxAndArgmax, lscalar, zvector, exp,
    get_scalar_constant_value, ivector, reshape, scalar_from_tensor, scal,
    iscalars, arange, dscalars, fvector, imatrix, numeric_grad,
    opt, lvector, true_div, max, min, Split, roll,
    tile, patternbroadcast, Eye, Shape, Dot, PermuteRowElements,
    ScalarFromTensor, TensorFromScalar, dtensor4, Rebroadcast, Alloc,
    dtensor3, SpecifyShape, Mean,
    itensor3, Tile, switch, ExtractDiag, AllocDiag,
    nonzero, flatnonzero, nonzero_values,
    stacklists, DimShuffle, hessian, ptp, power,
    swapaxes, choose, Choose, NoneConst, AllocEmpty,
    isclose, allclose, mgrid, ogrid, extract_constant,
    )
from theano.tests import unittest_tools as utt
from theano.tests.unittest_tools import attr
from theano import change_flags
imported_scipy_special =</b></font> False
mode_no_scipy = get_default_mode()
try:
    import scipy.special
    import scipy.stats
    imported_scipy_special = True
except ImportError:
    if config.mode == "FAST_COMPILE":
        mode_no_scipy = "FAST_RUN"
floatX = config.floatX
if config.mode == "FAST_COMPILE":
    mode_opt = "FAST_RUN"
else:
    mode_opt = get_default_mode()
utt.seed_rng()
test_rng = np.random.RandomState(seed=utt.fetch_seed())
if PY3:
    def L(i):
        return i
else:
    def L(i):
        return long(i)  # noqa for Python 3
def inplace_func(inputs, outputs, mode=None, allow_input_downcast=False,
                 on_unused_input='raise', name=None):
    if mode is None:
        mode = get_default_mode()
    return function(inputs, outputs,
                    mode=mode,
                    allow_input_downcast=allow_input_downcast,
                    accept_inplace=True,
                    on_unused_input=on_unused_input,
                    name=name)
def eval_outputs(outputs, ops=(), mode=None):
    f = inplace_func([], outputs, mode=mode)
    variables = f()
    if ops:
        assert any(isinstance(node.op, ops) for node in f.maker.fgraph.apply_nodes)
    if isinstance(variables, (tuple, list)) and len(variables) == 1:
        return variables[0]
    return variables
def get_numeric_subclasses(cls=np.number, ignore=None):
    if ignore is None:
        ignore = []
    rval = []
    dtype = np.dtype(cls)
    dtype_num = dtype.num
    if dtype_num not in ignore:
        np.array(0, dtype=dtype)
        rval.append(cls)
        ignore.append(dtype_num)
    for sub_ in cls.__subclasses__():
        rval += [c for c in get_numeric_subclasses(sub_, ignore=ignore)]
    return rval
def get_numeric_types(with_int=True, with_float=True, with_complex=False,
                      only_theano_types=True):
    if only_theano_types:
        theano_types = [d.dtype for d in theano.scalar.all_types]
    rval = []
    def is_within(cls1, cls2):
        return (cls1 is cls2 or
                issubclass(cls1, cls2) or
                isinstance(np.array([0], dtype=cls1)[0], cls2))
    for cls in get_numeric_subclasses():
        dtype = np.dtype(cls)
        if ((not with_complex and is_within(cls, np.complexfloating)) or
                (not with_int and is_within(cls, np.integer)) or
                (not with_float and is_within(cls, np.floating)) or
                (only_theano_types and dtype not in theano_types)):
            continue
        rval.append([str(dtype), dtype, dtype.num])
    return [x[1] for x in sorted(rval, key=str)]
def _numpy_checker(x, y):
    x, y = x[0], y[0]
    if (x.dtype != y.dtype or x.shape != y.shape or
            np.any(np.abs(x - y) &gt; 1e-10)):
        raise Exception("Output mismatch.", {'performlinker': x, 'clinker': y})
def safe_make_node(op, *inputs):
    node = op(*inputs)
    if isinstance(node, list):
        return node[0].owner
    else:
        return node.owner
def upcast_float16_ufunc(fn):
    def ret(*args, **kwargs):
        out_dtype = np.find_common_type(
            [a.dtype for a in args], [np.float16])
        if out_dtype == 'float16':
            sig = 'f' * fn.nin + '-&gt;' + 'f' * fn.nout
            kwargs.update(sig=sig)
        return fn(*args, **kwargs)
    return ret
def upcast_int8_nfunc(fn):
    def ret(*args, **kwargs):
        args = list(args)
        for i, a in enumerate(args):
            if getattr(a, 'dtype', None) in ('int8', 'uint8'):
                args[i] = a.astype('float32')
        return fn(*args, **kwargs)
    return ret
def makeTester(name, op, expected, checks=None, good=None, bad_build=None,
               bad_runtime=None, grad=None, mode=None, grad_rtol=None,
               eps=1e-10, skip=False, test_memmap=True, check_name=True,
               grad_eps=None):
    if checks is None:
        checks = {}
    if good is None:
        good = {}
    if bad_build is None:
        bad_build = {}
    if bad_runtime is None:
        bad_runtime = {}
    if grad is None:
        grad = {}
    if grad is True:
        grad = good
    _op, _expected, _checks, _good = op, expected, checks, good
    _bad_build, _bad_runtime, _grad = bad_build, bad_runtime, grad
    _mode, _grad_rtol, _eps, skip_ = mode, grad_rtol, eps, skip
    _test_memmap = test_memmap
    _check_name = check_name
    _grad_eps = grad_eps
    class Checker(unittest.TestCase):
        op = staticmethod(_op)
        expected = staticmethod(_expected)
        checks = _checks
        check_name = _check_name
        good = _good
        bad_build = _bad_build
        bad_runtime = _bad_runtime
        grad = _grad
        mode = _mode
        skip = skip_
        test_memmap = _test_memmap
        def setUp(self):
            if self.check_name:
                eval(self.__class__.__module__ + '.' + self.__class__.__name__)
            self.tmp_files = []
        def add_memmap_values(self, val_dict):
            if not self.test_memmap:
                return val_dict
            val_dict = val_dict.copy()
            for k, v in sorted(val_dict.items()):
                new_k = '_'.join((k, 'memmap'))
                if new_k in val_dict:
                    break
                new_v = []
                for inp in v:
                    if type(inp) is np.ndarray and inp.size &gt; 0:
                        f, fname = mkstemp()
                        self.tmp_files.append((f, fname))
                        new_inp = np.memmap(fname, dtype=inp.dtype,
                                            mode='w+', shape=inp.shape)
                        new_inp[...] = inp[...]
                        new_v.append(new_inp)
                    else:
                        new_v.append(inp)
                val_dict[new_k] = new_v
                break
            return val_dict
        def tearDown(self):
            import gc
            gc.collect()
            for f, fname in self.tmp_files:
                os.close(f)
                os.remove(fname)
        def test_good(self):
            if skip:
                raise SkipTest(skip)
            good = self.add_memmap_values(self.good)
            for testname, inputs in iteritems(good):
                inputs = [copy(input) for input in inputs]
                inputrs = [TensorType(
                    dtype=input.dtype,
                    broadcastable=[shape_elem == 1
                                   for shape_elem in input.shape]
                    )() for input in inputs]
                try:
                    node = safe_make_node(self.op, *inputrs)
                except Exception as exc:
                    err_msg = ("Test %s::%s: Error occurred while"
                               " making a node with inputs %s") % (
                                   self.op, testname, inputs)
                    exc.args += (err_msg,)
                    raise
                try:
                    f = inplace_func(inputrs, node.outputs, mode=mode, name='test_good')
                except Exception as exc:
                    err_msg = ("Test %s::%s: Error occurred while"
                               " trying to make a Function") % (self.op, testname)
                    exc.args += (err_msg,)
                    raise
                if (isinstance(self.expected, dict) and
                        testname in self.expected):
                    expecteds = self.expected[testname]
                    eps = 5e-9
                else:
                    expecteds = self.expected(*inputs)
                    eps = 1e-10
                if any([i.dtype in ('float32', 'int8', 'uint8', 'uint16')
                        for i in inputs]):
                    eps = 1e-6
                eps = np.max([eps, _eps])
                try:
                    variables = f(*inputs)
                except Exception as exc:
                    err_msg = ("Test %s::%s: Error occurred while calling"
                               " the Function on the inputs %s") % (
                                   self.op, testname, inputs)
                    exc.args += (err_msg,)
                    raise
                if not isinstance(expecteds, (list, tuple)):
                    expecteds = (expecteds, )
                for i, (variable, expected) in enumerate(
                        izip(variables, expecteds)):
                    if (variable.dtype != expected.dtype or
                            variable.shape != expected.shape or
                            not np.allclose(variable, expected,
                                            atol=eps, rtol=eps)):
                        self.fail(("Test %s::%s: Output %s gave the wrong"
                                   " value. With inputs %s, expected %s (dtype %s),"
                                   " got %s (dtype %s). eps=%f"
                                   " np.allclose returns %s %s") % (
                            self.op,
                            testname,
                            i,
                            inputs,
                            expected,
                            expected.dtype,
                            variable,
                            variable.dtype,
                            eps,
                            np.allclose(variable, expected,
                                        atol=eps, rtol=eps),
                            np.allclose(variable, expected)))
                for description, check in iteritems(self.checks):
                    if not check(inputs, variables):
                        self.fail(("Test %s::%s: Failed check: %s (inputs"
                                   " were %s, outputs were %s)") % (
                            self.op, testname, description,
                            inputs, variables))
        def test_bad_build(self):
            if skip:
                raise SkipTest(skip)
            for testname, inputs in iteritems(self.bad_build):
                inputs = [copy(input) for input in inputs]
                inputrs = [shared(input) for input in inputs]
                self.assertRaises(Exception,
                                  safe_make_node, self.op, *inputrs)
        @change_flags(compute_test_value='off')
        def test_bad_runtime(self):
            if skip:
                raise SkipTest(skip)
            for testname, inputs in iteritems(self.bad_runtime):
                inputrs = [shared(input) for input in inputs]
                try:
                    node = safe_make_node(self.op, *inputrs)
                except Exception as exc:
                    err_msg = ("Test %s::%s: Error occurred while trying"
                               " to make a node with inputs %s") % (
                        self.op, testname, inputs)
                    exc.args += (err_msg,)
                    raise
                try:
                    f = inplace_func([], node.outputs, mode=mode, name="test_bad_runtime")
                except Exception as exc:
                    err_msg = ("Test %s::%s: Error occurred while trying"
                               " to make a Function") % (self.op, testname)
                    exc.args += (err_msg,)
                    raise
                self.assertRaises(Exception, f, [])
        def test_grad(self):
            if skip:
                raise SkipTest(skip)
            backup = config.warn.sum_div_dimshuffle_bug
            config.warn.sum_div_dimshuffle_bug = False
            try:
                for testname, inputs in iteritems(self.grad):
                    inputs = [copy(input) for input in inputs]
                    try:
                        utt.verify_grad(self.op, inputs,
                                        mode=self.mode,
                                        rel_tol=_grad_rtol,
                                        eps=_grad_eps)
                    except Exception as exc:
                        err_msg = ("Test %s::%s: Error occurred while"
                                   " computing the gradient on the following"
                                   " inputs: %s") % (self.op, testname, inputs)
                        exc.args += (err_msg,)
                        raise
            finally:
                config.warn.sum_div_dimshuffle_bug = backup
        def test_grad_none(self):
            if skip:
                raise SkipTest(skip)
            if not hasattr(self.op, 'grad'):
                return
            for testname, inputs in iteritems(self.good):
                inputs = [copy(input) for input in inputs]
                inputrs = [TensorType(
                    dtype=input.dtype,
                    broadcastable=[shape_elem == 1
                                   for shape_elem in input.shape]
                    )() for input in inputs]
                if (isinstance(self.expected, dict) and
                        testname in self.expected):
                    expecteds = self.expected[testname]
                else:
                    expecteds = self.expected(*inputs)
                if not isinstance(expecteds, (list, tuple)):
                    expecteds = (expecteds, )
                out_grad_vars = []
                for out in expecteds:
                    if str(out.dtype) in tensor.discrete_dtypes:
                        dtype = floatX
                    else:
                        dtype = str(out.dtype)
                    bcast = [shape_elem == 1 for shape_elem in out.shape]
                    var = TensorType(dtype=dtype, broadcastable=bcast)()
                    out_grad_vars.append(var)
                try:
                    in_grad_vars = self.op.grad(inputrs, out_grad_vars)
                except (gof.utils.MethodNotDefined, NotImplementedError):
                    pass
                else:
                    assert None not in in_grad_vars
    Checker.__name__ = name
    if hasattr(Checker, '__qualname__'):
        Checker.__qualname__ = name
    return Checker
def rand(*shape):
    r = test_rng.rand(*shape) * 2 - 1
    return np.asarray(r, dtype=config.floatX)
def rand_nonzero(shape, eps=3e-4):
    r = np.asarray(test_rng.rand(*shape), dtype=config.floatX)
    r = r * (1 - eps) + eps * (r &gt;= 0.5)
    r = r * 2 - 1
    return r
def randint(*shape):
    return test_rng.randint(-5, 6, shape)
def randuint32(*shape):
    return np.array(test_rng.randint(5, size=shape), dtype=np.uint32)
def randuint16(*shape):
    return np.array(test_rng.randint(5, size=shape), dtype=np.uint16)
def randcomplex(*shape):
    r = np.asarray(test_rng.rand(*shape), dtype=config.floatX)
    return np.complex128(2 * r - 1)
def randcomplex_nonzero(shape, eps=1e-4):
    return np.complex128(rand_nonzero(shape, eps))
def randint_nonzero(*shape):
    r = test_rng.randint(-5, 5, shape)
    return r + (r == 0) * 5
def rand_ranged(min, max, shape):
    return np.asarray(test_rng.rand(*shape) * (max - min) + min,
                      dtype=config.floatX)
def randint_ranged(min, max, shape):
    return test_rng.randint(min, max + 1, shape)
def randc128_ranged(min, max, shape):
    return np.asarray(test_rng.rand(*shape) * (max - min) + min,
                      dtype='complex128')
def rand_of_dtype(shape, dtype):
    if dtype in tensor.discrete_dtypes:
        return randint(*shape).astype(dtype)
    elif dtype in tensor.float_dtypes:
        return rand(*shape).astype(dtype)
    elif dtype in tensor.complex_dtypes:
        return randcomplex(*shape).astype(dtype)
    else:
        raise TypeError()
_eps = 1e-2
def makeBroadcastTester(op, expected, checks=None, name=None, **kwargs):
    if checks is None:
        checks = {}
    if name is None:
        name = str(op)
    capitalize = False
    if name.startswith('Elemwise{') and name.endswith(',no_inplace}'):
        name = name[9:-12]
        capitalize = True
    elif name.endswith('_inplace'):
        capitalize = True
    if capitalize:
        name = ''.join([x.capitalize() for x in name.split('_')])
    if not name.endswith('Tester'):
        name += "Tester"
    if 'inplace' in kwargs:
        if kwargs['inplace']:
            _expected = expected
            if not isinstance(_expected, dict):
                def expected(*inputs):
                    return np.array(_expected(*inputs), dtype=inputs[0].dtype)
            def inplace_check(inputs, outputs):
                return np.all(inputs[0] == outputs[0])
            checks = dict(checks, inplace_check=inplace_check)
        del kwargs['inplace']
    return makeTester(name, op, expected, checks, **kwargs)
_good_broadcast_binary_normal = dict(
    same_shapes=(rand(2, 3), rand(2, 3)),
    not_same_dimensions=(rand(2, 2), rand(2)),
    scalar=(rand(2, 3), rand(1, 1)),
    row=(rand(2, 3), rand(1, 3)),
    column=(rand(2, 3), rand(2, 1)),
    integers=(randint(2, 3), randint(2, 3)),
    uint32=(randuint32(2, 3), randuint32(2, 3)),
    uint16=(randuint16(2, 3), randuint16(2, 3)),
    dtype_mixup_1=(rand(2, 3), randint(2, 3)),
    dtype_mixup_2=(randint(2, 3), rand(2, 3)),
    complex1=(randcomplex(2, 3), randcomplex(2, 3)),
    complex2=(randcomplex(2, 3), rand(2, 3)),
    empty=(np.asarray([], dtype=config.floatX),
           np.asarray([1], dtype=config.floatX)),
    )
<a name="30"></a>_bad_build_broadcast_binary_normal = dict()
_bad_runtime_broadcast_binary_normal = dict(
    bad_shapes=(rand(2, 3), rand<font color="#ae694a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, 2)),
    bad_row=(rand(2, 3), rand(1, 2)))
_grad_broadcast_binary_normal = dict(
    same_shapes=(rand(2, 3), rand(2, 3)),
    scalar=</b></font>(rand(2, 3), rand(1, 1)),
    row=(rand(2, 3), rand(1, 3)),
    column=(rand(2, 3), rand(2, 1)),
    )
def check_floatX(inputs, rval):
    if (isinstance(rval, np.ndarray) and
            rval.dtype == 'float64' and
            config.cast_policy == 'numpy+floatX' and
            config.floatX == 'float32' and
            all(x.dtype != 'float64' for x in inputs)):
        return rval.astype('float32')
    else:
        return rval
AddTester = makeBroadcastTester(
    op=add,
    expected=lambda *inputs: check_floatX(
        inputs, reduce(lambda x, y: x + y, inputs)),
    good=dict(
        three_inputs_same_shapes=(rand(2, 3),
                                  rand(2, 3),
                                  rand(2, 3)),
        three_inputs_same_shapes_uint=(randuint32(2, 3),
                                       randuint32(2, 3),
                                       randuint32(2, 3)),
        four_inputs_broadcast=(rand(2, 3),
                               rand(1, 3),
                               rand(2, 1),
                               rand(1, 1)),
        **_good_broadcast_binary_normal),
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal)
AddInplaceTester = makeBroadcastTester(
    op=inplace.add_inplace,
    expected=lambda x, y: x + y,
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    inplace=True)
SubTester = makeBroadcastTester(
    op=sub,
    expected=lambda x, y: check_floatX((x, y), x - y),
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    grad=_grad_broadcast_binary_normal)
SubInplaceTester = makeBroadcastTester(op=inplace.sub_inplace,
                                       expected=lambda x, y: x - y,
                                       good=_good_broadcast_binary_normal,
                                       bad_build=_bad_build_broadcast_binary_normal,
                                       bad_runtime=_bad_runtime_broadcast_binary_normal,
                                       inplace=True)
SwitchTester = makeBroadcastTester(
    op=switch,
    expected=np.where,
    good=dict(all_true=(np.asarray(1, dtype=config.floatX),
                        rand(4, 5), rand(4, 5)),
              false_true=(np.asarray(0, dtype=config.floatX),
                          rand(4, 5), rand(4, 5)),
              mixed=(randint_ranged(0, 1, (4, 5)),
                     rand(4, 5), rand(4, 5))
              ),
    bad_build=dict(all_true=(np.asarray(1, dtype=config.floatX),
                             rand(4, 5))),
    bad_runtime=dict(all_true=(np.asarray(1, dtype=config.floatX),
                               rand(3, 5), rand(4, 5)),
                     false_true=(np.asarray(0, dtype=config.floatX),
                                 rand(4, 6), rand(4, 5)),
                     ),
    grad=dict(all_true=(np.asarray(1, dtype=config.floatX),
                        rand(4, 5), rand(4, 5)),
              ),
)
MaximumTester = makeBroadcastTester(
    op=maximum,
    expected=lambda *inputs: check_floatX(inputs, np.maximum(*inputs)),
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    grad=_grad_broadcast_binary_normal)
MaximumInplaceTester = makeBroadcastTester(
    op=inplace.maximum_inplace,
    expected=np.maximum,
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    inplace=True)
def test_maximum_minimum_grad():
    x, y = tensor.vectors('xy')
    for op in [tensor.maximum, tensor.minimum]:
        o = op(x, y)
        g = theano.grad(o.sum(), [x, y])
        f = theano.function([x, y], g)
        assert np.allclose(f([1], [1]), [[1], [0]])
MinimumTester = makeBroadcastTester(
    op=minimum,
    expected=lambda *inputs: check_floatX(inputs, np.minimum(*inputs)),
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    grad=_grad_broadcast_binary_normal)
MinimumInplaceTester = makeBroadcastTester(
    op=inplace.minimum_inplace,
    expected=np.minimum,
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    inplace=True)
MulTester = makeBroadcastTester(
    op=mul,
    expected=lambda *inputs: check_floatX(inputs, reduce(lambda x, y: x * y, inputs)),
    good=dict(three_inputs_same_shapes=(rand(2, 3), rand(2, 3), rand(2, 3)),
              four_inputs_broadcast=(rand(2, 3), rand(1, 3), rand(2, 1), rand(1, 1)),
              **_good_broadcast_binary_normal),
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    grad=dict(three_inputs_same_shapes=(rand(2, 3), rand(2, 3), rand(2, 3)),
              four_inputs_broadcast=(rand(2, 3), rand(1, 3), rand(2, 1), rand(1, 1)),
              **_grad_broadcast_binary_normal))
MulInplaceTester = makeBroadcastTester(
    op=inplace.mul_inplace,
    expected=lambda x, y: x * y,
    good=_good_broadcast_binary_normal,
    bad_build=_bad_build_broadcast_binary_normal,
    bad_runtime=_bad_runtime_broadcast_binary_normal,
    inplace=True)
def copymod(dct, without=None, **kwargs):
    if without is None:
        without = []
    rval = copy(dct)
    for a in without:
        if a in rval:
            del rval[a]
    for kw, val in iteritems(kwargs):
<a name="7"></a>        rval[kw] = val
    return rval
_good_broadcast_div_mod_normal_float_no_complex = dict<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(
    same_shapes=(rand(2, 3), rand_nonzero((2, 3))),
    scalar=(rand(2, 3), rand_nonzero((1, 1))),
    row=(rand(2, 3), rand_nonzero((1</b></font>, 3))),
    column=(rand(2, 3), rand_nonzero((2, 1))),
    dtype_mixup_1=(rand(2, 3), randint_nonzero(2, 3)),
    dtype_mixup_2=(randint_nonzero(2, 3), rand_nonzero((2, 3))),
    integer=(randint(2, 3), randint_nonzero(2, 3)),
    uint8=(randint(2, 3).astype("uint8"),
           randint_nonzero(2, 3).astype("uint8")),
    uint16=(randint(2, 3).astype("uint16"),
            randint_nonzero(2, 3).astype("uint16")),
    int8=[np.tile(np.arange(-127, 128, dtype='int8'), [254, 1]).T,
          np.tile(np.array(list(range(-127, 0)) + list(range(1, 128)),
                           dtype='int8'),
                  [255, 1])],
    )
if PY3:
    _good_broadcast_div_mod_normal_float_inplace = copymod(
        _good_broadcast_div_mod_normal_float_no_complex,
        empty1=(np.asarray([]), np.asarray([1])),
        )
else:
    _good_broadcast_div_mod_normal_float_inplace = copymod(
        _good_broadcast_div_mod_normal_float_no_complex,
        empty1=(np.asarray([], dtype=config.floatX),
                np.asarray([1], dtype=config.floatX)),
        complex1=(randcomplex(2, 3), randcomplex_nonzero((2, 3))),
        complex2=(randcomplex(2, 3), rand_nonzero((2, 3))),
        )
_good_broadcast_div_mod_normal_float = copymod(
    _good_broadcast_div_mod_normal_float_inplace,
    empty2=(np.asarray([0], dtype=config.floatX),
            np.asarray([], dtype=config.floatX))
    )
_grad_broadcast_div_mod_normal = dict(
    same_shapes=(rand(2, 3), rand_nonzero((2, 3))),
    scalar=(rand(2, 3), rand_nonzero((1, 1))),
    row=(rand(2, 3), rand_nonzero((1, 3))),
    column=(rand(2, 3), rand_nonzero((2, 1))),
    )
div_grad_rtol = None
if config.floatX == 'float32':
    div_grad_rtol = 0.025
def _numpy_true_div(x, y):
    out = np.true_divide(x, y)
    if x.dtype in tensor.discrete_dtypes and y.dtype in tensor.discrete_dtypes:
        out = theano._asarray(out, dtype=config.floatX)
    return out
TrueDivTester = makeBroadcastTester(
    op=tensor.true_div,
    expected=_numpy_true_div,
    good=_good_broadcast_div_mod_normal_float_no_complex,
    grad=_grad_broadcast_div_mod_normal,
    grad_rtol=div_grad_rtol,
    )
TrueDivInplaceTester = makeBroadcastTester(
    op=inplace.true_div_inplace,
    expected=_numpy_true_div,
    good=copymod(
        _good_broadcast_div_mod_normal_float_inplace,
        without=['integer', 'uint8', 'uint16', 'int8']),
    grad_rtol=div_grad_rtol,
    inplace=True)
_good_inv = dict(
    normal=[5 * rand_nonzero((2, 3))],
    integers=[randint_nonzero(2, 3)],
    int8=[np.array(list(range(-127, 0)) + list(range(1, 127)), dtype='int8')],
    uint8=[np.array(list(range(0, 255)), dtype='uint8')],
    uint16=[np.array(list(range(0, 65535)), dtype='uint16')],
    complex=[randcomplex_nonzero((2, 3))],
    empty=[np.asarray([], dtype=config.floatX)])
_good_inv_inplace = copymod(_good_inv, without=['integers', 'int8', 'uint8', 'uint16', 'complex'])
_grad_inv = copymod(_good_inv,
                    without=['integers', 'int8', 'uint8', 'uint16', 'complex', 'empty'])
_bad_runtime_inv = dict(
    float=[np.zeros((2, 3))],
    integers=[np.zeros((2, 3), dtype='int64')],
    int8=[np.zeros((2, 3), dtype='int8')],
    complex=[np.zeros((2, 3), dtype='complex128')])
InvTester = makeBroadcastTester(
    op=tensor.inv,
    expected=lambda x: upcast_int8_nfunc(np.true_divide)(np.int8(1), x),
    good=_good_inv,
    bad_runtime=_bad_runtime_inv,
    grad=_grad_inv,
    grad_rtol=div_grad_rtol)
InvInplaceTester = makeBroadcastTester(
    op=inplace.inv_inplace,
    expected=lambda x: _numpy_true_div(np.int8(1), x),
    good=_good_inv_inplace,
    bad_runtime=_bad_runtime_inv,
    grad_rtol=div_grad_rtol,
    inplace=True)
CeilIntDivTester = makeBroadcastTester(
    op=tensor.ceil_intdiv,
    expected=lambda x, y: check_floatX((x, y), (x // y) + ((x % y) != 0)),
    good=_good_broadcast_div_mod_normal_float_no_complex,
    name='CeilIntDiv',
    )
ModTester = makeBroadcastTester(
    op=tensor.mod,
    expected=lambda x, y: np.asarray(
        x % y, dtype=theano.scalar.basic.upcast(x.dtype, y.dtype)),
    good=copymod(_good_broadcast_div_mod_normal_float,
                 ['complex1', 'complex2']),
    grad=_grad_broadcast_div_mod_normal,
    grad_eps=1e-5,
    )
ModInplaceTester = makeBroadcastTester(
    op=inplace.mod_inplace,
    expected=lambda x, y: np.asarray(
        x % y, dtype=theano.scalar.basic.upcast(x.dtype, y.dtype)),
    good=copymod(_good_broadcast_div_mod_normal_float_inplace,
                 ["complex1", "complex2"]),
    grad_eps=1e-5,
    inplace=True)
_good_broadcast_pow_normal_float = dict(
    same_shapes=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 3))),
    scalar=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 1))),
    row=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 3))),
<a name="3"></a>    column=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 1))),
    dtype_mixup=(rand_ranged(-3, 3, (2, 3)), randint_ranged(-3, 3, (2, 3))),
    complex1=(randcomplex(2, 3), randcomplex(2, 3)),
    complex2=(randcomplex(2, 3), rand<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(2, 3)),
    empty1=(np.asarray([], dtype=config.floatX),
            np.asarray([1], dtype=config.floatX)),
    empty2=(np.asarray([0], dtype=config.floatX),
            np.asarray([], dtype=config.</b></font>floatX)),
    empty3=(np.asarray([], dtype=config.floatX),
            np.asarray([], dtype=config.floatX)),
    )
_grad_broadcast_pow_normal = dict(
    same_shapes=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 3))),
    scalar=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 1))),
    row=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 3))),
    column=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 1))),
    x_eq_zero=(
        np.asarray([0.], dtype=config.floatX),
        np.asarray([2.], dtype=config.floatX)
        ),  # Test for issue 1780
    )
_good_broadcast_pow_normal_float_pow = copy(_good_broadcast_pow_normal_float)
del _good_broadcast_pow_normal_float_pow["empty2"]
m = copy(theano.compile.get_default_mode())
m.check_isfinite = False
PowTester = makeBroadcastTester(
    op=pow,
    expected=lambda x, y: check_floatX((x, y), x ** y),
    good=_good_broadcast_pow_normal_float,
    grad=_grad_broadcast_pow_normal,
    name='Pow',
    mode=m
)
PowInplaceTester = makeBroadcastTester(
    op=inplace.pow_inplace,
    expected=lambda x, y: x ** y,
    good=_good_broadcast_pow_normal_float_pow,
    inplace=True,
    mode=m
)
corner_case = np.asarray(
    [-2.5, -2., -1.5, -1., -0.5, -.51, -.49, 0,
     0.49, 0.5, 0.9, 1, 1.5, 2, 2.5],
    dtype=floatX)
corner_case_grad = np.asarray(
    [-2.5, -2., -1.5, -1., -0.5, -.51, -.49,
     0.49, 0.5, 0.9, 1, 1.5, 2, 2.5],
    dtype=floatX)
_good_broadcast_unary_normal_float = dict(
    normal=[rand_ranged(-5, 5, (2, 3))],
    corner_case=[corner_case],
<a name="21"></a>    complex=[randcomplex(2, 3)],
    empty=[np.asarray([], dtype=config.floatX)])
_good_broadcast_unary_normal_float_no_empty <font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= copymod(
    _good_broadcast_unary_normal_float,
    without=['empty'])
_good_broadcast_unary_normal_float_no_empty_no_complex = copymod(
    _good_broadcast_unary_normal_float_no_empty,
    without=['complex'])
_good_broadcast_unary_normal_float_no_complex = copymod(
    _good_broadcast_unary_normal_float,
    without=['complex'])
_good_broadcast_unary_normal_float_no_complex_small_neg_range = dict(
    normal=</b></font>[rand_ranged(-2, 5, (2, 3))],
    corner_case=[corner_case],
    empty=[np.asarray([], dtype=config.floatX)])
_good_broadcast_unary_normal = dict(
    normal=[np.asarray(rand_ranged(-5, 5, (2, 3)),
                       dtype=config.floatX)],
    integers=[randint_ranged(-5, 5, (2, 3))],
    int8=[np.arange(-127, 128, dtype='int8')],
    uint8=[np.arange(0, 255, dtype='uint8')],
    uint16=[np.arange(0, 65535, dtype='uint16')],
    corner_case=[corner_case],
    complex=[randcomplex(2, 3)],
    empty=[np.asarray([], dtype=config.floatX)],
    )
_good_broadcast_unary_normal_no_complex = dict(
    normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
    integers=[randint_ranged(-5, 5, (2, 3))],
    int8=[np.arange(-127, 128, dtype='int8')],
    uint8=[np.arange(0, 89, dtype='uint8')],
    uint16=[np.arange(0, 89, dtype='uint16')],
    corner_case=[corner_case],
    empty=[np.asarray([], dtype=config.floatX)],
    )
_grad_broadcast_unary_normal_no_complex = dict(
    normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
    corner_case=[corner_case_grad])
_grad_broadcast_unary_normal = dict(
    normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
    corner_case=[corner_case_grad],
    )
_grad_broadcast_unary_normal_noint = dict(
    normal=[(rand_ranged(_eps, 1 - _eps, (2, 3)) + randint(2, 3))
            .astype(floatX)])
_grad_broadcast_unary_normal_small_neg_range = dict(
    normal=[np.asarray(rand_ranged(-2, 5, (2, 3)), dtype=floatX)],
    corner_case=[corner_case_grad])
_grad_broadcast_unary_normal_no_complex_no_corner_case = copymod(
    _grad_broadcast_unary_normal_no_complex,
    without=['corner_case'])
_grad_broadcast_unary_abs1_no_complex = dict(
    normal=[np.asarray(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)), dtype=floatX)],
    )
_grad_broadcast_unary_0_2_no_complex = dict(
    normal=[np.asarray(rand_ranged(_eps, 1 - _eps, (2, 3)), dtype=floatX)],
    )
AbsTester = makeBroadcastTester(
    op=tensor.abs_,
    expected=lambda x: abs(x),
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal)
_good_broadcast_unary_normal_abs = copy(_good_broadcast_unary_normal)
del _good_broadcast_unary_normal_abs['complex']
AbsInplaceTester = makeBroadcastTester(
    op=inplace.abs__inplace,
    expected=lambda x: np.abs(x),
    good=_good_broadcast_unary_normal_abs,
    inplace=True)
NegTester = makeBroadcastTester(
    op=tensor.neg,
    expected=lambda x: -x,
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal)
NegInplaceTester = makeBroadcastTester(
    op=inplace.neg_inplace,
    expected=lambda x: -x,
    good=_good_broadcast_unary_normal,
    inplace=True)
SgnTester = makeBroadcastTester(
    op=tensor.sgn,
    expected=np.sign,
    good=_good_broadcast_unary_normal_no_complex,
    grad=_grad_broadcast_unary_normal,)
SgnInplaceTester = makeBroadcastTester(
    op=inplace.sgn_inplace,
    expected=np.sign,
    good=_good_broadcast_unary_normal_no_complex,
    inplace=True)
IntDivTester = makeBroadcastTester(
    op=tensor.int_div,
    expected=lambda x, y: check_floatX((x, y), x // y),
    good=_good_broadcast_div_mod_normal_float,
    )
IntDivInplaceTester = makeBroadcastTester(
    op=inplace.int_div_inplace,
    expected=lambda x, y: check_floatX((x, y), x // y),
    good=_good_broadcast_div_mod_normal_float_inplace,
    inplace=True
    )
CeilTester = makeBroadcastTester(
    op=tensor.ceil,
    expected=upcast_float16_ufunc(np.ceil),
    good=_good_broadcast_unary_normal_no_complex,
    grad=copymod(_grad_broadcast_unary_normal_noint,
                 extra=[np.asarray([-2.5, -1.5, -1.51, 0.49, .98, 1.02],
                                   dtype=floatX)]))
CeilInplaceTester = makeBroadcastTester(
    op=inplace.ceil_inplace,
    expected=upcast_float16_ufunc(np.ceil),
    good=copymod(_good_broadcast_unary_normal_no_complex,
                 without=['integers', 'int8', 'uint8', 'uint16']),
    inplace=True)
FloorTester = makeBroadcastTester(
    op=tensor.floor,
    expected=upcast_float16_ufunc(np.floor),
    good=_good_broadcast_unary_normal_no_complex,
    grad=_grad_broadcast_unary_normal_noint)
FloorInplaceTester = makeBroadcastTester(
    op=inplace.floor_inplace,
    expected=upcast_float16_ufunc(np.floor),
    good=copymod(_good_broadcast_unary_normal_no_complex,
                 without=["integers", "int8", "uint8", "uint16"]),
    inplace=True)
TruncInplaceTester = makeBroadcastTester(
    op=inplace.trunc_inplace,
    expected=upcast_float16_ufunc(np.trunc),
    good=_good_broadcast_unary_normal_no_complex,
    inplace=True)
TruncTester = makeBroadcastTester(
    op=tensor.trunc,
    expected=upcast_float16_ufunc(np.trunc),
    good=_good_broadcast_unary_normal_no_complex)
RoundHalfToEvenTester = makeBroadcastTester(
    op=tensor.round_half_to_even,
    expected=np.round,
    good=_good_broadcast_unary_normal_float_no_complex,
    grad=_grad_broadcast_unary_normal_no_complex_no_corner_case)
RoundHalfToEvenInplaceTester = makeBroadcastTester(
    op=inplace.round_half_to_even_inplace,
    expected=np.round,
    good=_good_broadcast_unary_normal_float_no_complex,
    inplace=True)
RoundHalfAwayFromZeroTester = makeBroadcastTester(
    op=tensor.round_half_away_from_zero,
    expected=lambda a: theano.scalar.basic.round_half_away_from_zero_vec(a),
    good=_good_broadcast_unary_normal_float_no_empty_no_complex,
    grad=_grad_broadcast_unary_normal_no_complex_no_corner_case)
RoundHalfAwayFromZeroInplaceTester = makeBroadcastTester(
    op=inplace.round_half_away_from_zero_inplace,
    expected=lambda a: theano.scalar.basic.round_half_away_from_zero_vec(a),
    good=_good_broadcast_unary_normal_float_no_empty_no_complex,
    inplace=True)
SqrTester = makeBroadcastTester(
    op=tensor.sqr,
    expected=np.square,
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal)
SqrInplaceTester = makeBroadcastTester(
    op=inplace.sqr_inplace,
    expected=np.square,
    good=_good_broadcast_unary_normal,
    inplace=True)
ExpTester = makeBroadcastTester(
    op=tensor.exp,
    expected=upcast_float16_ufunc(np.exp),
    good=dict(_good_broadcast_unary_normal,
              int8=[np.arange(-127, 89, dtype='int8')],
              uint8=[np.arange(0, 89, dtype='uint8')],
              uint16=[np.arange(0, 89, dtype='uint16')]),
    grad=_grad_broadcast_unary_normal)
ExpInplaceTester = makeBroadcastTester(
    op=inplace.exp_inplace,
    expected=np.exp,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
Exp2Tester = makeBroadcastTester(
    op=tensor.exp2,
    expected=upcast_float16_ufunc(np.exp2),
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal)
Exp2InplaceTester = makeBroadcastTester(
    op=inplace.exp2_inplace,
    expected=np.exp2,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
Expm1Tester = makeBroadcastTester(
    op=tensor.expm1,
    expected=upcast_float16_ufunc(np.expm1),
    good=dict(_good_broadcast_unary_normal,
              int8=[np.arange(-127, 89, dtype='int8')],
              uint8=[np.arange(0, 89, dtype='uint8')],
              uint16=[np.arange(0, 89, dtype='uint16')]),
    grad=_grad_broadcast_unary_normal)
Expm1InplaceTester = makeBroadcastTester(
    op=inplace.expm1_inplace,
    expected=np.expm1,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
_good_broadcast_unary_positive = dict(
    normal=(rand_ranged(0.001, 5, (2, 3)),),
    integers=(randint_ranged(1, 5, (2, 3)),),
    uint8=[np.arange(1, 256, dtype='uint8')],
    complex=(randc128_ranged(1, 5, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),
    )
_good_broadcast_unary_positive_float = copymod(
    _good_broadcast_unary_positive,
    without=['integers', 'uint8'])
_grad_broadcast_unary_positive = dict(normal=(rand_ranged(_eps, 5, (2, 3)),),)
LogTester = makeBroadcastTester(
    op=tensor.log,
    expected=upcast_float16_ufunc(np.log),
    good=_good_broadcast_unary_positive,
    grad=_grad_broadcast_unary_positive)
LogInplaceTester = makeBroadcastTester(
    op=inplace.log_inplace,
    expected=np.log,
    good=_good_broadcast_unary_positive_float,
    inplace=True)
Log2Tester = makeBroadcastTester(
    op=tensor.log2,
    expected=upcast_float16_ufunc(np.log2),
    good=_good_broadcast_unary_positive,
    grad=_grad_broadcast_unary_positive)
Log2InplaceTester = makeBroadcastTester(
    op=inplace.log2_inplace,
    expected=np.log2,
    good=_good_broadcast_unary_positive_float,
    inplace=True)
Log10Tester = makeBroadcastTester(
    op=tensor.log10,
    expected=upcast_float16_ufunc(np.log10),
    good=_good_broadcast_unary_positive,
    grad=_grad_broadcast_unary_positive)
Log10InplaceTester = makeBroadcastTester(
    op=inplace.log10_inplace,
    expected=np.log10,
    good=_good_broadcast_unary_positive_float,
    inplace=True)
Log1pTester = makeBroadcastTester(
    op=tensor.log1p,
    expected=upcast_float16_ufunc(np.log1p),
    good=_good_broadcast_unary_positive,
    grad=_grad_broadcast_unary_positive)
Log1pInplaceTester = makeBroadcastTester(
    op=inplace.log1p_inplace,
    expected=np.log1p,
    good=_good_broadcast_unary_positive_float,
    inplace=True)
SqrtTester = makeBroadcastTester(
    op=tensor.sqrt,
    expected=upcast_float16_ufunc(np.sqrt),
    good=_good_broadcast_unary_positive,
    grad=_grad_broadcast_unary_positive)
SqrtInplaceTester = makeBroadcastTester(
    op=inplace.sqrt_inplace,
    expected=np.sqrt,
    good=_good_broadcast_unary_positive_float,
    inplace=True)
_good_broadcast_unary_wide = dict(
    normal=(rand_ranged(-1000, 1000, (2, 3)),),
    integers=(randint_ranged(-1000, 1000, (2, 3)),),
    int8=[np.arange(-127, 128, dtype='int8')],
    uint8=[np.arange(0, 255, dtype='uint8')],
    uint16=[np.arange(0, 65535, dtype='uint16')],
    complex=(randc128_ranged(-1000, 1000, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),)
_good_broadcast_unary_wide_float = copymod(
    _good_broadcast_unary_wide,
    without=['integers', 'int8', 'uint8', 'uint16'])
_grad_broadcast_unary_wide = dict(normal=(rand_ranged(-1000, 1000, (2, 3)),),)
if theano.config.floatX == 'float32':
    angle_eps = 1e-4
else:
    angle_eps = 1e-10
Deg2radTester = makeBroadcastTester(
    op=tensor.deg2rad,
    expected=upcast_float16_ufunc(np.deg2rad),
    good=_good_broadcast_unary_normal_no_complex,
    grad=_grad_broadcast_unary_normal_no_complex,
    eps=angle_eps)
Deg2radInplaceTester = makeBroadcastTester(
    op=inplace.deg2rad_inplace,
    expected=np.deg2rad,
    good=_good_broadcast_unary_normal_float_no_complex,
    inplace=True,
    eps=angle_eps)
Rad2degTester = makeBroadcastTester(
    op=tensor.rad2deg,
    expected=upcast_float16_ufunc(np.rad2deg),
    good=_good_broadcast_unary_normal_no_complex,
    grad=_grad_broadcast_unary_normal_no_complex,
    eps=angle_eps)
Rad2degInplaceTester = makeBroadcastTester(
    op=inplace.rad2deg_inplace,
    expected=np.rad2deg,
    good=_good_broadcast_unary_normal_float_no_complex,
    inplace=True,
    eps=angle_eps)
SinTester = makeBroadcastTester(
    op=tensor.sin,
    expected=upcast_float16_ufunc(np.sin),
    good=_good_broadcast_unary_wide,
    grad=_grad_broadcast_unary_wide)
SinInplaceTester = makeBroadcastTester(
    op=inplace.sin_inplace,
    expected=np.sin,
    good=_good_broadcast_unary_wide_float,
    inplace=True)
_good_broadcast_unary_arcsin = dict(
    normal=(rand_ranged(-1, 1, (2, 3)),),
    integers=(randint_ranged(-1, 1, (2, 3)),),
    int8=[np.arange(-1, 2, dtype='int8')],
    uint8=[np.arange(0, 2, dtype='uint8')],
    uint16=[np.arange(0, 2, dtype='uint16')],
    complex=(randc128_ranged(-1, 1, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),)
_good_broadcast_unary_arcsin_float = copymod(
    _good_broadcast_unary_arcsin,
    without=['integers', 'int8', 'uint8', 'uint16'])
_grad_broadcast_unary_arcsin = dict(normal=(rand_ranged(-0.9, 0.9, (2, 3)),),)
ArcsinTester = makeBroadcastTester(
    op=tensor.arcsin,
    expected=upcast_float16_ufunc(np.arcsin),
    good=_good_broadcast_unary_arcsin,
    grad=_grad_broadcast_unary_arcsin)
ArcsinInplaceTester = makeBroadcastTester(
    op=inplace.arcsin_inplace,
    expected=np.arcsin,
    good=_good_broadcast_unary_arcsin_float,
    inplace=True)
CosTester = makeBroadcastTester(
    op=tensor.cos,
    expected=upcast_float16_ufunc(np.cos),
    good=_good_broadcast_unary_wide,
    grad=_grad_broadcast_unary_wide)
CosInplaceTester = makeBroadcastTester(
    op=inplace.cos_inplace,
    expected=np.cos,
    good=_good_broadcast_unary_wide_float,
    inplace=True)
def test_py_c_match():
    a = tensor.TensorType(dtype='int8', broadcastable=(False,))()
    f = theano.function([a], tensor.arccos(a), mode='DebugMode')
    f(np.asarray([1, 0, -1], dtype='int8'))
ArccosTester = makeBroadcastTester(
    op=tensor.arccos,
    expected=upcast_float16_ufunc(np.arccos),
    good=_good_broadcast_unary_arcsin,
    grad=_grad_broadcast_unary_arcsin)
ArccosInplaceTester = makeBroadcastTester(
    op=inplace.arccos_inplace,
    expected=np.arccos,
    good=_good_broadcast_unary_arcsin_float,
    inplace=True)
_good_broadcast_unary_tan = dict(
    normal=(rand_ranged(-3.14, 3.14, (2, 3)),),
    shifted=(rand_ranged(3.15, 6.28, (2, 3)),),
    integers=(randint_ranged(-3, 3, (2, 3)),),
    int8=[np.arange(-3, 4, dtype='int8')],
    uint8=[np.arange(0, 4, dtype='uint8')],
    uint16=[np.arange(0, 4, dtype='uint16')],
    complex=(randc128_ranged(-3.14, 3.14, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),)
_grad_broadcast_unary_tan = dict(normal=(rand_ranged(-1.5, 1.5, (2, 3)),),
                                 shifted=(rand_ranged(1.6, 4.6, (2, 3)),))
TanTester = makeBroadcastTester(
    op=tensor.tan,
    expected=upcast_float16_ufunc(np.tan),
    good=_good_broadcast_unary_tan,
    grad=_grad_broadcast_unary_tan)
TanInplaceTester = makeBroadcastTester(
    op=inplace.tan_inplace,
    expected=np.tan,
    good=copymod(_good_broadcast_unary_tan, without=['integers', 'int8', 'uint8', 'uint16']),
    inplace=True)
ArctanTester = makeBroadcastTester(
    op=tensor.arctan,
    expected=upcast_float16_ufunc(np.arctan),
    good=_good_broadcast_unary_wide,
    grad=_grad_broadcast_unary_wide)
ArctanInplaceTester = makeBroadcastTester(
    op=inplace.arctan_inplace,
    expected=np.arctan,
    good=_good_broadcast_unary_wide_float,
    inplace=True)
_good_broadcast_binary_arctan2 = dict(
    same_shapes=(rand(2, 3), rand(2, 3)),
    not_same_dimensions=(rand(2, 2), rand(2)),
    scalar=(rand(2, 3), rand(1, 1)),
    row=(rand(2, 3), rand(1, 3)),
    column=(rand(2, 3), rand(2, 1)),
    integers=(randint(2, 3), randint(2, 3)),
    int8=[np.arange(-127, 128, dtype='int8'),
          np.arange(-127, 128, dtype='int8')[:, np.newaxis]],
    uint8=[np.arange(0, 128, dtype='uint8'),
           np.arange(0, 128, dtype='uint8')[:, np.newaxis]],
    uint16=[np.arange(0, 128, dtype='uint16'),
            np.arange(0, 128, dtype='uint16')[:, np.newaxis]],
    dtype_mixup_1=(rand(2, 3), randint(2, 3)),
    dtype_mixup_2=(randint(2, 3), rand(2, 3)),
    empty=(np.asarray([], dtype=config.floatX),
           np.asarray([1], dtype=config.floatX)),
    )
_grad_broadcast_binary_arctan2 = dict(
    same_shapes=(rand(2, 3), rand(2, 3)),
    scalar=(rand(2, 3), rand(1, 1)),
    row=(rand(2, 3), rand(1, 3)),
    column=(rand(2, 3), rand(2, 1)),
    )
Arctan2Tester = makeBroadcastTester(
    op=tensor.arctan2,
    expected=upcast_float16_ufunc(np.arctan2),
    good=_good_broadcast_binary_arctan2,
    grad=_grad_broadcast_binary_arctan2)
Arctan2InplaceTester = makeBroadcastTester(
    op=inplace.arctan2_inplace,
    expected=np.arctan2,
    good=copymod(_good_broadcast_binary_arctan2,
                 without=['integers', 'int8', 'uint8',
                          'uint16', 'dtype_mixup_2']),
    inplace=True)
CoshTester = makeBroadcastTester(
    op=tensor.cosh,
    expected=upcast_float16_ufunc(np.cosh),
    good=dict(_good_broadcast_unary_normal,
              int8=[np.arange(-89, 90, dtype='int8')],
              uint8=[np.arange(0, 90, dtype='uint8')],
              uint16=[np.arange(0, 90, dtype='uint16')]),
    grad=_grad_broadcast_unary_normal)
CoshInplaceTester = makeBroadcastTester(
    op=inplace.cosh_inplace,
    expected=np.cosh,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
_good_broadcast_unary_arccosh = dict(
    normal=(rand_ranged(1, 1000, (2, 3)),),
    integers=(randint_ranged(1, 1000, (2, 3)),),
    uint8=[np.arange(1, 256, dtype='uint8')],
    complex=(randc128_ranged(1, 1000, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),)
_grad_broadcast_unary_arccosh = dict(normal=(rand_ranged(1 + _eps, 1000, (2, 3)),),)
ArccoshTester = makeBroadcastTester(
    op=tensor.arccosh,
    expected=upcast_float16_ufunc(np.arccosh),
    good=_good_broadcast_unary_arccosh,
    grad=_grad_broadcast_unary_arccosh)
ArccoshInplaceTester = makeBroadcastTester(
    op=inplace.arccosh_inplace,
    expected=np.arccosh,
    good=copymod(_good_broadcast_unary_arccosh, without=['integers', 'uint8']),
    inplace=True)
SinhTester = makeBroadcastTester(
    op=tensor.sinh,
    expected=upcast_float16_ufunc(np.sinh),
    good=dict(_good_broadcast_unary_normal,
              int8=[np.arange(-89, 90, dtype='int8')],
              uint8=[np.arange(0, 90, dtype='uint8')],
              uint16=[np.arange(0, 90, dtype='uint16')]),
    grad=_grad_broadcast_unary_normal)
SinhInplaceTester = makeBroadcastTester(
    op=inplace.sinh_inplace,
    expected=np.sinh,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
ArcsinhTester = makeBroadcastTester(
    op=tensor.arcsinh,
    expected=upcast_float16_ufunc(np.arcsinh),
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal)
ArcsinhInplaceTester = makeBroadcastTester(
    op=inplace.arcsinh_inplace,
    expected=np.arcsinh,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
TanhTester = makeBroadcastTester(
    op=tensor.tanh,
    expected=upcast_float16_ufunc(np.tanh),
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal)
TanhInplaceTester = makeBroadcastTester(
    op=inplace.tanh_inplace,
    expected=np.tanh,
    good=_good_broadcast_unary_normal_float,
    inplace=True)
_good_broadcast_unary_arctanh = dict(
    normal=(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
    integers=(randint_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
    int8=[np.arange(0, 1, dtype='int8')],
    uint8=[np.arange(0, 1, dtype='uint8')],
    uint16=[np.arange(0, 1, dtype='uint16')],
    complex=(randc128_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),)
_grad_broadcast_unary_arctanh = dict(
    normal=(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)),),)
ArctanhTester = makeBroadcastTester(
    op=tensor.arctanh,
    expected=upcast_float16_ufunc(np.arctanh),
    good=_good_broadcast_unary_arctanh,
    grad=_grad_broadcast_unary_arctanh)
ArctanhInplaceTester = makeBroadcastTester(
    op=inplace.arctanh_inplace,
    expected=np.arctanh,
    good=copymod(_good_broadcast_unary_arctanh, without=['integers', 'int8', 'uint8', 'uint16']),
    inplace=True)
<a name="5"></a>if imported_scipy_special:
    expected_erf = scipy.special.erf
    expected_erfc = scipy.special.erfc
    expected_erfinv = scipy.special<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.erfinv
    expected_erfcinv = scipy.special.erfcinv
    expected_gamma = scipy.special.gamma
<a name="12"></a>    expected_gammaln = scipy.special.gammaln
    expected_psi = scipy.special.psi
    expected_tri_gamma = partial(scipy.</b></font>special.polygamma, 1)
    expected_chi2sf = scipy.stats.chi2<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.sf
    expected_j0 = scipy.special.j0
    expected_j1 = scipy.special.j1
    expected_jv = scipy.special.jv
    expected_i0 = scipy.special.i0
    expected_i1 =</b></font> scipy.special.i1
    expected_iv = scipy.special.iv
    skip_scipy = False
    expected_erfcx = scipy.special.erfcx
else:
    expected_erf = []
    expected_erfc = []
    expected_erfcx = []
    expected_erfinv = []
    expected_erfcinv = []
    expected_gamma = []
    expected_gammaln = []
    expected_psi = []
    expected_tri_gamma = []
    expected_chi2sf = []
    expected_j0 = []
    expected_j1 = []
    expected_jv = []
    expected_i0 = []
    expected_i1 = []
    expected_iv = []
    skip_scipy = "scipy is not present"
ErfTester = makeBroadcastTester(
    op=tensor.erf,
    expected=expected_erf,
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
ErfInplaceTester = makeBroadcastTester(
    op=inplace.erf_inplace,
    expected=expected_erf,
    good=_good_broadcast_unary_normal_float,
    mode=mode_no_scipy,
    eps=2e-10,
    inplace=True,
    skip=skip_scipy)
ErfcTester = makeBroadcastTester(
    op=tensor.erfc,
    expected=expected_erfc,
    good=_good_broadcast_unary_normal_float_no_complex,
    grad=_grad_broadcast_unary_normal,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
ErfcInplaceTester = makeBroadcastTester(
    op=inplace.erfc_inplace,
    expected=expected_erfc,
    good=_good_broadcast_unary_normal_float_no_complex,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
ErfcxTester = makeBroadcastTester(
    op=tensor.erfcx,
    expected=expected_erfcx,
    good=_good_broadcast_unary_normal_float_no_complex_small_neg_range,
    grad=_grad_broadcast_unary_normal_small_neg_range,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
ErfcxInplaceTester = makeBroadcastTester(
    op=inplace.erfcx_inplace,
    expected=expected_erfcx,
    good=_good_broadcast_unary_normal_float_no_complex_small_neg_range,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
ErfinvTester = makeBroadcastTester(
    op=tensor.erfinv,
    expected=expected_erfinv,
    good={'normal': [rand_ranged(-.9, .9, (2, 3))],
          'empty': [np.asarray([], dtype=config.floatX)]},
    grad=_grad_broadcast_unary_abs1_no_complex,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
ErfcinvTester = makeBroadcastTester(
    op=tensor.erfcinv,
    expected=expected_erfcinv,
    good={'normal': [rand_ranged(0.001, 1.9, (2, 3))],
          'empty': [np.asarray([], dtype=config.floatX)]},
    grad=_grad_broadcast_unary_0_2_no_complex,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
_good_broadcast_unary_gammaln = dict(
    normal=(rand_ranged(-1 + 1e-2, 10, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),
    int=(randint_ranged(1, 10, (2, 3)),),
    uint8=(randint_ranged(1, 6, (2, 3)).astype('uint8'),),
    uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),),
    uint64=(randint_ranged(1, 10, (2, 3)).astype('uint64'),))
_grad_broadcast_unary_gammaln = dict(
    normal=(rand_ranged(1e-1, 8, (2, 3)),),)
GammaTester = makeBroadcastTester(
    op=tensor.gamma,
    expected=expected_gamma,
    good=_good_broadcast_unary_gammaln,
    grad=_grad_broadcast_unary_gammaln,
    mode=mode_no_scipy,
    eps=1e-5,
    skip=skip_scipy)
GammaInplaceTester = makeBroadcastTester(
    op=inplace.gamma_inplace,
    expected=expected_gamma,
    good=_good_broadcast_unary_gammaln,
    mode=mode_no_scipy,
    eps=1e-5,
    inplace=True,
    skip=skip_scipy)
GammalnTester = makeBroadcastTester(
    op=tensor.gammaln,
    expected=expected_gammaln,
    good=_good_broadcast_unary_gammaln,
    grad=_grad_broadcast_unary_gammaln,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
GammalnInplaceTester = makeBroadcastTester(
    op=inplace.gammaln_inplace,
    expected=expected_gammaln,
    good=_good_broadcast_unary_gammaln,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
_good_broadcast_unary_psi = dict(
    normal=(rand_ranged(1, 10, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),
    int=(randint_ranged(1, 10, (2, 3)),),
    uint8=(randint_ranged(1, 10, (2, 3)).astype('uint8'),),
    uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),))
PsiTester = makeBroadcastTester(
    op=tensor.psi,
    expected=expected_psi,
    good=_good_broadcast_unary_psi,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
PsiInplaceTester = makeBroadcastTester(
    op=inplace.psi_inplace,
    expected=expected_psi,
    good=_good_broadcast_unary_psi,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
_good_broadcast_unary_tri_gamma = _good_broadcast_unary_psi
TriGammaTester = makeBroadcastTester(
    op=tensor.tri_gamma,
    expected=expected_tri_gamma,
    good=_good_broadcast_unary_psi,
    eps=2e-8,
    mode=mode_no_scipy,
    skip=skip_scipy)
TriGammaInplaceTester = makeBroadcastTester(
    op=inplace.tri_gamma_inplace,
    expected=expected_tri_gamma,
    good=_good_broadcast_unary_tri_gamma,
    eps=2e-8,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
<a name="27"></a>
_good_broadcast_unary_chi2sf = dict(
    normal=(rand_ranged(1, 10, (2, 3)),
            np.asarray(1, dtype<font color="#e77471"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=config.floatX)),
    empty=(np.asarray([], dtype=config.floatX),
           np.asarray(1, dtype=config.</b></font>floatX)),
    integers=(randint_ranged(1, 10, (2, 3)),
              np.asarray(1, dtype=config.floatX)),
    uint8=(randint_ranged(1, 10, (2, 3)).astype('uint8'),
           np.asarray(1, dtype=config.floatX)),
    uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),
            np.asarray(1, dtype=config.floatX)))
Chi2SFTester = makeBroadcastTester(
    op=tensor.chi2sf,
    expected=expected_chi2sf,
    good=_good_broadcast_unary_chi2sf,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy,
    name='Chi2SF')
Chi2SFInplaceTester = makeBroadcastTester(
    op=inplace.chi2sf_inplace,
    expected=expected_chi2sf,
    good=_good_broadcast_unary_chi2sf,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy,
    name='Chi2SF')
_good_broadcast_unary_bessel = dict(
    normal=(rand_ranged(-10, 10, (2, 3)),),
    empty=(np.asarray([], dtype=config.floatX),),
    int=(randint_ranged(-10, 10, (2, 3)),),
    uint8=(randint_ranged(0, 10, (2, 3)).astype('uint8'),),
    uint16=(randint_ranged(0, 10, (2, 3)).astype('uint16'),))
_grad_broadcast_unary_bessel = dict(
    normal=(rand_ranged(-10., 10., (2, 3)),),)
_good_broadcast_binary_bessel = dict(
    normal=(rand_ranged(-5, 5, (2, 3)),
            rand_ranged(0, 10, (2, 3))),
    empty=(np.asarray([], dtype=config.floatX),
           np.asarray([], dtype=config.floatX)),
    integers=(randint_ranged(-5, 5, (2, 3)),
              randint_ranged(-10, 10, (2, 3))),
    uint8=(randint_ranged(0, 5, (2, 3)).astype('uint8'),
           randint_ranged(0, 10, (2, 3)).astype('uint8')),
    uint16=(randint_ranged(0, 5, (2, 3)).astype('uint16'),
            randint_ranged(0, 10, (2, 3)).astype('uint16')))
_grad_broadcast_binary_bessel = dict(
    normal=(rand_ranged(1, 5, (2, 3)),
            rand_ranged(0, 10, (2, 3))))
J0Tester = makeBroadcastTester(
    op=tensor.j0,
    expected=expected_j0,
    good=_good_broadcast_unary_bessel,
    grad=_grad_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
J0InplaceTester = makeBroadcastTester(
    op=inplace.j0_inplace,
    expected=expected_j0,
    good=_good_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
J1Tester = makeBroadcastTester(
    op=tensor.j1,
    expected=expected_j1,
    good=_good_broadcast_unary_bessel,
    grad=_grad_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
J1InplaceTester = makeBroadcastTester(
    op=inplace.j1_inplace,
    expected=expected_j1,
    good=_good_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
JvTester = makeBroadcastTester(
    op=tensor.jv,
    expected=expected_jv,
<a name="31"></a>    good=_good_broadcast_binary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    skip<font color="#3ea99f"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=skip_scipy)
JvInplaceTester = makeBroadcastTester(
    op=inplace.jv_inplace,
    expected=expected_jv,
    good=_good_broadcast_binary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
def</b></font> test_verify_jv_grad():
    if skip_scipy:
        raise SkipTest("SciPy needed")
    v_val, x_val = _grad_broadcast_binary_bessel['normal']
    def fixed_first_input_jv(x):
        return tensor.jv(v_val, x)
    utt.verify_grad(fixed_first_input_jv, [x_val])
I0Tester = makeBroadcastTester(
    op=tensor.i0,
    expected=expected_i0,
    good=_good_broadcast_unary_bessel,
    grad=_grad_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
I0InplaceTester = makeBroadcastTester(
    op=inplace.i0_inplace,
    expected=expected_i0,
    good=_good_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
I1Tester = makeBroadcastTester(
    op=tensor.i1,
    expected=expected_i1,
    good=_good_broadcast_unary_bessel,
    grad=_grad_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    skip=skip_scipy)
I1InplaceTester = makeBroadcastTester(
    op=inplace.i1_inplace,
    expected=expected_i1,
    good=_good_broadcast_unary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
IvTester = makeBroadcastTester(
    op=tensor.iv,
    expected=expected_iv,
<a name="29"></a>    good=_good_broadcast_binary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    skip<font color="#af7a82"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=skip_scipy)
IvInplaceTester = makeBroadcastTester(
    op=inplace.iv_inplace,
    expected=expected_iv,
    good=_good_broadcast_binary_bessel,
    eps=2e-10,
    mode=mode_no_scipy,
    inplace=True,
    skip=skip_scipy)
def</b></font> test_verify_iv_grad():
    if skip_scipy:
        raise SkipTest("SciPy needed")
    v_val, x_val = _grad_broadcast_binary_bessel['normal']
    def fixed_first_input_iv(x):
        return tensor.iv(v_val, x)
    utt.verify_grad(fixed_first_input_iv, [x_val])
ZerosLikeTester = makeBroadcastTester(
    op=tensor.zeros_like,
    expected=np.zeros_like,
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal,
    name='ZerosLike')
OnesLikeTester = makeBroadcastTester(
    op=tensor.ones_like,
    expected=np.ones_like,
    good=_good_broadcast_unary_normal,
    grad=_grad_broadcast_unary_normal,
    name='OnesLike')
_good_complex_from_polar = dict(
    same_shapes=(abs(rand(2, 3)), rand(2, 3)),
    not_same_dimensions=(abs(rand(2, 2)), rand(2)),
    scalar=(abs(rand(2, 3)), rand(1, 1)),
    row=(abs(rand(2, 3)), rand(1, 3)),
    column=(abs(rand(2, 3)), rand(2, 1)),
    integers=(abs(randint(2, 3)), randint(2, 3)),
    empty=(np.asarray([], dtype=config.floatX),
           np.asarray([1], dtype=config.floatX)),)
_grad_complex_from_polar = dict(
    same_shapes=(abs(rand(2, 3)), rand(2, 3)),
    scalar=(abs(rand(2, 3)), rand(1, 1)),
    row=(abs(rand(2, 3)), rand(1, 3)),
    column=(abs(rand(2, 3)), rand(2, 1)))
ComplexFromPolarTester = makeBroadcastTester(
    op=tensor.complex_from_polar,
    expected=lambda r, theta: r * np.cos(theta) + 1j * r * np.sin(theta),
    good=_good_complex_from_polar)
ConjTester = makeBroadcastTester(
    op=tensor.conj,
    expected=np.conj,
    good=_good_broadcast_unary_normal)
ConjInplaceTester = makeBroadcastTester(
    op=inplace.conj_inplace,
    expected=np.conj,
    good=_good_broadcast_unary_normal,
    inplace=True)
DotTester = makeTester(
    name='DotTester',
    op=dot,
    expected=lambda x, y: np.dot(x, y),
    checks={},
    good=dict(correct1=(rand(5, 7), rand(7, 5)),
              correct2=(rand(5, 7), rand(7, 9)),
              correct3=(rand(5, 7), rand(7)),
              correct4=(rand(5), rand(5, 7)),
              mixed1=(rand(5).astype('float32'), rand(5, 7)),
              mixed2=(rand(5).astype('float64'), rand(5, 7)),
              complex1=(randcomplex(5, 7), randcomplex(7)),
              complex2=(rand(5, 7), randcomplex(7)),
              complex3=(randcomplex(5, 7), rand(7)),
              empty1=(np.asarray([], dtype=config.floatX),
                      np.asarray([], dtype=config.floatX)),
              empty2=(rand(5, 0), rand(0, 2)),
              empty3=(rand(0, 5), rand(5, 0)),
              ),
    bad_build=dict(),
    bad_runtime=dict(bad1=(rand(5, 7), rand(5, 7)),
                     bad2=(rand(5, 7), rand(8, 3))))
BatchedDotTester = makeTester(
    name='BatchedDotTester',
    op=batched_dot,
    expected=(lambda xs, ys:
              np.asarray(
                  list(x * y if x.ndim == 0 or y.ndim == 0 else np.dot(x, y)
                       for x, y in zip(xs, ys)),
                  dtype=theano.scalar.upcast(xs.dtype, ys.dtype))),
    checks={},
    grad=dict(correct1=(rand(3, 5, 7), rand(3, 7, 5)),
              correct2=(rand(3, 5, 7), rand(3, 7, 9)),
              correct3=(rand(3, 5, 7), rand(3, 7)),
              correct4=(rand(3, 5), rand(3, 5, 7)),
              correct5=(rand(3), rand(3, 5, 7)),
              correct6=(rand(3, 5), rand(3)),
              correct7=(rand(3, 5), rand(3, 5)),
              correct8=(rand(3), rand(3)),
              correct9=(rand(3, 5, 7, 11), rand(3)),
              correct10=(rand(3, 2, 6, 5), rand(3, 5)),
              correct11=(rand(3, 2, 6, 5), rand(3, 5, 7)),
              correct12=(rand(3, 2, 6, 5), rand(3, 7, 5, 8)),
              mixed1=(rand(3, 5).astype('float32'),
                      rand(3, 5, 7)),
              mixed2=(rand(3, 5).astype('float64'),
                      rand(3, 5, 7))),
    good=dict(correct1=(rand(3, 5, 7), rand(3, 7, 5)),
              correct2=(rand(3, 5, 7), rand(3, 7, 9)),
              correct3=(rand(3, 5, 7), rand(3, 7)),
              correct4=(rand(3, 5), rand(3, 5, 7)),
              correct5=(rand(3), rand(3, 5, 7)),
              correct6=(rand(3, 5), rand(3)),
              correct7=(rand(3, 5), rand(3, 5)),
              correct8=(rand(3), rand(3)),
              correct9=(rand(3, 5, 7, 11), rand(3)),
              correct10=(rand(3, 7, 11, 5), rand(3, 5)),
              correct11=(rand(3, 7, 11, 5), rand(3, 5, 13)),
              correct12=(rand(3, 7, 11, 5), rand(3, 13, 5, 17)),
              mixed1=(rand(3, 5).astype('float32'),
<a name="28"></a>                      rand(3, 5, 7)),
              mixed2=(rand(3, 5).astype('float64'),
                      rand(3, 5, 7))),
    bad_build=dict(no_batch_axis2=(rand(), rand<font color="#717d7d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, 5)),
                   no_batch_axis3=(rand(3, 5), rand())),
    bad_runtime=dict(batch_dim_mismatch1=(rand(2, 5, 7), rand(3, 7, 9)),
                     batch_dim_mismatch2=</b></font>(rand(3, 5, 7), rand(2, 7, 9)),
                     batch_dim_mismatch3=(rand(3), rand(5)),
                     bad_dim1=(rand(3, 5, 7), rand(3, 5, 7)),
                     bad_dim2=(rand(3, 5, 7), rand(3, 8, 3)),
                     bad_dim3=(rand(3, 5), rand(3, 7)),
                     bad_dim4=(rand(3, 5, 7, 11), rand(3, 5)),
                     bad_dim5=(rand(3, 5, 7, 11), rand(3, 5, 13)),
                     bad_dim6=(rand(3, 5, 7, 11), rand(3, 13, 5, 17))))
def _numpy_second(x, y):
    return np.broadcast_arrays(x, y)[1]
ALL_DTYPES = ('int8', 'int16', 'int32', 'int64', 'float32', 'float64',
              'uint8', 'uint16',
              'complex64', 'complex128')
REAL_DTYPES = ALL_DTYPES[:6]
COMPLEX_DTYPES = ALL_DTYPES[-2:]
def multi_dtype_checks(shape1, shape2, dtypes=ALL_DTYPES, nameprefix=''):
    for dtype1, dtype2 in itertools.combinations(dtypes, 2):
        name1 = '%s_%s_%s' % (nameprefix, dtype1, dtype2)
        name2 = '%s_%s_%s' % (nameprefix, dtype2, dtype1)
        obj1 = rand_of_dtype(shape1, dtype1)
        obj2 = rand_of_dtype(shape2, dtype2)
        yield (name1, (obj1, obj2))
        yield (name2, (obj2, obj1))
def multi_dtype_cast_checks(shape, dtypes=ALL_DTYPES, nameprefix=''):
    for dtype1, dtype2 in itertools.combinations(dtypes, 2):
        name1 = '%s_%s_%s' % (nameprefix, dtype1, dtype2)
        name2 = '%s_%s_%s' % (nameprefix, dtype2, dtype1)
        obj1 = rand_of_dtype(shape, dtype1)
        obj2 = rand_of_dtype(shape, dtype2)
        yield (name1, (obj1, dtype2))
        yield (name2, (obj2, dtype1))
SecondBroadcastTester = makeTester(
    name='SecondBroadcastTester',
    op=second,
    expected=_numpy_second,
    good=dict(itertools.chain(
        multi_dtype_checks((4, 5), (5,)),
        multi_dtype_checks((2, 3, 2), (3, 2)),
        multi_dtype_checks((2, 3, 2), (2,)),
        )),
    bad_runtime=dict(
        fail1=(rand(5, 4), rand(5)),
        fail2=(rand(3, 2, 3), rand(6, 9)),
        fail3=(randint(6, 2, 9), rand(3, 2)),
        )
    )
SecondSameRankTester = makeTester(
    name='SecondSameRankTester',
    op=second,
    expected=_numpy_second,
    good=dict(itertools.chain(
        multi_dtype_checks((4, 5), (4, 5)),
        multi_dtype_checks((1, 2), (3, 2)),
        multi_dtype_checks((3, 2), (1, 2)),
        )),
    bad_runtime=dict(itertools.chain(
        multi_dtype_checks((4, 5), (5, 4)),
        multi_dtype_checks((1, 5), (5, 4)),
        )),
    mode=get_default_mode().excluding(
        'local_fill_to_alloc',
        'local_useless_fill')
    )
AllocTester = makeBroadcastTester(
    name='AllocTester',
    op=alloc,
    expected=(lambda x, *shp: np.zeros(shp, dtype=x.dtype) + x),
    good=dict(
        correct01=(rand(), np.int32(7)),
        correct01_bcast=(rand(1), np.int32(7)),
        correct02=(rand(), np.int32(4), np.int32(7)),
        correct12=(rand(7), np.int32(4), np.int32(7)),
        correct13=(rand(7), np.int32(2), np.int32(4), np.int32(7)),
        correct23=(rand(4, 7), np.int32(2), np.int32(4), np.int32(7)),
        correctb1=(rand(1, 7), np.int32(4), np.int32(7)),
        correctb2=(rand(1, 7), np.int32(2), np.int32(4), np.int32(7)),
        correctb3=(rand(7, 1), np.int32(7), np.int32(4)),
        correctb4=(rand(7, 1), np.int32(2), np.int32(7), np.int32(4)),
        ),
    bad_runtime=dict(
        bad_shape12=(rand(7), np.int32(7), np.int32(5)),
        ),
    bad_build=dict(
        vec=(rand(1), [np.int32(2)]),
        too_big32=(rand(6, 2, 4), np.int32(6), np.int32(2)),
        too_big32b=(rand(6, 2, 4), np.int32(6), np.int32(4)),
        too_big32c=(rand(6, 2, 4), np.int32(2), np.int32(4)),
        too_big32d=(rand(6, 2, 4), np.int32(2), np.int32(6)),
        too_big32e=(rand(6, 2, 4), np.int32(4), np.int32(6)),
        too_big32f=(rand(6, 2, 4), np.int32(4), np.int32(2)),
        ),
    )
s1, s2, s3 = randint_ranged(1, 13, (3,))
Alloc01GradTester = makeBroadcastTester(
    name='Alloc01GradTester',
    op=(lambda x: alloc(x, s1)),
    expected=(lambda x: np.zeros((s1,), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(),),
        x2=(rand(),),
        x3=(rand(),),
        ),
    )
Alloc13GradTester = makeBroadcastTester(
    name='Alloc13GradTester',
    op=(lambda x: alloc(x, s1, s2, s3)),
    expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(s3),),
        x2=(rand(s3),),
        x3=(rand(s3),),
        ),
    )
Allocb1GradTester = makeBroadcastTester(
    name='Allocb1GradTester',
    op=lambda x: alloc(x, s1, s2),
    expected=(lambda x: np.zeros((s1, s2), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(1, s2),),
        x2=(rand(1, s2),),
        x3=(rand(1, s2),),
    ),
)
Allocb2GradTester = makeBroadcastTester(
    name='Allocb2GradTester',
    op=lambda x: alloc(x, s1, s2, s3),
    expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(1, s3),),
        x2=(rand(1, s3),),
        x3=(rand(1, s3),),
    ),
)
Allocb3GradTester = makeBroadcastTester(
    name='Allocb3GradTester',
    op=lambda x: alloc(x, s1, s2),
    expected=(lambda x: np.zeros((s1, s2), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(s1, 1),),
        x2=(rand(s1, 1),),
        x3=(rand(s1, 1),),
    ),
)
Allocb4GradTester = makeBroadcastTester(
    name='Allocb4GradTester',
    op=lambda x: alloc(x, s1, s2, s3),
    expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(s2, 1),),
        x2=(rand(s2, 1),),
        x3=(rand(s2, 1),),
    ),
)
AllocDimshuffleGradTester = makeBroadcastTester(
    name='Allocb4GradTester',
    op=lambda x: alloc(x.dimshuffle('x', 'x', 0), 1, s2, s3),
    expected=(lambda x: np.zeros((1, s2, s3), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(s3),),
        x2=(rand(s3),),
        x3=(rand(s3),),
    ),
)
AllocDimshuffleGradTester2 = makeBroadcastTester(
    name='Allocb4GradTester',
    op=lambda x: alloc(x.dimshuffle('x', 0), 1, s2, s3),
    expected=(lambda x: np.zeros((1, s2, s3), dtype=x.dtype) + x),
    grad=dict(
        x1=(rand(s3),),
        x2=(rand(s3),),
        x3=(rand(s3),),
    ),
)
class ApplyDefaultTestOp(theano.Op):
    def __init__(self, id):
        self.default_output = id
    def make_node(self, x):
        x = theano.tensor.as_tensor_variable(x)
        return theano.Apply(self, [x], [x.type()])
class TestAsTensorVariable(unittest.TestCase):
    def setUp(self):
        self.x = tensor.scalar('x')
    def test_one_output(self):
        good_apply_var = ApplyDefaultTestOp(0).make_node(self.x)
        as_tensor_variable(good_apply_var)
    def test_below_zero_output(self):
        bad_apply_var = ApplyDefaultTestOp(-1).make_node(self.x)
        self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
    def test_above_output_len(self):
        bad_apply_var = ApplyDefaultTestOp(2).make_node(self.x)
        self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
    def test_list(self):
        bad_apply_var = ApplyDefaultTestOp([0, 1]).make_node(self.x)
        self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
    def test_strip_leading_broadcastable(self):
        x = tensor.TensorType(config.floatX, (True, False))('x')
        x = as_tensor_variable(x, ndim=1)
        assert(x.ndim == 1)
        x = tensor.matrix('x', dtype=config.floatX)
        self.assertRaises(ValueError, as_tensor_variable, x, ndim=1)
class TestAlloc(unittest.TestCase):
    dtype = config.floatX
    mode = mode_opt
    shared = staticmethod(theano.shared)
    allocs = [tensor.Alloc()] * 3
    def setUp(self):
        self.rng = np.random.RandomState(seed=utt.fetch_seed())
    def test_alloc_constant_folding(self):
        test_params = np.asarray(self.rng.randn(50 * 60),
                                 self.dtype)
        some_vector = vector('some_vector', dtype=self.dtype)
        some_matrix = some_vector.reshape((60, 50))
        variables = self.shared(np.ones((50,), dtype=self.dtype))
        idx = tensor.constant(np.arange(50))
        for alloc_, (subtensor, n_alloc) in zip(self.allocs, [
                (some_matrix[:60], 2),
                (some_matrix[arange(60)], 2),
                (some_matrix[idx, idx], 1)
        ]):
            derp = sum(dot(subtensor, variables))
            fobj = theano.function([some_vector], derp, mode=self.mode)
            grad_derp = theano.grad(derp, some_vector)
            fgrad = theano.function([some_vector], grad_derp,
                                    mode=self.mode)
            topo_obj = fobj.maker.fgraph.toposort()
            assert np.sum([isinstance(node.op, type(alloc_))
                           for node in topo_obj]) == 0
            topo_grad = fgrad.maker.fgraph.toposort()
            assert np.sum([isinstance(node.op, type(alloc_))
                           for node in topo_grad]) == n_alloc, (
                               alloc_, subtensor, n_alloc, topo_grad)
            fobj(test_params)
            fgrad(test_params)
    def test_alloc_output(self):
        val = tensor.constant(self.rng.randn(1, 1), dtype=self.dtype)
        for alloc_ in self.allocs:
            out = alloc_(val, 50, 60)
            f = theano.function([], out, mode=self.mode)
            topo = f.maker.fgraph.toposort()
            assert np.sum([isinstance(node.op, type(alloc_))
                           for node in topo]) == 1
            assert not isinstance(topo[0].op, DeepCopyOp)
    def test_ones(self):
        for shp in [[], 1, [1], [1, 2], [1, 2, 3]]:
            ones = theano.function([], [tensor.ones(shp)], mode=self.mode)
            assert np.allclose(ones(), np.ones(shp))
        x = scalar()
        shp = []
        ones_scalar = theano.function([], [tensor.ones(x.shape)],
                                      mode=self.mode)
        assert np.allclose(ones_scalar(), np.ones(shp))
        for (typ, shp) in [(vector, [3]), (matrix, [3, 4])]:
            x = typ()
            ones_tensor = theano.function([x], [tensor.ones(x.shape)],
                                          mode=self.mode)
            inp = np.zeros(shp, dtype=config.floatX)
            assert np.allclose(ones_tensor(inp),
                               np.ones(shp))
    def test_zeros(self):
        for shp in [[], 1, [1], [1, 2], [1, 2, 3]]:
            zeros = theano.function([], [tensor.zeros(shp)],
                                    mode=self.mode)
            assert np.allclose(zeros(), np.zeros(shp))
        x = scalar()
        shp = []
        zeros_scalar = theano.function([], [tensor.zeros(x.shape)],
                                       mode=self.mode)
        assert np.allclose(zeros_scalar(), np.zeros(shp))
        for (typ, shp) in [(vector, [3]), (matrix, [3, 4])]:
            x = typ()
            zeros_tensor = theano.function([x], [tensor.zeros(x.shape)],
                                           mode=self.mode)
            inp = np.zeros(shp, dtype=config.floatX)
            assert np.allclose(zeros_tensor(inp),
                               np.zeros(shp))
def test_eye():
    def check(dtype, N, M_=None, k=0):
        M = M_
        if M is None and theano.config.mode in ['DebugMode', 'DEBUG_MODE']:
            M = N
        N_symb = tensor.iscalar()
        M_symb = tensor.iscalar()
        k_symb = tensor.iscalar()
        f = function([N_symb, M_symb, k_symb],
                     eye(N_symb, M_symb, k_symb, dtype=dtype))
        result = f(N, M, k)
        assert np.allclose(result, np.eye(N, M_, k, dtype=dtype))
        assert result.dtype == np.dtype(dtype)
    for dtype in ALL_DTYPES:
        yield check, dtype, 3
        yield check, dtype, 3, 5
        yield check, dtype, 5, 3
        yield check, dtype, 3, 3, 1
        yield check, dtype, 3, 3, -1
        yield check, dtype, 3, 5, 1
        yield check, dtype, 3, 5, -1
        yield check, dtype, 5, 3, 1
        yield check, dtype, 5, 3, -1
class test_triangle(unittest.TestCase):
    def test_tri(self):
        def check(dtype, N, M_=None, k=0):
            M = M_
            if M is None and theano.config.mode in ['DebugMode', 'DEBUG_MODE']:
                M = N
            N_symb = tensor.iscalar()
            M_symb = tensor.iscalar()
            k_symb = tensor.iscalar()
            f = function([N_symb, M_symb, k_symb],
                         tri(N_symb, M_symb, k_symb, dtype=dtype))
            result = f(N, M, k)
            self.assertTrue(
                np.allclose(result, np.tri(N, M_, k, dtype=dtype)))
            self.assertTrue(result.dtype == np.dtype(dtype))
        for dtype in ALL_DTYPES:
            yield check, dtype, 3
            yield check, dtype, 3, 5
            yield check, dtype, 5, 3
            yield check, dtype, 3, 3, 1
            yield check, dtype, 3, 3, -1
            yield check, dtype, 3, 5, 1
            yield check, dtype, 3, 5, -1
            yield check, dtype, 5, 3, 1
            yield check, dtype, 5, 3, -1
    def test_tril_triu(self):
        def check_l(m, k=0):
            m_symb = matrix(dtype=m.dtype)
            k_symb = iscalar()
            f = function([m_symb, k_symb], tril(m_symb, k_symb))
            result = f(m, k)
            self.assertTrue(np.allclose(result, np.tril(m, k)))
            self.assertTrue(result.dtype == np.dtype(dtype))
        def check_u(m, k=0):
            m_symb = matrix(dtype=m.dtype)
            k_symb = iscalar()
            f = function([m_symb, k_symb], triu(m_symb, k_symb))
            result = f(m, k)
            self.assertTrue(np.allclose(result, np.triu(m, k)))
            self.assertTrue(result.dtype == np.dtype(dtype))
        for dtype in ALL_DTYPES:
            m = rand_of_dtype((10, 10), dtype)
            yield check_l, m, 0
            yield check_l, m, 1
            yield check_l, m, -1
            yield check_u, m, 0
            yield check_u, m, 1
            yield check_u, m, -1
            m = rand_of_dtype((10, 5), dtype)
            yield check_l, m, 0
            yield check_l, m, 1
            yield check_l, m, -1
            yield check_u, m, 0
            yield check_u, m, 1
            yield check_u, m, -1
class test_nonzero(unittest.TestCase):
    def test_nonzero(self):
        def check(m):
            m_symb = theano.tensor.tensor(dtype=m.dtype,
                                          broadcastable=(False,) * m.ndim)
            f_tuple = function([m_symb], nonzero(m_symb, return_matrix=False))
            f_matrix = function([m_symb], nonzero(m_symb, return_matrix=True))
            self.assertTrue(np.allclose(f_matrix(m),
                                        np.vstack(np.nonzero(m))))
            for i, j in zip(f_tuple(m), np.nonzero(m)):
                self.assertTrue(np.allclose(i, j))
        rand0d = np.array(rand())
        self.assertRaises(ValueError, check, rand0d)
        rand1d = rand(8)
        rand1d[:4] = 0
        check(rand1d)
        rand2d = rand(8, 9)
        rand2d[:4] = 0
        check(rand2d)
        rand3d = rand(8, 9, 10)
        rand3d[:4] = 0
        check(rand3d)
        rand4d = rand(8, 9, 10, 11)
        rand4d[:4] = 0
        check(rand4d)
    def test_flatnonzero(self):
        def check(m):
            m_symb = theano.tensor.tensor(dtype=m.dtype,
                                          broadcastable=(False,) * m.ndim)
            f = function([m_symb], flatnonzero(m_symb))
            result = f(m)
            assert np.allclose(result, np.flatnonzero(m))
        rand0d = np.array(rand())
        self.assertRaises(ValueError, check, rand0d)
        rand1d = rand(8)
        rand1d[:4] = 0
        check(rand1d)
        rand2d = rand(8, 9)
        rand2d[:4] = 0
        check(rand2d)
        rand3d = rand(8, 9, 10)
        rand3d[:4] = 0
        check(rand3d)
        rand4d = rand(8, 9, 10, 11)
        rand4d[:4] = 0
        check(rand4d)
    def test_nonzero_values(self):
        def check(m):
            m_symb = theano.tensor.tensor(dtype=m.dtype,
                                          broadcastable=(False,) * m.ndim)
            f = function([m_symb], nonzero_values(m_symb))
            result = f(m)
            assert np.allclose(result, m[np.nonzero(m)])
        rand0d = rand()
        self.assertRaises(ValueError, check, rand0d)
        rand1d = rand(8)
        rand1d[:4] = 0
        check(rand1d)
        rand2d = rand(8, 9)
        rand2d[:4] = 0
        check(rand2d)
        rand3d = rand(8, 9, 10)
        rand3d[:4] = 0
        check(rand3d)
        rand4d = rand(8, 9, 10, 11)
        rand4d[:4] = 0
        check(rand4d)
def test_identity():
    def check(dtype):
        obj = rand_of_dtype((2,), dtype)
        sym = tensor.vector(dtype=dtype)
        f = function([sym], tensor_copy(sym))
        assert np.all(obj == f(obj))
        assert obj.dtype == f(obj).dtype
        topo = f.maker.fgraph.toposort()
        assert len(topo) == 1
        if theano.config.mode != 'FAST_COMPILE':
            assert isinstance(topo[0].op, DeepCopyOp)
    for dtype in ALL_DTYPES:
        yield check, dtype
class CastTester(unittest.TestCase):
    def test_good_between_real_types(self):
        good = itertools.chain(
            multi_dtype_cast_checks((2,), dtypes=REAL_DTYPES),
            [('%s_%s' % (rand_of_dtype((2,), dtype), dtype),
              (rand_of_dtype((2,), dtype), dtype))
             for dtype in ALL_DTYPES])
        for testname, (obj, dtype) in good:
            inp = tensor.vector(dtype=obj.dtype)
            out = tensor.cast(inp, dtype=dtype)
            f = function([inp], out)
            assert f(obj).dtype == np.dtype(dtype)
            out2 = inp.astype(dtype=dtype)
            assert out2.type == out.type
    def test_cast_from_real_to_complex(self):
        for real_dtype in REAL_DTYPES:
            for complex_dtype in COMPLEX_DTYPES:
                inp = tensor.vector(dtype=real_dtype)
                out = tensor.cast(inp, dtype=complex_dtype)
                f = function([inp], out)
                obj = rand_of_dtype((2, ), real_dtype)
                assert f(obj).dtype == np.dtype(complex_dtype)
    def test_cast_from_complex_to_real_raises_error(self):
        for real_dtype in REAL_DTYPES:
            for complex_dtype in COMPLEX_DTYPES:
                inp = tensor.vector(dtype=real_dtype)
                self.assertRaises(TypeError, tensor.cast(
                    inp, dtype=complex_dtype))
ClipTester = makeTester(
    name='ClipTester',
    op=clip,
    expected=lambda x, y, z: np.clip(x, y, z),
    good=dict(correct1=((5 * rand(5, 5)).astype('float32'),
                        np.array(-1, dtype='float32'),
                        np.array(1, dtype='float32')),
              correct2=((5 * rand(5, 5)).astype('float64'),
                        np.array(-1, dtype='float64'),
                        np.array(1, dtype='float64')),
              correct3=(randint(5, 5).astype('int8'),
                        np.array(-1, dtype='int8'),
                        np.array(1, dtype='int8')),
              correct4=(randint(5, 5).astype('int16'),
                        np.array(-1, dtype='int16'),
                        np.array(1, dtype='int16')),
              correct5=(randint(5, 5).astype('int32'),
                        np.array(-1, dtype='int32'),
                        np.array(1, dtype='int32')),
              correct6=(randint(5, 5).astype('int64'),
                        np.array(-1, dtype='int64'),
                        np.array(1, dtype='int64')),
              correct8=(randint(0, 5).astype('uint8'),
                        np.array(2, dtype='uint8'),
                        np.array(4, dtype='uint8')),
              correct9=(randint(0, 5).astype('uint16'),
                        np.array(2, dtype='uint16'),
                        np.array(4, dtype='uint16')),)
    )
BackwardsClipTester = makeTester(
    name='BackwardsClipTester',
    op=clip,
    expected=lambda x, y, z: np.where(x &lt; y, y, np.minimum(x, z)),
    good=dict(correct7=((5 * rand(5, 5)).astype('float64'),
                        np.array(1, dtype='float64'),
                        np.array(-1, dtype='float64')),)
    )
class T_Clip(unittest.TestCase):
    def test_complex_value(self):
        for dtype in ['complex64', 'complex128']:
            a = tensor.vector(dtype=dtype)
            b = tensor.scalar()
            c = tensor.scalar()
            self.assertRaises(TypeError, clip, a, b, c)
    def test_clip_repeat_grad(self):
        x, y = tensor.vectors('xy')
        a = clip(x, y, x)
        g = theano.gradient.grad(a.sum(), x)
        fn = theano.function([x, y], [g])
        a2 = clip(x, x, y)
        g2 = theano.gradient.grad(a2.sum(), x)
        fn2 = theano.function([x, y], [g2])
        a3 = theano.tensor.clip(x, x, x)
        g3 = theano.gradient.grad(a3.sum(), x)
        fn3 = theano.function([x], [g3])
        rng = np.random.RandomState(utt.fetch_seed())
        nvals = 50
        xval = rng.rand(nvals).astype(config.floatX)
        yval_mn = rng.rand(nvals).astype(config.floatX) - 1.0
        yval_mx = rng.rand(nvals).astype(config.floatX) + 1.0
        aval, = fn(xval, yval_mn)
        aval2, = fn2(xval, yval_mx)
        aval3, = fn3(xval)
        self.assertTrue(np.all(aval == 1.))
        self.assertTrue(np.all(aval2 == 1.))
        self.assertTrue(np.all(aval3 == 1.))
    def test_clip_repeat_verify_grad(self):
        utt.verify_grad(
            op=lambda x: clip(x, 0, x),
            pt=[rand_nonzero((3, 7))])
        utt.verify_grad(
            op=lambda x: clip(x, x, 0),
            pt=[rand_nonzero((3, 7))])
        utt.verify_grad(
            op=lambda x: clip(0, x, x),
            pt=[rand_nonzero((3, 7))])
        utt.verify_grad(
            op=lambda x: clip(x, x, x),
            pt=[rand_nonzero((3, 7))])
def test_batched_dot():
    first = theano.tensor.tensor3("first")
    second = theano.tensor.tensor3("second")
    output = theano.tensor.basic.batched_dot(first, second)
    first_val = np.random.rand(10, 10, 20).astype(config.floatX)
    second_val = np.random.rand(10, 20, 5).astype(config.floatX)
    result_fn = theano.function([first, second], output)
    result = result_fn(first_val, second_val)
    assert result.shape[0] == first_val.shape[0]
    assert result.shape[1] == first_val.shape[1]
    assert result.shape[2] == second_val.shape[2]
    first_mat = theano.tensor.dmatrix("first")
    second_mat = theano.tensor.dmatrix("second")
    output = theano.tensor.basic.batched_dot(first_mat, second_mat)
    first_mat_val = np.random.rand(10, 10).astype(config.floatX)
    second_mat_val = np.random.rand(10, 10).astype(config.floatX)
    result_fn = theano.function([first_mat, second_mat], output)
    result = result_fn(first_mat_val, second_mat_val)
    assert result.shape[0] == first_mat_val.shape[0]
def test_batched_dot_not_contiguous():
    def np_genarray(*_shape):
        size = 1
        for dimsize in _shape:
            size *= dimsize
        return np.arange(size, dtype=floatX).reshape(_shape)
    X = tensor3()
    W = tensor3()
    Z = batched_dot(X, W)
    f = function([X, W], Z)
    w = np_genarray(30, 10, 5)
    reversed_x_container = np_genarray(20, 40, 30)
    x_container = reversed_x_container.T
    def check_first_dim(inverted):
        direction = -1 if inverted else 1
        x = x_container[::direction, ::2, ::2]
        assert x.shape == (30, 20, 10)
        assert x.strides[0] == direction * np.dtype(floatX).itemsize
        assert not (x.flags['C_CONTIGUOUS'] or x.flags['F_CONTIGUOUS'])
        result = f(x, w)
        ref_result = np.asarray(list(np.dot(u, v) for u, v in zip(x, w)))
        utt.assert_allclose(ref_result, result)
    for inverted in (0, 1):
        yield (check_first_dim, inverted)
def test_batched_tensordot():
    first = theano.tensor.tensor4("first")
    second = theano.tensor.tensor4("second")
    axes = [[1, 2], [3, 1]]
    output = theano.tensor.basic.batched_tensordot(first, second, axes)
    first_val = np.random.rand(8, 10, 20, 3).astype(config.floatX)
    second_val = np.random.rand(8, 20, 5, 10).astype(config.floatX)
    result_fn = theano.function([first, second], output)
    result = result_fn(first_val, second_val)
    assert result.shape[0] == first_val.shape[0]
    assert result.shape[1] == first_val.shape[3]
    assert result.shape[2] == second_val.shape[2]
    first_mat = theano.tensor.dmatrix("first")
    second_mat = theano.tensor.dmatrix("second")
    axes = 1
    output = theano.tensor.basic.batched_tensordot(first_mat, second_mat, axes)
    first_mat_val = np.random.rand(10, 4).astype(config.floatX)
    second_mat_val = np.random.rand(10, 4).astype(config.floatX)
    result_fn = theano.function([first_mat, second_mat], output)
    result = result_fn(first_mat_val, second_mat_val)
    assert result.shape[0] == first_mat_val.shape[0]
    assert len(result.shape) == 1
def test_tensor_values_eq_approx():
    a = np.asarray([-np.inf, -1, 0, 1, np.inf, np.nan])
    assert TensorType.values_eq_approx(a, a)
    b = np.asarray([np.inf, -1, 0, 1, np.inf, np.nan])
    assert not TensorType.values_eq_approx(a, b)
    b = np.asarray([-np.inf, -1, 0, 1, -np.inf, np.nan])
    assert not TensorType.values_eq_approx(a, b)
    b = np.asarray([np.inf, -1, 0, 1, 5, np.nan])
    assert TensorType.values_eq_approx(a, b, allow_remove_inf=True)
    b = np.asarray([np.inf, -1, 0, 1, 5, 6])
    assert not TensorType.values_eq_approx(a, b, allow_remove_inf=True)
    b = np.asarray([np.inf, -1, 0, 1, 5, np.nan])
    assert not TensorType.values_eq_approx(a, b, allow_remove_nan=False)
    b = np.asarray([-np.inf, -1, 0, 1, np.inf, 6])
    assert not TensorType.values_eq_approx(a, b, allow_remove_nan=False)
<a name="6"></a>def test_nan_inf_constant_signature():
    test_constants <font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [
        [np.nan, np.inf, 0, 1],
        [np.nan, np.inf, -np.inf, 1],
        [0, np.inf, -np.inf, 1],
        [0, 3, -np.inf, 1],
        [0, 3, np.</b></font>inf, 1],
        [np.nan, 3, 4, 1],
        [0, 3, 4, 1],
        np.nan,
        np.inf,
        -np.inf,
        0,
        1,
        ]
    n = len(test_constants)
    for i in xrange(n):
        for j in xrange(n):
            x = constant(test_constants[i])
            y = constant(test_constants[j])
            assert (x.signature() == y.signature()) == (i == j)
    x = tensor.scalar()
    mode = get_default_mode()
    if isinstance(mode, theano.compile.debugmode.DebugMode):
        mode = copy(mode)
        mode.check_isfinite = False
    f = theano.function([x], eq(x, np.nan), mode=mode)
    assert f(0) == 0
    assert f(np.nan) == 0
def test_isnan():
    for x in [tensor.matrix(), tensor.imatrix(), tensor.matrix(dtype='bool')]:
        y = tensor.isnan(x)
        assert isinstance(y.owner.op, tensor.Elemwise) == (
            x.dtype not in tensor.discrete_dtypes)
        assert y.dtype == 'bool'
        y = tensor.isnan_(x)
        assert isinstance(y.owner.op, tensor.Elemwise)
        assert y.dtype == 'bool'
        f = theano.function([x], y, allow_input_downcast=True)
        f([[0, 1, 2]])
class T_Shape(unittest.TestCase):
    def test_basic0(self):
        s = shape(np.ones((5, 3)))
        self.assertTrue((eval_outputs([s]) == [5, 3]).all())
    def test_basic1(self):
        s = shape(np.ones((2)))
        self.assertTrue((eval_outputs([s]) == [2]).all())
    def test_basic2(self):
        s = shape(np.ones((5, 3, 10)))
        self.assertTrue((eval_outputs([s]) == [5, 3, 10]).all())
class T_max_and_argmax(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
        MaxAndArgmax.debug = 0
    def test0(self):
        n = as_tensor_variable(5.0)
        v, i = eval_outputs(max_and_argmax(n))
        self.assertTrue(v == 5.0)
        self.assertTrue(i == 0)
        assert i.dtype == 'int64'
        v = eval_outputs(max_and_argmax(n)[0].shape)
        assert len(v) == 0
        v = eval_outputs(max_and_argmax(n)[1].shape)
        assert len(v) == 0
    def test1(self):
        n = as_tensor_variable([1, 2, 3, 2, -6])
        v, i = eval_outputs(max_and_argmax(n))
        self.assertTrue(v == 3)
        self.assertTrue(i == 2)
        assert i.dtype == 'int64'
        v = eval_outputs(max_and_argmax(n)[0].shape)
        assert len(v) == 0
    def test2(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
                                ([0, 1], None), ([1, 0], None),
                                (NoneConst.clone(), None),
                                (constant(0), 0)]:
            v, i = eval_outputs(max_and_argmax(n, axis))
            assert i.dtype == 'int64'
            self.assertTrue(np.all(v == np.max(data, np_axis)))
            self.assertTrue(np.all(i == np.argmax(data, np_axis)))
            v_shape = eval_outputs(max_and_argmax(n, axis)[0].shape)
            assert tuple(v_shape) == np.max(data, np_axis).shape
    def test2_float16(self):
        data = (rand(20, 30).astype("float16") - 0.5) * 20
        n = shared(data)
        for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
                                ([0, 1], None), ([1, 0], None),
                                (NoneConst.clone(), None),
                                (constant(0), 0)]:
            v, i = eval_outputs(max_and_argmax(n, axis), (MaxAndArgmax,))
            assert i.dtype == 'int64'
            self.assertTrue(np.all(v == np.max(data, np_axis)))
            self.assertTrue(np.all(i == np.argmax(data, np_axis)))
            v_shape = eval_outputs(max_and_argmax(n, axis)[0].shape)
            assert tuple(v_shape) == np.max(data, np_axis).shape
    def test2_invalid(self):
        n = as_tensor_variable(rand(2, 3))
        _logger = logging.getLogger('theano.gof.opt')
        oldlevel = _logger.level
        _logger.setLevel(logging.CRITICAL)
        try:
            try:
                eval_outputs(max_and_argmax(n, 3))
                assert False
            except ValueError:
                pass
        finally:
            _logger.setLevel(oldlevel)
    def test2_invalid_neg(self):
        n = as_tensor_variable(rand(2, 3))
        old_stderr = sys.stderr
        sys.stderr = StringIO()
        try:
            try:
                eval_outputs(max_and_argmax(n, -3))
                assert False
            except ValueError:
                pass
        finally:
            sys.stderr = old_stderr
    def test2_valid_neg(self):
        n = as_tensor_variable(rand(2, 3))
        v, i = eval_outputs(max_and_argmax(n, -1))
        assert i.dtype == 'int64'
        self.assertTrue(v.shape == (2,))
        self.assertTrue(i.shape == (2,))
        self.assertTrue(np.all(v == np.max(n.value, -1)))
        self.assertTrue(np.all(i == np.argmax(n.value, -1)))
        v, i = eval_outputs(max_and_argmax(n, -2))
        assert i.dtype == 'int64'
        self.assertTrue(v.shape == (3,))
        self.assertTrue(i.shape == (3,))
        self.assertTrue(np.all(v == np.max(n.value, -2)))
        self.assertTrue(np.all(i == np.argmax(n.value, -2)))
        v = eval_outputs(max_and_argmax(n, -1)[0].shape)
        assert v == (2)
        v = eval_outputs(max_and_argmax(n, -2)[0].shape)
        assert v == (3)
<a name="24"></a>    def test3(self):
        data = rand(2, 3, 4)
        n = as_tensor_variable(data)
        for (<font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
                                ([0, 1, 2], None), ([1, 2, 0], None)]:
            v, i = eval_outputs(</b></font>max_and_argmax(n, axis))
            assert i.dtype == 'int64'
            self.assertTrue(np.all(v == np.max(data, np_axis)))
            self.assertTrue(np.all(i == np.argmax(data, np_axis)))
            v = eval_outputs(max_and_argmax(n, axis)[0].shape)
            assert tuple(v) == np.max(data, np_axis).shape
    def test_arg_grad(self):
        x = matrix()
        cost = argmax(x, axis=0).sum()
        gx = grad(cost, x)
        val = tensor.get_scalar_constant_value(gx)
        assert val == 0.0
    def test_grad(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        def safe_verify_grad(func, data):
            data_tensor, = data
            data_vector = data_tensor.flatten()
            diff = np.abs(data_vector.reshape((-1, 1)) - data_vector)
            for i in xrange(len(diff)):
                diff[i, i] = 1
            eps = builtin_min(numeric_grad.type_eps[config.floatX],
                              diff.min() / 2)
            utt.verify_grad(func, data, eps=eps)
        def check_grad_max(data, max_grad_data, axis=None):
            assert axis in [0, None]
            z = np.zeros_like(data)
            z = z.flatten()
            argmax = np.argmax(data, axis=axis)
            if argmax.ndim == 0:
                z[argmax] += 1
            else:
                for id, v in enumerate(argmax):
                    z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
                      id] += 1
            z = z.reshape(data.shape)
            assert np.all(max_grad_data == z)
        for axis in (-1, 0, 1, None):
            for j in xrange(2):
                safe_verify_grad(lambda v: max_and_argmax(v, axis=axis)[j],
                                 [data])
                if axis != 1:
                    safe_verify_grad(lambda v: max_and_argmax(v.flatten(),
                                                              axis=axis)[j],
                                     [data])
            if axis in (0, None):
                check_grad_max(data, eval_outputs(grad(
                    max_and_argmax(n, axis=axis)[0].sum(), n)), axis=axis)
            check_grad_max(data, eval_outputs(grad(
                max_and_argmax(n.flatten())[0], n)))
        data = rand(3, 4, 5)
        for i in [0, 1, 2]:
            safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[0], [data])
            safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[1], [data])
        data = rand(2, 3, 4, 5)
        for i in [0, 1, 2, 3]:
            safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[0], [data])
            safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[1], [data])
        for i in [[0, 1], [0, 0]]:
            safe_verify_grad(lambda v: max_and_argmax(v, axis=i)[0], [data])
            safe_verify_grad(lambda v: max_and_argmax(v, axis=i)[1], [data])
    def test_preserve_broadcastable(self):
        x = tensor.matrix().dimshuffle('x', 0, 'x', 1, 'x')
        y = x.max(axis=1)
        assert y.type.broadcastable == (True, True, False, True)
    def test_multiple_axes(self):
        data = np.arange(24).reshape(3, 2, 4)
        x = as_tensor_variable(data)
        v, i = eval_outputs(max_and_argmax(x, [1, -1]))
        assert np.all(v == np.array([7, 15, 23]))
        assert np.all(i == np.array([7, 7, 7]))
        v = eval_outputs(max_and_argmax(x, [1, -1])[0].shape)
        assert tuple(v) == np.max(data, (1, -1)).shape
    def test_zero_shape(self):
        x = tensor.matrix()
        m, i = max_and_argmax(x, axis=1)
        f = theano.function([x], [m, i])
        xv = np.zeros((0, 4), dtype=floatX)
        mv, iv = f(xv)
        assert mv.shape == (0,)
        assert iv.shape == (0,)
    def test_numpy_input(self):
        ar = np.array([1, 2, 3])
        max, argmax = max_and_argmax(ar, axis=None)
        self.assertEqual(max.eval(), 3)
        self.assertEqual(argmax.eval(), 2)
class T_argmin_argmax(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
        MaxAndArgmax.debug = 0
    def test_scalar(self):
        for fct in [argmin, argmax]:
            n = as_tensor_variable(5.0)
            i = eval_outputs(fct(n))
            self.assertTrue(i == 0)
            v = eval_outputs(fct(n).shape)
            assert len(v) == 0
    def test_list(self):
        n = as_tensor_variable([1, 2, 3, 2, -6])
        i = eval_outputs(argmin(n))
        self.assertTrue(i == 4)
        v = eval_outputs(argmin(n).shape)
        assert len(v) == 0
        n = as_tensor_variable([1, 2, 3, 2, -6])
        i = eval_outputs(argmax(n))
        self.assertTrue(i == 2)
        v = eval_outputs(argmax(n).shape)
        assert len(v) == 0
    def test2(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
            for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
                                    ([0, 1], None), ([1, 0], None)]:
                v = eval_outputs(fct(n, axis))
                self.assertTrue(np.all(v == nfct(data, np_axis)))
                v_shape = eval_outputs(fct(n, axis).shape)
                assert tuple(v_shape) == nfct(data, np_axis).shape
    def test2_float16(self):
        data = (rand(20, 30).astype("float16") - 0.5) * 20
        n = shared(data)
        mode = get_default_mode().including("local_max_and_argmax", "uncanonicalize")
        for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
            for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
                                    ([0, 1], None), ([1, 0], None)]:
                v = eval_outputs(fct(n, axis), (Argmax,), mode=mode)
                self.assertTrue(np.all(v == nfct(data, np_axis)))
                v_shape = eval_outputs(fct(n, axis).shape, mode=mode)
                assert tuple(v_shape) == nfct(data, np_axis).shape
    def test2_invalid(self):
        for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
            n = as_tensor_variable(rand(2, 3))
            _logger = logging.getLogger('theano.gof.opt')
            oldlevel = _logger.level
            _logger.setLevel(logging.CRITICAL)
            try:
                try:
                    eval_outputs(fct(n, 3))
                    assert False
                except ValueError:
                    pass
            finally:
                _logger.setLevel(oldlevel)
    def test2_invalid_neg(self):
        for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
            n = as_tensor_variable(rand(2, 3))
            old_stderr = sys.stderr
            sys.stderr = StringIO()
            try:
                try:
                    eval_outputs(fct(n, -3))
                    assert False
                except ValueError:
                    pass
            finally:
                sys.stderr = old_stderr
    def test2_valid_neg(self):
        for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
            n = as_tensor_variable(rand(2, 3))
            i = eval_outputs(fct(n, -1))
            self.assertTrue(i.shape == (2,))
            self.assertTrue(np.all(i == nfct(n.value, -1)))
            i = eval_outputs(fct(n, -2))
            self.assertTrue(i.shape == (3,))
            self.assertTrue(np.all(i == nfct(n.value, -2)))
            v = eval_outputs(fct(n, -1).shape)
            assert v == (2)
            v = eval_outputs(fct(n, -2).shape)
            assert v == (3)
    def test3(self):
        data = rand(2, 3, 4)
        n = as_tensor_variable(data)
        for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
            for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (2, 2),
                                    (None, None), ([0, 1, 2], None),
                                    ([1, 0, 2], None)]:
                v = eval_outputs(fct(n, axis))
                self.assertTrue(np.all(v == nfct(data, np_axis)))
                v_shape = eval_outputs(fct(n, axis).shape)
                assert tuple(v_shape) == nfct(data, np_axis).shape
    def test_grad_argmin(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        n.name = 'n'
        utt.verify_grad(lambda v: argmin(v, axis=-1), [data])
        utt.verify_grad(lambda v: argmin(v, axis=[0]), [data])
        utt.verify_grad(lambda v: argmin(v, axis=[1]), [data])
        utt.verify_grad(lambda v: argmin(v.flatten()), [data])
        try:
            cost = argmin(n, axis=-1)
            cost.name = None
            grad(cost, n)
            raise Exception('Expected an error')
        except TypeError:
            pass
    def test_grad_argmax(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        utt.verify_grad(lambda v: argmax(v, axis=-1), [data])
        utt.verify_grad(lambda v: argmax(v, axis=[0]), [data])
        utt.verify_grad(lambda v: argmax(v, axis=[1]), [data])
        utt.verify_grad(lambda v: argmax(v.flatten()), [data])
        try:
            grad(argmax(n, axis=-1), n)
            raise Exception('Expected an error')
        except TypeError:
            pass
    def test_uint(self):
        for dtype in ('uint8', 'uint16', 'uint32', 'uint64'):
            itype = np.iinfo(dtype)
            data = np.array([itype.min + 3, itype.min, itype.max - 5, itype.max], dtype)
            n = as_tensor_variable(data)
            i = eval_outputs(argmin(n))
            self.assertEqual(i, 1)
            i = eval_outputs(argmax(n))
            self.assertEqual(i, 3)
    def test_bool(self):
        data = np.array([True, False], 'bool')
        n = as_tensor_variable(data)
        i = eval_outputs(argmin(n))
        self.assertEqual(i, 1)
        i = eval_outputs(argmax(n))
        self.assertEqual(i, 0)
class T_min_max(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
        MaxAndArgmax.debug = 0
    def test_scalar(self):
        for fct in [max, min]:
            n = as_tensor_variable(5.0)
            v = eval_outputs(fct(n))
            self.assertTrue(v == 5.0)
            v = eval_outputs(fct(n).shape)
            assert len(v) == 0
    def test_list(self):
        for fct, nfct in [(max, np.max), (min, np.min)]:
            n = as_tensor_variable([1, 2, 3, 2, -6])
            v = eval_outputs([fct(n)])
            self.assertTrue(v == nfct(n.value))
            v = eval_outputs(fct(n).shape)
            assert len(v) == 0
    def test2(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        for fct, nfct in [(max, np.max), (min, np.min)]:
            for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
                                    ([0, 1], None), ([1, 0], None)]:
                v = eval_outputs(fct(n, axis))
                self.assertTrue(np.all(v == nfct(data, np_axis)))
                v_shape = eval_outputs(fct(n, axis).shape)
                assert tuple(v_shape) == nfct(data, np_axis).shape
    def test2_invalid(self):
        for fct in [max, min]:
            n = as_tensor_variable(rand(2, 3))
            _logger = logging.getLogger('theano.gof.opt')
            oldlevel = _logger.level
            _logger.setLevel(logging.CRITICAL)
            try:
                try:
                    eval_outputs(fct(n, 3))
                    assert False
                except ValueError:
                    pass
            finally:
                _logger.setLevel(oldlevel)
    def test2_invalid_neg(self):
        for fct in [max, min]:
            n = as_tensor_variable(rand(2, 3))
            old_stderr = sys.stderr
            sys.stderr = StringIO()
            try:
                try:
                    eval_outputs(fct(n, -3))
                    assert False
                except ValueError:
                    pass
            finally:
                sys.stderr = old_stderr
    def test2_valid_neg(self):
        for fct, nfct in [(max, np.max), (min, np.min)]:
            n = as_tensor_variable(rand(2, 3))
            v = eval_outputs(fct(n, -1))
            self.assertTrue(v.shape == (2,))
            self.assertTrue(np.all(v == nfct(n.value, -1)))
            v = eval_outputs(fct(n, -2))
            self.assertTrue(v.shape == (3,))
            self.assertTrue(np.all(v == nfct(n.value, -2)))
            v = eval_outputs(fct(n, -1).shape)
            assert v == (2)
            v = eval_outputs(fct(n, -2).shape)
            assert v == (3)
    def test3(self):
        data = rand(2, 3, 4)
        n = as_tensor_variable(data)
        for fct, nfct in [(max, np.max), (min, np.min)]:
            for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (2, 2),
                                    (None, None), ([0, 1, 2], None),
                                    ([1, 0, 2], None)]:
                v = eval_outputs(fct(n, axis))
                self.assertTrue(np.all(v == nfct(data, np_axis)))
                v_shape = eval_outputs(fct(n, axis).shape)
                assert tuple(v_shape) == nfct(data, np_axis).shape
    def test3b(self):
        data = rand(2, 3, 4)
        n = as_tensor_variable(data)
        for fct, nfct in [(max, np.max), (min, np.min)]:
            for axis in [[0, 1], [1, 2], [0, 2]]:
                v = eval_outputs(fct(n, axis))
                np_v = nfct(nfct(data, axis[1]), axis[0])
                self.assertTrue(np.all(v == np_v))
                v_shape = eval_outputs(fct(n, axis).shape)
                assert tuple(v_shape) == np_v.shape
    def test_grad_max(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        def check_grad_max(data, max_grad_data, axis=None):
            assert axis in [0, None]
            z = np.zeros_like(data)
            z = z.flatten()
            argmax = np.argmax(data, axis=axis)
            if argmax.ndim == 0:
                z[np.argmax(data, axis=axis)] += 1
            else:
                for id, v in enumerate(argmax):
                    z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
                      id] += 1
            z = z.reshape(data.shape)
            assert np.all(max_grad_data == z)
        utt.verify_grad(lambda v: max(v, axis=-1), [data])
        utt.verify_grad(lambda v: max(v, axis=[0]), [data])
        check_grad_max(data, eval_outputs(grad(max(n, axis=0).sum(), n)),
                       axis=0)
        utt.verify_grad(lambda v: max(v, axis=[1]), [data])
        utt.verify_grad(lambda v: max(v.flatten()), [data])
        check_grad_max(data, eval_outputs(grad(max(n.flatten()), n)))
    def test_grad_min(self):
        data = rand(2, 3)
        n = as_tensor_variable(data)
        def check_grad_min(data, min_grad_data, axis=None):
            assert axis in [0, None]
            z = np.zeros_like(data)
            z = z.flatten()
            argmin = np.argmin(data, axis=axis)
            if argmin.ndim == 0:
                z[np.argmin(data, axis=axis)] += 1
            else:
                for id, v in enumerate(argmin):
                    z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
                      id] += 1
            z = z.reshape(data.shape)
            assert np.all(min_grad_data == z)
        utt.verify_grad(lambda v: min(v, axis=-1), [data])
        utt.verify_grad(lambda v: min(v, axis=[0]), [data])
        check_grad_min(data, eval_outputs(grad(min(n, axis=0).sum(), n)),
                       axis=0)
        utt.verify_grad(lambda v: min(v, axis=[1]), [data])
        utt.verify_grad(lambda v: min(v.flatten()), [data])
        check_grad_min(data, eval_outputs(grad(min(n.flatten()), n)))
    def _grad_list(self):
        data = rand(2, 3)
        for fct in [max_and_argmax, max, min]:
            utt.verify_grad(lambda v: fct(v, axis=[0, 1]), [data])
    def test_uint(self):
        for dtype in ('uint8', 'uint16', 'uint32', 'uint64'):
            itype = np.iinfo(dtype)
            data = np.array([itype.min + 3, itype.min, itype.max - 5, itype.max], dtype)
            n = as_tensor_variable(data)
            self.assertEqual(min(n).dtype, dtype)
            i = eval_outputs(min(n))
            self.assertEqual(i, itype.min)
            self.assertEqual(max(n).dtype, dtype)
            i = eval_outputs(max(n))
            self.assertEqual(i, itype.max)
    def test_bool(self):
        data = np.array([True, False], 'bool')
        n = as_tensor_variable(data)
        self.assertEqual(min(n).dtype, 'bool')
        i = eval_outputs(min(n))
        self.assertEqual(i, False)
        self.assertEqual(max(n).dtype, 'bool')
        i = eval_outputs(max(n))
        self.assertEqual(i, True)
def test_basic_allclose():
    assert tensor.basic._allclose(-0.311023883434, -0.311022856884)
class T_outer(unittest.TestCase):
    def test_outer(self):
        for m in range(4):
            for n in range(4):
                x = tensor.tensor(dtype='floatX', broadcastable=(False,) * m)
                y = tensor.tensor(dtype='floatX', broadcastable=(False,) * n)
                s1 = np.random.randint(1, 10, m)
                s2 = np.random.randint(1, 10, n)
                v1 = np.asarray(np.random.rand(*s1)).astype(floatX)
                v2 = np.asarray(np.random.rand(*s2)).astype(floatX)
                o = tensor.outer(x, y).eval({x: v1, y: v2})
                assert_allclose(o, np.outer(v1, v2))
    def test_grad(self):
        for shp0, shp1 in [((1,), (2,)),
                           ((3,), (1,)),
                           ((1,), (1,)),
                           ((3,), (2,)),
                           ((3, 2), (1, 1)),
                           ((3, 2), (1, 4)),
                           ((3, 2), (4, 1)),
                           ((3, 2), (4, 5)),
                           ((1, 2), (4, 5)),
                           ((3, 1), (4, 5)),
                           ((1, 1), (4, 5)),
                           ((1, 1), (1, 1)),
                           ]:
            data0 = np.random.rand(*shp0).astype(floatX)
            data1 = np.random.rand(*shp1).astype(floatX)
            utt.verify_grad(tensor.outer, [data0, data1])
class T_GetVectorLength(unittest.TestCase):
    def test_get_vector_length(self):
        x = theano.shared(np.zeros((2, 3, 4, 5)))
        assert len(list(x.shape)) == 4
        assert len(list(x.shape[2:4])) == 2
        assert len(list(x.shape[2:])) == 2
        assert len(list(x.shape[1:4])) == 3
        assert len(list(x.shape[2:2])) == 0
        assert len(list(x.shape[1:5])) == 3
        assert len(list(x.shape[1:10])) == 3
        assert len(list(x.shape[1:10:2])) == 2
        assert len(list(x.shape[-1:4])) == 1
        assert len(list(x.shape[-6:4])) == 4
        assert len(list(x.shape[1:-2])) == 1
        assert len(list(x.shape[1:-1])) == 2
class T_Join_and_Split(unittest.TestCase):
    def setUp(self):
        Join.debug = False
        utt.seed_rng()
        self.mode = theano.compile.get_default_mode().excluding(
            'constant_folding')
        self.join_op = Join()
        self.split_op_class = Split
        self.make_vector_op = opt.MakeVector()
        self.floatX = config.floatX
        self.hide_error = theano.config.mode not in [
            'DebugMode', 'DEBUG_MODE', 'FAST_COMPILE']
        self.shared = shared
    def eval_outputs_and_check_join(self, outputs):
        f = theano.function([], outputs, self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        variables = f()
        if isinstance(variables, (tuple, list)) and len(variables) == 1:
            return variables[0]
        return variables
    def eval_outputs_and_check_vector(self, outputs,
                                      make_vector_op=None):
        if make_vector_op is None:
            make_vector_op = self.make_vector_op
        f = theano.function([], outputs, self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(make_vector_op))]
        variables = f()
        if isinstance(variables, (tuple, list)) and len(variables) == 1:
            return variables[0]
        return variables
    def test_join_scalar(self):
        a = as_tensor_variable(1)
        b = as_tensor_variable(2)
        self.assertRaises(TypeError, join, 0, a, b)
    def test_stack_mixed_type_constants(self):
        a = as_tensor_variable(1)
        b = as_tensor_variable(2.0)
        c = tensor._shared(np.asarray(3.0, dtype=self.floatX))
        s = stack([a, b, c])
        want = np.array([1, 2, 3])
        out = self.eval_outputs_and_check_vector([s], opt.MakeVector())
        self.assertTrue((out == want).all())
    def test_stack_scalar(self):
        a = self.shared(np.asarray(1., dtype=self.floatX))
        b = as_tensor_variable(2.)
        c = as_tensor_variable(3.)
        s = stack([a, b, c])
        want = np.array([1, 2, 3])
        out = self.eval_outputs_and_check_vector([s])
        self.assertTrue((out == want).all())
    def test_stack_scalar_make_vector(self):
        a = tensor.scalar('a', dtype=self.floatX)
        b = tensor.scalar('b', dtype=self.floatX)
        s = stack([a, b, a, b])
        f = function([a, b], s, mode=self.mode)
        val = f(1, 2)
        self.assertTrue(np.all(val == [1, 2, 1, 2]))
        topo = f.maker.fgraph.toposort()
        assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
        assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
        assert f.maker.fgraph.outputs[0].dtype == self.floatX
    def test_stack_scalar_make_vector_dtype(self):
        a = tensor.iscalar('a')
        b = tensor.lscalar('b')
        s = stack([a, b, a, b])
        f = function([a, b], s, mode=self.mode)
        val = f(1, 2)
        self.assertTrue(np.all(val == [1, 2, 1, 2]))
        topo = f.maker.fgraph.toposort()
        assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
        assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
        assert f.maker.fgraph.outputs[0].dtype == 'int64'
    def test_stack_scalar_make_vector_constant(self):
        a = tensor.iscalar('a')
        b = tensor.lscalar('b')
        s = stack([10, a, b, np.int8(3)])
        f = function([a, b], s, mode=self.mode)
        val = f(1, 2)
        self.assertTrue(np.all(val == [10, 1, 2, 3]))
        topo = f.maker.fgraph.toposort()
        assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
        assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
        assert f.maker.fgraph.outputs[0].dtype == 'int64'
    def test_stack_new_interface(self):
        warnings.simplefilter('always', DeprecationWarning)
        a = tensor.imatrix('a')
        b = tensor.imatrix('b')
        s1 = stack(a, b)
        s2 = stack([a, b])
        f = function([a, b], [s1, s2], mode=self.mode)
        v1, v2 = f([[1, 2]], [[3, 4]])
        self.assertTrue(v1.shape == v2.shape)
        self.assertTrue(np.all(v1 == v2))
        s3 = stack([a, b], 1)
        f = function([a, b], s3, mode=self.mode)
        v3 = f([[1, 2]], [[3, 4]])
        v4 = np.array([[[1, 2], [3, 4]]])
        self.assertTrue(v3.shape == v4.shape)
        self.assertTrue(np.all(v3 == v4))
        v1 = [[1, 2, 3], [4, 5, 6]]
        v2 = [[7, 8, 9], [10, 11, 12]]
        s = stack([a, b], axis=-1)
        f = function([a, b], s, mode=self.mode)
        v = np.zeros((2, 3, 2))
        v[:, :, 0] = v1
        v[:, :, 1] = v2
        out = f(v1, v2)
        self.assertTrue(v.shape == out.shape)
        self.assertTrue(np.all(v == out))
        s = stack([a, b], axis=-2)
        f = function([a, b], s, mode=self.mode)
        v = np.zeros((2, 2, 3))
        v[:, 0, :] = v1
        v[:, 1, :] = v2
        out = f(v1, v2)
        self.assertTrue(v.shape == out.shape)
        self.assertTrue(np.all(v == out))
        self.assertRaises(IndexError, stack, [a, b], 4)
        self.assertRaises(IndexError, stack, [a, b], -4)
        with warnings.catch_warnings(record=True) as w:
            s = stack(a, b)
            assert len(w) == 1
            assert issubclass(w[-1].category, DeprecationWarning)
        with warnings.catch_warnings(record=True) as w:
            s = stack([a, b])
            s = stack([a, b], 1)
            s = stack([a, b], axis=1)
            s = stack(tensors=[a, b])
            s = stack(tensors=[a, b], axis=1)
            assert not w
    def test_stack_hessian(self):
        a = tensor.dvector('a')
        b = tensor.dvector('b')
        A = stack([a, b])
        B = A.T.dot(A)
        Ha, Hb = hessian(B.sum(), [a, b])
        a_v = np.random.rand(4)
        b_v = np.random.rand(4)
        f = theano.function([a, b], [Ha, Hb])
        Ha_v, Hb_v = f(a_v, b_v)
        assert Ha_v.shape == (4, 4)
        assert Hb_v.shape == (4, 4)
        assert np.allclose(Ha_v, 2.)
        assert np.allclose(Hb_v, 2.)
    def test_stack_hessian2(self):
        a = tensor.dvector('a')
        b = tensor.dvector('b')
        A = stack([a, b])
        Ha, Hb = hessian(A.sum(), [a, b])
        a_v = np.random.rand(4)
        b_v = np.random.rand(4)
        f = theano.function([a, b], [Ha, Hb])
        Ha_v, Hb_v = f(a_v, b_v)
        assert Ha_v.shape == (4, 4)
        assert Hb_v.shape == (4, 4)
        assert np.allclose(Ha_v, 0.)
        assert np.allclose(Hb_v, 0.)
    def test_join_concatenate_one_element(self):
        m = tensor.fmatrix()
        c = tensor.concatenate([m])
        f = theano.function(inputs=[m], outputs=[c],
                            mode=self.mode.including('local_join_1'))
        topo = f.maker.fgraph.toposort()
        assert len(topo) == 1
        assert isinstance(topo[0].op, DeepCopyOp)
    def test_join_vector(self):
        a = self.shared(np.array([1, 2, 3], dtype=self.floatX))
        b = as_tensor_variable(np.array([7, 8, 9], dtype=self.floatX))
        s = join(0, a, b)
        want = np.array([1, 2, 3, 7, 8, 9])
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
    def test_roll(self):
        for get_shift in [lambda a:a, lambda x:theano.shared(x)]:
            a = self.shared(np.array([1, 2, 3, 4, 5, 6], dtype=self.floatX))
            b = roll(a, get_shift(2))
            want = np.array([5, 6, 1, 2, 3, 4])
            out = theano.function([], b)()
            assert (out == want).all()
            b = roll(a, get_shift(-1), 0)
            want = np.array([2, 3, 4, 5, 6, 1])
            out = theano.function([], b)()
            assert (out == want).all()
            a = self.shared(np.arange(21).reshape((3, 7)).astype(self.floatX))
            b = roll(a, get_shift(-2), 1)
            want = np.roll(a.get_value(borrow=True), -2, 1)
            out = theano.function([], b)()
            assert (out == want).all()
            a = self.shared(np.arange(24).reshape((3, 2, 4)).astype(self.floatX))
            b = roll(a, get_shift(-2), -2)
            want = np.roll(a.get_value(borrow=True), -2, -2)
            out = theano.function([], b)()
            assert (out == want).all()
            want = np.roll(a.get_value(borrow=True), -2, 0)
            b = roll(a, get_shift(-2), 0)
            out = theano.function([], b)()
            assert (out == want).all()
            want = np.roll(a.get_value(borrow=True), 2)
            b = roll(a, get_shift(2))
            out = theano.function([], b)()
            assert (out == want).all()
            want = np.roll(a.get_value(borrow=True), 4, 0)
            b = roll(a, get_shift(4), 0)
            out = theano.function([], b)()
            assert (out == want).all()
            want = np.roll(a.get_value(borrow=True), -4, 0)
            b = roll(a, get_shift(-4), 0)
            out = theano.function([], b)()
            assert (out == want).all()
    def test_stack_vector(self):
        a = self.shared(np.array([1, 2, 3], dtype=self.floatX))
        b = as_tensor_variable(np.array([7, 8, 9], dtype=self.floatX))
        s = stack([a, b])
        want = np.array([[1, 2, 3], [7, 8, 9]])
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
    def test_join_matrix0(self):
        a = self.shared(np.array([[1, 2, 3], [4, 5, 6]],
                                 dtype=self.floatX))
        b = as_tensor_variable(np.array([[7, 8, 9]], dtype=self.floatX))
        s = join(0, a, b)
        want = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
    def test_join_matrix1(self):
        av = np.array([[.1, .2, .3], [.4, .5, .6]], dtype='float32')
        bv = np.array([[.7], [.8]], dtype='float32')
        a = self.shared(av)
        b = as_tensor_variable(bv)
        s = join(1, a, b)
        want = np.array([[.1, .2, .3, .7], [.4, .5, .6, .8]],
                        dtype='float32')
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
        utt.verify_grad(lambda a, b: join(1, a, b), [av, bv],
                        mode=self.mode)
    def test_join_matrix_dtypes(self):
        if "float32" in self.shared.__name__:
            raise SkipTest(
                "The shared variable constructor"
                " need to support other dtype then float32")
        av = np.array([[1, 2, 3], [4, 5, 6]], dtype='int8')
        bv = np.array([[7], [8]], dtype='float32')
        a = self.shared(av)
        b = as_tensor_variable(bv)
        s = join(1, a, b)
        want = np.array([[1, 2, 3, 7], [4, 5, 6, 8]], dtype='float32')
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
        grad(s.sum(), b)
        grad(s.sum(), a)
        utt.verify_grad(lambda b: join(1, a, b), [bv],
                        eps=1.0e-2, mode=self.mode)
    def test_join_matrix_ints(self):
        if "float32" in self.shared.__name__:
            raise SkipTest(
                "The shared variable constructor"
                " need to support other dtype then float32")
        av = np.array([[1, 2, 3], [4, 5, 6]], dtype='int8')
        bv = np.array([[7], [8]], dtype='int32')
        a = self.shared(av)
        b = as_tensor_variable(bv)
        s = join(1, a, b)
        want = np.array([[1, 2, 3, 7], [4, 5, 6, 8]], dtype='float32')
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
        assert (np.asarray(grad(s.sum(), b).eval()) == 0).all()
        assert (np.asarray(grad(s.sum(), a).eval()) == 0).all()
    def test_join_matrix1_using_vertical_stack(self):
        a = self.shared(np.array([[1, 2, 3], [4, 5, 6]], dtype=self.floatX))
        b = as_tensor_variable(np.array([[7, 8, 9]], dtype=self.floatX))
        c = as_tensor_variable(np.array([[9, 8, 7]], dtype=self.floatX))
        s = vertical_stack(a, b, c)
        want = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [9, 8, 7]])
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
    def test_join_matrix1_using_horizontal_stack(self):
        av = np.array([[.1, .2, .3], [.4, .5, .6]], dtype='float32')
        bv = np.array([[.7], [.8]], dtype='float32')
        cv = np.array([[.3, .2, .1], [.6, .5, .4]], dtype='float32')
        a = self.shared(av)
        b = as_tensor_variable(bv)
        c = as_tensor_variable(cv)
        s = horizontal_stack(a, b, c)
        want = np.array([[.1, .2, .3, .7, .3, .2, .1],
                         [.4, .5, .6, .8, .6, .5, .4]],
                        dtype='float32')
        out = self.eval_outputs_and_check_join([s])
        self.assertTrue((out == want).all())
        utt.verify_grad(lambda a, b: join(1, a, b), [av, bv],
                        mode=self.mode)
    def test_join_matrixV(self):
        v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
        a = self.shared(v)
        b = as_tensor_variable(v)
        ax = lscalar()
        s = join(ax, a, b)
        f = inplace_func([ax], [s], mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        want = np.array([[.1, .2, .3], [.4, .5, .6],
                         [.1, .2, .3], [.4, .5, .6]])
        got = f(0)
        assert np.allclose(got, want)
        want = np.array([[.1, .2, .3, .1, .2, .3],
                         [.4, .5, .6, .4, .5, .6]])
        got = f(1)
        assert np.allclose(got, want)
        utt.verify_grad(lambda a, b: join(0, a, b), [v, 2 * v], mode=self.mode)
        utt.verify_grad(lambda a, b: join(1, a, b), [v, 2 * v], mode=self.mode)
    def test_join_matrixV_negative_axis(self):
        v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
        a = self.shared(v)
        b = as_tensor_variable(v)
        ax = lscalar()
        s = join(ax, a, b)
        f = inplace_func([ax], [s], mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        want = np.array([[.1, .2, .3, .1, .2, .3],
                         [.4, .5, .6, .4, .5, .6]])
        got = f(-1)
        assert np.allclose(got, want)
        want = np.array([[.1, .2, .3], [.4, .5, .6],
                         [.1, .2, .3], [.4, .5, .6]])
        got = f(-2)
        assert np.allclose(got, want)
        self.assertRaises(IndexError, f, -3)
    def test_join_matrixC_negative_axis(self):
        v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
        a = self.shared(v)
        b = as_tensor_variable(v)
        s = join(-1, a, b)
        f = theano.function([], [s], mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        want = np.array([[.1, .2, .3, .1, .2, .3],
                         [.4, .5, .6, .4, .5, .6]])
        got = f()
        assert np.allclose(got, want)
        s = join(-2, a, b)
        f = theano.function([], [s], mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        want = np.array([[.1, .2, .3], [.4, .5, .6],
                         [.1, .2, .3], [.4, .5, .6]])
        got = f()
        assert np.allclose(got, want)
        self.assertRaises(IndexError, join, -3, a, b)
        utt.verify_grad(lambda a, b: join(-1, a, b), [v, 2 * v],
                        mode=self.mode)
    def test_vector_len(self):
        x = lscalar('x')
        y = dscalar('y')
        triple = as_tensor_variable((x, y, 9.0))
        assert 3 == get_vector_length(triple)
        a, b, c = triple
        f = function([x, y], [b, c, a], mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo if isinstance(node.op, opt.MakeVector)]
        assert np.allclose(f(4, 5), [5, 9, 4])
    def test_broadcastable_flag_assignment_mixed_otheraxes(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a_val = rng.rand(1, 4, 1).astype(self.floatX)
        b_val = rng.rand(1, 3, 1).astype(self.floatX)
        a = self.shared(a_val, broadcastable=(False, False, True))
        b = self.shared(b_val, broadcastable=(True, False, True))
        c = self.join_op(1, a, b)
        assert c.type.broadcastable[0] and c.type.broadcastable[2]
        assert not c.type.broadcastable[1]
        c = self.join_op(theano.tensor.constant(1), a, b)
        assert c.type.broadcastable[0] and c.type.broadcastable[2]
        assert not c.type.broadcastable[1]
        c = self.join_op(theano.tensor.cast(theano.tensor.constant(1),
                                            dtype="int32"),
                         a, b)
        assert c.type.broadcastable[0] and c.type.broadcastable[2]
        assert not c.type.broadcastable[1]
        f = function([], c, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        f()
        utt.verify_grad((lambda a, b: join(1, a, b)), [a_val, b_val], rng=rng,
                        mode=self.mode)
        a.set_value(rng.rand(2, 4, 1).astype(self.floatX))
        self.assertRaises(ValueError, f)
    def test_broadcastable_flag_assignment_mixed_thisaxes(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a_val = rng.rand(2, 4, 1).astype(self.floatX)
        b_val = rng.rand(1, 4, 1).astype(self.floatX)
        a = self.shared(a_val, broadcastable=(False, False, True))
        b = self.shared(b_val, broadcastable=(True, False, True))
        c = self.join_op(0, a, b)
        assert not c.type.broadcastable[0]
        f = function([], c, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        f()
        utt.verify_grad((lambda a, b: join(0, a, b)), [a_val, b_val], rng=rng,
                        mode=self.mode)
        self.assertRaises(TypeError, b.set_value,
                          rng.rand(3, 4, 1).astype(self.floatX))
        a = TensorType(dtype=self.floatX, broadcastable=[0, 0, 1])()
        b = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1])()
        c = self.join_op(0, a, b)
        f = function([a, b], c, mode=self.mode)
        bad_b_val = rng.rand(3, 4, 1).astype(self.floatX)
        self.assertRaises(TypeError, f, a_val, bad_b_val)
    def test_broadcastable_flags_all_broadcastable_on_joinaxis(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a_val = rng.rand(1, 4, 1).astype(self.floatX)
        b_val = rng.rand(1, 4, 1).astype(self.floatX)
        a = self.shared(a_val, broadcastable=(True, False, True))
        b = self.shared(b_val, broadcastable=(True, False, True))
        c = self.join_op(0, a, b)
        assert not c.type.broadcastable[0]
        f = function([], c, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        f()
        utt.verify_grad((lambda a, b: join(0, a, b)), [a_val, b_val], rng=rng,
                        mode=self.mode)
    def test_broadcastable_single_input_broadcastable_dimension(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a_val = rng.rand(1, 4, 1).astype(self.floatX)
        a = self.shared(a_val, broadcastable=(True, False, True))
        b = self.join_op(0, a)
        assert b.type.broadcastable[0]
        assert b.type.broadcastable[2]
        assert not b.type.broadcastable[1]
        f = function([], b, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        if theano.config.mode != 'FAST_COMPILE':
            assert not [True for node in topo
                        if isinstance(node.op, type(self.join_op))]
        f()
        utt.verify_grad((lambda a: join(0, a)), [a_val], rng=rng,
                        mode=self.mode)
        self.assertRaises(TypeError, a.set_value,
                          rng.rand(2, 4, 1).astype(self.floatX))
    def test_broadcastable_flags_many_dims_and_inputs(self):
        a = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 0, 0, 0])()
        b = TensorType(dtype=self.floatX, broadcastable=[1, 1, 1, 0, 0, 0])()
        c = TensorType(dtype=self.floatX, broadcastable=[1, 0, 0, 0, 0, 0])()
        d = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 1, 0, 1])()
        e = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 0, 0, 1])()
        f = self.join_op(0, a, b, c, d, e)
        fb = f.type.broadcastable
        assert not fb[0] and fb[1] and fb[2] and fb[3] and not fb[4] and fb[5]
        g = self.join_op(1, a, b, c, d, e)
        gb = g.type.broadcastable
        assert gb[0] and not gb[1] and gb[2] and gb[3] and not gb[4] and gb[5]
        h = self.join_op(4, a, b, c, d, e)
        hb = h.type.broadcastable
        assert hb[0] and hb[1] and hb[2] and hb[3] and not hb[4] and hb[5]
        f = function([a, b, c, d, e], f, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        assert [True for node in topo
                if isinstance(node.op, type(self.join_op))]
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
        b_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
        c_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
        d_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
        e_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
        f(a_val, b_val, c_val, d_val, e_val)
        utt.verify_grad((lambda a, b, c, d, e: join(0, a, b, c, d, e)),
                        [a_val, b_val, c_val, d_val, e_val], rng=rng,
                        mode=self.mode)
        bad_val = rng.rand(2, 1, 1, 1, 2, 1).astype(self.floatX)
        self.assertRaises(TypeError, f, bad_val, b_val, c_val, d_val, e_val)
        self.assertRaises(TypeError, f, a_val, bad_val, c_val, d_val, e_val)
        self.assertRaises(TypeError, f, a_val, b_val, bad_val, d_val, e_val)
        self.assertRaises(TypeError, f, a_val, b_val, c_val, bad_val, e_val)
        self.assertRaises(TypeError, f, a_val, b_val, c_val, d_val, bad_val)
        bad_a_val = rng.rand(1, 2, 1, 1, 2, 1).astype(self.floatX)
        bad_b_val = rng.rand(1, 1, 1, 1, 2, 2).astype(self.floatX)
        bad_c_val = rng.rand(1, 1, 2, 1, 2, 1).astype(self.floatX)
        bad_d_val = rng.rand(1, 2, 1, 1, 2, 1).astype(self.floatX)
        bad_e_val = rng.rand(1, 1, 1, 2, 2, 1).astype(self.floatX)
        self.assertRaises(ValueError, f, bad_a_val, b_val, c_val, d_val, e_val)
        self.assertRaises(ValueError, f, a_val, bad_b_val, c_val, d_val, e_val)
        self.assertRaises(ValueError, f, a_val, b_val, bad_c_val, d_val, e_val)
        self.assertRaises(ValueError, f, a_val, b_val, c_val, bad_d_val, e_val)
        self.assertRaises(ValueError, f, a_val, b_val, c_val, d_val, bad_e_val)
    def test_infer_shape_join(self):
        def get_mat(s1, s2):
            return np.asarray(np.random.uniform(size=(s1, s2)),
                              dtype=self.floatX)
        x1 = self.shared(get_mat(3, 4))
        x2 = self.shared(get_mat(2, 4))
        x3 = self.shared(get_mat(1, 4))
        z = self.join_op(0, x1, x2, x3)
        f = theano.function([], z.shape, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        out = f()
        assert (out == [6, 4]).all()
        if theano.config.mode != 'FAST_COMPILE':
            for node in f.maker.fgraph.toposort():
                assert not isinstance(node.op, type(self.join_op))
        x1.set_value(get_mat(3, 4))
        x2.set_value(get_mat(3, 4))
        x3.set_value(get_mat(3, 5))
        z = self.join_op(1, x1, x2, x3)
        f = theano.function([], z.shape, mode=self.mode)
        topo = f.maker.fgraph.toposort()
        out = f()
        assert (out == [3, 13]).all()
        if theano.config.mode != 'FAST_COMPILE':
            for node in topo:
                assert not isinstance(node.op, type(self.join_op))
        with change_flags(compute_test_value='off'):
            x1.set_value(get_mat(3, 4))
            x2.set_value(get_mat(3, 4))
            x3.set_value(get_mat(2, 5))
            if not self.hide_error:
                self.assertRaises(ValueError, f)
            else:
                f()
    def test_rebroadcast(self):
        x = tensor.TensorType(self.floatX, [False, False, True])()
        u = tensor.TensorType(self.floatX, [False, False, True])()
        tensor.concatenate([x, -u], axis=2)
    def test_concatenate_same(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        T_shared = self.shared(rng.rand(3, 4).astype(self.floatX))
        Tout = tensor.concatenate([T_shared, T_shared])
        f = function([], Tout, mode=self.mode)
        out = f()
        if theano.config.mode != 'FAST_COMPILE':
            assert [True for node in f.maker.fgraph.toposort()
                    if isinstance(node.op, type(self.join_op))]
        assert np.allclose(out,
                           np.concatenate([T_shared.get_value(),
                                           T_shared.get_value()]))
    def test_mixed_ndim_error(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        v = self.shared(rng.rand(4).astype(self.floatX))
        m = self.shared(rng.rand(4, 4).astype(self.floatX))
        self.assertRaises(TypeError, self.join_op, 0, v, m)
    def test_split_0elem(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        m = self.shared(rng.rand(4, 6).astype(self.floatX))
        o = self.split_op_class(2)(m, 0, [4, 0])
        f = function([], o, mode=self.mode)
        assert any([isinstance(node.op, self.split_op_class)
                    for node in f.maker.fgraph.toposort()])
        o1, o2 = f()
        assert np.allclose(o1, m.get_value(borrow=True))
        assert np.allclose(o2, m.get_value(borrow=True)[4:])
    @change_flags(compute_test_value='off')
    def test_split_neg(self):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        m = self.shared(rng.rand(4, 6).astype(self.floatX))
        o = self.split_op_class(2)(m, 0, [5, -1])
        f = function([], o, mode=self.mode)
        assert any([isinstance(node.op, self.split_op_class)
                    for node in f.maker.fgraph.toposort()])
        self.assertRaises(ValueError, f)
def test_join_inplace():
    s = tensor.lscalar()
    x = tensor.vector('x')
    z = tensor.zeros((s,))
    join = Join(view=0)
    c = join(0, x, z, z)
    f = theano.function([theano.In(x, borrow=True), s], theano.Out(c, borrow=True))
    data = np.array([3, 4, 5], dtype=theano.config.floatX)
    print(f(data, 0))
    if theano.config.mode not in ["DebugMode", "DEBUG_MODE"]:
        assert f(data, 0) is data
    assert np.allclose(f(data, 0), [3, 4, 5])
def test_join_oneInput():
    x_0 <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.fmatrix()
    x_1 = theano.tensor.fmatrix()
    x_2 = theano.tensor.fvector()
    join_0 = theano.tensor.concatenate([x_0], axis=</b></font>1)
    join_1 = theano.tensor.concatenate([x_0, x_1, theano.tensor.shape_padright(x_2)],
                                       axis=1)
    assert join_0 is x_0
    assert join_1 is not x_0
class test_comparison(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
        self.mode = None
        self.shared = shared
        self.dtypes = ['float64', 'float32', 'complex64', 'complex128']
    def inplace_func(self, inputs, outputs, check_isfinite=None):
        mode = self.mode
        if check_isfinite is False:
            if mode is None:
                mode = get_default_mode()
            mode.check_isfinite = False
        f = inplace_func(inputs, outputs, mode=mode)
        return f
    def test_gt(self):
        for dtype in self.dtypes:
            l = np.asarray([0., -1., 1.], dtype=dtype)
            r = np.asarray([0., 1., -1.], dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (tensor.constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), tensor.constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], x &gt; y)
                    v = fn()
                    self.assertTrue(np.all(v == (l &gt; r)), (v, (l &gt; r)))
                except TypeError:
                    assert err
    def test_lt(self):
        for dtype in self.dtypes:
            l = np.asarray([0., -1., 1.], dtype=dtype)
            r = np.asarray([0., 1., -1.], dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)), self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (tensor.constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), tensor.constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], x &lt; y)
                    v = fn()
                    self.assertTrue(np.all(v == (l &lt; r)), (v, (l &lt; r)))
                except TypeError:
                    assert err
    def test_le(self):
        for dtype in self.dtypes:
            l = np.asarray([0., -1., 1.], dtype=dtype)
            r = np.asarray([0., 1., -1.], dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (tensor.constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), tensor.constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], x &lt;= y)
                    v = fn()
                    self.assertTrue(np.all(v == (l &lt;= r)), (v, (l &lt;= r)))
                except TypeError:
                    assert err
    def test_ge(self):
        for dtype in self.dtypes:
            l = np.asarray([0., -1., 1.], dtype=dtype)
            r = np.asarray([0., 1., -1.], dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (tensor.constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), tensor.constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], x &gt;= y)
                    v = fn()
                    self.assertTrue(np.all(v == (l &gt;= r)), (v, (l &gt;= r)))
                except TypeError:
                    assert err
    def test_eq(self):
        for dtype in self.dtypes:
            l = np.asarray([0., -1., 1.], dtype=dtype)
            r = np.asarray([0., 1., -1.], dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (tensor.constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), tensor.constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], eq(x, y))
                    v = fn()
                    self.assertTrue(np.all(v == (l == r)), (v, (l == r)))
                except TypeError:
                    assert err
    def test_neq(self):
        for dtype in self.dtypes:
            l = np.asarray([0., -1., 1.], dtype=dtype)
            r = np.asarray([0., 1., -1.], dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (tensor.constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), tensor.constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], neq(x, y))
                    v = fn()
                    self.assertTrue(np.all(v == (l != r)), (v, (l != r)))
                except TypeError:
                    assert err
    def test_isclose(self):
        for dtype in self.dtypes:
            l = np.asarray(
                [0., 1., -1., 0.,
                 np.nan, np.inf, -np.inf, np.inf],
                dtype=dtype)
            r = np.asarray(
                [0., 1.0001, -1.000000000001, np.nan,
                 np.nan, np.inf, np.inf, 0.],
                dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), constant(r), False),
            ]:
                try:
                    o1 = isclose(x, y, equal_nan=False)
                    fn1 = self.inplace_func([], o1, check_isfinite=False)
                    o2 = isclose(x, y, equal_nan=True)
                    fn2 = self.inplace_func([], o2, check_isfinite=False)
                    v1 = fn1()
                    v2 = fn2()
                    self.assertTrue(
                        np.all(
                            v1 == np.asarray(
                                [True, False, True, False,
                                 False, True, False, False],
                                dtype="bool"
                            )
                        ),
                        np.all(
                            v2 == np.asarray(
                                [True, False, True, False,
                                 True, True, False, False],
                                dtype="bool"
                            )
                        )
                    )
                except TypeError:
                    if not dtype.startswith('complex'):
                        raise
                        assert err
    def test_allclose(self):
        for dtype in self.dtypes:
            l = np.asarray(
                [0., 1., -1., 0.,
                 np.nan, np.inf, -np.inf, np.inf],
                dtype=dtype)
            r = np.asarray(
                [0., 1.0001, -1.000000000001, np.nan,
                 np.nan, np.inf, np.inf, 0.],
                dtype=dtype)
            for x, y, err in [
                (self.shared(l.astype(dtype)),
                 self.shared(r.astype(dtype)), False),
                (l, self.shared(r.astype(dtype)), True),
                (constant(l), self.shared(r.astype(dtype)), False),
                (self.shared(l.astype(dtype)), r, False),
                (self.shared(l.astype(dtype)), constant(r), False),
            ]:
                try:
                    fn = self.inplace_func([], allclose(x, y, equal_nan=False),
                                           check_isfinite=False)
                    v = fn()
                    self.assertTrue(np.all(v == np.allclose(l, r)))
                except TypeError:
                    if not dtype.startswith('complex'):
                        assert err
class test_bitwise(unittest.TestCase):
    dtype = ['int8', 'int16', 'int32', 'int64', ]
    def test_or(self):
        for dtype in self.dtype:
            x, y = vector(dtype=dtype), vector(dtype=dtype)
            fn = inplace_func([x, y], x | y)
            l = theano._asarray([0, 0, 1, 1], dtype=dtype)
            r = theano._asarray([0, 1, 0, 1], dtype=dtype)
            v = fn(l, r)
            self.assertTrue(np.all(v == (operator.or_(l, r))), (l, r, v))
    def test_xor(self):
        for dtype in self.dtype:
            x, y = vector(dtype=dtype), vector(dtype=dtype)
            fn = inplace_func([x, y], x ^ y)
            ix = x
            ix = inplace.xor_inplace(ix, y)
            gn = inplace_func([x, y], ix)
            l = theano._asarray([0, 0, 1, 1], dtype=dtype)
            r = theano._asarray([0, 1, 0, 1], dtype=dtype)
            v = fn(l, r)
            self.assertTrue(np.all(v == (operator.xor(l, r))), (l, r, v))
            v = gn(l, r)
            self.assertTrue(np.all(l == np.asarray([0, 1, 1, 0])), l)
    def test_and(self):
        for dtype in self.dtype:
            x, y = vector(dtype=dtype), vector(dtype=dtype)
            fn = inplace_func([x, y], x &amp; y)
            l = theano._asarray([0, 0, 1, 1], dtype=dtype)
            r = theano._asarray([0, 1, 0, 1], dtype=dtype)
            v = fn(l, r)
            self.assertTrue(np.all(v == (operator.and_(l, r))), (l, r, v))
    def test_inv(self):
        for dtype in self.dtype:
            x = vector(dtype=dtype)
            fn = inplace_func([x], ~x)
            for l in [[0, 0, 1, 1], [0, 1, 0, 1],
                      [0, 0, 1, 1], [0, 1, 0, 1],
                      [-1, 2 ** 16, 2 ** 16 - 1]
                      ]:
                l = theano._asarray([0, 0, 1, 1], dtype=dtype)
                v = fn(l)
                self.assertTrue(np.all(v == (~l)), (l, v))
    def test_eye(self):
        n = iscalar()
        m = iscalar()
        k = iscalar()
        fn = theano.function([m, n, k], eye(m, n, k))
        self.assertTrue(np.all(fn(5, 6, 1) == np.eye(5, 6, 1)))
class T_add(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
    def test_complex_all_ops(self):
        for nbits in (64, 128):
            a = shared(np.ones(3, dtype='complex%i' % nbits) + 0.5j)
            b = shared(np.ones(3, dtype='complex%i' % nbits) + 1.5j)
            tests = (("+", lambda x, y: x + y),
                     ("-", lambda x, y: x - y),
                     ("*", lambda x, y: x * y),
                     ("/", lambda x, y: x / y))
            for s, fn in tests:
                f = inplace_func([], fn(a, b))
                self.assertTrue(a.type.values_eq_approx(fn(
                    a.get_value(), b.get_value()), f()))
    def test_grad_scalar_l(self):
        utt.verify_grad(add, [np.asarray([3.0]), rand(3)])
    def test_grad_scalar_r(self):
        utt.verify_grad(add, [rand(3), np.asarray([3.0])])
    def test_grad_row(self):
        utt.verify_grad(add, [rand(3, 5), rand(1, 5)])
    def test_grad_col(self):
        utt.verify_grad(add, [rand(3, 5), rand(3, 1)])
class T_ceil(unittest.TestCase):
    def test_complex(self):
        self.assertRaises(TypeError, tensor.ceil, tensor.zvector())
class T_exp(unittest.TestCase):
    def test_grad_0(self):
        utt.verify_grad(exp, [
            np.asarray([[1.5089518, 1.48439076, -4.7820262],
                        [2.04832468, 0.50791564, -1.58892269]])])
    def test_grad_1(self):
        utt.verify_grad(inplace.exp_inplace, [
            np.asarray([[1.5089518, 1.48439076, -4.7820262],
                        [2.04832468, 0.50791564, -1.58892269]])])
    if theano.config.cycle_detection == 'fast' and theano.config.mode != 'FAST_COMPILE':
        test_grad_1 = unittest.expectedFailure(test_grad_1)
    def test_int(self):
        x = ivector()
        f = function([x], exp(x))
        exp_3 = f([3])
        assert exp_3.dtype == 'float64'
    def test_complex(self):
        x = zvector()
        assert exp(x).dtype == 'complex128'
        f = function([x], exp(x))
        exp_3 = f([3 + 2j])
        assert np.allclose(exp_3, np.exp(3 + 2j))
class T_divimpl(unittest.TestCase):
    def test_impls(self):
        i = iscalar()
        ii = lscalar()
        d = dscalar()
        f = fscalar()
        c = cscalar()
        assert np.allclose(function([i, d], i / d)(5, 7.0), (5.0 / 7.0))
        assert np.allclose(function([i, d], d / i)(5, 7.0), (7.0 / 5.0))
        assert np.allclose(function([i, f], i / f)(5, 11.0), (5.0 / 11.0))
        assert np.allclose(function([i, f], f / i)(5, 11.0), (11.0 / 5.0))
        assert np.allclose(function([i, ii], i // ii)(5, 3), (5 // 3))
        assert np.allclose(function([i, ii], ii // i)(5, 3), (3 // 5))
        assert np.allclose(function([i, ii], true_div(i, ii))(5, 3),
                           (5. / 3.))
        assert np.allclose(function([i, ii], true_div(ii, i))(5, 3),
                           (3. / 5.))
        assert np.allclose(function([i, c], i / c)(5, np.complex(5, 3)),
                           (5. / (5 + 3j)))
        assert np.allclose(function([i, c], c / i)(5, np.complex(5, 3)),
                           ((5 + 3j) / 5.))
class T_mean(unittest.TestCase):
    def test_regression_mean_of_ndarray_failure(self):
        try:
            tensor.mean(np.zeros(1))
        except AttributeError:
            self.fail()
    def test_mean_f16(self):
        x = tensor.vector(dtype='float16')
        y = x.mean()
        f = theano.function([x], y)
        utt.assert_allclose(f(np.ones((100000,), dtype='float16')), 1.0)
    def test0(self):
        x = tensor.vector()
        f = theano.function([x], tensor.mean(x))
        data = rand(50)
        assert np.allclose(f(data), np.mean(data))
    def test_list(self):
        ll = [theano.shared(0.), theano.shared(2.)]
        tensor.mean(ll).eval() == 1
class test_matinv(unittest.TestCase):
    def mat_reciprocal(self, dim):
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a, b = matrices('ab')
        ab = a * b
        diff = ab - as_tensor_variable(np.ones((dim, dim),
                                               dtype=config.floatX))
        ssdiff = sum((diff ** 2.0))
        g_b = grad(ssdiff, b)
        fn = inplace_func([a, b], [ssdiff, g_b])
        x = rng.rand(dim, dim) + 0.1      # Initialized s.t. x is not too tiny
        w = rng.rand(dim, dim)
        x = np.asarray(x, dtype=config.floatX)
        w = np.asarray(w, dtype=config.floatX)
        for i in xrange(100):
            ssd, gw = fn(x, w)
            if i == 0:
                ssd0 = ssd
            w -= 0.4 * gw
        return ssd0, ssd
    def test_reciprocal(self):
        ssd0, ssd = self.mat_reciprocal(3)
        rng = np.random.RandomState(seed=utt.fetch_seed())
        x = rng.rand(3, 3) + 0.1
        w = rng.rand(3, 3)
        x = np.asarray(x, dtype=config.floatX)
        w = np.asarray(w, dtype=config.floatX)
        ones = np.ones((3, 3), dtype=config.floatX)
        myssd0 = np.sum((x * w - ones) ** 2.0)
        for i in xrange(100):
            gw = 2 * (x * w - ones) * x  # derivative of dMSE/dw
            myssd = np.sum((x * w - ones) ** 2)
            w -= 0.4 * gw
        self.assertAlmostEqual(ssd0, myssd0)
        self.assertAlmostEqual(ssd, myssd)
class t_dot(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
    def cmp_dot(self, x, y):
        def spec(x):
            x = np.asarray(x)
            return type(x), x.dtype, x.shape
<a name="25"></a>        nz = np.dot(x, y)
        tz = eval_outputs([dot(as_tensor_variable(x), as_tensor_variable(y))])
        self.assertTrue(tz.dtype == nz.dtype,
                        (tz<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.dtype, tz.dtype.num, nz.dtype, nz.dtype.num))
        self.assertTrue(tz.shape == nz.shape, (tz.</b></font>shape, nz.shape))
        utt.assert_allclose(nz, tz, rtol=1e-4, atol=1e-4)
<a name="19"></a>
    def test_Op_dims(self):
        _dot = theano<font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.tensor.basic._dot
        d0 = scalar()
        d1 = vector()
        d2 = matrix()
        d3 = tensor3()
        self.assertRaises(</b></font>TypeError, _dot, d0, d0)
        self.assertRaises(TypeError, _dot, d0, d1)
        self.assertRaises(TypeError, _dot, d0, d2)
        self.assertRaises(TypeError, _dot, d0, d3)
        self.assertRaises(TypeError, _dot, d1, d0)
<a name="14"></a>        _dot(d1, d1)
        _dot(d1, d2)
        self.assertRaises(TypeError, _dot, d1, d3)
        self<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertRaises(TypeError, _dot, d2, d0)
        _dot(d2, d1)
        _dot(d2, d2)
        self.assertRaises(TypeError, _dot, d2, d3)
        self.assertRaises(TypeError, _dot, d3, d0)
        self.assertRaises(TypeError, _dot, d3, d1)
        self.assertRaises(TypeError, _dot, d3, d2)
        self.assertRaises(</b></font>TypeError, _dot, d3, d3)
    def test_dot_0d_0d(self):
        self.cmp_dot(rand(), rand())
    def test_dot_0d_1d(self):
        self.cmp_dot(rand(), rand(5))
    def test_dot_0d_2d(self):
        self.cmp_dot(rand(), rand(6, 7))
    def test_dot_0d_3d(self):
        self.cmp_dot(rand(), rand(8, 6, 7))
    def test_dot_1d_0d(self):
        self.cmp_dot(rand(5), rand())
    def test_dot_1d_1d(self):
        self.cmp_dot(rand(5), rand(5))
    def test_dot_1d0_1d0(self):
        self.cmp_dot(rand(0), rand(0))
    def test_dot_1d_1d0(self):
        self.assertRaises(ValueError, self.cmp_dot, rand(5), rand(0))
    def test_dot_1d0_1d(self):
        self.assertRaises(ValueError, self.cmp_dot, rand(0), rand(5))
    def test_dot_1d_2d(self):
        self.cmp_dot(rand(6), rand(6, 7))
    def test_dot_1d0_2d(self):
        self.cmp_dot(rand(0), rand(0, 7))
    def test_dot_1d_2d0(self):
        self.cmp_dot(rand(6), rand(6, 0))
    def test_dot_1d0_2d0(self):
        self.cmp_dot(rand(0), rand(0, 0))
    def test_dot_1d_3d(self):
        self.cmp_dot(rand(6), rand(8, 6, 7))
    def test_dot_2d_0d(self):
        self.cmp_dot(rand(5, 6), rand())
    def test_dot_2d_1d(self):
        self.cmp_dot(rand(5, 6), rand(6))
    def test_dot_2d0_1d(self):
        self.cmp_dot(rand(0, 6), rand(6))
    def test_dot_2d_1d0(self):
        self.cmp_dot(rand(5, 0), rand(0))
    def test_dot_2d0_1d0(self):
        self.cmp_dot(rand(0, 0), rand(0))
    def test_dot_2d_2d(self):
        self.cmp_dot(rand(5, 6), rand(6, 7))
    def test_dot_2d0_2d(self):
        self.cmp_dot(rand(0, 6), rand(6, 7))
    def test_dot_2d_2d0(self):
        self.cmp_dot(rand(5, 6), rand(6, 0))
    def test_dot_2d0_2d0(self):
        self.cmp_dot(rand(0, 6), rand(6, 0))
    def test_dot_2d_0_2d(self):
        self.cmp_dot(rand(5, 0), rand(0, 7))
    def test_dot_2d0_0_2d0(self):
        self.cmp_dot(rand(0, 6), rand(6, 0))
    def test_dot_2d_3d(self):
        self.cmp_dot(rand(5, 6), rand(8, 6, 7))
    def test_dot_3d_0d(self):
        self.cmp_dot(rand(4, 5, 6), rand())
    def test_dot_3d_1d(self):
        self.cmp_dot(rand(4, 5, 6), rand(6))
    def test_dot_3d_2d(self):
        self.cmp_dot(rand(4, 5, 6), rand(6, 7))
    def test_dot_3d_3d(self):
        self.cmp_dot(rand(4, 5, 6), rand(8, 6, 7))
    def not_aligned(self, x, y):
        ctv_backup = config.compute_test_value
        config.compute_test_value = 'off'
        try:
            z = dot(x, y)
        finally:
            config.compute_test_value = ctv_backup
        _logger = logging.getLogger('theano.gof.opt')
        oldlevel = _logger.level
        _logger.setLevel(logging.CRITICAL)
        try:
            try:
                eval_outputs([z])
                assert False    # should have raised exception
            except ValueError as e:
                e0 = exc_message(e)
                self.assertTrue(
                    e0.split()[1:4] == ['are', 'not', 'aligned'] or
                    e0.split()[0:2] == ['Shape', 'mismatch:'] or
                    (e0.split()[0:4] ==
                        ['Incompatible', 'shapes', 'for', 'gemv']) or
                    e)
        finally:
            _logger.setLevel(oldlevel)
    def test_align_1_1(self):
        self.not_aligned(rand(5), rand(6))
    def test_align_1_2(self):
        self.not_aligned(rand(5), rand(6, 4))
    def test_align_1_3(self):
        self.not_aligned(rand(5), rand(6, 4, 7))
    def test_align_2_1(self):
        self.not_aligned(rand(5, 4), rand(6))
    def test_align_2_2(self):
        self.not_aligned(rand(5, 4), rand(6, 7))
    def test_align_2_3(self):
        self.not_aligned(rand(5, 4), rand(6, 7, 8))
    def test_align_3_1(self):
        self.not_aligned(rand(5, 4, 3), rand(6))
    def test_align_3_2(self):
        self.not_aligned(rand(5, 4, 3), rand(6, 7))
    def test_align_3_3(self):
<a name="0"></a>        self.not_aligned(rand(5, 4, 3), rand(6, 7, 8))
    def test_grad(self):
        utt<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.verify_grad(dot, [rand(2, 3), rand(3, 2)])
        utt.verify_grad(dot, [rand(2), rand(2, 3)])
        utt.verify_grad(dot, [rand(3, 2), rand(2)])
        utt.verify_grad(dot, [rand(2), rand(2)])
        utt.verify_grad(dot, [rand(), rand(2)])
<a name="2"></a>        utt.verify_grad(dot, [rand(), rand(2, 5)])
        utt.verify_grad(dot, [rand(2), rand()])
        utt.verify_grad(dot, [rand(2, 5), rand()])
        utt.</b></font>verify_grad<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(dot, [rand(2, 3, 4), rand(4)])
        utt.verify_grad(dot, [rand(3), rand(2, 3, 4)])
        utt.verify_grad(dot, [rand(4, 3), rand(2, 3, 4)])
        utt.verify_grad(dot, [rand(2, 3, 4), rand(4, 5)])
        utt.verify_grad(dot, [rand(2, 3, 4), rand(</b></font>3, 4, 5)])
    @attr('slow')
    def test_broadcastable_patterns(self):
        def val_for(r):
            if r.dtype.startswith('complex'):
                if r.ndim == 0:
                    return np.asarray(np.complex(1.1, 2.1),
                                      dtype=r.dtype)
                if r.ndim == 1:
                    if r.dtype == 'complex64':
                        return np.complex64([np.complex(1.2, 2.2)])
                    elif r.dtype == 'complex128':
                        return np.complex128([np.complex(1.2, 2.2)])
                elif r.ndim == 2:
                    if r.dtype == 'complex64':
                        return np.complex64([[np.complex(1.3, 2.3)]])
                    elif r.dtype == 'complex128':
                        return np.complex128([[np.complex(1.3, 2.3)]])
            if r.ndim == 0:
                return np.asarray(1.1, dtype=r.dtype)
            if r.ndim == 1:
                return np.asarray([1.2], dtype=r.dtype)
            elif r.ndim == 2:
                return np.asarray([[1.3]], dtype=r.dtype)
            raise ValueError()
        for dtype0 in ('float32', 'float64', 'complex64'):
            for dtype1 in ('float32', 'complex64', 'complex128'):
                for bc0 in ((True,), (False,), (True, True),
                            (True, False), (False, True),
                            (False, False)):
                    x = TensorType(dtype=dtype0, broadcastable=bc0)()
                    for bc1 in ((True,), (False,), (True, True),
                                (True, False), (False, True),
                                (False, False)):
                        y = TensorType(dtype=dtype1, broadcastable=bc1)()
                        z = dot(x, y)
                        t = TensorType(dtype=dtype0,
                                       broadcastable=z.broadcastable)()
                        rval = z * 3 + 2 * t
                        f = function([x, y, t], rval)
                        xval = val_for(x)
                        yval = val_for(y)
                        tval = val_for(t)
                        f(xval, yval, tval)  # debugmode checks result
                        if (dtype0.startswith('float') and
                                dtype1.startswith('float')):
                            g = grad(z.sum(), x)
                            assert g.broadcastable == x.broadcastable
                            g = grad(z.sum(), y)
                            assert g.broadcastable == y.broadcastable
class T_tensorfromscalar(unittest.TestCase):
    def test0(self):
<a name="17"></a>        s = scal.constant(56)
        t = tensor_from_scalar(s)
        self.assertTrue(t.owner.op is tensor_from_scalar)
        self<font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
        self.assertTrue(t.type.ndim == 0, t.type.ndim)
        self.</b></font>assertTrue(t.type.dtype == s.type.dtype)
        v = eval_outputs([t])
        self.assertTrue(v == 56, v)
        self.assertTrue(isinstance(v, np.ndarray))
        self.assertTrue(v.shape == (), v.shape)
    def test1(self):
<a name="23"></a>        s = scal.constant(56)
        t = as_tensor_variable(s)
        self.assertTrue(t.owner.op is tensor_from_scalar)
        self<font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
        self.assertTrue(t.type.ndim == 0, t.type.</b></font>ndim)
        self.assertTrue(t.type.dtype == s.type.dtype)
        v = eval_outputs([t])
        self.assertTrue(v == 56, v)
        self.assertTrue(isinstance(v, np.ndarray))
        self.assertTrue(v.shape == (), v.shape)
        g = grad(t, s)
        self.assertTrue(eval_outputs([g]) == 0.)
    def test2(self):
        s = scal.constant(56.)
        t = as_tensor_variable(s)
        self.assertTrue(t.owner.op is tensor_from_scalar)
        self.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
        self.assertTrue(t.type.ndim == 0, t.type.ndim)
        self.assertTrue(t.type.dtype == s.type.dtype)
        v = eval_outputs([t])
        self.assertTrue(v == 56., v)
        self.assertTrue(isinstance(v, np.ndarray))
        self.assertTrue(v.shape == (), v.shape)
        g = grad(t, s)
        self.assertTrue(eval_outputs([g]) == 1.)
class T_scalarfromtensor(unittest.TestCase):
    def test0(self):
        tt = constant(56)  # scal.constant(56)
        ss = scalar_from_tensor(tt)
        self.assertTrue(ss.owner.op is scalar_from_tensor)
        self.assertTrue(ss.type.dtype == tt.type.dtype)
        v = eval_outputs([ss])
        self.assertTrue(v == 56, v)
        if config.cast_policy == 'custom':
            self.assertTrue(isinstance(v, np.int8))
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            self.assertTrue(isinstance(
                v, getattr(np, str(np.asarray(56).dtype))))
        else:
            raise NotImplementedError(config.cast_policy)
        self.assertTrue(v.shape == (), v.shape)
        tt = lscalar()
        ss = scalar_from_tensor(tt)
        ss.owner.op.grad([tt], [ss])
        fff = function([tt], ss)
        v = fff(np.asarray(5))
        self.assertTrue(v == 5, v)
        self.assertTrue(isinstance(v, np.int64))
        self.assertTrue(v.shape == (), v.shape)
class test_grad(unittest.TestCase):
    class Obj1(gof.op.Op):
        def __init__(self):
            self.gval0 = scalar('e')
            self.gval1 = scalar('f')
        def make_node(self):
            inputs = [scalar('a'), scalar('c')]
            outputs = [scalar('b'), scalar('d')]
            return gof.Apply(self, inputs, outputs)
        def grad(self, inp, grads):
            x0, x1 = inp
            gz0, gz1 = grads
            return self.gval0, self.gval1
    def test_1param(self):
        o = test_grad.Obj1()
        a1 = o.make_node()
        self.assertTrue(o.gval0 is tensor.grad(a1.outputs[0], a1.inputs[0]))
    def test_Nparam(self):
        o = test_grad.Obj1()
        a1 = o.make_node()
        g0, g1 = grad(a1.outputs[0], a1.inputs)
        g0.name = None
        self.assertTrue(o.gval0 is g0)
        self.assertTrue(o.gval1 is g1)
    def test_grad_keep_type(self):
        X = tensor.matrix()
        y = X.sum()
        G = tensor.grad(y, [X])
        assert isinstance(G, list)
        G = tensor.grad(y, X)
        assert not isinstance(G, list)
    def test_1None_rval(self):
        o = test_grad.Obj1()
        a1 = o.make_node()
        g = grad(a1.outputs[0], a1.outputs[1],
                 disconnected_inputs='ignore')
        self.assertTrue(g.owner.op == fill)
        self.assertTrue(g.owner.inputs[1].data == 0)
        self.assertRaises(TypeError, grad, a1.outputs[0], 'wtf')
    def test_NNone_rval(self):
        o = test_grad.Obj1()
        a1 = o.make_node()
        g0, g1, g2 = grad(a1.outputs[0], a1.inputs + [scalar('z')],
                          disconnected_inputs='ignore')
        self.assertTrue(o.gval0 is g0)
        self.assertTrue(o.gval1 is g1)
        self.assertTrue(g2.owner.op == fill)
        self.assertTrue(g2.owner.inputs[1].data == 0)
    def test_zero_gradient_shape(self):
        x = dmatrix()
        f = theano.function([x], grad(dscalar(), x,
                                      disconnected_inputs='ignore'))
        a = np.ones((3, 7))
        self.assertTrue((f(a) == 0).all())  # Zero gradient.
        self.assertTrue(a.shape == f(a).shape)  # With proper shape.
    def test_cost_is_scalar(self):
        v = vector()
        m = matrix()
        self.assertRaises(TypeError, grad, v, v)
        self.assertRaises(TypeError, grad, m, m)
class T_op_cache(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
    def test0(self):
        v = matrix()
        v.name = 'v'
        gv = fill(v / v, 1.0) / v - (fill(v / v, 1.0) * v) / (v * v)
        fn_py = inplace_func([v], gv)
        fn_c_or_py = inplace_func([v], gv)
        a = rand(5, 2).astype(config.floatX)
        self.assertTrue(np.all(fn_py(a) == fn_c_or_py(a)))
class T_reshape(utt.InferShapeTester, utt.TestOptimizationMixin):
    def __init__(self, name, shared=tensor._shared, op=Reshape, mode=None,
                 ignore_topo=(DeepCopyOp, opt.MakeVector,
                              opt.Shape_i, DimShuffle, theano.tensor.Elemwise)):
        self.shared = shared
        self.op = op
        self.mode = mode
        self.ignore_topo = ignore_topo
        super(T_reshape, self).__init__(name)
    def function(self, inputs, outputs, ignore_empty=False):
        f = function(inputs, outputs, mode=self.mode)
        if self.mode is not None or theano.config.mode != "FAST_COMPILE":
            topo = f.maker.fgraph.toposort()
            topo_ = [node for node in topo if not isinstance(node.op,
                                                             self.ignore_topo)]
            if ignore_empty:
                assert len(topo_) &lt;= 1, topo_
            else:
                assert len(topo_) == 1, topo_
            if len(topo_) &gt; 0:
                assert type(topo_[0].op) is self.op
        return f
    def test_reshape(self):
        a = dvector()
        b = dmatrix()
        d = dmatrix()
        c = reshape(b, as_tensor_variable(6), ndim=1)
        f = self.function([b], c)
        b_val1 = np.asarray([[0, 1, 2], [3, 4, 5]])
        c_val1 = np.asarray([0, 1, 2, 3, 4, 5])
        b_val2 = b_val1.T
        c_val2 = np.asarray([0, 3, 1, 4, 2, 5])
        f_out1 = f(b_val1)
        f_out2 = f(b_val2)
        assert np.all(f_out1 == c_val1), (f_out1, c_val1)
        assert np.all(f_out2 == c_val2), (f_out2, c_val2)
        c = reshape(b, (as_tensor_variable(6),), ndim=1)
        f = self.function([b], c)
        assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
                      np.asarray([0, 1, 2, 3, 4, 5]))
        c = reshape(b, d.shape)
        f = self.function([b, d], c)
        assert np.all(f(np.asarray<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>([[0, 1, 2], [3, 4, 5]]),
                        [[0, 1], [2, 3], [4, 5]]) ==
                      np.asarray([[0</b></font>, 1], [2, 3], [4, 5]]))
        c = reshape(a, [2, 3])
        f = self.function([a], c)
        assert np.all(f(np.asarray([0, 1, 2, 3, 4, 5])) ==
                      np.asarray([[0, 1, 2], [3, 4, 5]]))
        a_val = np.asarray([0, 1, 2, 3, 4, 5])
        a_val_copy = np.asarray([0, 1, 2, 3, 4, 5])
        b_val = np.asarray([[0, 1, 2], [3, 4, 5]])
        f_sub = self.function([a, b], c - b)
        assert np.all(f_sub(a_val, b_val) == 0.0)
        assert np.all(a_val == a_val_copy)
        a_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
        a_val_copy = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
        b_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
        f_sub = self.function([a, b], c - b)
        assert np.all(f_sub(a_val, b_val) == 0.0)
        assert np.all(a_val == a_val_copy)
        def just_vals(v):
            return Reshape(2)(v, theano._asarray([2, 3], dtype='int32'))
        utt.verify_grad(just_vals, [a_val], mode=self.mode)
        self._compile_and_check([a], [c], (a_val,), self.op)
        c = reshape(b, (b.shape[0], b.shape[1], 1))
        f = self.function([b], c, ignore_empty=True)
        assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
                      np.asarray([[[0], [1], [2]], [[3], [4], [5]]]))
        assert (f.maker.fgraph.toposort()[-1].outputs[0].type.broadcastable ==
                (False, False, True))
        c = reshape(b, (b.shape[1], b.shape[0], 1))
        f = self.function([b], c, ignore_empty=True)
        assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
                      np.asarray([[[0], [1]], [[2], [3]], [[4], [5]]]))
        assert (f.maker.fgraph.toposort()[-1].outputs[0].type.broadcastable ==
                (False, False, True))
    def test_m1(self):
        t = tensor3()
        rng = np.random.RandomState(seed=utt.fetch_seed())
        val = rng.uniform(size=(3, 4, 5)).astype(config.floatX)
        for out in [t.reshape([-1]), t.reshape([-1, 5]),
                    t.reshape([5, -1]), t.reshape([5, -1, 3])]:
            self._compile_and_check([t], [out], [val], self.op)
    def test_reshape_long_in_shape(self):
        v = dvector('v')
        r = v.reshape((v.shape[0], L(1)))
        print(r.eval({v: np.arange(5.)}))
        assert np.allclose(r.eval({v: np.arange(5.)}).T,
                           np.arange(5.))
    def test_bad_shape(self):
        a = matrix('a')
        shapes = ivector('shapes')
        rng = np.random.RandomState(seed=utt.fetch_seed())
        a_val = rng.uniform(size=(3, 4)).astype(config.floatX)
        r = a.reshape(shapes, ndim=1)
        f = self.function([a, shapes], r)
        self.assertRaises(ValueError, f, a_val, [13])
        r = a.reshape(shapes, ndim=2)
        f = self.function([a, shapes], r)
        self.assertRaises(ValueError, f, a_val, [-1, 5])
        self.assertRaises(ValueError, f, a_val, [7, -1])
        self.assertRaises(ValueError, f, a_val, [7, 5])
        self.assertRaises(ValueError, f, a_val, [-1, -1])
    def test_0(self):
        x = fvector('x')
        f = self.function([x], x.reshape((0, 100)))
        assert f(np.ndarray((0,), dtype='float32')).shape == (0, 100)
    def test_empty_shp(self):
        const = theano.tensor.constant([1]).reshape(())
        f = function([], const)
        assert f().shape == ()
def test_make_column_matrix_broadcastable():
    a = tensor.dmatrix()
    b = a.reshape((a.shape[0], )).dimshuffle(0, 'x')
    f = function([a], b)
    assert (f(np.zeros((3, 1))) + np.ones(2) == np.ones((3, 2))).all()
def test_flatten_outdimNone():
    a = dmatrix()
    c = flatten(a)
    f = inplace_func([a], c)
    a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
    c_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
    assert np.all(f(a_val) == c_val)
    f = inplace_func([a], c)
    assert np.all(f(a_val) == c_val)
    utt.verify_grad(flatten, [a_val])
def test_flatten_scalar():
    a = dscalar()
    c = flatten(a)
    f = inplace_func([a], c)
    a_val = theano._asarray(3.0, dtype='float64')
    c_val = theano._asarray([3.0], dtype='float64')
    assert np.all(f(a_val) == c_val)
    f = inplace_func([a], c)
    assert np.all(f(a_val) == c_val)
def test_flatten_ndim1():
    a = dmatrix()
    c = flatten(a, 1)
    f = inplace_func([a], c)
    a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
    c_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
    assert np.all(f(a_val) == c_val)
    f = inplace_func([a], c)
    assert np.all(f(a_val) == c_val)
    utt.verify_grad(flatten, [a_val])
def test_flatten_ndim2():
    a = dmatrix()
    c = flatten(a, 2)
    f = inplace_func([a], c)
    a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
    assert np.all(f(a_val) == a_val)
    f = inplace_func([a], c)
    assert np.all(f(a_val) == a_val)
    flatten_2 = partial(flatten, ndim=2)
    utt.verify_grad(flatten_2, [a_val])
def test_flatten_ndim2_of_3():
    a = TensorType('float64', (False, False, False))()
    c = flatten(a, 2)
    f = inplace_func([a], c)
    a_val = theano._asarray([[[0, 1], [2, 3]], [[4, 5], [6, 7]]],
                            dtype='float64')
    c_val = theano._asarray([[0, 1, 2, 3], [4, 5, 6, 7]], dtype='float64')
    assert np.all(f(a_val) == c_val)
    f = inplace_func([a], c)
    assert np.all(f(a_val) == c_val)
    flatten_2 = partial(flatten, ndim=2)
    utt.verify_grad(flatten_2, [a_val])
    flatten_2 = partial(flatten, outdim=2)
    utt.verify_grad(flatten_2, [a_val])
def test_flatten_broadcastable():
    inp = TensorType('float64', (False, False, False, False))()
    out = flatten(inp, ndim=2)
    assert out.broadcastable == (False, False)
    inp = TensorType('float64', (False, False, False, True))()
    out = flatten(inp, ndim=2)
    assert out.broadcastable == (False, False)
    inp = TensorType('float64', (False, True, False, True))()
    out = flatten(inp, ndim=2)
    assert out.broadcastable == (False, False)
    inp = TensorType('float64', (False, True, True, True))()
    out = flatten(inp, ndim=2)
    assert out.broadcastable == (False, True)
    inp = TensorType('float64', (True, False, True, True))()
    out = flatten(inp, ndim=3)
    assert out.broadcastable == (True, False, True)
def test_flatten_ndim_invalid():
    a = dmatrix()
    assert_raises(ValueError, flatten, a, 3)
    assert_raises(ValueError, flatten, a, 0)
def test_is_flat():
    assert tensor.is_flat(tensor.as_tensor_variable(np.zeros((10))))
    assert tensor.is_flat(tensor.as_tensor_variable(np.zeros((10, 10, 10))),
                          ndim=3)
    assert not tensor.is_flat(
        tensor.as_tensor_variable(np.zeros((10, 10, 10))))
    assert tensor.is_flat(tensor.vector())
    assert tensor.is_flat(tensor.tensor3(), ndim=3)
    assert not tensor.is_flat(tensor.tensor3())
    X = tensor.tensor4()
    assert tensor.is_flat(X.reshape((-1, )))
    assert tensor.is_flat(X.reshape((10, 10, -1)), ndim=3)
    assert not tensor.is_flat(X.reshape((10, 10, -1)))
    X = tensor.tensor4()
    assert tensor.is_flat(X.reshape((tensor.iscalar(), )))
    assert tensor.is_flat(X.reshape((tensor.iscalar(), ) * 3), ndim=3)
    assert not tensor.is_flat(X.reshape((tensor.iscalar(), ) * 3))
def test_tile():
    def run_tile(x, x_, reps, use_symbolic_reps):
        if use_symbolic_reps:
            rep_symbols = [iscalar() for _ in range(len(reps))]
            f = function([x] + rep_symbols, tile(x, rep_symbols))
            return f(*([x_] + list(reps)))
        else:
            f = function([x], tile(x, reps))
            return f(x_)
    rng = np.random.RandomState(utt.fetch_seed())
    for use_symbolic_reps in [False, True]:
        x = vector()
        x_ = rng.randn(5).astype(config.floatX)
        assert np.all(run_tile(x, x_, (2,), use_symbolic_reps) ==
                      np.tile(x_, (2,)))
        x = matrix()
        x_ = rng.randn(2, 4).astype(config.floatX)
        assert np.all(run_tile(x, x_, (2, 3), use_symbolic_reps) ==
                      np.tile(x_, (2, 3)))
        x = tensor3()
        x_ = rng.randn(2, 4, 3).astype(config.floatX)
        assert np.all(run_tile(x, x_, (2, 3, 4), use_symbolic_reps) ==
                      np.tile(x_, (2, 3, 4)))
        x = tensor4()
        x_ = rng.randn(2, 4, 3, 5).astype(config.floatX)
        assert np.all(run_tile(x, x_, (2, 3, 4, 6), use_symbolic_reps) ==
                      np.tile(x_, (2, 3, 4, 6)))
    test_shape = [2, 4, 3, 5]
    k = 0
    for xtype in [vector(), matrix(), tensor3(), tensor4()]:
        x = xtype
        k = k + 1
        x_ = rng.randn(*test_shape[0:k]).astype(config.floatX)
        reps_ = 2
        f = function([x], tile(x, reps_))
        assert np.all(f(x_) == np.tile(x_, reps_))
        reps = iscalar()
        reps_ = 2
        f = function([x, reps], tile(x, reps))
        assert np.all(f(x_, reps_) == np.tile(x_, reps_))
        reps = ivector()
        reps_ = [2] if k == 1 or k == 2 else [2, 3]
        ndim_ = k
        f = function([x, reps], tile(x, reps, ndim_))
        assert np.all(f(x_, reps_) == np.tile(x_, reps_))
        reps_ = [2, 3, 4]
        f = function([x], tile(x, reps_))
        assert np.all(f(x_) == np.tile(x_, reps_))
        d = iscalar()
        reps = [2, d, 4]
        f = function([x, d], tile(x, reps))
        reps_ = [2, 3, 4]
        assert np.all(f(x_, 3) == np.tile(x_, reps_))
        r = [2, 3, 4, 5, 6]
        reps_ = r[:k + 1]  # len(reps_) = x.ndim+1
        f = function([x], tile(x, reps_))
        assert np.all(f(x_) == np.tile(x_, reps_))
        ndim_ = len(reps_)
        f = function([x], tile(x, reps_, ndim_))
        assert np.all(f(x_) == np.tile(x_, reps_))
        ndim_ = len(reps_) + 1
        f = function([x], tile(x, reps_, ndim_))
        assert np.all(f(x_) == np.tile(x_, [1] + reps_))
        r = [2, 3, 4, 5]
        if k &gt; 1:
            ndim_ = k + 1
            reps_ = r[:k - 1]
            f = function([x], tile(x, reps_, ndim_))
            assert np.all(f(x_) == np.tile(x_, [1, 1] + reps_))
        reps = ivector()
        np.testing.assert_raises(ValueError, tile, x, reps)
        for reps in [2.5, fscalar(), fvector()]:
            np.testing.assert_raises(ValueError, tile, x, reps)
        reps = imatrix()
        np.testing.assert_raises(ValueError, tile, x, reps)
        for reps in [[2, 3, 4], iscalar(), ivector()]:
            if k &gt; 1:
                ndim = k - 1
                np.testing.assert_raises(ValueError, tile, x, reps, ndim)
        r = [2, 3, 4, 5, 6]
        reps = r[:k + 1]
        ndim = k
        np.testing.assert_raises(ValueError, tile, x, reps, ndim)
        reps = ivector()
        r = [2, 3, 4, 5, 6, 7]
        reps_ = r[:k + 2]
        ndim_ = k + 1
        f = function([x, reps], tile(x, reps, ndim_))
        np.testing.assert_raises(AssertionError, f, x_, reps_)
def test_tile_grad():
    def grad_tile(x, reps, np_x):
        y = tile(x, reps)
        z = y.sum()
        g = theano.function([x], grad(z, x))
        grad_res = g(np_x)
        assert np.all(grad_res == np.prod(reps))
    rng = np.random.RandomState(utt.fetch_seed())
    grad_tile(vector('x'), [3], rng.randn(5).astype(config.floatX))
    grad_tile(matrix('x'), [3, 4], rng.randn(2, 3).astype(config.floatX))
    grad_tile(tensor3('x'), [3, 4, 5],
              rng.randn(2, 4, 3).astype(config.floatX))
    grad_tile(tensor4('x'), [3, 4, 5, 6],
              rng.randn(2, 4, 3, 5).astype(config.floatX))
class TestARange(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
    def test_Op_integers(self):
        start, stop, step = iscalars('start', 'stop', 'step')
        out = ARange(start.type.dtype)(start, stop, step)
        f = function([start, stop, step], out)
        assert np.all(f(0, 5, 1) == np.arange(0, 5, 1))
        assert np.all(f(2, 11, 4) == np.arange(2, 11, 4))
        assert np.all(f(-5, 1, 1) == np.arange(-5, 1, 1))
        assert np.all(f(10, 2, -2) == np.arange(10, 2, -2))
        assert np.all(f(10, 2, 2) == np.arange(10, 2, 2))
        assert np.all(f(0, 0, 1) == np.arange(0, 0, 1))
    def test_grads(self):
        def f(start, stop, step):
            return ARange(start.type.dtype)(start, stop, step)
        rng = np.random.RandomState(utt.fetch_seed())
        for start, stop, step in [(0, 4.9, 1),
                                  (5.1, 0, -0.5),
                                  (1, 5.1, 0.5)]:
            utt.verify_grad(f, [np.asarray(start).astype(config.floatX),
                                np.asarray(stop).astype(config.floatX),
                                np.asarray(step).astype(config.floatX)],
                            rng=rng)
    def test_integers(self):
        start, stop, step = iscalars('start', 'stop', 'step')
        out = arange(start, stop, step)
        f = function([start, stop, step], out)
        if config.cast_policy == 'custom':
            assert out.dtype == 'int64'
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            numpy_dtype = np.arange(np.array(1, dtype='int32')).dtype
            assert out.dtype == numpy_dtype
        else:
            raise NotImplementedError(config.cast_policy)
        assert np.all(f(0, 5, 1) == np.arange(0, 5, 1))
        assert np.all(f(2, 11, 4) == np.arange(2, 11, 4))
        assert np.all(f(-5, 1, 1) == np.arange(-5, 1, 1))
        assert np.all(f(10, 2, -2) == np.arange(10, 2, -2))
        assert np.all(f(10, 2, 2) == np.arange(10, 2, 2))
        assert np.all(f(0, 0, 1) == np.arange(0, 0, 1))
    def test_float32(self):
        start, stop, step = fscalars('start', 'stop', 'step')
        out = arange(start, stop, step)
        f = function([start, stop, step], out)
<a name="15"></a>        if config.cast_policy == 'custom':
            assert out.dtype == start.type.dtype
        elif config.cast_policy == 'numpy':
            numpy_dtype = np.arange(np<font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.array(0, dtype=start.dtype),
                                    np.array(1, dtype=stop.dtype),
                                    np.array(1, dtype=step.dtype)).</b></font>dtype
            assert out.dtype == numpy_dtype
        elif config.cast_policy == 'numpy+floatX':
            assert out.dtype == config.floatX
        else:
            raise NotImplementedError(config.cast_policy)
        arg_vals = [(0, 5, 1), (2, 11, 4), (-5, 1.1, 1.2), (1.3, 2, -2.1),
                    (10, 2, 2)]
        for arg_v in arg_vals:
            start_v, stop_v, step_v = arg_v
            start_v_, stop_v_, step_v_ = np.asarray(arg_v,
                                                    dtype=start.type.dtype)
            f_val = f(start_v_, stop_v_, step_v_)
            if config.cast_policy == 'custom':
                expected_val = np.arange(start_v, stop_v, step_v,
                                         dtype=start.type.dtype)
            elif config.cast_policy in ('numpy', 'numpy+floatX'):
                expected_val = np.arange(start_v_, stop_v_, step_v_,
                                         dtype=out.dtype)
            else:
                raise NotImplementedError(config.cast_policy)
            assert np.all(f_val == expected_val)
    def test_float64(self):
        start, stop, step = dscalars('start', 'stop', 'step')
        out = arange(start, stop, step)
        f = function([start, stop, step], out)
        assert out.dtype == start.type.dtype
        arg_vals = [(0, 5, 1), (2, 11, 4), (-5, 1.1, 1.2),
                    (1.3, 2, -2.1), (10, 2, 2)]
        for arg_v in arg_vals:
            start_v, stop_v, step_v = arg_v
            start_v_, stop_v_, step_v_ = np.asarray(arg_v,
                                                    dtype=start.type.dtype)
            f_val = f(start_v_, stop_v_, step_v_)
            if config.cast_policy == 'custom':
                expected_val = np.arange(start_v, stop_v, step_v,
                                         dtype=start.type.dtype)
            elif config.cast_policy in ('numpy', 'numpy+floatX'):
                expected_val = np.arange(start_v_, stop_v_, step_v_)
            else:
                raise NotImplementedError(config.cast_policy)
            assert np.all(f_val == expected_val)
    def test_default_step(self):
        start, stop = iscalars('start', 'stop')
        out = arange(start, stop)
        f = function([start, stop], out)
        if config.cast_policy == 'custom':
            assert out.dtype == 'int64'
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            assert out.dtype == np.arange(np.int32(0),
                                          np.int32(1)).dtype
        else:
            raise NotImplementedError(config.cast_policy)
        assert np.all(f(0, 5) == np.arange(0, 5))
        assert np.all(f(-5, 1) == np.arange(-5, 1))
        assert np.all(f(0, 0) == np.arange(0, 0))
        dstart, dstop = dscalars('start', 'stop')
        dout = arange(dstart, dstop)
        df = function([dstart, dstop], dout)
        assert dout.dtype == dstart.type.dtype
        assert np.all(df(0.2, 5.3) == np.arange(0.2, 5.3))
        assert np.all(df(0.8, 5.3) == np.arange(0.8, 5.3))
        assert np.all(df(-0.7, 5.3) == np.arange(-0.7, 5.3))
    def test_default_start(self):
        stop = iscalar('stop')
        out = arange(stop)
        f = function([stop], out)
        if config.cast_policy == 'custom':
            assert out.dtype == 'int64'
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            assert out.dtype == np.arange(np.int32(1)).dtype
        else:
            raise NotImplementedError(config.cast_policy)
        assert np.all(f(8) == np.arange(8))
        assert np.all(f(-2) == np.arange(-2))
        fstop = fscalar('stop')
        fout = arange(fstop)
        ff = function([fstop], fout)
        if config.cast_policy == 'custom':
            assert fout.dtype == fstop.type.dtype
        elif config.cast_policy == 'numpy':
            assert fout.dtype == np.arange(np.float32(1)).dtype
        elif config.cast_policy == 'numpy+floatX':
            if config.floatX == 'float32':
                assert fout.dtype == 'float32'
            else:
                assert fout.dtype == np.arange(np.float32(1)).dtype
        else:
            raise NotImplementedError(config.cast_policy)
        fstop_values = [0.2, -0.7, 8.5]
        for fstop_v in fstop_values:
            fstop_v32 = np.float32(fstop_v)
            assert np.all(ff(fstop_v32) == np.arange(fstop_v))
    def test_upcast(self):
        if config.cast_policy == 'custom':
            assert arange(iscalar()).dtype == 'int64'
            assert arange(fscalar()).dtype == fscalar().dtype
            assert arange(dscalar()).dtype == dscalar().dtype
            assert arange(iscalar(), fscalar()).dtype == dscalar().dtype
            assert arange(iscalar(), dscalar()).dtype == dscalar().dtype
            assert arange(fscalar(), dscalar()).dtype == dscalar().dtype
            assert arange(iscalar(), fscalar(), dscalar()).dtype == \
                dscalar().dtype
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            for dtype in get_numeric_types():
                arange_dtype = arange(scalar(dtype=str(dtype))).dtype
                numpy_dtype = np.arange(np.array(1, dtype=dtype)).dtype
                if (dtype != 'float64' and
                        numpy_dtype == 'float64' and
                        config.cast_policy == 'numpy+floatX' and
                        config.floatX == 'float32'):
                    assert arange_dtype == 'float32'
                else:
                    assert arange_dtype == numpy_dtype
                for stop_dtype in get_numeric_types():
                    arange_dtype = arange(
                        start=scalar(dtype=str(dtype)),
                        stop=scalar(dtype=str(stop_dtype))).dtype
                    numpy_dtype = np.arange(
                        start=np.array(0, dtype=dtype),
                        stop=np.array(1, dtype=stop_dtype)).dtype
                    if (dtype != 'float64' and
                            stop_dtype != 'float64' and
                            numpy_dtype == 'float64' and
                            config.cast_policy == 'numpy+floatX' and
                            config.floatX == 'float32'):
                        assert arange_dtype == 'float32'
                    else:
                        assert arange_dtype == numpy_dtype
<a name="11"></a>
                    for step_dtype in get_numeric_types():
                        arange_dtype <font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= arange(
                            start=scalar(dtype=str(dtype)),
                            stop=scalar(dtype=str(stop_dtype)),
                            step=scalar(dtype=str(step_dtype))).</b></font>dtype
                        numpy_dtype = np.arange(
                            start=np.array(0, dtype=dtype),
                            stop=np.array(1, dtype=stop_dtype),
                            step=np.array(1, dtype=step_dtype)).dtype
                        if (dtype != 'float64' and
                                stop_dtype != 'float64' and
                                step_dtype != 'float64' and
                                numpy_dtype == 'float64' and
                                config.cast_policy == 'numpy+floatX' and
                                config.floatX == 'float32'):
                            assert arange_dtype == 'float32'
                        else:
                            assert arange_dtype == numpy_dtype
        else:
            raise NotImplementedError(config.cast_policy)
    def test_dtype_cache(self):
        start, stop, step = iscalars('start', 'stop', 'step')
        out1 = arange(start, stop, step)
        out2 = arange(start, stop, step, dtype=out1.dtype)
        out3 = arange(start, stop, 2., dtype=out1.dtype)
        out4 = arange(start, stop, 2.)
        assert out1.owner.op is out2.owner.op
        assert out2.owner.op is out3.owner.op
        assert out3.owner.op is not out4.owner.op
    def test_infer_shape(self):
        start, stop, step = iscalars('start', 'stop', 'step')
        out = arange(start, stop, step)
        mode = theano.config.mode
        if mode == 'FAST_COMPILE':
            mode = 'FAST_RUN'
        mode = compile.mode.get_mode(mode).excluding('fusion')
        f = function([start, stop, step], out.shape, mode=mode)
        assert len(f.maker.fgraph.toposort()) == 9
        if config.cast_policy == 'custom':
            assert out.dtype == 'int64'
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            numpy_dtype = np.arange(np.array(0, dtype=start.dtype),
                                    np.array(1, dtype=stop.dtype),
                                    np.array(1, dtype=step.dtype)).dtype
            assert out.dtype == numpy_dtype
        else:
            raise NotImplementedError(config.cast_policy)
        assert np.all(f(0, 5, 1) == len(np.arange(0, 5, 1)))
        assert np.all(f(2, 11, 4) == len(np.arange(2, 11, 4)))
        assert np.all(f(-5, 1, 1) == len(np.arange(-5, 1, 1)))
        assert np.all(f(10, 2, -2) == len(np.arange(10, 2, -2)))
        assert np.all(f(10, 2, 2) == len(np.arange(10, 2, 2)))
        assert np.all(f(0, 0, 1) == len(np.arange(0, 0, 1)))
        out = arange(start, stop, 1)
        f = function([start, stop], out.shape, mode=mode)
        assert len(f.maker.fgraph.toposort()) == 5
        if config.cast_policy == 'custom':
            assert out.dtype == 'int64'
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            assert out.dtype == np.arange(
                np.int32(0), np.int32(1), np.int32(1)).dtype
        else:
            raise NotImplementedError(config.cast_policy)
        assert np.all(f(0, 5) == len(np.arange(0, 5)))
        assert np.all(f(2, 11) == len(np.arange(2, 11)))
        assert np.all(f(-5, 1) == len(np.arange(-5, 1)))
        assert np.all(f(10, 2) == len(np.arange(10, 2)))
        assert np.all(f(10, 2) == len(np.arange(10, 2)))
        assert np.all(f(0, 0) == len(np.arange(0, 0)))
        assert np.all(f(-64, 64) == len(np.arange(-64, 64)))
        assert arange(-64, 64).shape.eval() == [128]
        assert arange(-64, 64, 2).shape.eval() == [64]
        out = arange(0, stop, 1)
        f = function([stop], out.shape, mode=mode)
        assert len(f.maker.fgraph.toposort()) == 2
        if config.cast_policy == 'custom':
            assert out.dtype == 'int64'
        elif config.cast_policy in ('numpy', 'numpy+floatX'):
            numpy_dtype = np.arange(0,
                                    np.array(1, dtype=stop.dtype),
                                    1).dtype
            assert out.dtype == numpy_dtype
        else:
            raise NotImplementedError(config.cast_policy)
        assert np.all(f(5) == len(np.arange(0, 5)))
        assert np.all(f(11) == len(np.arange(0, 11)))
        assert np.all(f(1) == len(np.arange(0, 1)))
        assert np.all(f(2) == len(np.arange(0, 2)))
        assert np.all(f(2) == len(np.arange(0, 2)))
        assert np.all(f(0) == len(np.arange(0, 0)))
class TestNdGrid(unittest.TestCase):
    def setUp(self):
        pass
    def test_mgrid_numpy_equiv(self):
        nmgrid = (np.mgrid[0:1:.1, 1:10:1., 10:100:10.],
                  np.mgrid[0:2:1, 1:10:1, 10:100:10])
        tmgrid = (mgrid[0:1:.1, 1:10:1., 10:100:10.],
                  mgrid[0:2:1, 1:10:1, 10:100:10])
        for n, t in zip(nmgrid, tmgrid):
            for ng, tg in zip(n, t):
                utt.assert_allclose(ng, tg.eval())
    def test_ogrid_numpy_equiv(self):
        nogrid = (np.ogrid[0:1:.1, 1:10:1., 10:100:10.],
                  np.ogrid[0:2:1, 1:10:1, 10:100:10])
        togrid = (ogrid[0:1:.1, 1:10:1., 10:100:10.],
                  ogrid[0:2:1, 1:10:1, 10:100:10])
        for n, t in zip(nogrid, togrid):
            for ng, tg in zip(n, t):
                utt.assert_allclose(ng, tg.eval())
    def test_mgrid_theano_variable_numpy_equiv(self):
        nfmgrid = np.mgrid[0:1:.1, 1:10:1., 10:100:10.]
        nimgrid = np.mgrid[0:2:1, 1:10:1, 10:100:10]
        i, j, k = dscalars('i', 'j', 'k')
        l, m, n = iscalars('l', 'm', 'n')
        tfmgrid = mgrid[i:1:.1, 1:j:1., 10:100:k]
        timgrid = mgrid[l:2:1, 1:m:1, 10:100:n]
        ff = theano.function([i, j, k], tfmgrid)
        fi = theano.function([l, m, n], timgrid)
        for n, t in zip((nfmgrid, nimgrid), (ff(0, 10, 10.), fi(0, 10, 10))):
            for ng, tg in zip(n, t):
                utt.assert_allclose(ng, tg)
    def test_ogrid_theano_variable_numpy_equiv(self):
        nfogrid = np.ogrid[0:1:.1, 1:10:1., 10:100:10.]
        niogrid = np.ogrid[0:2:1, 1:10:1, 10:100:10]
        i, j, k = dscalars('i', 'j', 'k')
        l, m, n = iscalars('l', 'm', 'n')
        tfogrid = ogrid[i:1:.1, 1:j:1., 10:100:k]
        tiogrid = ogrid[l:2:1, 1:m:1, 10:100:n]
        ff = theano.function([i, j, k], tfogrid)
        fi = theano.function([l, m, n], tiogrid)
        for n, t in zip((nfogrid, niogrid), (ff(0, 10, 10.), fi(0, 10, 10))):
            for ng, tg in zip(n, t):
                utt.assert_allclose(ng, tg)
<a name="26"></a>
class TestInversePermutation(unittest.TestCase):
    <font color="#68818b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def setUp(self):
        utt.seed_rng()
    def test_dim1(self):
        p = ivector()
        inv = inverse_permutation(p)
        assert inv.dtype == p.</b></font>dtype
        f_inverse = function([p], inv)
        rng = np.random.RandomState(utt.fetch_seed())
        p_val = rng.permutation(10).astype('int32')
        inv_val = f_inverse(p_val)
        assert np.all(f_inverse(inv_val) == p_val)
        assert np.all(p_val[inv_val] == np.arange(10))
        assert np.all(inv_val[p_val] == np.arange(10))
    def test_dim2(self):
        p = imatrix()
        inv = inverse_permutation(p)
        f_inverse = function([p], inv)
        rng = np.random.RandomState(utt.fetch_seed())
        p_val = np.asarray([rng.permutation(10) for i in range(7)],
                           dtype='int32')
        inv_val = f_inverse(p_val)
        assert np.all(f_inverse(inv_val) == p_val)
        for p_row, i_row in zip(p_val, inv_val):
            assert np.all(p_row[i_row] == np.arange(10))
            assert np.all(i_row[p_row] == np.arange(10))
class TestPermuteRowElements(unittest.TestCase):
    def setUp(self):
        utt.seed_rng()
    def test_1_1(self):
        input = dvector()
        p = ivector()
        out = permute_row_elements(input, p)
        permute = function([input, p], out)
        rng = np.random.RandomState(utt.fetch_seed())
        input_val = rng.uniform(size=(5,))
        p_val = rng.permutation(5).astype('int32')
        out_val = permute(input_val, p_val)
        out_bis = input_val[p_val]
        assert np.all(out_val == out_bis)
        def permute_fixed(s_input):
            return permute_row_elements(s_input, p_val)
        utt.verify_grad(permute_fixed, [input_val])
    def test_2_1(self):
        input = matrix()
        p = ivector()
        out = permute_row_elements(input, p)
        permute = function([input, p], out)
        rng = np.random.RandomState(utt.fetch_seed())
        input_val = rng.uniform(size=(3, 5)).astype(config.floatX)
        p_val = rng.permutation(5).astype('int32')
        out_val = permute(input_val, p_val)
        out_bis = np.asarray([r[p_val] for r in input_val])
        assert np.all(out_val == out_bis)
        def permute_fixed(s_input):
            return permute_row_elements(s_input, p_val)
        utt.verify_grad(permute_fixed, [input_val])
    def test_2_2(self):
        input = matrix()
        p = imatrix()
        out = permute_row_elements(input, p)
        permute = function([input, p], out)
        rng = np.random.RandomState(utt.fetch_seed())
        input_val = rng.uniform(size=(3, 5)).astype(config.floatX)
        p_val = np.asarray([rng.permutation(5) for i in range(3)],
                           dtype='int32')
        out_val = permute(input_val, p_val)
        out_bis = np.asarray([i_row[p_row]
                              for i_row, p_row in zip(input_val, p_val)])
        assert np.all(out_val == out_bis)
        def permute_fixed(s_input):
            return permute_row_elements(s_input, p_val)
        utt.verify_grad(permute_fixed, [input_val])
    def test_1_2(self):
        input = vector()
        p = imatrix()
        out = permute_row_elements(input, p)
        permute = function([input, p], out)
        rng = np.random.RandomState(utt.fetch_seed())
        input_val = rng.uniform(size=(5,)).astype(config.floatX)
        p_val = np.asarray([rng.permutation(5)
                            for i in range(3)], dtype='int32')
        out_val = permute(input_val, p_val)
        out_bis = np.asarray([input_val[p_row] for p_row in p_val])
        assert np.all(out_val == out_bis)
        def permute_fixed(s_input):
            return permute_row_elements(s_input, p_val)
        utt.verify_grad(permute_fixed, [input_val])
    def test_3b_2(self):
        input = TensorType('floatX', (False, True, False))()
        p = imatrix()
        out = permute_row_elements(input, p)
        permute = function([input, p], out)
        rng = np.random.RandomState(utt.fetch_seed())
        input_val = rng.uniform(size=(4, 1, 5)).astype(config.floatX)
        p_val = np.asarray([rng.permutation(5) for i in range(3)],
                           dtype='int32')
        out_val = permute(input_val, p_val)
        out_bis = np.asarray([[in_mat[0, p_row] for p_row in p_val]
                              for in_mat in input_val])
        assert np.all(out_val == out_bis)
        def permute_fixed(s_input):
            return permute_row_elements(s_input, p_val)
        utt.verify_grad(permute_fixed, [input_val])
class test_tensordot(unittest.TestCase):
    def TensorDot(self, axes):
        return lambda a, b: tensordot(a, b, axes)
    def setUp(self):
        utt.seed_rng()
    def test0(self):
        avec = vector()
        bvec = vector()
        axes = ((0, ), (0, ))
        c = tensordot(avec, bvec, axes)
        f1 = inplace_func([avec, bvec], c)
        aval = rand(5)
        bval = rand(5)
        out0 = np.tensordot(aval, bval, axes)
        out1 = f1(aval, bval)
        utt.assert_allclose(out0, out1)
        utt.verify_grad(self.TensorDot(axes), [aval, bval])
        bmat = matrix()
        axes = ((0, ), (1, ))
        c = tensordot(avec, bmat, axes)
        f2 = inplace_func([avec, bmat], c)
        aval = rand(5)
        bval = rand(8, 5)
        utt.assert_allclose(np.tensordot(aval, bval, axes),
                            f2(aval, bval))
        utt.verify_grad(self.TensorDot(axes), [aval, bval])
        amat = matrix()
<a name="9"></a>        for axes, shps in [[((0,), (0,)), [(4, 7), (4, 9)]],
                           [((0,), (1,)), [(4, 7), (9, 4)]],
                           [((1,), (0,)), [(4, 7), (7, 9)]],
                           [((<font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>1,), (1,)), [(4, 7), (9, 7)]],
                           [((0, 1), (0, 1)), [(4, 7), (4, 7)]],
                           ]:
            c = tensordot(amat, bmat, axes)
            f3 =</b></font> inplace_func([amat, bmat], c)
            aval = rand(*shps[0])
            bval = rand(*shps[1])
            utt.assert_allclose(np.tensordot(aval, bval, axes),
                                f3(aval, bval))
            utt.verify_grad(self.TensorDot(axes), [aval, bval])
        for axes, shps in [[((2,), (1,)), [(1, 2, 3, 4), (2, 3)]],
                           [((0,), (1,)), [(1, 2, 3, 4), (3, 1)]],
                           [((<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0,), (0,)), [(1, 2, 3, 4), (1, 3)]],
                           [((3,), (0,)), [(1, 2, 3, 4), (4, 1)]],
                           ]:
            atens = tensor4()
            c =</b></font> tensordot(atens, bmat, axes)
            f4 = inplace_func([atens, bmat], c)
            aval = rand(*shps[0])
            bval = rand(*shps[1])
            utt.assert_allclose(np.tensordot(aval, bval, axes),
                                f4(aval, bval))
            utt.verify_grad(self.TensorDot(axes), [aval, bval])
        atens = tensor4()
        btens = tensor3()
        axes = ((1, 3), (0, 2))
        c = tensordot(atens, btens, axes)
        f5 = inplace_func([atens, btens], c)
        aval = rand(4, 3, 5, 2)
        bval = rand(3, 4, 2)
        utt.assert_allclose(np.tensordot(aval, bval, axes),
                            f5(aval, bval))
        utt.verify_grad(self.TensorDot(axes), [aval, bval])
        axes = (axes[1], axes[0])
        c = tensordot(btens, atens, axes)
        f6 = inplace_func([btens, atens], c)
        utt.assert_allclose(np.tensordot(bval, aval, axes),
                            f6(bval, aval))
        utt.verify_grad(self.TensorDot(axes), [bval, aval])
    def test_raise_error(self):
        amat = matrix()
        bmat = matrix()
        bvec = vector()
        self.assertRaises(ValueError, tensordot, amat, bmat, (0, 1, 2))
        self.assertRaises(ValueError, tensordot, amat, bmat, ((0, 1), (0)))
        self.assertRaises(ValueError, tensordot, amat, bmat, ((0, 1, 2), (0, 1, 2)))
        self.assertRaises(ValueError, tensordot, amat, bvec, (0, 1))
        self.assertRaises(ValueError, tensordot, amat, bvec, 2)
    def test_weird_valid_axes(self):
        amat = matrix()
        bmat = matrix()
        for axes in [0,
                     (1, 0),
                     [1, 0],
                     (1, (0, )),
                     ((1, ), 0),
                     ([1], [0]),
                     ([], [])]:
            c = tensordot(amat, bmat, axes)
            f3 = inplace_func([amat, bmat], c)
            aval = rand(4, 7)
            bval = rand(7, 9)
            utt.assert_allclose(np.tensordot(aval, bval, axes),
                                f3(aval, bval))
            utt.verify_grad(self.TensorDot(axes), [aval, bval])
    def test_scalar_axes(self):
        amat = fmatrix()
        bmat = dmatrix()
        axes = 1
        aval = rand(4, 5).astype('float32')
        bval = rand(5, 3)
        c = tensordot(amat, bmat, axes)
        f3 = inplace_func([amat, bmat], c)
        self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
                                    f3(aval, bval)))
        utt.verify_grad(self.TensorDot(axes), [aval, bval])
        amat = tensor3()
        bmat = tensor3()
        axes = 2
        aval = rand(3, 4, 5)
        bval = rand(4, 5, 3)
        c = tensordot(amat, bmat, axes)
        f3 = inplace_func([amat, bmat], c)
        self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
                                    f3(aval, bval)))
        utt.verify_grad(self.TensorDot(axes), [aval, bval])
    def test_scalar0(self):
        amat = matrix()
        bmat = matrix()
        axes = 0
        aval = rand(4, 5)
        bval = rand(5, 4)
        c = tensordot(amat, bmat, axes)
        f3 = inplace_func([amat, bmat], c)
        self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
                                    f3(aval, bval)))
        utt.verify_grad(self.TensorDot(axes), [aval, bval])
    def test_broadcastable1(self):
        x = TensorType(dtype=floatX, broadcastable=(True, False, False))('x')
        y = tensor3('y')
        z = tensordot(x, y)
        assert z.broadcastable == (True, False)
        f = inplace_func([x, y], z)
        xv = rand(1, 3, 4)
        yv = rand(3, 4, 5)
        zv = f(xv, yv)
        self.assertTrue(np.allclose(np.tensordot(xv, yv), zv))
    def test_broadcastable2(self):
        x = TensorType(dtype=floatX, broadcastable=(True, False, False))('x')
        y = tensor3('y')
        axes = [[2, 1], [0, 1]]
        z = tensordot(x, y, axes=axes)
        assert z.broadcastable == (True, False)
        f = inplace_func([x, y], z)
        xv = rand(1, 3, 4)
        yv = rand(4, 3, 5)
        zv = f(xv, yv)
        self.assertTrue(np.allclose(np.tensordot(xv, yv, axes=axes), zv))
def test_smallest_stack():
    sx, sy = dscalar(), dscalar()
    rval = inplace_func([sx, sy], stack([sx, sy]))(-4.0, -2.0)
    assert type(rval) == np.ndarray
    assert [-4, -2] == list(rval)
def test_smallest():
    x = dvector()
    y = dvector()
    z = dvector()
    f1 = inplace_func([x], smallest(x))
    assert np.all([1, 2, 3] == f1([1, 2, 3]))
    f3 = inplace_func([x, y, z], smallest(x, y, z))
    assert np.all([1, 2, 3] == f3([1, 3, 9], [7, 7, 7], [8, 2, 3]))
    sx, sy = dscalar(), dscalar()
    assert -4 == inplace_func([sx, sy], smallest(sx, sy))(-4.0, -2.0)
def test_reshape_member_fn():
    x = dmatrix()
    y = x.reshape((4, 5, 6))
    assert y.owner.op == Reshape(3)
def test_var():
    a = Tensor(dtype='float64', broadcastable=[False, False, False])()
    f = function([a], var(a))
    a_val = np.arange(60).reshape(3, 4, 5)
    assert np.allclose(np.var(a_val), f(a_val))
    f = function([a], var(a, axis=0))
    assert np.allclose(np.var(a_val, axis=0), f(a_val))
    f = function([a], var(a, axis=1))
    assert np.allclose(np.var(a_val, axis=1), f(a_val))
    f = function([a], var(a, axis=2))
    assert np.allclose(np.var(a_val, axis=2), f(a_val))
    f = function([a], var(a, axis=0, ddof=0))
    assert np.allclose(np.var(a_val, axis=0, ddof=0), f(a_val))
    f = function([a], var(a, axis=1, ddof=1))
    assert np.allclose(np.var(a_val, axis=1, ddof=1), f(a_val))
    f = function([a], var(a, axis=2, ddof=1))
    assert np.allclose(np.var(a_val, axis=2, ddof=1), f(a_val))
    f = function([a], var(a, ddof=0, corrected=True))
    mean_a = np.mean(a_val)
    centered_a = a_val - mean_a
    v = np.mean(centered_a ** 2)
    error = (np.mean(centered_a)) ** 2
    v = v - error
    assert np.allclose(v, f(a_val))
    f = function([a], var(a, axis=2, ddof=1, corrected=True))
    mean_a = np.mean(a_val, axis=2, keepdims=True)
    centered_a = a_val - mean_a
    v = np.var(a_val, axis=2, ddof=1)
    shp_inp = np.shape(a_val)
    shp = shp_inp - np.array(1)
    error = (np.sum(centered_a, axis=2)) ** 2
    error = np.true_divide(error, shp[1] * shp_inp[1])
    v = v - error
    assert np.allclose(v, f(a_val))
    assert theano.tensor.vector(dtype='float16').var().dtype == 'float16'
class T_sum(unittest.TestCase):
    def test_sum_overflow(self):
        a = Tensor(dtype='int8', broadcastable=[False])()
        f = function([a], sum(a))
        assert f([1] * 300) == 300
    def test_list(self):
        ll = [theano.shared(0.), theano.shared(2.)]
        tensor.sum(ll).eval() == 2
@dec.skipif(
    isinstance(get_default_mode(), theano.compile.debugmode.DebugMode),
    ("This test fails in DEBUG_MODE, but the generated code is OK. "
     "It is actually a problem of DEBUG_MODE, see #626."))
def test_default():
    x, y = scalars('xy')
    z = default(x, y)
    f = function([x, y], z)
    assert f(1, 2) == 1
    assert f(None, 2) == 2
    assert f(1, None) == 1
@dec.skipif(
    isinstance(get_default_mode(), theano.compile.debugmode.DebugMode),
    ("This test fails in DEBUG_MODE, but the generated code is OK. "
     "It is actually a problem of DEBUG_MODE, see #626."))
def test_default_state():
    x, y = scalars('xy')
    z = default(x, 3.8)
    new_x = y + z
    f = function([y, compile.In(x, update=new_x, value=12.0)], new_x)
    assert f(3) == 15
    f['x'] = None
    assert np.allclose(f(1), 4.8)
    assert np.allclose(f(np.asarray(2.2, dtype=config.floatX)), 7)
def test_autocast():
    backup_config = config.cast_policy
    for autocast_cfg in (
            'custom',
            'numpy+floatX',
            ):
        config.cast_policy = autocast_cfg
        try:
            eval('_test_autocast_' + autocast_cfg.replace('+', '_'))()
        finally:
            config.cast_policy = backup_config
def _test_autocast_custom():
    assert config.cast_policy == 'custom'
    orig_autocast = autocast_float.dtypes
    with autocast_float_as('float32'):
        assert autocast_float.dtypes == ('float32',)
    assert autocast_float.dtypes == orig_autocast
    with autocast_float_as('float64'):
        assert autocast_float.dtypes == ('float64',)
    assert autocast_float.dtypes == orig_autocast
    with autocast_float_as('float32'):
        assert autocast_float.dtypes == ('float32',)
        with autocast_float_as('float64'):
            assert autocast_float.dtypes == ('float64',)
        assert autocast_float.dtypes == ('float32',)
    assert autocast_float.dtypes == orig_autocast
    with autocast_float_as('float32'):
        assert (dvector() + 1.1).dtype == 'float64'
        assert (fvector() + 1.1).dtype == 'float32'
        assert ((fvector() + theano._asarray(1.1, dtype='float64')).dtype ==
                'float64')
        assert ((fvector() + theano._asarray(1.1, dtype='float32')).dtype ==
                'float32')
        assert (dvector() + 1).dtype == 'float64'
        assert (fvector() + 1).dtype == 'float32'
    with autocast_float_as('float64'):
        assert (dvector() + 1.1).dtype == 'float64'
        assert (fvector() + 1.1).dtype == 'float64'
        assert (fvector() + 1.0).dtype == 'float64'
        assert ((fvector() + theano._asarray(1.1, dtype='float64')).dtype ==
                'float64')
        assert ((fvector() + theano._asarray(1.1, dtype='float32')).dtype ==
                'float32')
        assert (dvector() + 1).dtype == 'float64'
        assert (fvector() + 1).dtype == 'float32'
    with autocast_float_as('float32', 'float64'):
        assert (dvector() + 1.1).dtype == 'float64'
        assert (fvector() + 1.1).dtype == theano.config.floatX
        assert (fvector() + 1.0).dtype == 'float32'
        assert (dvector() + np.float32(1.1)).dtype == 'float64'
        assert (dvector() + np.float64(1.1)).dtype == 'float64'
        assert (dvector() + np.float(1.1)).dtype == 'float64'
        assert (fvector() + np.float32(1.1)).dtype == 'float32'
        assert (fvector() + np.float64(1.1)).dtype == 'float64'
        assert (fvector() + np.float(1.1)).dtype == theano.config.floatX
        assert (lvector() + np.int64(1)).dtype == 'int64'
        assert (lvector() + np.int32(1)).dtype == 'int64'
        assert (lvector() + np.int16(1)).dtype == 'int64'
        assert (lvector() + np.int8(1)).dtype == 'int64'
        assert (ivector() + np.int8(1)).dtype == 'int32'
        assert (wvector() + np.int8(1)).dtype == 'int16'
        assert (bvector() + np.int8(1)).dtype == 'int8'
        with autocast_float_as('float64'):
            assert (fvector() + 1.0).dtype == 'float64'
def _test_autocast_numpy():
    assert config.cast_policy == 'numpy'
    def ok(z):
        assert tensor.constant(z).dtype == np.asarray(z).dtype
    for x in ([2 ** i for i in xrange(63)] +
              [0, L(0), L(1), L(2 ** 63 - 1)] +
              [0., 1., 1.1, 1.5]):
        n_x = np.asarray(x)
        ok(x)
        ok(-x)
        ok(x - 1)
        ok(-x + 1)
        ok(n_x)
def _test_autocast_numpy_floatX():
    assert config.cast_policy == 'numpy+floatX'
    backup_floatX = config.floatX
    def ok(z, floatX):
        if (isinstance(z, float) and
                floatX == 'float32' and
                not hasattr(z, 'dtype')):
            assert tensor.constant(z).dtype == 'float32'
        else:
            assert tensor.constant(z).dtype == np.asarray(z).dtype
    try:
        for floatX in ('float32', 'float64'):
            config.floatX = floatX
            for x in ([2 ** i - 1 for i in xrange(64)] +
                      [0, L(0), L(1), L(2 ** 63 - 1)] +
                      [0., 1., 1.1, 1.5]):
                ok(x, floatX)
                ok(-x, floatX)
                ok(x - 1, floatX)
                ok(-x + 1, floatX)
                ok(np.asarray(x), floatX)
                ok(np.float64(x), floatX)
    finally:
        config.floatX = backup_floatX
class test_arithmetic_cast(unittest.TestCase):
    def test_arithmetic_cast(self):
        backup_config = config.cast_policy
        dtypes = get_numeric_types(with_complex=True)
        def theano_scalar(dtype):
            return tensor.scalar(dtype=str(dtype))
        def numpy_scalar(dtype):
            return np.array(1, dtype=dtype)
        def theano_array(dtype):
            return tensor.vector(dtype=str(dtype))
        def numpy_array(dtype):
            return np.array([1], dtype=dtype)
        def theano_i_scalar(dtype):
            return theano.scalar.Scalar(str(dtype))()
        def numpy_i_scalar(dtype):
            return numpy_scalar(dtype)
        if config.int_division == 'int':
            warnings.filterwarnings('ignore', message='Division of two integer',
                                    category=DeprecationWarning)
        try:
            for cfg in ('numpy+floatX', ):  # Used to test 'numpy' as well.
                config.cast_policy = cfg
                for op in (operator.add, operator.sub, operator.mul,
                           operator_div, operator.floordiv):
                    for a_type in dtypes:
                        for b_type in dtypes:
                            is_int_division = (
                                op is operator_div and
                                a_type in tensor.discrete_dtypes and
                                b_type in tensor.discrete_dtypes)
                            for combo in (
                                    ('scalar', 'scalar'),
                                    ('array', 'array'),
                                    ('scalar', 'array'),
                                    ('array', 'scalar'),
                                    ('i_scalar', 'i_scalar'),
                                    ):
                                theano_args = list(
                                    map(eval, ['theano_%s' % c for c in combo]))
                                numpy_args = list(
                                    map(eval, ['numpy_%s' % c for c in combo]))
                                try:
                                    theano_dtype = op(
                                        theano_args[0](a_type),
                                        theano_args[1](b_type)).type.dtype
                                    assert not (is_int_division and
                                                config.int_division == 'raise')
                                except theano.scalar.IntegerDivisionError:
                                    assert (is_int_division and
                                            config.int_division == 'raise')
                                    continue
                                numpy_dtypes = [
                                    op(numpy_args[0](a_type),
                                       numpy_args[1](b_type)).dtype,
                                    op(numpy_args[1](b_type),
                                       numpy_args[0](a_type)).dtype]
                                numpy_dtype = theano.scalar.upcast(
                                    *list(map(str, numpy_dtypes)))
                                if numpy_dtype == theano_dtype:
                                    continue
                                if (cfg == 'numpy+floatX' and
                                        config.floatX == 'float32' and
                                        a_type != 'float64' and
                                        b_type != 'float64' and
                                        numpy_dtype == 'float64'):
                                    assert theano_dtype == 'float32'
                                    continue
                                if 'array' in combo and 'scalar' in combo:
                                    array_type, scalar_type = (
                                        (a_type, b_type)[list(combo).index(arg)]
                                        for arg in ('array', 'scalar'))
                                    up_type = theano.scalar.upcast(array_type,
                                                                   scalar_type)
                                    if (
                                            scalar_type != array_type and
                                            array_type != up_type and
                                            theano_dtype == up_type and
                                            array_type == numpy_dtype):
                                        continue
                                if (is_int_division and
                                        config.int_division == 'floatX'):
                                    assert theano_dtype == config.floatX
                                    continue
                                if (cfg == 'numpy+floatX' and
                                        a_type == 'complex128' and
                                        (b_type == 'float32' or
                                         b_type == 'float16') and
                                        combo == ('scalar', 'array') and
                                        theano_dtype == 'complex128' and
                                        numpy_dtype == 'complex64'):
                                    raise SkipTest("Known issue with"
                                                   "numpy see #761")
                                assert False
        finally:
            config.cast_policy = backup_config
            if config.int_division == 'int':
                warnings.filterwarnings(
                    'default',
                    message='Division of two integer',
                    category=DeprecationWarning)
class T_long_tensor(unittest.TestCase):
    def test_fit_int64(self):
        bitwidth = theano.configdefaults.python_int_bitwidth()
        for exponent in xrange(bitwidth):
            val = L(2 ** exponent - 1)
            scalar_ct = constant(val)
            assert scalar_ct.dtype in tensor.int_dtypes, (exponent, val, scalar_ct.dtype)
            assert scalar_ct.value == val
            vector_ct = constant([val, val])
            if PY3 and bitwidth == 32:
                assert vector_ct.dtype == 'int32'
            else:
                assert vector_ct.dtype == 'int64'
            assert np.all(vector_ct.value == val)
            matrix_ct = constant([[val, val]])
            if PY3 and bitwidth == 32:
                assert matrix_ct.dtype == 'int32'
            else:
                assert matrix_ct.dtype == 'int64'
            assert np.all(matrix_ct.value == val)
    def test_too_big(self):
        val = L(2 ** 64)
        self.assertRaises(Exception, constant, val)
        self.assertRaises(Exception, constant, [val, val])
        self.assertRaises(Exception, constant, [[val, val]])
class test_broadcast(unittest.TestCase):
    def test_broadcast_bigdim(self):
        def f():
            x = matrix()
            addbroadcast(x, 2)
        self.assertRaises(ValueError, f)
    def test_unbroadcast_addbroadcast(self):
        x = matrix()
        assert unbroadcast(x, 0) is x
        assert unbroadcast(x, 1) is x
        assert unbroadcast(x, 1, 0) is x
        assert unbroadcast(x, 0, 1) is x
        assert addbroadcast(x, 0) is not x
        assert addbroadcast(x, 1) is not x
        assert addbroadcast(x, 1, 0).owner.inputs[0] is x
        assert unbroadcast(addbroadcast(x, 0), 0) is x
        assert addbroadcast(unbroadcast(x, 0), 0) is not x
        x = row()
        assert unbroadcast(x, 0) is not x
        assert unbroadcast(x, 1) is x
        assert unbroadcast(x, 1, 0) is not x
        assert unbroadcast(x, 0, 1) is not x
        assert addbroadcast(x, 0) is x
        assert addbroadcast(x, 1).owner.inputs[0] is x
        assert addbroadcast(x, 1, 0).owner.inputs[0] is x
        assert addbroadcast(x, 0, 1).owner.inputs[0] is x
        assert unbroadcast(addbroadcast(x, 1), 1) is x
        assert addbroadcast(unbroadcast(x, 1), 1) is not x
        assert unbroadcast(unbroadcast(x, 0), 0).owner.inputs[0] is x
        x = TensorType(dtype='float64', broadcastable=(True, True))()
        assert unbroadcast(unbroadcast(x, 1), 0).owner.inputs[0] is x
        assert addbroadcast(unbroadcast(x, 1), 0).owner.inputs[0] is x
        assert addbroadcast(unbroadcast(x, 0), 0) is x
    def test_patternbroadcast(self):
        x = scalar('x')
        m = tensor.matrix('m')
        s = patternbroadcast(m, x.broadcastable)
        assert s is m
        x2 = patternbroadcast(x, x.broadcastable)
        assert x2 is x
    def test_infer_shape(self):
        x = matrix()
        y = addbroadcast(x, 0)
        f = theano.function([x], y.shape)
        assert (f(np.zeros((1, 5), dtype=config.floatX)) == [1, 5]).all()
        topo = f.maker.fgraph.toposort()
        if theano.config.mode != 'FAST_COMPILE':
            assert len(topo) == 2
            assert isinstance(topo[0].op, opt.Shape_i)
            assert isinstance(topo[1].op, opt.MakeVector)
        x = matrix()
        y = unbroadcast(x, 0)
        f = theano.function([x], y.shape)
        assert (f(np.zeros((2, 5), dtype=config.floatX)) == [2, 5]).all()
        topo = f.maker.fgraph.toposort()
        if theano.config.mode != 'FAST_COMPILE':
            assert len(topo) == 3
            assert isinstance(topo[0].op, opt.Shape_i)
            assert isinstance(topo[1].op, opt.Shape_i)
            assert isinstance(topo[2].op, opt.MakeVector)
        x = row()
        y = unbroadcast(x, 0)
        f = theano.function([x], y.shape)
        assert (f(np.zeros((1, 5), dtype=config.floatX)) == [1, 5]).all()
        topo = f.maker.fgraph.toposort()
        if theano.config.mode != 'FAST_COMPILE':
            assert len(topo) == 2
            assert isinstance(topo[0].op, opt.Shape_i)
            assert isinstance(topo[1].op, opt.MakeVector)
def test_len():
    for shape_ in [(5,), (3, 4), (7, 4, 6)]:
        x = tensor.tensor(dtype='floatX', broadcastable=(False,) * len(shape_))
        assert_raises(TypeError, len, x)
def test_mod():
    x, y = fscalars('xy')
    fn = gof.DualLinker().accept(
        gof.FunctionGraph([x, y], [x % y])).make_function()
    for a, b in ((0, 1), (1, 1), (0, -1), (1, -1), (-1, -1),
                 (1, 2), (-1, 2), (1, -2), (-1, -2),
                 (5, 3), (-5, 3), (5, -3), (-5, -3)
                 ):
        assert fn(a, b) == a % b, (a,)
def test_divmod():
    x, y = fscalars('xy')
<a name="20"></a>    d, r = divmod(x, y)
    fn = gof.DualLinker().accept(
        gof.FunctionGraph([x, y], [d, r])).make_function()
    for a, b in ((0, 1), (1, 1), (0, -1), (<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>1, -1), (-1, -1),
                 (1, 2), (-1, 2), (1, -2), (-1, -2),
                 (5, 3), (-5, 3), (5, -3), (-5, -3)
                 ):
        d_v, r_v = fn(a, b)
        d_vp, r_vp =</b></font> divmod(a, b)
        assert d_v == d_vp and r_v == r_vp, (a,)
def test_mod_compile():
    x = tensor.vector()
    y = tensor.vector()
    out = tensor.switch(tensor.eq(3 % x.shape[0], 0), y, y[:-1])
    theano.function([x, y], out)
def test_unalign():
    if config.floatX == 'float64':
        dtype = "b1,f8"
    else:
        dtype = "b1,f4"
    a = np.empty(10000, dtype=dtype)['f1']
    b = np.empty(10000, dtype=dtype)['f1']
    assert not a.flags.aligned
    assert not b.flags.aligned
    a[:] = rand(len(a))
    b[:] = rand(len(b))
    out_numpy = 2 * a + 3 * b
    av, bv = tensor.vectors('ab')
    f = theano.function([av, bv], 2 * av + 3 * bv)
    f.maker.fgraph.toposort()
    try:
        out_theano = f(a, b)
        assert not a.flags.aligned
        assert not b.flags.aligned
        assert np.allclose(out_numpy, out_theano)
        assert False
    except TypeError:
        pass
    a = np.empty((), dtype=dtype)['f1']
    b = np.empty((), dtype=dtype)['f1']
    assert not a.flags.aligned
    assert not b.flags.aligned
    out_numpy = 2 * a + 3 * b
    av, bv = tensor.scalars('ab')
    f = theano.function([av, bv], 2 * av + 3 * bv)
    f.maker.fgraph.toposort()
    try:
        out_theano = f(a, b)
        assert not a.flags.aligned
        assert not b.flags.aligned
        assert np.allclose(out_numpy, out_theano)
        assert False
    except TypeError:
        pass
def test_dimshuffle_duplicate():
    x = tensor.vector()
    success = False
    try:
        tensor.DimShuffle((False, ), (0, 0))(x)
    except ValueError as e:
        assert str(e).find("may not appear twice") != -1
        success = True
    assert success
class T_get_scalar_constant_value(unittest.TestCase):
    def test_get_scalar_constant_value(self):
        a = tensor.stack([1, 2, 3])
        assert get_scalar_constant_value(a[0]) == 1
        assert get_scalar_constant_value(a[1]) == 2
        assert get_scalar_constant_value(a[2]) == 3
        b = tensor.iscalar()
        a = tensor.stack([b, 2, 3])
        self.assertRaises(tensor.basic.NotScalarConstantError, get_scalar_constant_value, a[0])
        assert get_scalar_constant_value(a[1]) == 2
        assert get_scalar_constant_value(a[2]) == 3
        v = tensor.ivector()
        a = tensor.stack([v, [2], [3]])
        self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[0])
        self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[1])
        self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[2])
        v = tensor.row()
        assert get_scalar_constant_value(v.shape[0]) == 1
    def test_subtensor_of_constant(self):
        c = constant(rand(5))
        for i in range(c.value.shape[0]):
            assert get_scalar_constant_value(c[i]) == c.value[i]
        c = constant(rand(5, 5))
        for i in range(c.value.shape[0]):
            for j in range(c.value.shape[1]):
                assert get_scalar_constant_value(c[i, j]) == c.value[i, j]
    def test_numpy_array(self):
        assert get_scalar_constant_value(np.array(3)) == 3
        self.assertRaises(
            tensor.NotScalarConstantError,
            get_scalar_constant_value,
            np.array([0, 1]))
        self.assertRaises(
            tensor.EmptyConstantError,
            get_scalar_constant_value,
            np.array([]))
    def test_make_vector(self):
        mv = opt.make_vector(1, 2, 3)
        self.assertRaises(
            tensor.NotScalarConstantError,
            get_scalar_constant_value,
            mv)
        assert get_scalar_constant_value(mv[0]) == 1
        assert get_scalar_constant_value(mv[1]) == 2
        assert get_scalar_constant_value(mv[2]) == 3
        assert get_scalar_constant_value(mv[np.int32(0)]) == 1
        assert get_scalar_constant_value(mv[np.int64(1)]) == 2
        assert get_scalar_constant_value(mv[np.uint(2)]) == 3
        t = theano.scalar.Scalar('int64')
        self.assertRaises(
            tensor.NotScalarConstantError,
            get_scalar_constant_value,
            mv[t()])
    def test_shape_i(self):
        c = theano.tensor.constant(np.random.rand(3, 4))
        s = opt.Shape_i(0)(c)
        assert get_scalar_constant_value(s) == 3
        s = opt.Shape_i(1)(c)
        assert get_scalar_constant_value(s) == 4
        d = theano.shared(np.random.randn(1, 1), broadcastable=(True, True))
        f = theano.tensor.basic.ScalarFromTensor()(opt.Shape_i(0)(d))
        assert get_scalar_constant_value(f) == 1
    def test_elemwise(self):
        c = theano.tensor.constant(np.random.rand())
        s = c + 1
        assert np.allclose(get_scalar_constant_value(s), c.data + 1)
        s = c - 1
        assert np.allclose(get_scalar_constant_value(s), c.data - 1)
        s = c * 1.2
        assert np.allclose(get_scalar_constant_value(s), c.data * 1.2)
        s = c &lt; 0.5
        assert np.allclose(get_scalar_constant_value(s), int(c.data &lt; 0.5))
        s = tensor.second(c, .4)
        assert np.allclose(get_scalar_constant_value(s), .4)
    def test_assert(self):
        c = theano.tensor.constant(2)
        x = theano.tensor.scalar()
        a = opt.Assert()(c, c &gt; 1)
        assert get_scalar_constant_value(a) == 2
        with change_flags(compute_test_value='off'):
            a = opt.Assert()(c, c &gt; 2)
            self.assertRaises(
                tensor.NotScalarConstantError,
                get_scalar_constant_value, a)
        a = opt.Assert()(c, c &gt; x)
        self.assertRaises(
            tensor.NotScalarConstantError,
            get_scalar_constant_value, a)
    def test_second(self):
        c = theano.tensor.constant(np.random.rand())
        shp = theano.tensor.vector()
        s = theano.tensor.second(shp, c)
        assert get_scalar_constant_value(s) == c.data
    def test_copy(self):
        c = theano.tensor.constant(3)
        d = extract_constant(c)
        d += 1
        e = extract_constant(c)
        self.assertTrue(e == 3, (c, d, e))
class T_as_tensor_variable(unittest.TestCase):
    def test_bool(self):
        self.assertRaises(TypeError, as_tensor_variable, True)
        self.assertRaises(TypeError, as_tensor_variable, False)
    def test_ndarray_bool(self):
        ten = as_tensor_variable(np.array([True, False, False, True, True]))
        assert ten.type.dtype == 'bool'
    def test_memmap(self):
        inp = np.random.rand(4, 3)
        f, fname = mkstemp()
        new_inp = np.memmap(fname, dtype=inp.dtype,
                            mode='w+', shape=inp.shape)
        new_inp[...] = inp
        as_tensor_variable(new_inp)
    def test_empty_dtype(self):
        old = theano.config.floatX
        for dtype in ['float16', 'float32', 'float64']:
            try:
                theano.config.floatX = dtype
                assert theano.tensor.as_tensor_variable(()).dtype == dtype
                assert theano.tensor.as_tensor_variable([]).dtype == dtype
            finally:
                theano.config.floatX = old
class test_complex_mod(unittest.TestCase):
    def test_fail(self):
        x = vector(dtype='complex64')
        try:
            x % 5
            assert False
        except theano.scalar.ComplexError:
            pass
class test_size(unittest.TestCase):
    def test_matrix(self):
        x = tensor.matrix()
        y = np.zeros((5, 7), dtype=config.floatX)
        assert y.size == function([x], x.size)(y)
    def test_vector(self):
        x = tensor.vector()
        y = np.zeros(7, dtype=config.floatX)
        assert y.size == function([x], x.size)(y)
    def test_scalar(self):
        x = tensor.scalar()
        y = np.array(7, dtype=config.floatX)
        assert y.size == function([x], x.size)(y)
    def test_shared(self):
        y = np.zeros((1, 2, 3, 4), dtype=config.floatX)
        x = theano.shared(y)
        assert y.size == function([], x.size)()
class test_diag(unittest.TestCase):
    def __init__(self, name, mode=None, shared=tensor._shared,
                 floatX=None, type=tensor.TensorType):
        self.mode = mode
        self.shared = shared
        if floatX is None:
            floatX = config.floatX
        self.floatX = floatX
        self.type = type
        super(test_diag, self).__init__(name)
    def test_diag(self):
        rng = np.random.RandomState(utt.fetch_seed())
        x = theano.tensor.vector()
        g = diag(x)
        assert isinstance(g.owner.op, AllocDiag)
        f = theano.function([x], g)
        for shp in [5, 0, 1]:
            m = rng.rand(shp).astype(self.floatX)
            v = np.diag(m)
            r = f(m)
            assert (r == v).all()
        xx = self.shared(rng.rand(3, 5))
        g = diag(xx)
        assert isinstance(g.owner.op, ExtractDiag)
        f = theano.function([], g)
        for shp in [(5, 3), (3, 5), (5, 1), (1, 5), (5, 0), (0, 5),
                    (1, 0), (0, 1)]:
            m = rng.rand(*shp).astype(self.floatX)
            xx.set_value(m)
            v = np.diag(m)
            r = f()
            assert (r == v).all()
        xx = theano.tensor.scalar()
        np.testing.assert_raises(ValueError, diag, xx)
    def test_infer_shape(self):
        rng = np.random.RandomState(utt.fetch_seed())
        x = theano.tensor.vector()
        g = diag(x)
        f = theano.function([x], g.shape)
        topo = f.maker.fgraph.toposort()
        if config.mode != 'FAST_COMPILE':
            assert np.sum(
                [isinstance(node.op, AllocDiag) for node in topo]) == 0
        for shp in [5, 0, 1]:
            m = rng.rand(shp).astype(self.floatX)
            assert (f(m) == np.diag(m).shape).all()
        x = theano.tensor.matrix()
        g = diag(x)
        f = theano.function([x], g.shape)
        topo = f.maker.fgraph.toposort()
        if config.mode != 'FAST_COMPILE':
            assert np.sum(
                [isinstance(node.op, ExtractDiag) for node in topo]) == 0
        for shp in [(5, 3), (3, 5), (5, 1), (1, 5), (5, 0), (0, 5),
                    (1, 0), (0, 1)]:
            m = rng.rand(*shp).astype(self.floatX)
            assert (f(m) == np.diag(m).shape).all()
    def test_diag_grad(self):
        rng = np.random.RandomState(utt.fetch_seed())
        x = rng.rand(5)
        tensor.verify_grad(diag, [x], rng=rng)
        x = rng.rand(5, 3)
        tensor.verify_grad(diag, [x], rng=rng)
class TestAllocDiag(unittest.TestCase):
    def __init__(self, name, alloc_diag=AllocDiag, mode=None):
        self.alloc_diag = alloc_diag
        if mode is None:
            mode = theano.compile.mode.get_default_mode()
        self.mode = mode
        super(TestAllocDiag, self).__init__(name)
    def _generator(self):
        dims = 4
        shape = (5,) * dims
        xv = np.random.randn(*shape).astype(config.floatX)
        for d in xrange(1, dims + 1):
            x = TensorType(dtype=config.floatX, broadcastable=(False,) * d)('x')
            test_val = xv[((0,) * (dims - d))]
            yield x, test_val
    def test_alloc_diag_values(self):
        for x, test_val in self._generator():
            for offset, axis1, axis2 in [(0, 0, 1), (0, 1, 2), (1, 0, 1),
                                         (0, 1, 3), (0, 2, 3), (1, 2, 3),
                                         (-1, 0, 1), (-2, 0, 1), (-1, 1, 2)]:
                if np.maximum(axis1, axis2) &gt; len(test_val.shape):
                    continue
                adiag_op = self.alloc_diag(offset=offset,
                                           axis1=axis1,
                                           axis2=axis2)
                f = theano.function([x], adiag_op(x))
                diag_arr = f(test_val)
                rediag = np.diagonal(
                    diag_arr,
                    offset=offset,
                    axis1=axis1,
                    axis2=axis2
                )
                assert np.all(rediag == test_val)
                f_shape = theano.function([x], adiag_op(x).shape, mode='FAST_RUN')
                theano.printing.debugprint(f_shape.maker.fgraph.outputs[0])
                output_shape = f_shape(test_val)
                assert not any(isinstance(node.op, self.alloc_diag)
                               for node in f_shape.maker.fgraph.toposort())
                rediag_shape = np.diagonal(
                    np.ones(output_shape),
                    offset=offset,
                    axis1=axis1,
                    axis2=axis2
                ).shape
                assert np.all(rediag_shape == test_val.shape)
                diag_x = adiag_op(x)
                sum_diag_x = tensor.sum(diag_x)
                grad_x = tensor.grad(sum_diag_x, x)
                grad_diag_x = tensor.grad(sum_diag_x, diag_x)
                f_grad_x = theano.function([x], grad_x, mode=self.mode)
                f_grad_diag_x = theano.function([x], grad_diag_x, mode=self.mode)
                grad_input = f_grad_x(test_val)
                grad_diag_input = f_grad_diag_x(test_val)
                true_grad_input = np.diagonal(
                    grad_diag_input,
                    offset=offset,
                    axis1=axis1,
                    axis2=axis2
                )
                assert np.all(true_grad_input == grad_input)
class test_numpy_assumptions(unittest.TestCase):
    def test_ndarray_copy(self):
        assert copy(np.ndarray) is np.ndarray
        assert deepcopy(np.ndarray) is np.ndarray
    def test_dtype_equality(self):
        dtypes = get_numeric_types(with_complex=True)
        for dtype1_idx, dtype1 in enumerate(dtypes):
            for dtype2 in dtypes[dtype1_idx + 1:]:
                assert (dtype1 == dtype2) == (str(dtype1) == str(dtype2))
def test_transpose():
    x1 = tensor.dvector('x1')
    x2 = tensor.dmatrix('x2')
    x3 = tensor.dtensor3('x3')
    x1v = np.arange(24)
    x2v = np.arange(24).reshape(2, 12)
    x3v = np.arange(24).reshape(2, 3, 4)
    f = theano.function([x1, x2, x3], [
        tensor.transpose(x1),
        tensor.transpose(x2),
        tensor.transpose(x3),
        x1.transpose(),
        x2.transpose(),
        x3.transpose(),
        x2.transpose(0, 1),
        x3.transpose((0, 2, 1)),
        tensor.transpose(x2, [0, 1]),
        tensor.transpose(x3, [0, 2, 1]),
        ])
    t1, t2, t3, t1b, t2b, t3b, t2c, t3c, t2d, t3d = f(x1v, x2v, x3v)
    assert t1.shape == np.transpose(x1v).shape
    assert t2.shape == np.transpose(x2v).shape
    assert t3.shape == np.transpose(x3v).shape
    assert np.all(t1 == np.transpose(x1v))
    assert np.all(t2 == np.transpose(x2v))
    assert np.all(t3 == np.transpose(x3v))
    assert np.all(t1b == x1v.transpose())
    assert np.all(t2b == x2v.transpose())
    assert np.all(t3b == x3v.transpose())
    assert t2c.shape == (2, 12)
    assert t3c.shape == (2, 4, 3)
    assert np.all(t2c == x2v.transpose([0, 1]))
    assert np.all(t3c == x3v.transpose([0, 2, 1]))
    assert t2d.shape == (2, 12)
    assert t3d.shape == (2, 4, 3)
    assert np.all(t2d == np.transpose(x2v, [0, 1]))
    assert np.all(t3d == np.transpose(x3v, [0, 2, 1]))
    assert tensor.transpose(x1).name == 'x1.T'
    assert tensor.transpose(x2).name == 'x2.T'
    assert tensor.transpose(x3).name == 'x3.T'
    assert tensor.transpose(tensor.dmatrix()).name is None
def test_stacklists():
    a, b, c, d = map(scalar, 'abcd')
    X = stacklists([[a, b],
                    [c, d]])
    f = function([a, b, c, d], X)
    result = f(1, 2, 3, 4)
    assert result.shape == (2, 2)
    assert np.allclose(f(1, 2, 3, 4), np.asarray([[1, 2], [3, 4]]))
    X = stacklists([a, b, c, d])
    f = function([a, b, c, d], X)
    result = f(1, 2, 3, 4)
    assert result.shape == (4,)
    assert np.allclose(f(1, 2, 3, 4), np.asarray([[1, 2, 3, 4]]))
    X = stacklists([[[a], [b]], [[c], [d]]])
    f = function([a, b, c, d], X)
    result = f(1, 2, 3, 4)
    assert result.shape == (2, 2, 1)
    a, b, c, d = [matrix(x) for x in 'abcd']
    X = stacklists([[a, b],
                    [c, d]])
    f = function([a, b, c, d], X)
    x = np.ones((4, 4), 'float32')
    assert f(x, x, x, x).shape == (2, 2, 4, 4)
class TestSpecifyShape(unittest.TestCase):
    mode = None
    input_type = TensorType
    def shortDescription(self):
        return None
    def test_bad_shape(self):
        specify_shape = SpecifyShape()
        x = vector()
        xval = np.random.rand(2).astype(floatX)
        f = theano.function([x], specify_shape(x, [2]), mode=self.mode)
        f(xval)
        xval = np.random.rand(3).astype(floatX)
        self.assertRaises(AssertionError, f, xval)
        assert isinstance([n for n in f.maker.fgraph.toposort()
                           if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
                          self.input_type)
        x = matrix()
        xval = np.random.rand(2, 3).astype(floatX)
        f = theano.function([x], specify_shape(x, [2, 3]), mode=self.mode)
        assert isinstance([n for n in f.maker.fgraph.toposort()
                           if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
                          self.input_type)
        f(xval)
        for shape_ in [(1, 3), (2, 2), (5, 5)]:
            xval = np.random.rand(*shape_).astype(floatX)
            self.assertRaises(AssertionError, f, xval)
    def test_bad_number_of_shape(self):
        specify_shape = SpecifyShape()
        x = vector()
        shape_vec = ivector()
        xval = np.random.rand(2).astype(floatX)
        self.assertRaises(AssertionError, specify_shape, x, [])
        self.assertRaises(AssertionError, specify_shape, x, [2, 2])
        f = theano.function([x, shape_vec], specify_shape(x, shape_vec),
                            mode=self.mode)
        assert isinstance([n for n in f.maker.fgraph.toposort()
                           if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
                          self.input_type)
        self.assertRaises(AssertionError, f, xval, [])
        self.assertRaises(AssertionError, f, xval, [2, 2])
        x = matrix()
        xval = np.random.rand(2, 3).astype(floatX)
        for shape_ in [(),
                       (1,),
                       (2, 3, 4)]:
            self.assertRaises(AssertionError, specify_shape, x, shape_)
            f = theano.function([x, shape_vec], specify_shape(x, shape_vec),
                                mode=self.mode)
            assert isinstance([n for n in f.maker.fgraph.toposort()
                               if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
                              self.input_type)
            self.assertRaises(AssertionError, f, xval, shape_)
class TestInferShape(utt.InferShapeTester):
    def test_infer_shape(self):
        atens3 = tensor3()
        atens3_val = rand(4, 5, 3)
        for outdim in (3, 2, 1):
            self._compile_and_check([atens3],
                                    [flatten(atens3, outdim)],
                                    [atens3_val], Reshape,
                                    excluding=['local_useless_reshape'])
        amat = matrix()
        amat_val = rand(4, 5)
        for outdim in (2, 1):
            self._compile_and_check([amat],
                                    [flatten(amat, outdim)],
                                    [amat_val], Reshape,
                                    excluding=['local_useless_reshape'])
        avec = vector()
        avec_val = rand(4)
        outdim = 1
        self._compile_and_check([avec],
                                [flatten(avec, outdim)],
                                [avec_val], Reshape,
                                excluding=['local_useless_reshape'])
        aiscal = iscalar()
        biscal = iscalar()
        ciscal = iscalar()
        self._compile_and_check([aiscal, biscal, ciscal],
                                [Eye()(aiscal, biscal, ciscal)],
                                [4, 4, 0], Eye)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [Eye()(aiscal, biscal, ciscal)],
                                [4, 5, 0], Eye)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [Eye()(aiscal, biscal, ciscal)],
                                [3, 5, 0], Eye)
        aiscal = iscalar()
        biscal = iscalar()
        ciscal = iscalar()
        self._compile_and_check([aiscal, biscal, ciscal],
                                [Tri()(aiscal, biscal, ciscal)],
                                [4, 4, 0], Tri)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [Tri()(aiscal, biscal, ciscal)],
                                [4, 5, 0], Tri)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [Tri()(aiscal, biscal, ciscal)],
                                [3, 5, 0], Tri)
        atens3 = tensor3()
        atens3_val = rand(4, 5, 3)
        atens3_diag = ExtractDiag()(atens3)
        self._compile_and_check([atens3], [atens3_diag],
                                [atens3_val], ExtractDiag)
        atens3_diag = ExtractDiag(1)(atens3)
        self._compile_and_check([atens3], [atens3_diag],
                                [atens3_val], ExtractDiag)
        atens3_diag = ExtractDiag(-1)(atens3)
        self._compile_and_check([atens3], [atens3_diag],
                                [atens3_val], ExtractDiag)
        atens3_diag = ExtractDiag(1, 0, 2)(atens3)
        self._compile_and_check([atens3], [atens3_diag],
                                [atens3_val], ExtractDiag)
        atens3_diag = ExtractDiag(1, 1, 2)(atens3)
        self._compile_and_check([atens3], [atens3_diag],
                                [atens3_val], ExtractDiag)
        atens3_diag = ExtractDiag(1, 2, 0)(atens3)
        self._compile_and_check([atens3], [atens3_diag],
                                [atens3_val], ExtractDiag)
        advec = dvector()
        advec_val = rand(4)
        self._compile_and_check([advec], [AllocDiag()(advec)],
                                [advec_val], AllocDiag)
        adtens = tensor3()
        adtens_val = rand(4, 5, 3)
        self._compile_and_check([adtens],
                                [Shape()(adtens)],
                                [adtens_val], (opt.MakeVector, Shape))
        advec = dvector()
        bdvec = dvector()
        advec_val = rand(4)
        bdvec_val = rand(4)
        self._compile_and_check([advec, bdvec],
<a name="18"></a>                                [Dot()(advec, bdvec)],
                                [advec_val, bdvec_val],
                                (Dot, tensor.blas.Dot22,
                                 tensor.blas<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Gemv, tensor.blas_c.CGemv))
        admat = dmatrix()
        bdmat = dmatrix()
        admat_val = rand(4, 5)
        bdmat_val = rand(5, 3)
        self._compile_and_check(</b></font>[admat, bdmat],
                                [Dot()(admat, bdmat)],
                                [admat_val, bdmat_val],
                                (Dot, tensor.blas.Dot22))
        bdmat_val = rand(4, 5)
        self._compile_and_check([advec, bdmat],
                                [Dot()(advec, bdmat)],
                                [advec_val, bdmat_val],
                                (Dot, tensor.blas.Dot22,
                                 tensor.blas.Gemv, tensor.blas_c.CGemv))
        admat_val = rand(5, 4)
        self._compile_and_check([admat, bdvec],
                                [Dot()(admat, bdvec)],
                                [admat_val, bdvec_val],
                                (Dot, tensor.blas.Dot22,
                                 tensor.blas.Gemv, tensor.blas_c.CGemv))
        aivec = ivector()
        adtens_val = rand(4, 10, 3)
        aivec_val = [2, 5, 3]
        for aiscal_val in [1, -2]:
            self._compile_and_check(
                [adtens, aiscal, aivec],
                [Split(3)(adtens, aiscal, aivec)[0]],
                [adtens_val, aiscal_val, aivec_val], (Split))
        cdmat = dmatrix()
        admat_val = rand(1, 3)
        bdmat_val = rand(2, 3)
        cdmat_val = rand(4, 3)
        for aiscal_val in [0, -2]:
            self._compile_and_check(
                [aiscal, admat, bdmat, cdmat],
                [Join()(aiscal, admat, bdmat, cdmat)],
                [aiscal_val, admat_val, bdmat_val, cdmat_val], Join)
        admat_val = rand(4, 1)
        bdmat_val = rand(4, 3)
        cdmat_val = rand(4, 2)
        for aiscal_val in [-1, 1]:
            self._compile_and_check(
                [aiscal, admat, bdmat, cdmat],
                [Join()(aiscal, admat, bdmat, cdmat)],
                [aiscal_val, admat_val, bdmat_val, cdmat_val], Join)
        abool = True
        rng = np.random.RandomState(utt.fetch_seed())
        advec_val = rand(5)
        aivec_val = rng.permutation(5).astype('int32')
        self._compile_and_check([advec, aivec],
                                [PermuteRowElements()(advec, aivec, abool)],
                                [advec_val, aivec_val], PermuteRowElements)
        admat_val = rand(3, 5)
        self._compile_and_check([admat, aivec],
                                [PermuteRowElements()(admat, aivec, abool)],
                                [admat_val, aivec_val], PermuteRowElements)
        adtens3 = dtensor3()
        adtens3_val = rand(3, 2, 5)
        self._compile_and_check([adtens3, aivec],
                                [PermuteRowElements()(adtens3, aivec, abool)],
                                [adtens3_val, aivec_val], PermuteRowElements)
        aimat = imatrix()
        perma = rng.permutation(5).astype('int32')
        permb = rng.permutation(5).astype('int32')
        permc = rng.permutation(5).astype('int32')
        aimat_val = np.vstack((perma, permb, permc))
        admat_val = rand(3, 5)
        self._compile_and_check([admat, aimat],
                                [PermuteRowElements()(admat, aimat, abool)],
                                [admat_val, aimat_val], PermuteRowElements)
        aitens3 = itensor3()
        perma = rng.permutation(5).astype('int32')
        permb = rng.permutation(5).astype('int32')
        permc = rng.permutation(5).astype('int32')
        bimat_val = np.vstack((perma, permb, permc))
        aitens3_val = np.empty((2, 3, 5), 'int32')
        aitens3_val[0, ::, ::] = aimat_val
        aitens3_val[1, ::, ::] = bimat_val
        self._compile_and_check([admat, aitens3],
                                [PermuteRowElements()(admat, aitens3, abool)],
                                [admat_val, aitens3_val], PermuteRowElements)
        aiscal = iscalar()
        self._compile_and_check([aiscal],
                                [TensorFromScalar()(ScalarFromTensor()(aiscal))],
                                [45], ScalarFromTensor,
                                excluding=["local_tensor_scalar_tensor"])
        aiscal = scal.float64()
        self._compile_and_check([aiscal],
                                [TensorFromScalar()(aiscal)],
                                [4.], TensorFromScalar)
        adtens4 = dtensor4()
        adict = [(0, False), (1, True), (2, False), (3, True)]
        adtens4_val = rand(2, 1, 3, 1)
        self._compile_and_check([adtens4],
                                [Rebroadcast(*adict)(adtens4)],
                                [adtens4_val], Rebroadcast,
                                warn=False)
        adtens4_bro = TensorType('float64', (True, True, True, False))()
        bdict = [(0, True), (1, False), (2, False), (3, False)]
        adtens4_bro_val = rand(1, 1, 1, 3)
        self._compile_and_check([adtens4_bro],
                                [Rebroadcast(*bdict)(adtens4_bro)],
                                [adtens4_bro_val], Rebroadcast)
<a name="13"></a>
        randint = np.random.randint
        adscal <font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= dscalar()
        aiscal = lscalar()
        biscal = lscalar()
        ciscal = lscalar()
        discal = lscalar()
        adscal_val = rand()
        aiscal_val = randint(</b></font>3, 6, size=())
        biscal_val = randint(3, 6, size=())
        ciscal_val = randint(3, 6, size=())
        discal_val = randint(3, 6, size=())
        self._compile_and_check(
            [adscal, aiscal, biscal, ciscal, discal],
            [Alloc()(adscal, aiscal, biscal, ciscal, discal)],
            [adscal_val, aiscal_val, biscal_val, ciscal_val, discal_val],
            Alloc)
        adtens3_val = rand(4, 5, 3)
        self._compile_and_check([adtens3],
                                max_and_argmax(adtens3, None),
                                [adtens3_val], MaxAndArgmax)
        self._compile_and_check([adtens3],
                                max_and_argmax(adtens3, 0),
                                [adtens3_val], MaxAndArgmax)
        self._compile_and_check([adtens3],
                                max_and_argmax(adtens3, 1),
                                [adtens3_val], MaxAndArgmax)
        self._compile_and_check([adtens3],
                                max_and_argmax(adtens3, 2),
                                [adtens3_val], MaxAndArgmax)
        self._compile_and_check([adtens3],
                                max_and_argmax(adtens3, [0, 1, 2]),
                                [adtens3_val], MaxAndArgmax)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [ARange('int64')(aiscal, biscal, ciscal)],
                                [0, 5, 1], ARange)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [ARange('int64')(aiscal, biscal, ciscal)],
                                [2, 11, 4], ARange)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [ARange('int64')(aiscal, biscal, ciscal)],
                                [-5, 1, 1], ARange)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [ARange('int64')(aiscal, biscal, ciscal)],
                                [10, 2, -2], ARange)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [ARange('int64')(aiscal, biscal, ciscal)],
                                [10, 2, 2], ARange)
        self._compile_and_check([aiscal, biscal, ciscal],
                                [ARange('int64')(aiscal, biscal, ciscal)],
                                [0, 0, 1], ARange)
        aivec_val = [3, 4, 2, 5]
        adtens4_val = rand(*aivec_val)
        self._compile_and_check([adtens4, aivec],
                                [SpecifyShape()(adtens4, aivec)],
                                [adtens4_val, aivec_val], SpecifyShape)
        adtens3_val = rand(3, 4, 5)
        aiscal_val = 2
        self._compile_and_check([adtens3],
                                [Mean(None)(adtens3)],
                                [adtens3_val], Mean)
        self._compile_and_check([adtens3],
                                [Mean(aiscal_val)(adtens3)],
                                [adtens3_val], Mean)
        admat = dmatrix()
        aivec = ivector()
        ndim = 1
        admat_val = rand(3, 4)
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [12])],
                                [admat_val], Reshape)
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [-1])],
                                [admat_val], Reshape)
        ndim = 2
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [4, 3])],
                                [admat_val], Reshape)
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [4, -1])],
                                [admat_val], Reshape)
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [3, -1])],
                                [admat_val], Reshape)
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [-1, 3])],
                                [admat_val], Reshape)
        self._compile_and_check([admat],
                                [Reshape(ndim)(admat, [-1, 4])],
                                [admat_val], Reshape)
        adtens4 = dtensor4()
        ndim = 4
        adtens4_val = rand(2, 4, 3, 5)
        self._compile_and_check([adtens4],
                                [Reshape(ndim)(adtens4, [1, -1, 10, 4])],
                                [adtens4_val], Reshape)
        self._compile_and_check([adtens4],
                                [Reshape(ndim)(adtens4, [1, 3, 10, 4])],
                                [adtens4_val], Reshape)
        advec = dvector()
        advec_val = rand(5)
        aivec_val = [3]
        ndim = 1
        self._compile_and_check([advec],
                                [Tile(ndim)(advec, aivec_val)],
                                [advec_val], Tile)
        admat = dmatrix()
        admat_val = rand(2, 4)
        aivec_val = [2, 3]
        ndim = 2
        self._compile_and_check([admat],
                                [Tile(ndim)(admat, aivec_val)],
                                [admat_val], Tile)
        adtens4 = dtensor4()
        adtens4_val = rand(2, 4, 3, 5)
        aivec_val = [2, 3, 1, 4]
        ndim = 4
        self._compile_and_check([adtens4],
                                [Tile(ndim)(adtens4, aivec_val)],
                                [adtens4_val], Tile)
class TestTensorInstanceMethods(unittest.TestCase):
    def setUp(self):
        self.vars = matrices('X', 'Y')
        self.vals = [m.astype(floatX) for m in [rand(2, 2), rand(2, 2)]]
    def test_argmin(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.argmin().eval({X: x}), x.argmin())
    def test_argmax(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.argmax().eval({X: x}), x.argmax())
    def test_argsort(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.argsort().eval({X: x}), x.argsort())
        assert_array_equal(X.argsort(1).eval({X: x}), x.argsort(1))
    def test_clip(self):
        X, Y = self.vars
        x, y = self.vals
        Z = X.clip(Y - 0.5, Y + 0.5)
        z = x.clip(y - 0.5, y + 0.5)
        assert_array_equal(Z.eval({X: x, Y: y}), z)
    def test_dot(self):
        X, Y = self.vars
        x, y = self.vals
        assert_allclose(x.dot(y), X.dot(Y).eval({X: x, Y: y}))
        Z = X.dot(Y)
        z = x.dot(y)
        assert_allclose(x.dot(z), X.dot(Z).eval({X: x, Z: z}))
    def test_real_imag(self):
        X, Y = self.vars
        x, y = self.vals
        Z = X + Y * 1j
        z = x + y * 1j
        assert_array_equal(Z.real.eval({Z: z}), x)
        assert_array_equal(Z.imag.eval({Z: z}), y)
    def test_conj(self):
        X, Y = self.vars
        x, y = self.vals
        Z = X + Y * 1j
        z = x + y * 1j
        assert_array_equal(Z.conj().eval({Z: z}), z.conj())
        assert_array_equal(Z.conjugate().eval({Z: z}), z.conj())
    def test_round(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.round().eval({X: x}), x.round())
    def test_std(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_allclose(X.std().eval({X: x}), x.std())
    def test_repeat(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.repeat(2).eval({X: x}), x.repeat(2))
    def test_trace(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.trace().eval({X: x}), x.trace())
    def test_ravel(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.ravel().eval({X: x}), x.ravel())
    def test_diagonal(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.diagonal().eval({X: x}), x.diagonal())
        assert_array_equal(X.diagonal(1).eval({X: x}), x.diagonal(1))
        assert_array_equal(X.diagonal(-1).eval({X: x}), x.diagonal(-1))
        for offset, axis1, axis2 in [(1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0)]:
            assert_array_equal(X.diagonal(offset, axis1, axis2).eval({X: x}),
                               x.diagonal(offset, axis1, axis2))
    def test_take(self):
        X, _ = self.vars
        x, _ = self.vals
        indices = [1, 0, 3]
        assert_array_equal(X.take(indices).eval({X: x}), x.take(indices))
        indices = [1, 0, 1]
        assert_array_equal(X.take(indices, 1).eval({X: x}), x.take(indices, 1))
        indices = np.array([-10, 5, 12], dtype='int32')
        assert_array_equal(X.take(indices, 1, mode='wrap').eval({X: x}),
                           x.take(indices, 1, mode='wrap'))
        assert_array_equal(X.take(indices, -1, mode='wrap').eval({X: x}),
                           x.take(indices, -1, mode='wrap'))
        assert_array_equal(X.take(indices, 1, mode='clip').eval({X: x}),
                           x.take(indices, 1, mode='clip'))
        assert_array_equal(X.take(indices, -1, mode='clip').eval({X: x}),
                           x.take(indices, -1, mode='clip'))
        self.assertRaises(IndexError, X.take(indices).eval, {X: x})
        self.assertRaises(IndexError, (2 * X.take(indices)).eval, {X: x})
        self.assertRaises(TypeError, X.take, [0.0])
        indices = [[1, 0, 1], [0, 1, 1]]
        assert_array_equal(X.take(indices, 1).eval({X: x}), x.take(indices, 1))
        assert_array_equal(X[:, indices].eval({X: x}), x[:, indices])
    def test_cumsum(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.cumsum().eval({X: x}), x.cumsum())
    def test_cumprod(self):
        X, _ = self.vars
        x, _ = self.vals
        assert_array_equal(X.cumprod().eval({X: x}), x.cumprod())
def test_norm():
    x = theano.tensor.vector('x')
    n = x.norm(2)
    f = theano.function([x], n)
    assert np.allclose(f([1, 1]), np.sqrt(2))
class test_cov(unittest.TestCase):
    def test_core(self):
        x = theano.tensor.matrix('x')
        c = theano.tensor.cov(x)
        f = theano.function([x], c)
        data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
        assert np.allclose(f(data), np.cov(data))
        data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
        assert np.allclose(f(data), np.cov(data))
        data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
        assert np.allclose(f(data), np.cov(data))
        data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
        assert np.allclose(f(data), np.cov(data))
        data = np.asarray(np.random.rand(1, 2), dtype=config.floatX)
        assert np.allclose(f(data), np.cov(data))
    def test_rowvar(self):
        for rowvar in [True, False]:
            x = theano.tensor.matrix('x')
            c = theano.tensor.cov(x, rowvar=rowvar)
            f = theano.function([x], c)
            data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
            assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
            data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
            assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
            data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
            assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
            data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
            assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
        x = theano.tensor.matrix('x')
        c = theano.tensor.cov(x, rowvar=False)
        f = theano.function([x], c)
        data = np.asarray(np.random.rand(2, 1), dtype=config.floatX)
        assert np.allclose(f(data), np.cov(data, rowvar=False))
<a name="16"></a>
    def test_y(self):
        x <font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.matrix('x')
        y = theano.tensor.matrix('y')
        c = theano.tensor.cov(x, y=</b></font>y)
        f = theano.function([x, y], c)
        data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
        y = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
        assert np.allclose(f(data, y), np.cov(data, y=y))
        data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
        y = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
        assert np.allclose(f(data, y), np.cov(data, y=y))
        data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
        y = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
        assert np.allclose(f(data, y), np.cov(data, y=y))
        data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
        y = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
        assert np.allclose(f(data, y), np.cov(data, y=y))
    def test_ddof(self):
        for ddof in range(0, 5):
            x = theano.tensor.matrix('x')
            c = theano.tensor.cov(x, ddof=ddof)
            f = theano.function([x], c)
            data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
            assert np.allclose(f(data), np.cov(data, ddof=ddof))
    def test_bias(self):
        for bias in [True, False]:
            x = theano.tensor.matrix('x')
            c = theano.tensor.cov(x, bias=bias)
            f = theano.function([x], c)
            data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
            assert np.allclose(f(data), np.cov(data, bias=bias))
        for ddof in range(0, 5):
            for bias in [True, False]:
                x = theano.tensor.matrix('x')
                c = theano.tensor.cov(x, ddof=ddof, bias=bias)
                f = theano.function([x], c)
                data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
                assert np.allclose(f(data), np.cov(data, ddof=ddof, bias=bias))
class test_ptp(unittest.TestCase):
    def test_scalar(self):
        x = scalar('x')
        p = ptp(x)
        f = theano.function([x], p)
        y = np.asarray(rand() * 2000 - 1000, dtype=config.floatX)
        result = f(y)
        numpyResult = np.ptp(y)
        self.assertTrue(np.array_equal(result, numpyResult))
    def test_vector(self):
        x = vector('x')
        p = ptp(x, 0)
        f = theano.function([x], p)
        y = rand_ranged(-1000, 1000, [100])
        result = f(y)
        numpyResult = np.ptp(y, 0)
        self.assertTrue(np.array_equal(result, numpyResult))
    def test_matrix_first_axis(self):
        x = matrix('x')
        p = ptp(x, 1)
        f = theano.function([x], p)
        y = rand_ranged(-1000, 1000, [100, 100])
        result = f(y)
        numpyResult = np.ptp(y, 1)
        self.assertTrue(np.array_equal(result, numpyResult))
    def test_matrix_second_axis(self):
        x = matrix('x')
        p = ptp(x, 0)
        f = theano.function([x], p)
        y = rand_ranged(-1000, 1000, [100, 100])
        result = f(y)
        numpyResult = np.ptp(y, 0)
        self.assertTrue(np.array_equal(result, numpyResult))
    def test_matrix_neg_axis(self):
        x = matrix('x')
        p = ptp(x, -1)
        f = theano.function([x], p)
        y = rand_ranged(-1000, 1000, [100, 100])
        result = f(y)
        numpyResult = np.ptp(y, -1)
        self.assertTrue(np.array_equal(result, numpyResult))
    def test_matrix_no_axis(self):
        x = matrix('x')
        p = ptp(x)
        f = theano.function([x], p)
        y = rand_ranged(-1000, 1000, [100, 100])
        result = f(y)
        numpyResult = np.ptp(y)
        self.assertTrue(np.array_equal(result, numpyResult))
    def test_interface(self):
        x = matrix('x')
        p = x.ptp(1)
        f = theano.function([x], p)
        y = rand_ranged(-1000, 1000, [100, 100])
        result = f(y)
        numpyResult = np.ptp(y, 1)
        self.assertTrue(np.array_equal(result, numpyResult))
if __name__ == '__main__':
    t = TestInferShape('setUp')
    t.setUp()
    t.test_infer_shape()
class T_swapaxes(unittest.TestCase):
    def test_no_dimensional_input(self):
        self.assertRaises(IndexError, swapaxes, 2, 0, 1)
    def test_unidimensional_input(self):
        self.assertRaises(IndexError, swapaxes, [2, 1], 0, 1)
    def test_not_enough_dimension(self):
        self.assertRaises(IndexError, swapaxes, [[2, 1], [3, 4]], 3, 4)
    def test_doubleswap(self):
        y = matrix()
        n = swapaxes(y, 0, 1)
        f = function([y], n)
        testMatrix = [[2, 1], [3, 4]]
        self.assertTrue(np.array_equal(testMatrix, f(f(testMatrix))))
    def test_interface(self):
        x = theano.tensor.matrix()
        x.swapaxes(0, 1)
    def test_numpy_compare(self):
        rng = np.random.RandomState(utt.fetch_seed())
        A = tensor.matrix("A", dtype=theano.config.floatX)
        Q = swapaxes(A, 0, 1)
        fn = function([A], [Q])
        a = rng.rand(4, 4).astype(theano.config.floatX)
        n_s = np.swapaxes(a, 0, 1)
        t_s = fn(a)
        assert np.allclose(n_s, t_s)
class T_Power(unittest.TestCase):
    def test_numpy_compare(self):
        rng = np.random.RandomState(utt.fetch_seed())
        A = tensor.matrix("A", dtype=theano.config.floatX)
        Q = power(A, 3)
        fn = function([A], [Q])
        a = rng.rand(4, 4).astype(theano.config.floatX)
        n_p = np.power(a, 3)
        t_p = fn(a)
        assert np.allclose(n_p, t_p)
    def test_multiple_power(self):
        x = tensor.vector()
        y = [1, 2, 3]
        z = power(x, y)
        f = function([x], z)
        assert np.allclose(f([1, 2, 3]), [1, 4, 27])
    def test_wrong_shape(self):
        x = tensor.vector()
        y = [1, 2, 3]
        z = power(x, y)
        f = function([x], z)
        self.assertRaises(ValueError, f, [1, 2, 3, 4])
class T_Choose(utt.InferShapeTester):
    op = staticmethod(choose)
    op_class = Choose
    modes = ['raise', 'wrap', 'clip']
    def test_numpy_compare(self):
        a = tensor.vector(dtype='int32')
        b = tensor.matrix(dtype='float32')
        A = np.random.randint(0, 4, 4).astype('int32')
        B = np.asarray(np.random.rand(4, 4), dtype='float32')
        for m in self.modes:
            f = function([a, b], choose(a, b, mode=m))
            t_c = f(A, B)
            n_c = np.choose(A, B, mode=m)
            assert np.allclose(t_c, n_c)
    def test_method(self):
        a = tensor.vector(dtype='int32')
        b = tensor.matrix(dtype='float32')
        A = np.random.randint(0, 4, 4).astype('int32')
        B = np.asarray(np.random.rand(4, 4), dtype='float32')
        for m in self.modes:
            f = function([a, b], a.choose(b, mode=m))
            t_c = f(A, B)
            n_c = A.choose(B, mode=m)
            assert np.allclose(t_c, n_c)
    def test_broadcasted(self):
        a = tensor.scalar(dtype='int32')
        b = tensor.matrix(dtype='float32')
        A = 3
        B = np.asarray(np.random.rand(4, 4), dtype='float32')
        for m in self.modes:
            f = function([a, b], choose(a, b, mode=m))
            t_c = f(A, B)
            n_c = np.choose(A, B, mode=m)
            assert np.allclose(t_c, n_c)
        b = theano.tensor.col(dtype='float32')
        B = np.asarray(np.random.rand(4, 1), dtype='float32')
        for m in self.modes:
            f = function([a, b], choose(a, b, mode=m))
            assert choose(a, b, mode=m).broadcastable[0]
            t_c = f(A, B)
            n_c = np.choose(A, B, mode=m)
            assert np.allclose(t_c, n_c)
    def test_dtype_error(self):
        a = tensor.scalar(dtype='float32')
        b = tensor.matrix(dtype='float32')
        self.assertRaises(TypeError, choose, a, b)
    def test_numpy_compare_tuple(self):
        a = tensor.tensor3(dtype='int32')
        b = tensor.tensor3(dtype='float32')
        c = tensor.tensor3(dtype='float32')
        A = np.random.randint(0, 2, (2, 1, 1)).astype('int32')
        B = np.asarray(np.random.rand(1, 6, 1), dtype='float32')
        C = np.asarray(np.random.rand(1, 1, 5), dtype='float32')
        for m in self.modes:
            f = function([a, b, c], choose(a, (b, c), mode=m))
            t_c = f(A, B, C)
            n_c = np.choose(A, (B, C), mode=m)
            assert np.allclose(t_c, n_c)
    def test_infer_shape(self):
        for shp1, shp2 in [
            ((5, 4), (7, 4)),
            ((1, 4), (7, 4)),
            ((5, 1), (7, 4)),
            ((5, 4), (1, 4)),
            ((5, 4), (7, 1)),
            ((5, 4), (4,)),
            ((1, 4), (4,)),
            ((5, 1), (4,)),
            ((5, 4), (1,)),
            ((4,), (5, 4)),
            ((1,), (5, 4)),
            ((4,), (1, 4)),
            ((4,), (3, 1)),
            ((4,), (4,)),
            ((1,), (4,)),
            ((4,), (1,)),
            ((1,), (1,)),
        ]:
            a = tensor.tensor(dtype='int32',
                              broadcastable=[n == 1 for n in shp1])
            c = tensor.tensor(dtype='float32',
                              broadcastable=[n == 1 for n in shp2])
            A = np.asarray(np.random.rand(*shp1) * shp2[0], dtype='int32')
            C = np.asarray(np.random.rand(*shp2) * shp2[0], dtype='float32')
            self._compile_and_check([a, c],  # theano.function inputs
                                    [self.op(a, c)],  # theano.function outputs
                                    [A, C],
                                    self.op_class)
    def ___test_infer_shape_tuple(self):
        a <font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= tensor.tensor3(dtype='int32')
        b = tensor.tensor3(dtype='int32')
        c = tensor.tensor3(dtype='int32')
        A = np.asarray(</b></font>[1, 0], dtype='int32').reshape((2, 1, 1))
        B = np.asarray(np.random.rand(1, 4, 1), dtype='int32')
        C = np.asarray(np.random.rand(1, 1, 7), dtype='int32')
        f = function([a, b, c], choose(a, (b, c)))
        shape = (2, 4, 7)
        assert np.allclose(f(A, B, C).shape, shape)
        self._compile_and_check([a, b, c],  # theano.function inputs
                                [self.op(a, (b, c))],  # theano.function outputs
                                [A, B, C],
                                self.op_class)
def test_allocempty():
    f = theano.function([], AllocEmpty("float32")(2, 3))
    assert len(f.maker.fgraph.apply_nodes) == 1
    out = f()
    assert out.shape == (2, 3)
    assert out.dtype == 'float32'
def test_symbolic_slice():
    x = theano.tensor.tensor4('x')
    a, b = x.shape[:2]
    output = a.eval({x: np.zeros((5, 4, 3, 2), dtype=theano.config.floatX)})
    assert output == np.array(5)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
