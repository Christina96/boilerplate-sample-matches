<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for dnn.py &amp; test_basic_3.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for dnn.py &amp; test_basic_3.py
      </h3>
<h1 align="center">
        3.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>dnn.py (7.747804%)<th>test_basic_3.py (2.481913%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(2214-2222)<td><a href="#" name="0">(5195-5203)</a><td align="center"><font color="#ff0000">41</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(5-56)<td><a href="#" name="1">(1-62)</a><td align="center"><font color="#c00000">31</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(2223-2227)<td><a href="#" name="2">(5203-5207)</a><td align="center"><font color="#950000">24</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(2893-2900)<td><a href="#" name="3">(1048-1053)</a><td align="center"><font color="#820000">21</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(4015-4019)<td><a href="#" name="4">(4547-4550)</a><td align="center"><font color="#690000">17</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(3285-3290)<td><a href="#" name="5">(1724-1729)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1470-1472)<td><a href="#" name="6">(3032-3037)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(1399-1401)<td><a href="#" name="7">(878-881)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(3237-3240)<td><a href="#" name="8">(6525-6532)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(3228-3231)<td><a href="#" name="9">(6508-6515)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(1973-1976)<td><a href="#" name="10">(8761-8765)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(1943-1950)<td><a href="#" name="11">(6141-6144)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(3391-3396)<td><a href="#" name="12">(1730-1735)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1999-2005)<td><a href="#" name="13">(8089-8095)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(1231-1235)<td><a href="#" name="14">(5036-5043)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(4049-4053)<td><a href="#" name="15">(5979-5981)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(4023-4026)<td><a href="#" name="16">(8443-8445)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(3814-3815)<td><a href="#" name="17">(5279-5281)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(3421-3426)<td><a href="#" name="18">(7957-7964)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(3316-3321)<td><a href="#" name="19">(5022-5028)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(3179-3182)<td><a href="#" name="20">(7224-7229)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(4039-4042)<td><a href="#" name="21">(1116-1129)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(3858-3859)<td><a href="#" name="22">(5514-5516)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(3712-3715)<td><a href="#" name="23">(5293-5294)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(3188-3190)<td><a href="#" name="24">(3202-3204)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(3133-3135)<td><a href="#" name="25">(5016-5017)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#68818b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#68818b"><font color="#68818b">-</font><td><a href="#" name="26">(3076-3083)<td><a href="#" name="26">(6299-6306)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#e77471")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#e77471"><font color="#e77471">-</font><td><a href="#" name="27">(2798-2801)<td><a href="#" name="27">(1920-1922)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#717d7d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#717d7d"><font color="#717d7d">-</font><td><a href="#" name="28">(762-764)<td><a href="#" name="28">(2222-2225)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#af7a82")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#af7a82"><font color="#af7a82">-</font><td><a href="#" name="29">(720-727)<td><a href="#" name="29">(2085-2097)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#ae694a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ae694a"><font color="#ae694a">-</font><td><a href="#" name="30">(659-661)<td><a href="#" name="30">(687-692)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#3ea99f")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3ea99f"><font color="#3ea99f">-</font><td><a href="#" name="31">(579-586)<td><a href="#" name="31">(2017-2029)</a><td align="center"><font color="#4a0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>dnn.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import os
3 import sys
4 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import warnings
5 import numpy as np
6 from six import integer_types
7 from six.moves import reduce
8 import theano
9 from theano import Op, Apply, tensor, config, Variable
10 from theano.scalar import (as_scalar, constant, Log, get_scalar_type,
11                            int32 as int_t, bool as bool_t, uint32 as uint32_t)
12 from theano.tensor import as_tensor_variable, Argmax
13 from theano.tensor.extra_ops import cpu_contiguous
14 from theano.gradient import DisconnectedType, grad_not_implemented
15 from theano.gof import Optimizer, local_optimizer, COp, ParamsType, EnumList
16 from theano.gof.cmodule import GCC_compiler
17 from theano.gof.type import CDataType, Generic
18 from theano.gof.opt import inherit_stack_trace
19 from theano.tensor.opt import Assert
20 from theano.compile import optdb
21 from theano.compile.ops import shape_i, shape_i_op
22 from theano.tensor.nnet import LogSoftmax, SoftmaxGrad
23 from theano.tensor.nnet.abstract_conv import (AbstractConv2d,
24                                               AbstractConv2d_gradWeights,
25                                               AbstractConv2d_gradInputs,
26                                               AbstractConv3d,
27                                               AbstractConv3d_gradWeights,
28                                               AbstractConv3d_gradInputs,
29                                               get_conv_output_shape,
30                                               assert_conv_shape)
31 from theano.tensor.signal.pool import (
32     Pool, MaxPoolGrad, AveragePoolGrad)
33 from . import pygpu, cudnn_defs
34 from .type import (get_context, gpu_context_type, list_contexts,
35                    GpuArraySharedVariable)
36 from .basic_ops import (as_gpuarray_variable, infer_context_name, gpuarray_helper_inc_dir,
37                         gpu_contiguous, GpuAllocEmpty,
38                         empty_like, GpuArrayType, HostFromGpu)
39 from .elemwise import GpuElemwise, GpuCAReduceCuda
40 from .reduction import GpuMaxAndArgmax
41 from .nnet import GpuSoftmax
42 from .opt import (gpu_seqopt, register_opt, pool_db, pool_db2,
43                   op_lifter, register_opt2, register_inplace)
44 from .opt_util import alpha_merge, output_merge, inplace_allocempty, pad_dims, unpad_dims
45 from theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME
46 import theano.pathparse
47 DNN_CONV_ALGO_CHOOSE_ONCE =</b></font> ['guess_once', 'time_once']
48 DNN_CONV_ALGO_CHOOSE_TIME = ['time_once', 'time_on_shape_change']
49 try:
50     from pygpu import gpuarray
51 except ImportError:
52     pass
53 WIN32_CUDNN_NAMES = ['cudnn64_7.dll', 'cudnn64_6.dll', 'cudnn64_5.dll']
54 if sys.platform == 'win32':
55     theano.pathparse.PathParser(theano.config.dnn.bin_path)
56 def _load_lib(name):
57     try:
58         return ctypes.cdll.LoadLibrary(name)
59     except OSError:
60         return None
61 def _dnn_lib():
62     if _dnn_lib.handle is None:
63         import ctypes.util
64         if config.dnn.bin_path != "":
65             if sys.platform == 'darwin':
66                 dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, 'libcudnn.dylib'))
67             elif sys.platform == 'win32':
68                 for name in WIN32_CUDNN_NAMES:
69                     dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, name))
70                     if dnn_handle is not None:
71                         break
72             else:
73                 dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, 'libcudnn.so'))
74         else:
75             lib_name = ctypes.util.find_library('cudnn')
76             if lib_name is None and sys.platform == 'win32':
77                 for name in WIN32_CUDNN_NAMES:
78                     lib_name = ctypes.util.find_library(name)
79                     if lib_name:
80                         break
81             if lib_name is None:
82                 raise RuntimeError(
83                     'Could not find cudnn library (looked for v5* to v7*).'
84                     ' Check your cudnn installation. Maybe using the Theano'
85                     ' flag dnn.base_path can help you. Current value "%s"' %
86                     config.dnn.base_path)
87             else:
88                 dnn_handle = ctypes.cdll.LoadLibrary(lib_name)
89         if dnn_handle is None:
90             raise RuntimeError('Could not load cudnn library. Check your cudnn'
91                                ' installation. Maybe using the Theano'
92                                ' flag dnn.base_path can help you. Current value "%s"' %
93                                config.dnn.base_path)
94         _dnn_lib.handle = dnn_handle
95         cudnn = _dnn_lib.handle
96         cudnn.cudnnCreate.argtypes = [ctypes.POINTER(ctypes.c_void_p)]
97         cudnn.cudnnCreate.restype = ctypes.c_int
98         cudnn.cudnnDestroy.argtypes = [ctypes.c_void_p]
99         cudnn.cudnnDestroy.restype = ctypes.c_int
100     return _dnn_lib.handle
101 _dnn_lib.handle = None
102 def _make_handle(ctx):
103     cudnn = _dnn_lib()
104     handle = ctypes.c_void_p()
105     with ctx:
106         err = cudnn.cudnnCreate(ctypes.byref(handle))
107     if err != 0:
108         raise RuntimeError("Error creating cudnn handle. "
109                            "This can be a sign of a too old driver.", err)
110     return handle
111 def _dnn_check_compile():
112     preambule = """
113     path_wrapper = "\"" if os.name == 'nt' else ""
114     params = ["-l", "cudnn"]
115     params.extend(['-I%s%s%s' % (path_wrapper, gpuarray_helper_inc_dir(), path_wrapper)])
116     if config.dnn.include_path:
117         params.extend(['-I%s%s%s' % (path_wrapper, config.dnn.include_path, path_wrapper)])
118     if config.cuda.include_path:
119         params.extend(['-I%s%s%s' % (path_wrapper, config.cuda.include_path, path_wrapper)])
120     if config.dnn.library_path:
121         params.extend(['-L%s%s%s' % (path_wrapper, config.dnn.library_path, path_wrapper)])
122     compiler_res = GCC_compiler.try_flags(
123         params, preambule=preambule, body=body,
124         try_run=False, output=True)
125     avail, out, err = compiler_res if isinstance(compiler_res, tuple) else (compiler_res, None, None)
126     if not avail:
127         return False, ("cannot compile with cuDNN. "
128                        "We got this error:\n" + str(err))
129     return True, None
130 def _dnn_check_version():
131     v = version()
132     if v &lt; 5000:
133         return False, "cuDNN version is too old. Update to v5* or higher, was %d." % v
134     if v &gt;= 7200:
135         warnings.warn("Your cuDNN version is more recent than "
136                       "Theano. If you encounter problems, try "
137                       "updating Theano or downgrading cuDNN to "
138                       "a version &gt;= v5 and &lt;= v7.")
139     return True, None
140 def dnn_present():
141     if dnn_present.avail is not None:
142         return dnn_present.avail
143     if config.dnn.enabled == "False":
144         dnn_present.msg = "Disabled by dnn.enabled flag"
145         dnn_present.avail = False
146         return False
147     if pygpu is None:
148         dnn_present.msg = "PyGPU not available"
149         dnn_present.avail = False
150         return False
151     if config.dnn.enabled == "no_check":
152         dnn_present.avail, dnn_present.msg = True, "presence check disabled by dnn.enabled flag"
153     else:
154         dnn_present.avail, dnn_present.msg = _dnn_check_compile()
155     if dnn_present.avail:
156         dnn_present.avail, dnn_present.msg = _dnn_check_version()
157         if not dnn_present.avail:
158             return False
159     return dnn_present.avail
160 dnn_present.avail = None
161 dnn_present.msg = None
162 def dnn_available(context_name):
163     if not dnn_present():
164         dnn_available.msg = dnn_present.msg
165         return False
166     ctx = get_context(context_name)
167     if not ctx.kind == b'cuda':
168         dnn_available.msg = "Not on a CUDA device."
169         return False
170     if int(ctx.bin_id[-2:]) &lt; 30:
171         dnn_available.msg = "Device not supported"
172         return False
173     if version() &lt; 7002:
174         if int(ctx.bin_id[-2:]) &gt;= 70:
175             dnn_available.msg = "Use cuDNN 7.0.2 or higher for Volta."
176             return False
177     return True
178 dnn_available.msg = None
179 def CUDNNDataType(name, freefunc=None):
180     cargs = []
181     if config.dnn.bin_path and sys.platform != 'win32':
182         cargs.append('-Wl,-rpath,' + config.dnn.bin_path)
183     return CDataType(name, freefunc,
184                      headers=['cudnn.h'],
185                      header_dirs=[config.dnn.include_path,
186                                   config.cuda.include_path],
187                      libraries=['cudnn'],
188                      lib_dirs=[config.dnn.library_path],
189                      compile_args=cargs,
190                      version=version(raises=False))
191 class DnnVersion(Op):
192     __props__ = ()
193     def c_headers(self):
194         return ['cudnn.h']
195     def c_header_dirs(self):
196         return [config.dnn.include_path, config.cuda.include_path]
197     def c_libraries(self):
198         return ['cudnn']
199     def c_lib_dirs(self):
200         return [config.dnn.library_path]
201     def c_compile_args(self):
202         if config.dnn.bin_path and sys.platform != 'win32':
203             return ['-Wl,-rpath,' + config.dnn.bin_path]
204         return []
205     def c_support_code(self):
206         return """
207     def do_constant_folding(self, node):
208         return False
209     def c_code_cache_version(self):
210         return None
211 def version(raises=True):
212     if not dnn_present():
213         if raises:
214             raise RuntimeError(
215                 "We can't determine the cudnn version as it is not available",
216                 dnn_available.msg)
217         else:
218             return -1
219     if version.v is None:
220         f = theano.function([], DnnVersion()(),
221                             theano.Mode(optimizer=None),
222                             profile=False)
223         v = f()
224         if v[0] != v[1]:
225             raise RuntimeError("Mixed dnn version. The header is version %s "
226                                "while the library is version %s." % v)
227         version.v = v[1]
228     return version.v
229 version.v = None
230 handle_type = CUDNNDataType('cudnnHandle_t', 'cudnnDestroy')
231 cudnn = cudnn_defs.get_definitions(version(raises=False))
232 def get_precision(precision, inputs, for_grad=False):
233     common_dtype = theano.scalar.upcast(*[i.dtype for i in inputs])
234     if not common_dtype.startswith('float'):
235         raise TypeError("cuDNN convolution only works on real numbers")
236     if precision is None:
237         precision = theano.config.dnn.conv.precision
238     if precision == 'as_input' or precision == 'as_input_f32':
239         if common_dtype == 'float16' and precision == 'as_input_f32':
240             precision = 'float32'
241         else:
242             precision = common_dtype
243     if for_grad and precision == 'float16':
244         raise TypeError("Float16 precision is disabled for cuDNN backward convolutions due to computation errors.")
245     return precision, common_dtype
246 class DnnBase(COp):
247     check_broadcast = False
248     params_type = handle_type
249     def dnn_context(self, node):
250         return node.outputs[0].type.context_name
251     def get_params(self, node):
252         ctx_name = self.dnn_context(node)
253         ctx = get_context(ctx_name)
254         if not hasattr(ctx, 'cudnn_handle_param'):
255             ptr = ctx.cudnn_handle.value
256             res = handle_type.make_value(ptr)
257             ctx.cudnn_handle_param = res
258         if isinstance(self.params_type, ParamsType):
259             if not self.params_type.has_type(handle_type):
260                 raise TypeError('DnnBase: params_type must take into account the cuDNN handle type.')
261             handle_field = self.params_type.get_field(handle_type)
262             return self.params_type.get_params(self, **{handle_field: ctx.cudnn_handle_param})
263         return ctx.cudnn_handle_param
264     def __init__(self, files=None, c_func=None):
265         if files is None:
266             files = []
267         COp.__init__(self, ["c_code/dnn_base.c"] + files, c_func)
268     def c_headers(self):
269         return ['gpuarray/types.h', 'gpuarray/array.h', 'gpuarray/kernel.h',
270                 'gpuarray/util.h', 'gpuarray/ext_cuda.h', 'gpuarray_api.h',
271                 'numpy_compat.h', 'cudnn.h', 'cudnn_helper.h',
272                 'gpuarray_helper.h']
273     def c_header_dirs(self):
274         return [gpuarray_helper_inc_dir(), pygpu.get_include(),
275                 config.dnn.include_path, config.cuda.include_path]
276     def c_libraries(self):
277         return ['cudnn', 'gpuarray']
278     def c_lib_dirs(self):
279         return [config.dnn.library_path]
280     def c_compile_args(self):
281         if config.dnn.bin_path and sys.platform != 'win32':
282             return ['-Wl,-rpath,' + config.dnn.bin_path]
283         return []
284     def c_code_cache_version(self):
285         return (super(DnnBase, self).c_code_cache_version(), version(), 4)
286 class GpuDnnConvDesc(COp):
287     __props__ = ('border_mode', 'subsample', 'dilation', 'conv_mode',
288                  'precision', 'num_groups')
289     params_type = ParamsType(pad0=int_t, pad1=int_t, pad2=int_t,
290                              sub0=int_t, sub1=int_t, sub2=int_t,
291                              dil0=int_t, dil1=int_t, dil2=int_t,
292                              nb_dims=int_t,
293                              bmode=EnumList(('BORDER_MODE_FULL', 'full'),
294                                             ('BORDER_MODE_VALID', 'valid'),
295                                             ('BORDER_MODE_HALF', 'half')),
296                              conv_mode=cudnn.cudnnConvolutionMode_t,
297                              precision=cudnn.cudnnDataType_t,
298                              num_groups=int_t)
299     def c_headers(self):
300         return ['cudnn.h', 'cudnn_helper.h']
301     def c_header_dirs(self):
302         return [gpuarray_helper_inc_dir(), config.dnn.include_path,
303                 config.cuda.include_path]
304     def c_libraries(self):
305         return ['cudnn']
306     def c_lib_dirs(self):
307         return [config.dnn.library_path]
308     def c_compile_args(self):
309         if config.dnn.bin_path and sys.platform != 'win32':
310             return ['-Wl,-rpath,' + config.dnn.bin_path]
311         return []
312     def do_constant_folding(self, node):
313         return False
314     def __init__(self, border_mode, subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
315                  precision="float32", num_groups=1):
316         COp.__init__(self, ["c_code/conv_desc.c"], "APPLY_SPECIFIC(conv_desc)")
317         if version() &lt; 6000 and any([d != 1 for d in dilation]):
318             raise RuntimeError("Dilation &gt; 1 not supported for cuDNN version &lt; 6.")
319         if isinstance(border_mode, integer_types):
320             border_mode = (border_mode,) * len(subsample)
321         if isinstance(border_mode, tuple):
322             assert len(border_mode) == len(subsample)
323             border_mode = tuple(map(int, border_mode))
324         if not ((isinstance(border_mode, tuple) and min(border_mode) &gt;= 0) or
325                 border_mode in ('valid', 'full', 'half')):
326             raise ValueError(
327                 'invalid border_mode {}, which must be either '
328                 '"valid", "full", "half", an integer or a pair of'
329                 ' integers'.format(border_mode))
330         self.border_mode = border_mode
331         assert len(subsample) in (2, 3)
332         self.subsample = subsample
333         assert cudnn.cudnnConvolutionMode_t.has_alias(conv_mode)
334         self.conv_mode = conv_mode
335         self.num_groups = num_groups
336         assert len(dilation) == len(subsample)
337         self.dilation = dilation
338         assert cudnn.cudnnDataType_t.has_alias(precision)
339         self.precision = precision
340     def make_node(self, kern_shape):
341         kern_shape = as_tensor_variable(kern_shape)
342         if kern_shape.type.ndim != 1 or kern_shape.dtype not in theano.tensor.basic.int_dtypes:
343             raise TypeError('kern must be an int64 1D shape tensor')
344         kern_shape = theano.tensor.basic.cast(kern_shape, 'int64')
345         node = Apply(self, [kern_shape],
346                      [CUDNNDataType("cudnnConvolutionDescriptor_t",
347                                     freefunc="cudnnDestroyConvolutionDescriptor")()])
348         out = node.outputs[0]
349         out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true
350         return node
351     bmode = property(lambda self: 'valid' if isinstance(self.border_mode, tuple) else self.border_mode)
352     pad0 = property(lambda self: self.border_mode[0] if isinstance(self.border_mode, tuple) else 0)
353     pad1 = property(lambda self: self.border_mode[1] if isinstance(self.border_mode, tuple) else 0)
354     pad2 = property(lambda self: self.border_mode[2] if (isinstance(self.border_mode, tuple) and
355                                                          len(self.border_mode) &gt; 2) else 0)
356     sub0 = property(lambda self: self.subsample[0])
357     sub1 = property(lambda self: self.subsample[1])
358     sub2 = property(lambda self: self.subsample[2] if len(self.subsample) &gt; 2 else 0)
359     dil0 = property(lambda self: self.dilation[0])
360     dil1 = property(lambda self: self.dilation[1])
361     dil2 = property(lambda self: self.dilation[2] if len(self.dilation) &gt; 2 else 0)
362     nb_dims = property(lambda self: len(self.subsample))
363     def c_code_cache_version(self):
364         return (super(GpuDnnConvDesc, self).c_code_cache_version(), version())
365     def __setstate__(self, d):
366         self.__dict__.update(d)
367         if not hasattr(self, "dilation"):
368             self.dilation = (1,) * len(self.subsample)
369         if not hasattr(self, "num_groups"):
370             self.num_groups = 1
371 _zero = constant(np.asarray(0.0, dtype='float64'))
372 _one = constant(np.asarray(1.0, dtype='float64'))
373 def ensure_dt(val, default, name, dtype):
374     if dtype == 'float16':
375         dtype = 'float32'
376     if val is None:
377         val = default.clone()
378     if not isinstance(val, Variable):
379         val = constant(val)
380     if hasattr(val, 'ndim') and val.ndim == 0:
381         val = as_scalar(val)
382     if not isinstance(val.type, theano.scalar.Scalar):
383         raise TypeError("%s: expected a scalar value" % (name,))
384     if not val.type.dtype == dtype:
385         val = val.astype(dtype)
386     return val
387 class GpuDnnConv(DnnBase):
388     __props__ = ('algo', 'inplace', 'num_groups')
389     check_input <font color="#3ea99f"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= False
390     params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionFwdAlgo_t,
391                              choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
392                              inplace=bool_t,
393                              handle=handle_type,
394                              num_groups=int_t)
395     def</b></font> __init__(self, algo=None, inplace=False, num_groups=1):
396         DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_fwd.c"],
397                          "APPLY_SPECIFIC(conv_fwd)")
398         if algo is None:
399             algo = config.dnn.conv.algo_fwd
400         self.algo = algo
401         self.inplace = bool(inplace)
402         if self.inplace:
403             self.destroy_map = {0: [2]}
404         assert cudnn.cudnnConvolutionFwdAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
405         self.conv_algo = cudnn.cudnnConvolutionFwdAlgo_t.CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
406         if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
407             self.conv_algo = self.algo
408         self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
409         self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
410         self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
411         self.num_groups = num_groups
412     def __setstate__(self, d):
413         self.__dict__.update(d)
414         if not hasattr(self, 'algo'):
415             if hasattr(self, 'workmem'):
416                 self.algo = self.workmem
417             else:
418                 self.algo = config.dnn.conv.algo_fwd
419         if not hasattr(self, 'inplace'):
420             self.inplace = False
421         if not hasattr(self, 'num_groups'):
422             self.num_groups = 1
423     def make_node(self, img, kern, output, desc, alpha=None, beta=None):
424         ctx_name = infer_context_name(img, kern, output)
425         img = as_gpuarray_variable(img, ctx_name)
426         kern = as_gpuarray_variable(kern, ctx_name)
427         output = as_gpuarray_variable(output, ctx_name)
428         if img.type.ndim not in (4, 5):
429             raise TypeError('img must be 4D or 5D tensor')
430         if kern.type.ndim not in (4, 5):
431             raise TypeError('kern must be 4D or 5D tensor')
432         if output.type.ndim not in (4, 5):
433             raise TypeError('output must be a 4D or 5D tensor')
434         if (img.type.ndim != kern.type.ndim or
435                 img.type.ndim != output.type.ndim):
436             raise TypeError("The number of dimensions of "
437                             "img, kern and output must match")
438         if img.type.ndim == 5 and self.algo not in (cudnn.conv3d_fwd_algorithms +
439                                                     SUPPORTED_DNN_CONV_ALGO_RUNTIME):
440             raise ValueError("convolution algo %s can't be used for "
441                              "3d convolutions", (self.algo,))
442         if (not isinstance(desc.type, CDataType) or
443                 desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
444             raise TypeError('desc must be cudnnConvolutionDescriptor_t')
445         alpha = ensure_dt(alpha, _one, 'alpha', img.dtype)
446         beta = ensure_dt(beta, _zero, 'beta', img.dtype)
447         return Apply(self, [img, kern, output, desc, alpha, beta],
448                      [output.type()])
449     def grad(self, inp, grads):
450         img, kerns, output, desc, alpha, beta = inp
451         top, = grads
452         top = gpu_contiguous(top)
453         d_img = GpuDnnConvGradI<font color="#ae694a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(num_groups=self.num_groups)(kerns, top, empty_like(img), desc)
454         d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(img, top, empty_like(kerns), desc)
455         d_alpha =</b></font> grad_not_implemented(self, 4, alpha)
456         d_beta = grad_not_implemented(self, 5, beta)
457         return [d_img * alpha, d_kerns * alpha, top * beta,
458                 DisconnectedType()(), d_alpha, d_beta]
459     def connection_pattern(self, node):
460         return [[1], [1], [1], [0], [1], [1]]
461     @staticmethod
462     def get_out_shape(ishape, kshape, border_mode, subsample, dilation):
463         if not isinstance(ishape, (list, tuple)):
464             ishape = [ishape[i] for i in range(len(subsample) + 2)]
465         if not isinstance(kshape, (list, tuple)):
466             kshape = [kshape[i] for i in range(len(subsample) + 2)]
467         return get_conv_output_shape(
468             ishape,
469             kshape,
470             border_mode,
471             subsample,
472             dilation)
473     def infer_shape(self, node, shape):
474         return [shape[2]]
475 class GpuDnnConvGradW(DnnBase):
476     __props__ = ('algo', 'inplace', 'num_groups')
477     check_input <font color="#af7a82"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= False
478     params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionBwdFilterAlgo_t,
479                              choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
480                              inplace=bool_t,
481                              handle=handle_type,
482                              num_groups=int_t)
483     def</b></font> __init__(self, inplace=False, algo=None, num_groups=1):
484         DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_gw.c"],
485                          "APPLY_SPECIFIC(conv_gw)")
486         self.inplace = bool(inplace)
487         if self.inplace:
488             self.destroy_map = {0: [2]}
489         if algo is None:
490             algo = config.dnn.conv.algo_bwd_filter
491         self.algo = algo
492         assert cudnn.cudnnConvolutionBwdFilterAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
493         self.conv_algo = cudnn.cudnnConvolutionBwdFilterAlgo_t.CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0
494         if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
495             self.conv_algo = self.algo
496         self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
497         self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
498         self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
499         self.num_groups = num_groups
500     def __setstate__(self, d):
501         self.__dict__.update(d)
502         if not hasattr(self, 'inplace'):
503             self.inplace = False
504         if not hasattr(self, 'algo'):
505             self.algo = config.dnn.conv.algo_bwd_filter
506         if not hasattr(self, 'num_groups'):
507             self.num_groups = 1
508     def grad(self, inp, grads):
509         img, top, output, desc, alpha, beta = inp
510         kerns, = grads
511         kerns = gpu_contiguous(kerns)
512         d_img = GpuDnnConvGradI<font color="#717d7d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(num_groups=self.num_groups)(kerns, top, empty_like(img), desc)
513         d_top = GpuDnnConv(num_groups=self.num_groups)(img, kerns, empty_like(top), desc)
514         d_alpha =</b></font> grad_not_implemented(self, 4, alpha)
515         d_beta = grad_not_implemented(self, 5, beta)
516         return (d_img * alpha, d_top * alpha, kerns * beta,
517                 DisconnectedType()(), d_alpha, d_beta)
518     def connection_pattern(self, node):
519         return [[1], [1], [1], [0], [1], [1]]
520     def op_may_fail_with_subsample(self, img, desc):
521         return (version() &lt; 6000 and
522                 img.type.dtype == 'float32' and
523                 img.type.ndim == 5 and
524                 self.algo != 'none' and
525                 desc.owner.op.subsample != (1, 1, 1))
526     def op_may_fail_with_beta(self, img, beta):
527         return (version() &lt; 6000 and
528                 img.type.dtype == 'float32' and
529                 self.algo not in ('none', 'deterministic', 'fft', 'small') and
530                 beta is not None and
531                 theano.tensor.extract_constant(beta) != 1)
532     def make_node(self, img, topgrad, output, desc, alpha=None, beta=None):
533         if self.op_may_fail_with_subsample(img, desc):
534             warnings.warn('cuDNN backward filter operation for 3D convolutions may produce bad results '
535                           'with certain cuDNN algorithms depending on the compute capability of your GPU '
536                           'if subsample is not (1, 1, 1). If you encounter problems, consider '
537                           'setting the theano flag "dnn.conv.algo_bwd_filter" to "none".')
538         if self.op_may_fail_with_beta(img, beta):
539             warnings.warn('cuDNN backward filter operation for convolutions may produce bad results '
540                           'with certain cuDNN algorithms depending on the compute capability of your GPU '
541                           'if beta != 1. If you encounter problems, consider '
542                           'setting the theano flag "dnn.conv.algo_bwd_filter" to '
543                           '"none", "deterministic", "fft", or "small".')
544         ctx_name = infer_context_name(img, topgrad, output)
545         img = as_gpuarray_variable(img, ctx_name)
546         topgrad = as_gpuarray_variable(topgrad, ctx_name)
547         output = as_gpuarray_variable(output, ctx_name)
548         if img.type.ndim not in (4, 5):
549             raise TypeError('img must be 4D or 5D tensor')
550         if topgrad.type.ndim not in (4, 5):
551             raise TypeError('topgrad must be 4D or 5D tensor')
552         if output.type.ndim not in (4, 5):
553             raise TypeError('output must be 4D or 5D tensor')
554         if (img.type.ndim != topgrad.type.ndim or
555                 img.type.ndim != output.type.ndim):
556             raise TypeError("The number of dimensions of "
557                             "img, topgrad and output must match")
558         if img.type.ndim == 5 and self.algo not in (cudnn.conv3d_bwd_filter_algorithms +
559                                                     SUPPORTED_DNN_CONV_ALGO_RUNTIME):
560             raise ValueError("convolution algo %s can't be used for "
561                              "3d convolutions", (self.algo,))
562         if (not isinstance(desc.type, CDataType) or
563                 desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
564             raise TypeError('desc must be cudnnConvolutionDescriptor_t')
565         alpha = ensure_dt(alpha, _one, 'alpha', img.dtype)
566         beta = ensure_dt(beta, _zero, 'beta', img.dtype)
567         return Apply(self, [img, topgrad, output, desc, alpha, beta],
568                      [output.type()])
569     def infer_shape(self, node, shape):
570         return [shape[2]]
571 class GpuDnnConvGradI(DnnBase):
572     _f16_ok = True
573     __props__ = ('algo', 'inplace', 'num_groups')
574     check_input = False
575     params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionBwdDataAlgo_t,
576                              choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
577                              inplace=bool_t,
578                              handle=handle_type,
579                              num_groups=int_t)
580     def __init__(self, inplace=False, algo=None, num_groups=1):
581         DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_gi.c"],
582                          "APPLY_SPECIFIC(conv_gi)")
583         self.inplace = bool(inplace)
584         if self.inplace:
585             self.destroy_map = {0: [2]}
586         if algo is None:
587             algo = config.dnn.conv.algo_bwd_data
588         self.algo = algo
589         assert cudnn.cudnnConvolutionBwdDataAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
590         self.conv_algo = cudnn.cudnnConvolutionBwdDataAlgo_t.CUDNN_CONVOLUTION_BWD_DATA_ALGO_0
591         if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
592             self.conv_algo = self.algo
593         self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
594         self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
595         self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
596         self.num_groups = num_groups
597     def __setstate__(self, d):
598         self.__dict__.update(d)
599         if not hasattr(self, 'algo'):
600             self.algo = config.dnn.conv.algo_bwd_data
601         if not hasattr(self, 'inplace'):
602             self.inplace = False
603         if not hasattr(self, 'num_groups'):
604             self.num_groups = 1
605     def grad(self, inp, grads):
606         kerns, top, output, desc, alpha, beta = inp
607         img, = grads
608         img = gpu_contiguous(img)
609         d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(img, top, empty_like(kerns), desc)
610         d_top = GpuDnnConv(num_groups=self.num_groups)(img, kerns, empty_like(top), desc)
611         d_alpha = grad_not_implemented(self, 4, alpha)
612         d_beta = grad_not_implemented(self, 5, beta)
613         return (d_kerns * alpha, d_top * alpha, img * beta,
614                 DisconnectedType()(), d_alpha, d_beta)
615     def connection_pattern(self, node):
616         return [[1], [1], [1], [0], [1], [1]]
617     def make_node(self, kern, topgrad, output, desc, alpha=None, beta=None):
618         ctx_name = infer_context_name(kern, topgrad, output)
619         kern = as_gpuarray_variable(kern, ctx_name)
620         topgrad = as_gpuarray_variable(topgrad, ctx_name)
621         output = as_gpuarray_variable(output, ctx_name)
622         if kern.type.ndim not in (4, 5):
623             raise TypeError('kern must be 4D or 5D tensor')
624         if topgrad.type.ndim not in (4, 5):
625             raise TypeError('topgrad must be 4D or 5D tensor')
626         if output.type.ndim not in (4, 5):
627             raise TypeError('output must be 4D or 5D tensor')
628         if (kern.type.ndim != topgrad.type.ndim or
629                 kern.type.ndim != output.type.ndim):
630             raise TypeError("The number of dimensions of "
631                             "kern, topgrad and output must match")
632         if kern.type.ndim == 5 and self.algo not in (cudnn.conv3d_bwd_data_algorithms +
633                                                      SUPPORTED_DNN_CONV_ALGO_RUNTIME):
634             raise ValueError("convolution algo %s can't be used for "
635                              "3d convolutions", (self.algo,))
636         if (not isinstance(desc.type, CDataType) or
637                 desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
638             raise TypeError('desc must be cudnnConvolutionDescriptor_t')
639         alpha = ensure_dt(alpha, _one, 'alpha', kern.dtype)
640         beta = ensure_dt(beta, _zero, 'beta', kern.dtype)
641         return Apply(self, [kern, topgrad, output, desc, alpha, beta],
642                      [output.type()])
643     def infer_shape(self, node, shape):
644         return [shape[2]]
645 def _dnn_conv(img, kerns, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1), dilation=(1, 1),
646               conv_mode='conv', algo=None, precision=None, num_groups=1):
647     ctx_name = infer_context_name(img, kerns)
648     img = as_gpuarray_variable(img, ctx_name)
649     kerns = as_gpuarray_variable(kerns, ctx_name)
650     precision, dt = get_precision(precision, [img, kerns])
651     img = gpu_contiguous(img.astype(dt))
652     kerns = gpu_contiguous(kerns.astype(dt))
653     desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
654                           conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns.shape)
655     desc_op = desc.owner.op
656     ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
657     kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
658     out_shp = get_conv_output_shape(ishape, kshape, desc_op.border_mode, desc_op.subsample, filter_dilation=dilation)
659     out_shp = assert_conv_shape(out_shp)
660     if beta == 0:
661         real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
662     else:
663         assert out is not None
664         out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
665         check = Assert('GpuDnnConv: given output (for beta not null) does not have expected shape')
666         real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, out_shp)))
667     return GpuDnnConv(algo=algo, num_groups=num_groups)(img, kerns, real_out, desc, alpha, beta)
668 def _dnn_gradweight(img, topgrad, kerns_shp, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1),
669                     dilation=(1, 1), conv_mode='conv', algo=None, precision=None, num_groups=1):
670     ctx_name = infer_context_name(img, topgrad)
671     img = as_gpuarray_variable(img, ctx_name)
672     topgrad = as_gpuarray_variable(topgrad, ctx_name)
673     kerns_shp = theano.tensor.as_tensor_variable(kerns_shp)
674     precision, dt = get_precision(precision, [img, topgrad], for_grad=True)
675     img = gpu_contiguous(img.astype(dt))
676     topgrad = gpu_contiguous(topgrad.astype(dt))
677     desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
678                           conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns_shp)
679     if beta == 0:
680         real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*kerns_shp)
681     else:
682         assert out is not None
683         out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
684         check = Assert('GpuDnnConvGradW: given output (for beta not null) does not have expected shape')
685         real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, kerns_shp)))
686     return GpuDnnConvGradW(algo=algo, num_groups=num_groups)(img, topgrad, real_out, desc, alpha, beta)
687 def _dnn_gradinput(kerns, topgrad, img_shp, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1),
688                    dilation=(1, 1), conv_mode='conv', algo=None, precision=None, num_groups=1):
689     ctx_name = infer_context_name(kerns, topgrad)
690     kerns = as_gpuarray_variable(kerns, ctx_name)
691     topgrad = as_gpuarray_variable(topgrad, ctx_name)
692     img_shp = theano.tensor.as_tensor_variable(img_shp)
693     precision, dt = get_precision(precision, [kerns, topgrad], for_grad=True)
694     kerns = gpu_contiguous(kerns.astype(dt))
695     topgrad = gpu_contiguous(topgrad.astype(dt))
696     desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
697                           conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns.shape)
698     if beta == 0:
699         real_out = GpuAllocEmpty(dtype=kerns.dtype, context_name=ctx_name)(*img_shp)
700     else:
701         assert out is not None
702         out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
703         check = Assert('GpuDnnConvGradI: given output (for beta not null) does not have expected shape')
704         real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, img_shp)))
705     return GpuDnnConvGradI(algo=algo, num_groups=num_groups)(kerns, topgrad, real_out, desc, alpha, beta)
706 def dnn_conv(img, kerns, border_mode='valid', subsample=(1, 1), dilation=(1, 1),
707              conv_mode='conv', direction_hint=None, workmem=None,
708              algo=None, precision=None, num_groups=1):
709     if workmem is not None:
710         if algo is not None:
711             raise ValueError("You can't use both algo and workmem")
712         warnings.warn("workmem is deprecated, use algo instead", stacklevel=2)
713         algo = workmem
714     fgraph = getattr(img, 'fgraph', None) or getattr(kerns, 'fgraph', None)
715     ctx_name = infer_context_name(img, kerns)
716     if (border_mode == 'valid' and subsample == (1, 1) and dilation == (1, 1) and
717             direction_hint == 'bprop weights' and num_groups == 1):
718         img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3))
719         if conv_mode == 'conv':
720             kerns = kerns[:, :, ::-1, ::-1]
721         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))
722         out_shp = (shape_i(kerns, 1, fgraph),
723                    shape_i(img, 1, fgraph),
724                    shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,
725                    shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1)
726         out_shp = assert_conv_shape(out_shp)
727         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
728         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
729         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1), dilation=(1, 1),
730                               num_groups=num_groups,
731                               conv_mode='cross', precision=precision)(out.shape)
732         conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)
733         return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)
734     elif (border_mode == 'full' and subsample == (1, 1) and
735           direction_hint != 'forward!' and num_groups == 1):
736         img = gpu_contiguous(img)  # cudnn v2 rc3 need contiguous data
737         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))
738         conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
739         out_shp = (shape_i(img, 0, fgraph),
740                    shape_i(kerns, 1, fgraph),
741                    shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],
742                    shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1])
743         out_shp = assert_conv_shape(out_shp)
744         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
745         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
746         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1), dilation=dilation,
747                               num_groups=num_groups,
748                               conv_mode=conv_mode, precision=precision)(kerns.shape)
749         return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)
750     return _dnn_conv(img, kerns, algo=algo, border_mode=border_mode, subsample=subsample, dilation=dilation,
751                      conv_mode=conv_mode, precision=precision, num_groups=num_groups)
752 def dnn_conv3d(img, kerns, border_mode='valid', subsample=(1, 1, 1), dilation=(1, 1, 1),
753                conv_mode='conv', direction_hint=None,
754                algo=None, precision=None, num_groups=1):
755     fgraph = getattr(img, 'fgraph', None) or getattr(kerns, 'fgraph', None)
756     ctx_name = infer_context_name(img, kerns)
757     if (border_mode == 'valid' and subsample == (1, 1, 1) and dilation == (1, 1, 1) and
758             direction_hint == 'bprop weights' and num_groups == 1):
759         img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3, 4))
760         if conv_mode == 'conv':
761             kerns = kerns[:, :, ::-1, ::-1, ::-1]
762         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))
763         out_shp = (shape_i(kerns, 1, fgraph),
764                    shape_i(img, 1, fgraph),
765                    shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,
766                    shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1,
767                    shape_i(img, 4, fgraph) - shape_i(kerns, 4, fgraph) + 1)
768         out_shp = assert_conv_shape(out_shp)
769         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
770         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
771         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1, 1), dilation=(1, 1, 1),
772                               num_groups=num_groups,
773                               conv_mode='cross', precision=precision)(out.shape)
774         conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)
775         return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)
776     elif (border_mode == 'full' and subsample == (1, 1, 1) and
777           direction_hint != 'forward!' and num_groups == 1):
778         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))
779         conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
780         out_shp = (<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>shape_i(img, 0, fgraph),
781                    shape_i(kerns, 1, fgraph),
782                    shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],
783                    shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1],
784                    shape_i(img, 4, fgraph) + (shape_i(</b></font>kerns, 4, fgraph) - 1) * dilation[2])
785         out_shp = assert_conv_shape(out_shp)
786         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
787         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
788         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1, 1), dilation=dilation,
789                               num_groups=num_groups,
790                               conv_mode=conv_mode, precision=precision)(kerns.shape)
791         return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)
792     return _dnn_conv(img, kerns, algo=algo, border_mode=border_mode, subsample=subsample, dilation=dilation,
793                      conv_mode=conv_mode, precision=precision, num_groups=num_groups)
794 def dnn_gradweight(img, topgrad, kerns_shp, border_mode='valid',
795                    subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
796                    precision=None, algo=None, num_groups=1):
797     return _dnn_gradweight(img, topgrad, kerns_shp, border_mode=border_mode, subsample=subsample, dilation=dilation,
798                            conv_mode=conv_mode, algo=algo, precision=precision, num_groups=num_groups)
799 def dnn_gradweight3d(img, topgrad, kerns_shp, border_mode='valid',
800                      subsample=(1, 1, 1), dilation=(1, 1, 1), conv_mode='conv',
801                      precision=None, algo=None, num_groups=1):
802     return dnn_gradweight(img, topgrad, kerns_shp, border_mode,
803                           subsample, dilation, conv_mode, precision,
804                           algo, num_groups)
805 def dnn_gradinput(kerns, topgrad, img_shp, border_mode='valid',
806                   subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
807                   precision=None, algo=None, num_groups=1):
808     return _dnn_gradinput(kerns, topgrad, img_shp, border_mode=border_mode, subsample=subsample, dilation=dilation,
809                           conv_mode=conv_mode, algo=algo, precision=precision, num_groups=num_groups)
810 def dnn_gradinput3d(kerns, topgrad, img_shp, border_mode='valid',
811                     subsample=(1, 1, 1), dilation=(1, 1, 1), conv_mode='conv',
812                     precision=None, algo=None, num_groups=1):
813     return dnn_gradinput(kerns, topgrad, img_shp, border_mode, subsample,
814                          dilation, conv_mode, precision, algo,
815                          num_groups)
816 class GpuDnnPoolDesc(Op):
817     __props__ = ('ws', 'stride', 'mode', 'pad')
818     def c_headers(self):
819         return ['cudnn.h', 'cudnn_helper.h']
820     def c_header_dirs(self):
821         return [gpuarray_helper_inc_dir(), config.dnn.include_path]
822     def c_libraries(self):
823         return ['cudnn']
824     def c_lib_dirs(self):
825         return [config.dnn.library_path]
826     def do_constant_folding(self, node):
827         return False
828     def __init__(self, ws=(1, 1), stride=(1, 1), mode='max', pad=(0, 0)):
829         if mode == 'average':
830             mode = 'average_inc_pad'
831         assert mode in ('max', 'average_inc_pad', 'average_exc_pad')
832         self.mode = mode
833         assert len(ws) == len(stride) and len(stride) == len(pad)
834         assert len(ws) in (2, 3)
835         self.ws = ws
836         self.stride = stride
837         self.pad = pad
838     def get_ndim(self):
839         return len(self.ws)
840     def __setstate__(self, d):
841         self.__dict__.update(d)
842         if not hasattr(self, 'pad'):
843             self.pad = (0, 0)
844     def make_node(self):
845         node = Apply(self, [],
846                      [CUDNNDataType("cudnnPoolingDescriptor_t",
847                                     freefunc="cudnnDestroyPoolingDescriptor")()])
848         out = node.outputs[0]
849         out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true
850         return node
851     def c_code(self, node, name, inputs, outputs, sub):
852         desc, = outputs
853         if self.mode == 'max':
854             mode_flag = 'CUDNN_POOLING_MAX'
855         elif self.mode == "average_inc_pad":
856             mode_flag = 'CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING'
857         elif self.mode == "average_exc_pad":
858             mode_flag = 'CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING'
859         else:
860             raise NotImplementedError("Unsupported pooling model.")
861         return """
862 {
863   cudnnStatus_t err;
864   if ((err = cudnnCreatePoolingDescriptor(&amp;%(desc)s)) != CUDNN_STATUS_SUCCESS) {
865     PyErr_Format(PyExc_MemoryError, "could not allocate pooling "
866                  "descriptor: %%s", cudnnGetErrorString(err));
867     %(fail)s
868   }
869   static const int win[%(nd)d] = {%(win)s};
870   static const int pad[%(nd)d] = {%(pad)s};
871   static const int str[%(nd)d] = {%(str)s};
872     err = cudnnSetPoolingNdDescriptor(%(desc)s, %(mode_flag)s, CUDNN_PROPAGATE_NAN, %(nd)d, win, pad, str);
873   if (err != CUDNN_STATUS_SUCCESS) {
874     PyErr_Format(PyExc_RuntimeError, "could not set op descriptor: %%s",
875                  cudnnGetErrorString(err));
876     %(fail)s
877 }
878 """ % dict(name=name, desc=desc, mode_flag=mode_flag, fail=sub['fail'],
879            nd=self.get_ndim<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(), win=', '.join(map(str, self.ws)),
880            pad=', '.join(map(str, self.pad)),
881            str=', '.join(map(str, self.</b></font>stride)))
882     def c_code_cache_version(self):
883         return (4, version())
884 class GpuDnnPoolBase(DnnBase):
885     """
886     Abstract base class for GpuDnnPool and GpuDnnPoolGrad.
887     """
888     c_file = None
889     c_function = None
890     _f16_ok = True
891     __props__ = ('mode',)
892     check_input = False
893     params_type = ParamsType(mode=cudnn.cudnnPoolingMode_t,
894                              handle=handle_type)
895     def __init__(self, mode='max'):
896         DnnBase.__init__(self, [self.c_file], self.c_function)
897         if mode == 'average':
898             mode = 'average_inc_pad'
899         assert cudnn.cudnnPoolingMode_t.has_alias(mode)
900         self.mode = mode
901 class GpuDnnPool(GpuDnnPoolBase):
902     """
903     Parameters
904     ----------
905     img : tensor
906         The image 4d or 5d tensor.
907     ws : tensor
908         Window size.
909     stride : tensor
910         (dx, dy) or (dx, dy, dz).
911     mode : {'max', 'average_inc_pad', 'average_exc_pad'}
912         The old deprecated name 'average' corresponds to 'average_inc_pad'.
913     pad : tensor
914         (padX, padY) or (padX, padY, padZ)
915     """
916     c_file = "c_code/dnn_pool.c"
917     c_function = "APPLY_SPECIFIC(dnn_pool)"
918     def make_node(self, img, ws, stride, pad):
919         ctx_name = infer_context_name(img)
920         img = as_gpuarray_variable(img, ctx_name)
921         ws = tensor.as_tensor_variable(ws)
922         stride = tensor.as_tensor_variable(stride)
923         pad = tensor.as_tensor_variable(pad)
924         assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim
925         assert ws.type.ndim == 1
926         return Apply(self, [img, ws, stride, pad], [img.type()])
927     def infer_shape(self, node, shape):
928         w = node.inputs[1]
929         p = node.inputs[3]
930         res <font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= [shape[0][0], shape[0][1],
931                (shape[0][2] + 2 * p[0] - w[0]) // s[0] + 1,
932                (shape[0][3] + 2 * p[1] - w[1]) // s[</b></font>1] + 1
933                ]
934         if node.inputs[0].ndim == 5:
935             res.append((shape[0][4] + 2 * p[2] - w[2]) // s[2] + 1)
936         return [res]
937     def L_op(self, inp, outputs, grads):
938         img, ws, stride, pad = inp
939         grad, = grads
940         grad = gpu_contiguous(grad)
941         out, = outputs
942         g_out = GpuDnnPoolGrad(mode=self.mode)(img, out, grad, ws, stride, pad)
943         return g_out, theano.gradient.DisconnectedType()(), theano.gradient.DisconnectedType()(), theano.gradient.DisconnectedType()()
944     def connection_pattern(self, node):
945         return [[1], [0], [0], [0]]
946 class GpuDnnPoolGrad(GpuDnnPoolBase):
947     """
948     The pooling gradient.
949     Parameters
950     ----------
951     inp
952         The input of the pooling.
953     out
954         The output of the pooling in the forward.
955     out_grad
956         Same size as out, but is the corresponding gradient information.
957     ws : tensor variable
958         Window size.
959     stride : tensor variable
960         (dx, dy) or (dx, dy, dz).
961     mode : {'max', 'average_inc_pad', 'average_exc_pad'}
962         The old deprecated name 'average' corresponds to 'average_inc_pad'.
963     pad : tensor
964         (padX, padY) or (padX, padY, padZ)
965     """
966     c_file = "c_code/dnn_pool_grad.c"
967     c_function = "APPLY_SPECIFIC(dnn_pool_grad)"
968     def make_node(self, inp, out, out_grad, ws, stride, pad):
969         ctx_name = infer_context_name(inp, out, out_grad)
970         inp = as_gpuarray_variable(inp, ctx_name)
971         assert (inp.ndim in [4, 5])
972         out_grad = as_gpuarray_variable(out_grad, ctx_name)
973         assert (out_grad.ndim in [4, 5])
974         out = as_gpuarray_variable(out, ctx_name)
975         assert(out.ndim in [4, 5])
976         assert (out_grad.ndim == inp.ndim)
977         assert (inp.ndim == out.ndim)
978         ws = tensor.as_tensor_variable(ws)
979         stride = tensor.as_tensor_variable(stride)
980         pad = tensor.as_tensor_variable(pad)
981         assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim
982         assert ws.type.ndim == 1
983         return Apply(self, [inp, out, out_grad, ws, stride, pad], [inp.type()])
984     def infer_shape(self, node, shape):
985         return [shape[0]]
986 def dnn_pool(img, ws, stride=None, mode='max', pad=None):
987     """
988     GPU pooling using cuDNN from NVIDIA.
989     The memory layout to use is 'bc01', that is 'batch', 'channel',
990     'first dim', 'second dim' in that order.
991     `ws`, `stride` and `pad` must have the same length.
992     Parameters
993     ----------
994     img
995         Images to do the pooling over.
996     ws : tuple
997         Subsampling window size.  Should have 2 or 3 elements.
998     stride : tuple
999         Subsampling stride (default: (1, 1) or (1, 1, 1)).
1000     mode : {'max', 'average_inc_pad', 'average_exc_pad', 'sum', 'max_deterministic'}
1001         **NB**: 'max_deterministic' is supported since cuDNN v6.
1002     pad : tuple
1003         (padX, padY) or (padX, padY, padZ)
1004         default: (0, 0) or (0, 0, 0)
1005     .. warning:: The cuDNN library only works with GPU that have a compute
1006         capability of 3.0 or higher.  This means that older GPU will not
1007         work with this Op.
1008     Notes
1009     -----
1010     This Op implements the ignore_border=True of max_pool_2d.
1011     """
1012     img = gpu_contiguous(img)
1013     if stride is None:
1014         stride = (1,) * len(ws)
1015     if pad is None:
1016         pad = (0,) * len(ws)
1017     if mode == "sum":
1018         ret = GpuDnnPool(mode="average_inc_pad")(img, ws, stride, pad)
1019         context_name = ret.type.context_name
1020         window_elem = theano.tensor.prod(ws).astype(ret.dtype)
1021         return as_gpuarray_variable(ret * window_elem, context_name)
1022     return GpuDnnPool(mode=mode)(img, ws, stride, pad)
1023 class GpuDnnSoftmaxBase(DnnBase):
1024     """
1025     Op for the cuDNN Softmax.
1026     Parameters
1027     ----------
1028     algo : {'fast', 'accurate', 'log'}
1029         Indicating whether, respectively, computations should be optimized for
1030         speed, for accuracy, or if cuDNN should rather compute the log-softmax instead.
1031     mode : {'instance', 'channel'}
1032         Indicating whether the softmax should be computed per image across 'c01'
1033         or per spatial location '01' per image across 'c'.
1034     """
1035     __props__ = ('mode', 'algo')
1036     check_input = False
1037     params_type = ParamsType(algo=cudnn.cudnnSoftmaxAlgorithm_t,
1038                              mode=cudnn.cudnnSoftmaxMode_t,
1039                              handle=handle_type)
1040     def __init__(self, algo, mode):
1041         DnnBase.__init__(self, [self.file], self.c_func)
1042         assert cudnn.cudnnSoftmaxAlgorithm_t.has_alias(algo)
1043         self.algo = algo
1044         assert cudnn.cudnnSoftmaxMode_t.has_alias(mode)
1045         self.mode = mode
1046     def infer_shape(self, node, shape):
1047         if self.direction == 'forward':
1048             return [shape[0]]
1049         else:
1050             return [shape[1]]
1051 class GpuDnnSoftmax(GpuDnnSoftmaxBase):
1052     """
1053     Op for the cuDNN Softmax.
1054     algo : {'fast', 'accurate', 'log'}
1055         Indicating whether, respectively, computations should be optimized for
1056         speed, for accuracy, or if cuDNN should rather compute the log-softmax instead.
1057     mode : {'instance', 'channel'}
1058         Indicating whether the softmax should be computed per image across 'c01'
1059         or per spatial location '01' per image across 'c'.
1060     """
1061     _f16_ok = True
1062     direction = "forward"
1063     file = "c_code/dnn_softmax.c"
1064     c_func = "APPLY_SPECIFIC(softmax)"
1065     def make_node(self, x):
1066         x = as_gpuarray_variable(x, infer_context_name(x))
1067         assert x.ndim == 4
1068         return Apply(self, [x], [x.type()])
1069     def L_op(self, inp, outputs, grads):
1070         x, = inp
1071         g_sm, = grads
1072         sm, = outputs
1073         return [GpuDnnSoftmaxGrad(
1074                 self.algo,
1075                 self.mode
1076                 )(g_sm, sm)]
1077 class GpuDnnSoftmaxGrad(GpuDnnSoftmaxBase):
1078     """
1079     Op for the cuDNN SoftmaxGrad.
1080     Parameters
1081     ----------
1082     algo
1083         'fast', 'accurate' or 'log' indicating whether, respectively,
1084         computations should be optimized for speed, for accuracy, or if cuDNN
1085         should rather compute the gradient of the log-softmax instead.
1086     mode
1087         'instance' or 'channel' indicating whether the softmax should
1088         be computed per image across 'c01' or per spatial location '01' per
1089         image across 'c'.
1090     """
1091     _f16_ok = True
1092     direction = 'backward'
1093     file = "c_code/dnn_softmax_grad.c"
1094     c_func = "APPLY_SPECIFIC(softmax_grad)"
1095     def make_node(self, dy, sm):
1096         ctx_name = infer_context_name(dy, sm)
1097         dy = as_gpuarray_variable(dy, ctx_name)
1098         sm = as_gpuarray_variable(sm, ctx_name)
1099         assert dy.ndim == 4
1100         assert sm.ndim == 4
1101         return Apply(self, [dy, sm], [sm.type()])
1102 class GpuDnnReduction(DnnBase):
1103     check_input = False
1104     _f16_ok = True
1105     _cop_num_outputs = 2
1106     __props__ = ('red_op', 'axis', 'acc_dtype', 'dtype', 'return_indices')
1107     params_type = ParamsType(red_op=cudnn.cudnnReduceTensorOp_t,
1108                              acc_dtype=cudnn.cudnnDataType_t,
1109                              c_axis=uint32_t,
1110                              handle=handle_type)
1111     def __init__(self, red_op, axis, acc_dtype, dtype, return_indices):
1112         DnnBase.__init__(self, ['c_code/dnn_redux.c'], 'APPLY_SPECIFIC(dnn_redux)')
1113         assert cudnn.cudnnReduceTensorOp_t.has_alias(red_op)
1114         self.red_op = red_op
1115         assert acc_dtype in ['float16', 'float32', 'float64']
1116         self.acc_dtype = acc_dtype
1117         assert dtype in ['float16', 'float32', 'float64']
1118         self.dtype = dtype
1119         if axis is not None:
1120             if len(axis) &gt; 8:
1121                 raise ValueError('Too many axes to reduce on')
1122             if any(a &gt;= 8 for a in axis):
1123                 raise ValueError('Axes larger than 8 not supported')
1124             axis = tuple(axis)
1125         self.c_axis = self._convert_axis(axis)
1126         self.axis = axis
1127         if return_indices and (red_op != 'maximum' and red_op != 'minimum'):
1128             raise ValueError("Can't request indices for something other than"
1129                              " minimum or maximum")
1130         self.return_indices = return_indices
1131     def _convert_axis(self, axis):
1132         if axis is None:
1133             return np.uint32(-1)
1134         else:
1135             return reduce(lambda a, b: a | b, map(lambda a: 1 &lt;&lt; a, axis), 0)
1136     def make_node(self, inp):
1137         ctx_name = infer_context_name(inp)
1138         inp = as_gpuarray_variable(inp, ctx_name)
1139         inp = gpu_contiguous(inp)
1140         if inp.ndim &gt; 8:
1141             raise ValueError("cuDNN reduction doesn't support nd &gt; 8")
1142         assert inp.dtype in ['float16', 'float32', 'float64']
1143         if inp.dtype == 'float64':
1144             assert self.acc_dtype == 'float64'
1145         if inp.dtype == 'float32':
1146             assert self.acc_dtype == 'float32'
1147         if inp.dtype == 'float16':
1148             assert self.acc_dtype != 'float64'
1149         bcast = []
1150         for i in range(inp.ndim):
1151             if not (self.c_axis &amp; (1 &lt;&lt; i)):
1152                 bcast.append(inp.broadcastable[i])
1153         outs = [inp.type.clone(dtype=self.dtype, broadcastable=bcast)()]
1154         if self.return_indices:
1155             outs.append(GpuArrayType(dtype='uint32', broadcastable=bcast,
1156                                      context_name=ctx_name)())
1157         return Apply(self, [inp], outs)
1158 class GpuDnnBatchNorm(DnnBase):
1159     """
1160     Base Op for cuDNN Batch Normalization.
1161     Parameters
1162     ----------
1163     mode : {'per-activation', 'spatial'}
1164         Whether to normalize per activation (in this mode, bias and scale
1165         tensor dimensions are 1xCxHxW) or share normalization factors across
1166         spatial dimensions (in this mode, bias and scale tensor dimensions
1167         are 1xCx1x1).
1168     epsilon
1169         Epsilon value used in the batch normalization formula. Minimum allowed
1170         value is 1e-5 (imposed by cuDNN).
1171     running_average_factor : float
1172         Factor for updating the values or `running_mean` and `running_var`.
1173         If the factor is close to one, the running averages will update quickly,
1174         if the factor is close to zero it will update slowly.
1175     running_mean : tensor or None
1176         Previous value of the running mean. If this is given, the new value
1177         ``running_mean * (1 - r_a_factor) + batch mean * r_a_factor``
1178         will be returned as one of the outputs of this function.
1179         `running_mean` and `running_var` should either both be given or
1180         both be None.
1181     running_var : tensor or None
1182         Previous value of the running variance. If this is given, the new value
1183         ``running_var * (1 - r_a_factor) + (m / (m - 1)) * batch var * r_a_factor``
1184         will be returned as one of the outputs of this function,
1185         where `m` is the product of lengths of the averaged-over dimensions.
1186         `running_mean` and `running_var` should either both be given or
1187         both be None.
1188     """
1189     __props__ = ('mode', 'running_averages', 'inplace_running_mean',
1190                  'inplace_running_var', 'inplace_output')
1191     _cop_num_inputs = 7
1192     _cop_num_outputs = 5
1193     check_input = False
1194     params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
1195                              inplace_output=bool_t,
1196                              inplace_running_mean=bool_t,
1197                              inplace_running_var=bool_t,
1198                              handle=handle_type)
1199     def __init__(self, mode='per-activation', running_averages=False,
1200                  inplace_running_mean=False, inplace_running_var=False,
1201                  inplace_output=False):
1202         DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm.c'],
1203                          'dnn_batchnorm_op')
1204         assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
1205         self.mode = mode
1206         self.running_averages = running_averages
1207         self.inplace_output = inplace_output
1208         self.inplace_running_mean = inplace_running_mean
1209         self.inplace_running_var = inplace_running_var
1210         self.destroy_map = {}
1211         if self.inplace_output:
1212             self.destroy_map[0] = [0]
1213         if self.running_averages and self.inplace_running_mean:
1214             self.destroy_map[3] = [5]
1215         if self.running_averages and self.inplace_running_var:
1216             self.destroy_map[4] = [6]
1217     def __setstate__(self, d):
1218         self.__dict__.update(d)
1219         if not hasattr(self, 'running_average_factor'):
1220             self.running_average_factor = 0
1221         if not hasattr(self, 'running_averages'):
1222             self.running_averages = False
1223         if not (hasattr(self, 'inplace_running_mean') and
1224                 hasattr(self, 'inplace_running_var') and
1225                 hasattr(self, 'inplace_output')):
1226             self.inplace_running_mean = False
1227             self.inplace_running_var = False
1228             self.inplace_output = False
1229             self.destroy_map = {}
1230     def infer_shape(self, node, shape):
1231         return [shape[0]] + [shape[1]] * (len(node.outputs) - 1)
1232     def make_node(self, x, scale, bias, epsilon=1e-4,
1233                   running_average_factor=0.1,
1234                   running_mean=None, running_var=None):
1235         assert x.ndim == scale.ndim == bias.ndim
1236         assert x.ndim in (4, 5)
1237         assert self.running_averages == (running_mean is not None) == (running_var is not None)
1238         assert (running_mean is None or running_mean.ndim == x.ndim)
1239         assert (running_var is None or running_var.ndim == x.ndim)
1240         ctx_name = infer_context_name(x, scale, bias)
1241         x = as_gpuarray_variable(x, ctx_name)
1242         scale = as_gpuarray_variable(scale, ctx_name)
1243         bias = as_gpuarray_variable(bias, ctx_name)
1244         epsilon = as_scalar(epsilon).astype('float64')
1245         running_average_factor = as_scalar(running_average_factor).astype('float64')
1246         inputs = [x, scale, bias, epsilon, running_average_factor]
1247         output_types = [x.type(), scale.type(), scale.type()]
1248         if running_mean is not None and running_var is not None:
1249             inputs.append(as_gpuarray_variable(running_mean, ctx_name))
1250             inputs.append(as_gpuarray_variable(running_var, ctx_name))
1251             output_types.append(scale.type())
1252             output_types.append(scale.type())
1253         return Apply(self, inputs, output_types)
1254     def L_op(self, inputs, outputs, grads):
1255         x, scale, bias, epsilon, running_average_factor = inputs[:5]
1256         dy = grads[0]
1257         _, x_mean, x_invstd = outputs[:3]
1258         disconnected_outputs = [
1259             DisconnectedType()(),  # epsilon
1260             DisconnectedType()()]  # running_average_factor
1261         for i in range(5, len(inputs)):
1262             disconnected_outputs.append(DisconnectedType()())
1263         return GpuDnnBatchNormGrad(self.mode)(
1264             x, dy, scale, x_mean, x_invstd, epsilon) + disconnected_outputs
1265     def connection_pattern(self, node):
1266         patterns = [[True, True, True],     # x
1267                     [True, True, True],     # scale
1268                     [True, True, True],     # bias
1269                     [False, False, False],  # epsilon
1270                     [False, False, False]]  # running_average_factor
1271         for i in range(5, len(node.inputs)):
1272             patterns[0].append(True)
1273             for pattern in patterns[1:]:
1274                 pattern.append(False)
1275             patterns.append([False] * (3 + i - 5) + [True])
1276         return patterns
1277 class GpuDnnBatchNormInference(DnnBase):
1278     """
1279     Base Op for cuDNN Batch Normalization.
1280     Parameters
1281     ----------
1282     mode : {'per-activation', 'spatial'}
1283         Whether to normalize per activation (in this mode, bias and scale
1284         tensor dimensions are 1xCxHxW) or share normalization factors across
1285         spatial dimensions (in this mode, bias and scale tensor dimensions
1286         are 1xCx1x1).
1287     epsilon
1288         Epsilon value used in the batch normalization formula. Minimum allowed
1289         value is 1e-5 (imposed by cuDNN).
1290     """
1291     __props__ = ('mode', 'inplace')
1292     check_input = False
1293     params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
1294                              inplace=bool_t,
1295                              handle=handle_type)
1296     def __init__(self, mode='per-activation', inplace=False):
1297         DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm_inf.c'],
1298                          'dnn_batchnorm_op')
1299         assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
1300         self.mode = mode
1301         self.inplace = bool(inplace)
1302         if self.inplace:
1303             self.destroy_map = {0: [0]}
1304     def __setstate__(self, d):
1305         self.__dict__.update(d)
1306         if not hasattr(self, 'inplace'):
1307             self.inplace = False
1308     def infer_shape(self, node, shape):
1309     def make_node(self, x, scale, bias, estimated_mean, estimated_variance, epsilon=1e-4):
1310         ctx_name <font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= infer_context_name(x, scale, bias, estimated_mean,
1311                                       estimated_variance)
1312         x = as_gpuarray_variable(x, ctx_name)
1313         scale = as_gpuarray_variable(scale, ctx_name)
1314         bias = as_gpuarray_variable(bias, ctx_name)
1315         estimated_mean = as_gpuarray_variable(estimated_mean, ctx_name)
1316         estimated_variance = as_gpuarray_variable(estimated_variance, ctx_name)
1317         epsilon = as_scalar(epsilon).</b></font>astype('float64')
1318         assert x.ndim == scale.ndim == bias.ndim == estimated_mean.ndim == estimated_variance.ndim
1319         assert x.ndim in (4, 5)
1320         return Apply(self, [x, scale, bias, estimated_mean, estimated_variance, epsilon], [x.type()])
1321     def grad(self, inputs, grads):
1322         x, scale, bias, est_mean, est_var, epsilon = inputs
1323         dy = grads[0]
1324         if self.mode == "per-activation":
1325             axes = (0,)
1326         elif self.mode == "spatial":
1327             axes = (0,) + tuple(range(2, x.ndim))
1328         scale, bias, est_mean, est_var = (theano.tensor.addbroadcast(t, *axes)
1329                                           for t in (scale, bias, est_mean, est_var))
1330         est_var_eps = est_var + epsilon
1331         est_std = theano.tensor.sqrt(est_var_eps)
1332         two = theano.tensor.constant(2.)
1333         dx = dy * (scale / est_std)
1334         dscale <font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (dy * (x - est_mean)).sum(axes, keepdims=True) / est_std
1335         dbias = dy.sum(axes, keepdims=True)
1336         dmean = -dy.sum(axes, keepdims=True) * (scale / est_std)
1337         dvar = -(dy * (x - est_mean)).sum(</b></font>axes, keepdims=True) * (scale / (two * est_var_eps * est_std))
1338         return [dx, dscale, dbias, dmean, dvar, DisconnectedType()()]
1339     def connection_pattern(self, node):
1340         return [[True], [True], [True], [True], [True], [False]]
1341 class GpuDnnBatchNormGrad(DnnBase):
1342     __props__ = ('mode',)
1343     check_input = False
1344     params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
1345                              handle=handle_type)
1346     def __init__(self, mode='per-activation'):
1347         DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm_grad.c'],
1348                          'dnn_batchnorm_grad')
1349         assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
1350     def make_node(self, x, dy, scale, x_mean, x_invstd, epsilon=1e-4):
1351         ctx_name <font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= infer_context_name(x, dy, scale, x_mean, x_invstd)
1352         x = as_gpuarray_variable(x, ctx_name)
1353         dy = as_gpuarray_variable(dy, ctx_name)
1354         scale = as_gpuarray_variable(scale, ctx_name)
1355         x_mean = as_gpuarray_variable(x_mean, ctx_name)
1356         x_invstd = as_gpuarray_variable(x_invstd, ctx_name)
1357         epsilon = as_scalar(</b></font>epsilon).astype('float64')
1358         assert x.ndim == dy.ndim == scale.ndim == x_mean.ndim == x_invstd.ndim
1359         assert x.ndim in (4, 5)
1360         return Apply(self, [x, dy, scale, x_mean, x_invstd, epsilon], [x.type(), scale.type(), scale.type()])
1361     def infer_shape(self, node, shape):
1362         return [shape[0], shape[2], shape[2]]
1363 gpudata_type = CDataType('gpudata *', 'gpudata_release')
1364 dropoutdesc_type = CUDNNDataType('cudnnDropoutDescriptor_t',
1365                                  'cudnnDestroyDropoutDescriptor')
1366 class GpuDnnDropoutOp(DnnBase):
1367     __props__ = ('inplace',)
1368     def __init__(self, inplace=False):
1369         DnnBase.__init__(self, ["c_code/dnn_dropout_fwd.c"], "dnn_dropout_fwd")
1370         self.inplace = inplace
1371         if self.inplace:
1372             self.destroy_map = {1: [2]}
1373     def make_node(self, inp, descriptor, state):
1374         ctx_name = infer_context_name(inp)
1375         inp = as_gpuarray_variable(inp, ctx_name)
1376         return Apply(self, [inp, descriptor, state],
1377                      [inp.type(), state.type(), gpudata_type()])
1378     def prepare_node(self, node, storage_map, compute_map, impl):
1379         assert self.inplace, "GpuDnnDropoutOp not inplace"
1380 class _DropoutDescriptor(DnnBase):
1381     __props__ = ('context_name',)
1382     def __init__(self, context_name):
1383         DnnBase.__init__(self, ["c_code/dnn_dropout_desc.c"], "dnn_dropout_desc")
1384         self.context_name = context_name
1385     def dnn_context(self, node):
1386         return self.context_name
1387     def do_constant_folding(self, node):
1388         return False
1389     def make_node(self, dropout, seed, context_name):
1390         dropout = as_scalar(dropout).astype('float32')
1391         seed = as_scalar(seed).astype('uint64')
1392         assert context_name == self.context_name
1393         context = gpu_context_type.make_constant(get_context(context_name))
1394         return Apply(self, [dropout, seed, context],
1395                      [dropoutdesc_type(),
1396                       GpuArrayType('uint8', (False,),
1397                                    context_name=context_name)()])
1398     def c_code_cache_version_apply(self, node):
1399         return None
1400 def _make_dropout_desc(dropout, seed, context_name):
1401     desc, states = theano.function(
1402         [],
1403         _DropoutDescriptor(context_name)(dropout, seed, context_name),
1404         theano.Mode(optimizer=None),
1405         profile=False)()
1406     return desc, states
1407 def dropout(x, dropout=0.0, seed=4242):
1408     desc, states = _make_dropout_desc(dropout, seed, x.type.context_name)
1409     y, odesc = GpuDnnDropoutOp()(x, desc)
1410     return y, desc, odesc, states
1411 rnndesc_type = CUDNNDataType('cudnnRNNDescriptor_t',
1412                              'cudnnDestroyRNNDescriptor')
1413 def as_i32(v):
1414     return as_scalar(v).astype('int32')
1415 class _RNNDescriptor(DnnBase):
1416     __props__ = ('context_name',)
1417     def __init__(self, context_name):
1418         if version() &lt; 5005:
1419             raise RuntimeError("cudnn RNN require cudnn v5 final or higher.")
1420         DnnBase.__init__(self, ["c_code/dnn_rnn_desc.c"], "dnn_rnn_desc")
1421         self.context_name = context_name
1422     def dnn_context(self, node):
1423         return self.context_name
1424     def do_constant_folding(self, node):
1425         return False
1426     def make_node(self, hidden_size, num_layers, ddesc, input_mode,
1427                   direction_mode, rnn_mode, dtype):
1428         hidden_size = as_i32(hidden_size)
1429         num_layers = as_i32(num_layers)
1430         if version() &lt; 5005:
1431             raise RuntimeError("cudnn RNN require cudnn v5 final or higher.")
1432         if input_mode == 'linear':
1433             input_mode = as_i32(0)
1434         elif input_mode == 'skip':
1435             input_mode = as_i32(1)
1436         else:
1437             raise ValueError("input_mode")
1438         if direction_mode == 'unidirectional':
1439             direction_mode = as_i32(0)
1440         elif direction_mode == 'bidirectional':
1441             direction_mode = as_i32(1)
1442         else:
1443             raise ValueError("direction_mode")
1444         if rnn_mode == 'rnn_relu':
1445             rnn_mode = as_i32(0)
1446         elif rnn_mode == 'rnn_tanh':
1447             rnn_mode = as_i32(1)
1448         elif rnn_mode == 'lstm':
1449             rnn_mode = as_i32(2)
1450         elif rnn_mode == 'gru':
1451             rnn_mode = as_i32(3)
1452         else:
1453             raise ValueError("rnn_mode")
1454         dtype = as_i32(gpuarray.dtype_to_typecode(dtype))
1455         return Apply(self, [hidden_size, num_layers,
1456                             dropoutdesc_type.make_constant(ddesc),
1457                             input_mode, direction_mode, rnn_mode, dtype],
1458                      [rnndesc_type()])
1459 def _make_rnn_desc(hidden_size, num_layers, ddesc, rnn_mode,
1460                    input_mode, direction_mode, dtype, context_name):
1461     desc = theano.function(
1462         [],
1463         _RNNDescriptor(context_name)(hidden_size, num_layers, ddesc,
1464                                      input_mode, direction_mode,
1465                                      rnn_mode, dtype),
1466         theano.Mode(optimizer=None),
1467         profile=False)()
1468     return desc
1469 class _RNNParamSize(DnnBase):
1470     __props__ = ('context_name',)
1471     def __init__(self, context_name):
1472         DnnBase.__init__(self, ["c_code/dnn_rnn_paramsize.c"],
1473                          "dnn_rnn_paramsize")
1474         self.context_name = context_name
1475     def dnn_context(self, node):
1476         return self.context_name
1477     def do_constant_folding(self, node):
1478         return False
1479     def make_node(self, desc, input_size, typecode):
1480         input_size = as_tensor_variable(input_size).astype('uint64')
1481         typecode = as_i32(typecode)
1482         return Apply(self, [rnndesc_type.make_constant(desc), input_size,
1483                             typecode],
1484                      [get_scalar_type('uint64')()])
1485 def _get_param_size(desc, input_size, dtype, context_name):
1486     typecode = gpuarray.dtype_to_typecode(dtype)
1487     return theano.function(
1488         [],
1489         _RNNParamSize(context_name)(desc, input_size, typecode),
1490         theano.Mode(optimizer=None),
1491         profile=False)()
1492 class _RNNSplitParams(DnnBase):
1493     __props__ = ('rnn_mode',)
1494     def __init__(self, rnn_mode):
1495         DnnBase.__init__(self)
1496         self.rnn_mode = rnn_mode
1497     def make_node(self, w, desc, layer, isize, typecode):
1498         w = as_gpuarray_variable(w, infer_context_name(w))
1499         assert w.ndim == 1
1500         layer = as_scalar(layer).astype('int32')
1501         isize = as_tensor_variable(isize).astype('uint64')
1502         assert isize.ndim == 1
1503         typecode = as_scalar(typecode).astype('int32')
1504         _1d = GpuArrayType(w.type.dtype, [False],
1505                            context_name=w.type.context_name)
1506         _2d = GpuArrayType(w.type.dtype, [False, False],
1507                            context_name=w.type.context_name)
1508         outputs = []
1509         if self.rnn_mode == 'rnn_relu' or self.rnn_mode == 'rnn_tanh':
1510             outputs.extend([_2d(), _1d()])  # recurrent
1511         elif self.rnn_mode == 'lstm':
1512             outputs<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.extend([_2d(), _1d()])  # input input
1513             outputs.extend([_2d(), _1d()])  # input forget
1514             outputs.extend([_2d(), _1d()])  # input newmem
1515             outputs.extend([_2d(), _1d()])  # input output
1516             outputs.extend([_2d(), _1d()])  # recur input
1517             outputs.extend([_2d(), _1d()])  # recur forget
1518             outputs.extend([_2d(), _1d()])  # recur output
1519         elif self.</b></font>rnn_mode == 'gru':
1520             outputs.extend<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>([_2d(), _1d()])  # input reset
1521             outputs.extend([_2d(), _1d()])  # input update
1522             outputs.extend([_2d(), _1d()])  # input newmem
1523             outputs.extend([_2d(), _1d()])  # recur reset
1524             outputs.extend([_2d(), _1d(</b></font>)])  # recur update
1525             outputs.extend([_2d(), _1d()])  # recur newmem
1526         return Apply(self, [w, layer, rnndesc_type.make_constant(desc),
1527                             isize, typecode], outputs)
1528     def c_code(self, node, name, inputs, outputs, sub):
1529         kw = dict(fail=sub['fail'], w=inputs[0], layer=inputs[1],
1530                   desc=inputs[2], isize=inputs[3], typecode=inputs[4],
1531                   handle=sub['params'])
1532         code = """
1533   cudnnTensorDescriptor_t xdesc;
1534   cudnnFilterDescriptor_t wdesc;
1535   cudnnFilterDescriptor_t odesc;
1536   size_t nshp[2];
1537   void *w;
1538   void *o;
1539   ptrdiff_t off;
1540   size_t bshp;
1541   cudnnStatus_t err;
1542   cudnnDataType_t dt;
1543   cudnnTensorFormat_t tf;
1544   int nd;
1545   int dims[3];
1546   int strs[3];
1547   if (PyArray_DIM(%(isize)s, 0) != 2) {
1548     PyErr_SetString(PyExc_ValueError, "input_size should be of length two");
1549     %(fail)s;
1550   }
1551   switch (%(typecode)s) {
1552   case GA_FLOAT:
1553     dt = CUDNN_DATA_FLOAT;
1554     break;
1555   case GA_DOUBLE:
1556     dt = CUDNN_DATA_DOUBLE;
1557     break;
1558   case GA_HALF:
1559     dt = CUDNN_DATA_HALF;
1560     break;
1561   default:
1562     PyErr_SetString(PyExc_ValueError, "Unsupported data type");
1563     %(fail)s;
1564   }
1565   err = cudnnCreateTensorDescriptor(&amp;xdesc);
1566   if (err != CUDNN_STATUS_SUCCESS) {
1567     PyErr_SetString(PyExc_RuntimeError, "Could not create xdesc");
1568     %(fail)s;
1569   }
1570   dims[0] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 0);
1571   dims[1] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 1);
1572   dims[2] = 1;
1573   strs[0] = dims[2] * dims[1];
1574   strs[1] = dims[2];
1575   strs[2] = 1;
1576   err = cudnnSetTensorNdDescriptor(xdesc, dt, 3, dims, strs);
1577   if (err != CUDNN_STATUS_SUCCESS) {
1578     cudnnDestroyTensorDescriptor(xdesc);
1579     PyErr_Format(PyExc_RuntimeError, "Could not set xdesc: %%s",
1580                  cudnnGetErrorString(err));
1581     %(fail)s;
1582   }
1583   if (c_make_filter(%(w)s, &amp;wdesc)) {
1584     cudnnDestroyTensorDescriptor(xdesc);
1585     %(fail)s
1586   }
1587   err = cudnnCreateFilterDescriptor(&amp;odesc);
1588   if (err != CUDNN_STATUS_SUCCESS) {
1589     PyErr_SetString(PyExc_RuntimeError, "could not create odesc");
1590     cudnnDestroyTensorDescriptor(xdesc);
1591     cudnnDestroyFilterDescriptor(wdesc);
1592     %(fail)s
1593   }
1594   w = PyGpuArray_DEV_DATA(%(w)s);
1595   nshp[0] = PyGpuArray_DIM(%(w)s, 0);
1596   nshp[1] = 1;
1597         """ % kw
1598         def get_params(id, m, b):
1599             kw2 = kw.copy()
1600             kw2['id'] = id
1601             kw2['m'] = m
1602             kw2['b'] = b
1603             return """
1604   err = cudnnGetRNNLinLayerBiasParams(%(handle)s, %(desc)s, %(layer)s, xdesc, wdesc, w, %(id)s, odesc, &amp;o);
1605   if (err != CUDNN_STATUS_SUCCESS) {
1606     cudnnDestroyTensorDescriptor(xdesc);
1607     cudnnDestroyFilterDescriptor(wdesc);
1608     cudnnDestroyFilterDescriptor(odesc);
1609     PyErr_SetString(PyExc_RuntimeError, "can't fetch bias for id %(id)s");
1610     %(fail)s
1611   }
1612   off = (intptr_t)o - (intptr_t)w;
1613   assert(off &gt;= 0 &amp;&amp; "bias");
1614   err = cudnnGetFilterNdDescriptor(odesc, 3, &amp;dt, &amp;tf, &amp;nd, dims);
1615   if (err != CUDNN_STATUS_SUCCESS) {
1616     cudnnDestroyTensorDescriptor(xdesc);
1617     cudnnDestroyFilterDescriptor(wdesc);
1618     cudnnDestroyFilterDescriptor(odesc);
1619     PyErr_SetString(PyExc_RuntimeError, "could not get bias shape for id %(id)s");
1620     %(fail)s;
1621   }
1622   // We assume that the typecode matches
1623   assert(dims[2] == 1 &amp;&amp; "bias");
1624   assert(dims[1] == 1 &amp;&amp; "bias");
1625   %(b)s = pygpu_view(%(w)s, Py_None);
1626   %(b)s-&gt;ga.offset += off;
1627   %(b)s-&gt;ga.dimensions[0] = dims[0];
1628   bshp = dims[0];
1629   assert(dims[0] == 1 &amp;&amp; "bias");
1630   assert(dims[2] == 1 &amp;&amp; "bias");
1631   %(b)s = pygpu_view(%(w)s, Py_None);
1632   %(b)s-&gt;ga.offset += off;
1633   %(b)s-&gt;ga.dimensions[0] = dims[1];
1634   GpuArray_fix_flags(&amp;%(b)s-&gt;ga);
1635   err = cudnnGetRNNLinLayerMatrixParams(%(handle)s, %(desc)s, %(layer)s, xdesc, wdesc, w, %(id)s, odesc, &amp;o);
1636   if (err != CUDNN_STATUS_SUCCESS) {
1637     cudnnDestroyTensorDescriptor(xdesc);
1638     cudnnDestroyFilterDescriptor(wdesc);
1639     cudnnDestroyFilterDescriptor(odesc);
1640     PyErr_SetString(PyExc_RuntimeError, "can't fetch matrix for id %(id)s");
1641     %(fail)s
1642   }
1643   off = (intptr_t)o - (intptr_t)w;
1644   assert(off &gt;= 0 &amp;&amp; "matrix");
1645   // This is 3d because of cudnn limitations.
1646   err = cudnnGetFilterNdDescriptor(odesc, 3, &amp;dt, &amp;tf, &amp;nd, dims);
1647   if (err != CUDNN_STATUS_SUCCESS) {
1648     cudnnDestroyTensorDescriptor(xdesc);
1649     cudnnDestroyFilterDescriptor(wdesc);
1650     cudnnDestroyFilterDescriptor(odesc);
1651     PyErr_SetString(PyExc_RuntimeError, "could not get matrix shape for id %(id)s");
1652     %(fail)s;
1653   }
1654   assert(dims[1] == 1 &amp;&amp; "matrix");
1655   assert(dims[2] == 1 &amp;&amp; "matrix");
1656   // We assume that the typecode matches
1657   %(m)s = pygpu_reshape(%(w)s, 2, nshp, GA_F_ORDER, 1, -1);
1658   %(m)s-&gt;ga.offset += off;
1659   assert(dims[0] %% bshp == 0);
1660   %(m)s-&gt;ga.dimensions[0] = dims[0] / bshp;
1661   %(m)s-&gt;ga.dimensions[1] = bshp;
1662   assert(dims[0] == 1 &amp;&amp; "matrix");
1663   // We assume that the typecode matches
1664   %(m)s = pygpu_reshape(%(w)s, 2, nshp, GA_F_ORDER, 1, -1);
1665   %(m)s-&gt;ga.offset += off;
1666   %(m)s-&gt;ga.dimensions[1] = dims[1];
1667   %(m)s-&gt;ga.dimensions[0] = dims[2];
1668   %(m)s-&gt;ga.strides[1] = %(m)s-&gt;ga.dimensions[0] * gpuarray_get_elsize(%(m)s-&gt;ga.typecode);
1669   GpuArray_fix_flags(&amp;%(m)s-&gt;ga);
1670             """ % kw2
1671         for i in range(len(outputs) // 2):
1672             code += get_params(i, outputs[2 * i], outputs[(2 * i) + 1])
1673         code += """
1674   cudnnDestroyTensorDescriptor(xdesc);
1675   cudnnDestroyFilterDescriptor(wdesc);
1676   cudnnDestroyFilterDescriptor(odesc);
1677         """
1678         return code
1679     def c_code_cache_version(self):
1680         return (5, version())
1681 def _split_rnn_params(w, desc, layer, input_size, dtype, rnn_mode):
1682     typecode = gpuarray.dtype_to_typecode(dtype)
1683     outs = _RNNSplitParams(rnn_mode)(w, desc, layer, input_size, typecode)
1684     outs = [theano.Out(o, borrow=True) for o in outs]
1685     return theano.function(
1686         [], outs,
1687         theano.Mode(optimizer=None),
1688         profile=False)()
1689 class GpuDnnRNNOp(DnnBase):
1690     __props__ = ()
1691     _cop_num_inputs = 6
1692     _cop_num_outputs = 4
1693     def __init__(self, rnn_mode, direction_mode):
1694         DnnBase.__init__(self, ["c_code/dnn_rnn_fwd.c"], 'dnn_rnn_fwd')
1695         self.rnn_mode = rnn_mode
1696         if direction_mode == 'bidirectional':
1697             self.num_dirs = 2
1698         elif direction_mode == 'unidirectional':
1699             self.num_dirs = 1
1700         else:
1701             raise ValueError('direction_mode is invalid (got %s)' % (direction_mode,))
1702     def dnn_context(self, node):
1703         return node.outputs[1].type.context_name
1704     def make_node(self, desc, w, x, hx, cx=None):
1705         if cx is None:
1706             context_name = infer_context_name(w, x, hx)
1707         else:
1708             context_name = infer_context_name(w, x, hx, cx)
1709         w = as_gpuarray_variable(w, context_name)
1710         x = as_gpuarray_variable(x, context_name)
1711         hx = as_gpuarray_variable(hx, context_name)
1712         inputs = [desc, as_i32(self.num_dirs), w, x, hx]
1713         assert w.ndim == 1
1714         assert x.ndim == 3  # seqLength, minibatch, inputSize
1715         assert hx.ndim == 3  # numLayers, minibatch, hiddenSize * bidi
1716         if self.rnn_mode == 'lstm':
1717             cx = as_gpuarray_variable(cx, context_name)
1718             assert cx.ndim == 3  # numLayers, minibatch, hiddenSize * bidi
1719             inputs.append(cx)
1720         _3d = GpuArrayType(dtype=x.dtype, broadcastable=(False, False, False),
1721                            context_name=context_name)
1722         reserve = gpudata_type()
1723         y = _3d()  # seqLength, minibatch, hiddenSize * bidi
1724         hy = _3d()  # numLayers, miniBatch, hiddenSize * bidi
1725         outputs = [reserve, y, hy]
1726         if self.rnn_mode == 'lstm':
1727             cy = _3d()  # numLayers, miniBatch, hiddenSize * bidi
1728             outputs.append(cy)
1729         return Apply(self, inputs, outputs)
1730     def L_op(self, inputs, outputs, output_grads):
1731         desc, numDirs, w, x, hx = inputs[:5]
1732         cx = inputs[5] if len(inputs) == 6 else None
1733         reserve, y, hy = outputs[:3]
1734         _, dy, dhy = output_grads[:3]
1735         dcy = output_grads[3] if len(output_grads) == 4 else None
1736         if isinstance(dy.type, DisconnectedType):
1737             dy = as_gpuarray_variable(y.zeros_like(),
1738                                       context_name=y.type.context_name)
1739         if isinstance(dhy.type, DisconnectedType):
1740             dhy = None
1741         if dcy and isinstance(dcy.type, DisconnectedType):
1742             dcy = None
1743         dinputs = GpuDnnRNNGradInputs(rnn_mode=self.rnn_mode,
1744                                       grad_h=(dhy is not None),
1745                                       grad_c=(dcy is not None))(
1746             desc, x, y, dy, dhy, dcy, w, hx, cx, reserve, return_list=True)
1747         reserve2, dx, dhx = dinputs[:3]
1748         dw = GpuDnnRNNGradWeights()(
1749             desc, x, hx, y, reserve2, w)
1750         res = [DisconnectedType()(), DisconnectedType()(), dw, dx, dhx]
1751         if cx is not None:
1752             res.append(dinputs[3])  # dcx
1753         return res
1754     def connection_pattern(self, node):
1755         deconn = [[False] * len(node.outputs)] * 2
1756         conn = [[True] * len(node.outputs)] * (len(node.inputs) - 2)
1757         return deconn + conn
1758 class GpuDnnRNNGradInputs(DnnBase):
1759     __props__ = ('rnn_mode', 'grad_c', 'grad_h')
1760     _cop_num_inputs = 10
1761     _cop_num_outputs = 4
1762     def __init__(self, rnn_mode, grad_h, grad_c):
1763         DnnBase.__init__(self, ['c_code/dnn_rnn_gi.c'], 'dnn_rnn_gi')
1764         self.rnn_mode = rnn_mode
1765         self.grad_h = grad_h
1766         self.grad_c = grad_c
1767         if self.grad_c:
1768             assert self.rnn_mode == 'lstm'
1769     def dnn_context(self, node):
1770         return node.outputs[1].type.context_name
1771     def make_node(self, desc, x, y, dy, dhy, dcy, w, hx, cx, reserve):
1772         xshp = as_scalar(x.shape[2]).astype('uint64')
1773         inputs = [desc, xshp, y, dy, w, hx, reserve]
1774         outputs = [reserve.type(), x.type(), hx.type()]
1775         if self.rnn_mode == 'lstm':
1776             inputs.append(cx)
1777             outputs.append(cx.type())
1778         if self.grad_h:
1779             inputs.append(dhy)
1780         if self.grad_c:
1781             inputs.append(dcy)
1782         return Apply(self, inputs, outputs)
1783     def format_c_function_args(self, inp, out):
1784         rinp = inp[:7]
1785         others = inp[7:]
1786         if self.rnn_mode == 'lstm':
1787             rinp.append(others.pop(0))
1788         else:
1789             rinp.append('NULL')
1790         if self.grad_h:
1791             rinp.append(others.pop(0))
1792         else:
1793             rinp.append('NULL')
1794         if self.grad_c:
1795             rinp.append(others.pop(0))
1796         else:
1797             rinp.append('NULL')
1798         assert len(others) == 0
1799         return COp.format_c_function_args(self, rinp, out)
1800 class GpuDnnRNNGradWeights(DnnBase):
1801     __props__ = ()
1802     def __init__(self):
1803         DnnBase.__init__(self, ['c_code/dnn_rnn_gw.c'], 'dnn_rnn_gw')
1804     def make_node(self, desc, x, hx, y, reserve, w):
1805         wsize = as_scalar(w.shape[0]).astype('uint64')
1806         inputs = [desc, wsize, x, hx, y, reserve]
1807         outputs = [w.type()]
1808         return Apply(self, inputs, outputs)
1809 class RNNBlock(object):
1810     """
1811     An object that allow us to use CuDNN RNN implementation.
1812     TODO: make an example how to use. You can check Theano tests
1813     test_dnn_rnn_gru() and test_dnn_rnn_lstm() in the file
1814     theano/gpuarray/tests/test_dnn.py for now.
1815     Parameters
1816     ----------
1817     dtype : data type of computation
1818     hidden_size : int
1819         hidden layer dimension.
1820     num_layers : int
1821         number of the recurrent layer you want to set.
1822     rnn_mode : {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'}
1823         rnn_relu: A single-gate recurrent neural network with a ReLU activation function.
1824         .. math::
1825         h_t=ReLU(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})
1826         rnn_tanh: A single-gate recurrent neural network with a tanh activation function.
1827         .. math::
1828         h_t=tanh(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})
1829         lstm: A four-gate Long Short-Term Memory network with no peephole connections.
1830         gru: A three-gate network consisting of Gated Recurrent Units.
1831     input_mode : {'linear', 'skip'}
1832         linear: input will be multiplied by a biased matrix
1833         skip: No operation is performed on the input.  The size must match the hidden size.
1834     direction_mode : {'unidirectional', 'bidirectional'}
1835         unidirectional: The network operates recurrently from the first input to the last.
1836         bidirectional: The network operates from first to last then from last to first and concatenates the results at each layer.
1837     """
1838     def __init__(self, dtype, hidden_size, num_layers, rnn_mode,
1839                  input_mode='linear', direction_mode='unidirectional',
1840                  context_name=None):
1841         ddesc, states = _make_dropout_desc(0, 4242, context_name)
1842         self.ddesc = ddesc
1843         self.dstates = states
1844         self.desc = _make_rnn_desc(hidden_size, num_layers,
1845                                    ddesc, rnn_mode, input_mode,
1846                                    direction_mode, dtype, context_name)
1847         self.rnn_mode = rnn_mode
1848         self.direction_mode = direction_mode
1849         self.context_name = context_name
1850         self.dtype = dtype
1851     def get_param_size(self, input_size):
1852         """
1853         Get the size of the shared variable for the parameters of the RNN.
1854         This will return a size (in items) necessary to store all the
1855         parameters for the RNN.  You should allocate a variable of
1856         that size to store those parameters.  The order and layout of
1857         the parameters is opaque.
1858         Parameters
1859         ----------
1860         input_size: (int, int)
1861             Size of the input blocks
1862         """
1863         bytesize = _get_param_size(self.desc, input_size, self.dtype,
1864                                    self.context_name)
1865         bytesize = int(bytesize)
1866         assert bytesize % np.dtype(self.dtype).itemsize == 0
1867         return bytesize // np.dtype(self.dtype).itemsize
1868     def split_params(self, w, layer, input_size):
1869         """
1870         Split the opaque parameter block into components.
1871         Parameters
1872         ----------
1873         w: GpuArraySharedVariable
1874             opaque parameter block
1875         layer: int
1876             ID of the layer
1877         input_size: (int, int)
1878             Size of the input blocks
1879         """
1880         if not isinstance(w, GpuArraySharedVariable):
1881             raise TypeError("split_params only works on gpuarray shared variables")
1882         return _split_rnn_params(w, self.desc, layer, input_size, self.dtype, self.rnn_mode)
1883     def apply(self, w, x, hx, cx=None):
1884         """
1885         Apply the RNN to some data
1886         Parameters
1887         ----------
1888         w:
1889             opaque parameter block
1890         x:
1891             input
1892         hx:
1893             initial hidden state
1894         cx:
1895             initial cell state (for LSTM)
1896         """
1897         return GpuDnnRNNOp(self.rnn_mode, self.direction_mode)(
1898             rnndesc_type.make_constant(self.desc),
1899             w, x, hx, cx, return_list=True)[1:]
1900 def dnn_batch_normalization_train(inputs, gamma, beta, mode='per-activation',
1901                                   epsilon=1e-4, running_average_factor=0.1,
1902                                   running_mean=None, running_var=None):
1903     """
1904     Performs batch normalization of the given inputs, using the mean and
1905     variance of the inputs.
1906     Parameters
1907     ----------
1908     mode : {'per-activation', 'spatial'}
1909         Whether to normalize per activation or share normalization factors
1910         across spatial dimensions (i.e., all dimensions past the second).
1911     gamma : tensor
1912         Learnable scale factors. Must match the dimensionality of `inputs`,
1913         but have sizes of `1` for all axes normalized over (i.e., in the first
1914         dimension for ``mode='per-activation'`, and additionally in all
1915         dimensions past the second for ``mode='spatial'``).
1916     beta : tensor
1917         Learnable biases. Must match the tensor layout of `gamma`.
1918     epsilon : float
1919         Epsilon value used in the batch normalization formula. Minimum allowed
1920         value is 1e-5 (imposed by cuDNN).
1921     running_average_factor : float
1922         Factor for updating the values or `running_mean` and `running_var`.
1923         If the factor is close to one, the running averages will update quickly,
1924         if the factor is close to zero it will update slowly.
1925     running_mean : tensor or None
1926         Previous value of the running mean. If this is given, the new value
1927         ``running_mean * (1 - r_a_factor) + batch mean * r_a_factor``
1928         will be returned as one of the outputs of this function.
1929         `running_mean` and `running_var` should either both be given or
1930         both be None.
1931     running_var : tensor or None
1932         Previous value of the running variance. If this is given, the new value
1933         ``running_var * (1 - r_a_factor) + (m / (m - 1)) * batch var * r_a_factor``
1934         will be returned as one of the outputs of this function,
1935         where `m` is the product of lengths of the averaged-over dimensions.
1936         `running_mean` and `running_var` should either both be given or
1937         both be None.
1938     Returns
1939     -------
1940     out : tensor
1941         Batch-normalized inputs.
1942     mean : tensor
1943         Means of `inputs` across the normalization axes.
1944     invstd : tensor
1945         Inverse standard deviations of `inputs` across the normalization axes.
1946     new_running_mean : tensor
1947         New value of the running mean (only if both `running_mean` and
1948         `running_var` were given).
1949     new_running_var : tensor
1950         New value of the running variance (only if both `running_var` and
1951         `running_mean` were given).
1952     Notes
1953     -----
1954     Requires cuDNN 5 and Theano 0.9dev2 or more recent.
1955     For 4d tensors, returned values are equivalent to:
1956     .. code-block:: python
1957         axes = 0 if mode == 'per-activation' else (0, 2, 3)
1958         mean = inputs.mean(axes, keepdims=True)
1959         var = inputs.var(axes, keepdims=True)
1960         invstd = T.inv(T.sqrt(var + epsilon))
1961         out = (inputs - mean) * gamma * invstd + beta
1962         m = T.cast(T.prod(inputs.shape) / T.prod(mean.shape), 'float32')
1963         running_mean = running_mean * (1 - running_average_factor) + \\
1964                        mean * running_average_factor
1965         running_var = running_var * (1 - running_average_factor) + \\
1966                       (m / (m - 1)) * var * running_average_factor
1967     For 5d tensors, the axes are (0, 2, 3, 4).
1968     """
1969     ndim = inputs.ndim
1970     if gamma.ndim != ndim or beta.ndim != ndim:
1971         raise ValueError("gamma and beta must be of the same dimensionality "
1972                          "as inputs; got %d and %d instead of %d" %
1973                          (gamma.ndim, beta.ndim, ndim))
1974     if (running_mean is None) != (running_var is None):
1975         raise ValueError("running_mean and running_var must either both be "
1976                          "given or both be None")
1977     if running_mean is not None and running_mean.ndim != ndim:
1978         raise ValueError("running_mean must be of the same dimensionality "
1979                          "as inputs; got %d instead of %d" %
1980                          (running_mean.ndim, ndim))
1981     if running_var is not None and running_var.ndim != ndim:
1982         raise ValueError("running_var must be of the same dimensionality "
1983                          "as inputs; got %d instead of %d" %
1984                          (running_var.ndim, ndim))
1985     if epsilon &lt; 1e-5:
1986         raise ValueError("epsilon must be at least 1e-5, got %f" % epsilon)
1987     running_averages = (running_mean is not None and running_var is not None)
1988     if ndim &lt; 4:
1989         inputs = theano.tensor.shape_padright(inputs, 4 - ndim)
1990         gamma = theano.tensor.shape_padright(gamma, 4 - ndim)
1991         beta = theano.tensor.shape_padright(beta, 4 - ndim)
1992         if running_averages:
1993             running_mean = theano.tensor.shape_padright(running_mean, 4 - ndim)
1994     elif ndim &gt; 5:
1995         inputs_shape = inputs.shape
1996         params_shape <font color="#e77471"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= gamma.shape
1997         inputs = theano.tensor.flatten(inputs, 5)
1998         gamma = theano.tensor.flatten(gamma, 5)
1999         beta = theano.</b></font>tensor.flatten(beta, 5)
2000         if running_averages:
2001             running_mean = theano.tensor.flatten(running_mean, 5)
2002             running_var = theano.tensor.flatten(running_var, 5)
2003     batchnorm_op = GpuDnnBatchNorm(mode=mode, running_averages=running_averages)
2004     if running_averages:
2005         out, mean, invstd, new_running_mean, new_running_var = batchnorm_op(
2006             gpu_contiguous(inputs), gpu_contiguous(gamma),
2007             gpu_contiguous(beta), epsilon=epsilon,
2008             running_average_factor=running_average_factor,
2009             running_mean=gpu_contiguous(running_mean),
2010             running_var=gpu_contiguous(running_var))
2011         if new_running_mean.broadcastable != running_mean.broadcastable:
2012             new_running_mean = tensor.patternbroadcast(new_running_mean, running_mean.broadcastable)
2013         if new_running_var.broadcastable != running_var.broadcastable:
2014             new_running_var = tensor.patternbroadcast(new_running_var, running_var.broadcastable)
2015         result = (out, mean, invstd, new_running_mean, new_running_var)
2016     else:
2017         result = batchnorm_op(gpu_contiguous(inputs), gpu_contiguous(gamma),
2018                               gpu_contiguous(beta), epsilon=epsilon)
2019     if ndim &lt; 4:
2020         result = tuple(theano.tensor.flatten(r, ndim) for r in result)
2021     elif ndim &gt; 5:
2022         result = (theano.tensor.reshape(result[0], inputs_shape),) + tuple(
2023             theano.tensor.reshape(r, params_shape) for r in result[1:])
2024     return result
2025 def dnn_batch_normalization_test(inputs, gamma, beta, mean, var,
2026                                  mode='per-activation', epsilon=1e-4):
2027     """
2028     Performs batch normalization of the given inputs, using the given mean and
2029     variance.
2030     Parameters
2031     ----------
2032     mode : {'per-activation', 'spatial'}
2033         Whether to normalize per activation or share normalization factors
2034         across spatial dimensions (i.e., all dimensions past the second).
2035     gamma : tensor
2036         Scale factors. Must match the dimensionality of `inputs`, but have
2037         sizes of `1` for all axes normalized over (i.e., in the first dimension
2038         for ``mode='per-activation'`, and additionally in all dimensions past
2039         the second for ``mode='spatial'``).
2040     beta : tensor
2041         Biases. Must match the tensor layout of `gamma`.
2042     mean : tensor
2043         Means. Usually these are running averages computed during training.
2044         Must match the tensor layout of `gamma`.
2045     var : tensor
2046         Variances. Usually these are running averages computed during training.
2047         Must match the tensor layout of `gamma`.
2048     epsilon : float
2049         Epsilon value used in the batch normalization formula. Minimum allowed
2050         value is 1e-5 (imposed by cuDNN).
2051     Returns
2052     -------
2053     out : tensor
2054         Batch-normalized inputs.
2055     Notes
2056     -----
2057     Requires cuDNN 5 and Theano 0.9dev2 or more recent.
2058     For 4d tensors, the returned value is equivalent to:
2059     .. code-block:: python
2060         axes = (0,) if mode == 'per-activation' else (0, 2, 3)
2061         gamma, beta, mean, var = (T.addbroadcast(t, *axes)
2062                                   for t in (gamma, beta, mean, var))
2063         out = (inputs - mean) * gamma / T.sqrt(var + epsilon) + beta
2064     For 5d tensors, the axes would be (0, 2, 3, 4).
2065     """
2066     ndim = inputs.ndim
2067     if gamma.ndim != ndim or beta.ndim != ndim:
2068         raise ValueError("gamma and beta must be of the same dimensionality "
2069                          "as inputs; got %d and %d instead of %d" %
2070                          (gamma.ndim, beta.ndim, ndim))
2071     if mean.ndim != ndim or var.ndim != ndim:
2072         raise ValueError("mean and var must be of the same dimensionality "
2073                          "as inputs; got %d and %d instead of %d" %
2074                          (mean.ndim, var.ndim, ndim))
2075     if epsilon &lt; 1e-5:
2076         raise ValueError("epsilon must be at least 1e-5, got %f" % epsilon)
2077         inputs = theano.tensor.shape_padright(inputs, 4 - ndim)
2078         gamma = theano.tensor.shape_padright(gamma, 4 - ndim)
2079         beta = theano.tensor.shape_padright<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(beta, 4 - ndim)
2080         mean = theano.tensor.shape_padright(mean, 4 - ndim)
2081         var = theano.tensor.shape_padright(var, 4 - ndim)
2082     elif ndim &gt; 5:
2083         inputs_shape = inputs.shape
2084         inputs = theano.tensor.flatten(inputs, 5)
2085         gamma = theano.tensor.flatten(gamma, 5)
2086         beta = theano.</b></font>tensor.flatten(beta, 5)
2087         mean = theano.tensor.flatten(mean, 5)
2088         var = theano.tensor.flatten(var, 5)
2089     batchnorm_op = GpuDnnBatchNormInference(mode=mode)
2090     result = batchnorm_op(gpu_contiguous(inputs), gpu_contiguous(gamma),
2091                           gpu_contiguous(beta), gpu_contiguous(mean),
2092                           gpu_contiguous(var), epsilon=epsilon)
2093     if ndim &lt; 4:
2094         result = theano.tensor.flatten(result, ndim)
2095     elif ndim &gt; 5:
2096         result = theano.tensor.reshape(result, inputs_shape)
2097     return result
2098 class GpuDnnTransformerGrid(DnnBase):
2099     """
2100     Grid generator Op for cuDNN Spatial Transformer.
2101     """
2102     __props__ = ()
2103     _cop_num_inputs = 2
2104     _cop_num_outputs = 1
2105     _f16_ok = True
2106     check_input = False
2107     def __init__(self):
2108         DnnBase.__init__(self, ["c_code/dnn_sptf_grid.c"], "APPLY_SPECIFIC(dnn_sptf_grid)")
2109     def make_node(self, theta, out_dims):
2110         """
2111         Create a grid generator node for a cuDNN Spatial Transformer
2112         Parameters
2113         ----------
2114         theta : tensor
2115             Affine transformation tensor containing one affine transformation
2116             matrix per image. ``theta`` is usually generated by the localization
2117             network.
2118         out_dims : tuple
2119             Dimensions of the transformed inputs, containing four elements, and is given
2120             by (N, C, H, W), where N is the number of inputs, C the number of channels,
2121             H and W are the height and width of each input.
2122         """
2123         context_name = infer_context_name(theta)
2124         theta = gpu_contiguous(as_gpuarray_variable(theta, context_name))
2125         assert theta.dtype in ('float16', 'float32', 'float64')
2126         assert theta.ndim == 3
2127         out_dims = cpu_contiguous(as_tensor_variable(out_dims))
2128         assert out_dims.dtype in theano.tensor.basic.integer_dtypes
2129         assert out_dims.ndim == 1
2130         out_dims = theano.tensor.basic.cast(out_dims, 'int64')
2131         grid = GpuArrayType(dtype=theta.dtype,
2132                             broadcastable=(theta.type.ndim + 1) * (False,),
2133                             context_name=context_name)()
2134         inputs = [theta, out_dims]
2135         outputs = [grid]
2136         return Apply(self, inputs, outputs)
2137     def grad(self, inputs, grads):
2138         theta, out_dims = inputs
2139         dgrid = grads[0]
2140         dtheta = GpuDnnTransformerGradT()(dgrid)
2141         return [dtheta, grad_not_implemented(self, 1, out_dims)]
2142 class GpuDnnTransformerSampler(DnnBase):
2143     """
2144     Grid sampler Op for cuDNN Spatial Transformer.
2145     """
2146     __props__ = ()
2147     _cop_num_inputs = 2
2148     _cop_num_outputs = 1
2149     _f16_ok = True
2150     check_input = False
2151     def __init__(self):
2152         DnnBase.__init__(self, ["c_code/dnn_sptf_sampler.c"], "APPLY_SPECIFIC(dnn_sptf_sampler)")
2153     def make_node(self, img, grid):
2154         """
2155         Create a grid sampler node for a cuDNN Spatial Transformer
2156         Parameters
2157         ----------
2158         img : tensor
2159             Images from which the pixels will be sampled. The implementation
2160             assumes the tensor is in NCHW format, where N is the number of images,
2161             C is the number of color channels, H is the height of the inputs, and
2162             W is width of the inputs.
2163         grid : GpuDnnTransformerGrid
2164             Grid that contains the coordinates of the pixels to be sampled from
2165             the inputs images.
2166         """
2167         context_name = infer_context_name(img, grid)
2168         img = gpu_contiguous(as_gpuarray_variable(img, context_name))
2169         if img.type.ndim != 4:
2170             raise TypeError('img must be a 4D tensor')
2171         elif img.dtype not in ('float16', 'float32', 'float64'):
2172             raise TypeError('img type must be floating-point')
2173         grid = gpu_contiguous(as_gpuarray_variable(grid, context_name))
2174         if grid.type.ndim != 4:
2175             raise TypeError('grid must be a 4D tensor')
2176         elif grid.dtype not in ('float16', 'float32', 'float64'):
2177             raise TypeError('grid type must be floating-point')
2178         out = GpuArrayType(dtype=img.dtype,
2179                            broadcastable=img.type.ndim * (False,),
2180                            context_name=context_name)()
2181         inputs = [img, grid]
2182         outputs = [out]
2183         return Apply(self, inputs, outputs)
2184     def grad(self, inputs, grads):
2185         img, grid = inputs
2186         dy = grads[0]
2187         dimg, dgrid = GpuDnnTransformerGradI()(img, grid, dy)
2188         return [dimg, dgrid]
2189 class GpuDnnTransformerGradI(DnnBase):
2190     """
2191     Gradient of inputs Op for cuDNN Spatial Transformer.
2192     """
2193     __props__ = ()
2194     _cop_num_inputs = 3
2195     _cop_num_outputs = 2
2196     _f16_ok = True
2197     check_input = False
2198     def __init__(self):
2199         DnnBase.__init__(self, ["c_code/dnn_sptf_gi.c"], "APPLY_SPECIFIC(dnn_sptf_gi)")
2200     def make_node(self, img, grid, dy):
2201         context_name = infer_context_name(img, grid, dy)
2202         img = as_gpuarray_variable(gpu_contiguous(img), context_name)
2203         if img.ndim != 4:
2204             raise TypeError('img must have 4 dimensions.')
2205         grid = as_gpuarray_variable(gpu_contiguous(grid), context_name)
2206         if img.ndim != grid.ndim:
2207             raise TypeError('grid should have the same number of dimensions as img')
2208         dy = as_gpuarray_variable(dy, context_name)
2209         if dy.ndim != 4:
2210             raise TypeError('dy must have 4 dimensions.')
2211         dimg = img.type()
2212         dgrid = grid.type()
2213         inputs = [img, grid, dy]
2214         outputs = [dimg, dgrid]
2215         return Apply(self, inputs, outputs)
2216 class GpuDnnTransformerGradT(DnnBase):
2217     """
2218     Gradient of affine transformations Op for cuDNN Spatial Transformer.
2219     """
2220     __props__ = ()
2221     _cop_num_inputs = 1
2222     _cop_num_outputs = 1
2223     check_input = False
2224     <font color="#68818b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def __init__(self):
2225         DnnBase.__init__(self, ["c_code/dnn_sptf_gt.c"], "APPLY_SPECIFIC(dnn_sptf_gt)")
2226     def make_node(self, dgrid):
2227         context_name = infer_context_name(dgrid)
2228         dgrid = as_gpuarray_variable(dgrid, context_name)
2229         assert dgrid.dtype in ('float16'</b></font>, 'float32', 'float64')
2230         assert dgrid.ndim == 4
2231         dtheta = GpuArrayType(dtype=dgrid.dtype,
2232                               broadcastable=(dgrid.type.ndim - 1) * (False,),
2233                               context_name=context_name)()
2234         inputs = [dgrid]
2235         outputs = [dtheta]
2236         return Apply(self, inputs, outputs)
2237 def dnn_spatialtf(img, theta, scale_width=1, scale_height=1):
2238     """
2239     GPU spatial transformer using cuDNN from NVIDIA.
2240     Parameters
2241     ----------
2242     img : tensor
2243         Images to which the transformations will be applied. The implementation
2244         assumes the tensor is in NCHW format, where N is the number of images,
2245         C is the number of color channels, H is the height of the inputs, and
2246         W is width of the inputs.
2247     theta : tensor
2248         Affine transformation tensor containing one affine transformation
2249         matrix per image. ``theta`` is usually generated by the localization
2250         network.
2251     scale_height: float
2252         A float specifying the scaling factor for the height of the output
2253         image. A value of 1 will keep the original height of the input. Values
2254         larger than 1 will upsample the input. Values below 1 will downsample
2255         the input.
2256     scale_width: float
2257         A float specifying the scaling factor for the width of the output
2258         image. A value of 1 will keep the original width of the input. Values
2259         larger than 1 will upsample the input. Values below 1 will downsample
2260         the input.
2261     Returns
2262     -------
2263     out : tensor
2264         Transformed images with width and height properly scaled.
2265     Notes
2266     -----
2267     Currently, cuDNN only supports 2D transformations with 2x3 affine
2268     transformation matrices.
2269     Bilinear interpolation is the only grid sampler method available.
2270     """
2271     out_dims = (<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>img.shape[0], img.shape[1],
2272                 theano.tensor.ceil(img.shape[2] * scale_height),
2273                 theano.tensor.</b></font>ceil(img.shape[3] * scale_width))
2274     out_dims = tuple([as_scalar(v).astype('int64') for v in out_dims])
2275     grid = GpuDnnTransformerGrid()(theta, out_dims)
2276     sampler = GpuDnnTransformerSampler()(img, grid)
2277     return sampler
2278 def local_abstractconv_cudnn_graph(op, context_name, inputs, outputs):
2279     if (not isinstance(op, (AbstractConv2d,
2280                             AbstractConv2d_gradWeights,
2281                             AbstractConv2d_gradInputs))):
2282         return
2283     if version(raises=False) &lt; 6000 and op.filter_dilation != (1, 1):
2284         return None
2285     if op.unshared:
2286         return None
2287     if isinstance(op.border_mode, tuple) and any(isinstance(p, tuple) for p in op.border_mode):
2288         return None
2289     inp1 = inputs[0]
2290     inp2 = inputs[1]
2291     if not dnn_available(inp1.type.context_name):
2292         return
2293     if op.filter_flip:
2294         conv_mode = 'conv'
2295     else:
2296         conv_mode = 'cross'
2297     if isinstance(op, AbstractConv2d):
2298         rval = dnn_conv(inp1, inp2,
2299                         border_mode=op.border_mode,
2300                         subsample=op.subsample,
2301                         dilation=op.filter_dilation,
2302                         direction_hint='forward!',
2303                         num_groups=op.num_groups)
2304     elif isinstance(op, AbstractConv2d_gradWeights):
2305         shape = (<font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[1], inp1.shape[1] // op.num_groups,
2306                  inputs[2][0], inputs[2][1])
2307         rval = dnn_gradweight(inp1, inp2, shape,
2308                               border_mode=</b></font>op.border_mode,
2309                               subsample=op.subsample,
2310                               dilation=op.filter_dilation,
2311                               num_groups=op.num_groups)
2312     elif isinstance(op, AbstractConv2d_gradInputs):
2313         shape = (<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[0], inp1.shape[1] * op.num_groups,
2314                  inputs[2][0], inputs[2][1])
2315         rval = dnn_gradinput(</b></font>inp1, inp2, shape,
2316                              border_mode=op.border_mode,
2317                              subsample=op.subsample,
2318                              dilation=op.filter_dilation,
2319                              conv_mode=conv_mode,
2320                              num_groups=op.num_groups)
2321     return [rval]
2322 def local_abstractconv3d_cudnn_graph(op, context_name, inputs, outputs):
2323     if (not isinstance(op, (AbstractConv3d,
2324                             AbstractConv3d_gradWeights,
2325                             AbstractConv3d_gradInputs))):
2326         return
2327     if version(raises=False) &lt; 6000 and op.filter_dilation != (1, 1, 1):
2328         return None
2329     inp1 = inputs[0]
2330     inp2 = inputs[1]
2331     if not dnn_available(inp1.type.context_name):
2332         return
2333     if op.filter_flip:
2334         conv_mode = 'conv'
2335     else:
2336         conv_mode = 'cross'
2337     if isinstance(op, AbstractConv3d):
2338         rval = dnn_conv3d(inp1, inp2,
2339                           border_mode=op.border_mode,
2340                           subsample=op.subsample,
2341                           dilation=op.filter_dilation,
2342                           direction_hint='forward!',
2343                           num_groups=op.num_groups)
2344     elif isinstance(op, AbstractConv3d_gradWeights):
2345         shape = (<font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[1], inp1.shape[1] // op.num_groups,
2346                  inputs[2][0], inputs[2][1], inputs[2][2])
2347         rval = dnn_gradweight3d(inp1, inp2, shape,
2348                                 border_mode=</b></font>op.border_mode,
2349                                 subsample=op.subsample,
2350                                 dilation=op.filter_dilation,
2351                                 num_groups=op.num_groups)
2352     elif isinstance(op, AbstractConv3d_gradInputs):
2353         shape = (<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[0], inp1.shape[1] * op.num_groups,
2354                  inputs[2][0], inputs[2][1], inputs[2][2])
2355         rval = dnn_gradinput3d(inp1, inp2, shape,
2356                                border_mode=</b></font>op.border_mode,
2357                                subsample=op.subsample,
2358                                dilation=op.filter_dilation,
2359                                conv_mode=conv_mode,
2360                                num_groups=op.num_groups)
2361     return [rval]
2362 @local_optimizer([AbstractConv2d, AbstractConv3d])
2363 def local_abstractconv_cudnn(node):
2364     ctx = infer_context_name(*node.inputs)
2365     if not isinstance(node.inputs[0].type, GpuArrayType):
2366         return
2367     if node.op.unshared:
2368         return None
2369     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
2370         return None
2371     if isinstance(node.op, AbstractConv2d):
2372         with inherit_stack_trace(node.outputs):
2373             return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2374     elif isinstance(node.op, AbstractConv3d):
2375         with inherit_stack_trace(node.outputs):
2376             return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2377 @local_optimizer([AbstractConv2d, AbstractConv2d_gradWeights, AbstractConv2d_gradInputs])
2378 def local_abstractconv_cudnn_alt(node):
2379     if(not isinstance(node.op, (AbstractConv2d, AbstractConv2d_gradWeights,
2380        AbstractConv2d_gradInputs))):
2381         return
2382     if version(raises=False) &lt; 6000 and node.op.filter_dilation != (1, 1):
2383         return None
2384     if node.op.unshared:
2385         return None
2386     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
2387         return None
2388     inp1 = node.inputs[0]
2389     inp2 = node.inputs[1]
2390         return
2391     op = node<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op
2392     border_mode = node.op.border_mode
2393     subsample = node.op.subsample
2394     filter_dilation = node.op.filter_dilation
2395     num_groups = node.op.num_groups
2396     precision, _ = get_precision(None, [inp1</b></font>, inp2])
2397     if node.op.filter_flip:
2398         conv_mode = 'conv'
2399     else:
2400         conv_mode = 'cross'
2401     if isinstance(op, AbstractConv2d):
2402         if border_mode == 'half' or subsample != (1, 1) or num_groups != 1:
2403             return None
2404         if border_mode == 'full':
2405             direction_hint = 'bprop inputs'
2406         elif border_mode == 'valid' and filter_dilation == (1, 1):
2407             direction_hint = 'bprop weights'
2408         else:
2409             return None
2410         rval = dnn_conv(inp1, inp2,
2411                         border_mode=border_mode,
2412                         subsample=subsample,
2413                         dilation=filter_dilation,
2414                         direction_hint=direction_hint,
2415                         conv_mode=conv_mode,
2416     elif isinstance(op, AbstractConv2d_gradWeights):
2417         if(<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>border_mode == 'valid' and subsample == (1, 1) and
2418            filter_dilation == (1, 1) and num_groups == 1):
2419             img = gpu_contiguous(inp1)
2420             topgrad = gpu_contiguous(inp2)
2421             ctx_name = infer_context_name(img, topgrad)
2422             img = gpu_contiguous(img.dimshuffle(</b></font>1, 0, 2, 3))
2423             topgrad = gpu_contiguous(topgrad.dimshuffle(1, 0, 2, 3))
2424             ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
2425             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
2426             out_shp = get_conv_output_shape(ishape,
2427                                             tshape,
2428                                             border_mode=border_mode,
2429                                             subsample=subsample,
2430                                             filter_dilation=filter_dilation)
2431             out_shp = assert_conv_shape(out_shp)
2432             out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
2433             desc = GpuDnnConvDesc(border_mode=border_mode,
2434                                   subsample=subsample,
2435                                   dilation=filter_dilation,
2436                                   conv_mode='cross',
2437                                   precision=precision)(out.shape)
2438             conv = GpuDnnConv(algo=None, num_groups=num_groups)(img, topgrad, out, desc)
2439             if conv_mode == 'conv':
2440                 conv = conv[:, :, ::-1, ::-1]
2441             rval = as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)
2442         else:
2443             return None
2444     elif isinstance(op, AbstractConv2d_gradInputs):
2445         if border_mode == 'valid' and subsample == (1, 1) and num_groups == 1:
2446             kerns = gpu_contiguous(inp1.dimshuffle(1, 0, 2, 3))
2447             topgrad = gpu_contiguous(inp2)
2448             ctx_name = infer_context_name(kerns, topgrad)
2449             conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
2450             desc = GpuDnnConvDesc(border_mode='full',
2451                                   subsample=subsample,
2452                                   dilation=filter_dilation,
2453                                   conv_mode=conv_mode,
2454                                   precision=precision)(kerns.shape)
2455             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
2456             kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
2457             shape = get_conv_output_shape(tshape,
2458                                           kshape,
2459                                           border_mode='full',
2460                                           subsample=subsample,
2461                                           filter_dilation=filter_dilation)
2462             shape = assert_conv_shape(shape)
2463             out = GpuAllocEmpty(dtype=topgrad.dtype, context_name=ctx_name)(*shape)
2464             rval = GpuDnnConv(algo=None, num_groups=num_groups)(topgrad, kerns, out, desc)
2465         else:
2466             return None
2467     return [rval]
2468 @local_optimizer([AbstractConv3d, AbstractConv3d_gradWeights, AbstractConv3d_gradInputs])
2469 def local_abstractconv3d_cudnn_alt(node):
2470     if(not isinstance(node.op, (AbstractConv3d,
2471                                 AbstractConv3d_gradWeights,
2472                                 AbstractConv3d_gradInputs))):
2473         return
2474     if version(raises=False) &lt; 6000 and node.op.filter_dilation != (1, 1, 1):
2475         return None
2476     inp1 = node.inputs[0]
2477     inp2 = node.inputs[1]
2478         return
2479     op = node<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op
2480     border_mode = node.op.border_mode
2481     subsample = node.op.subsample
2482     filter_dilation = node.op.filter_dilation
2483     num_groups = node.op.num_groups
2484     precision, _ =</b></font> get_precision(None, [inp1, inp2])
2485     if node.op.filter_flip:
2486         conv_mode = 'conv'
2487     else:
2488         conv_mode = 'cross'
2489     if isinstance(op, AbstractConv3d):
2490         if border_mode == 'half' or subsample != (1, 1, 1) or num_groups &gt; 1:
2491             return None
2492         if border_mode == 'full':
2493             direction_hint = 'bprop inputs'
2494         elif border_mode == 'valid' and filter_dilation == (1, 1, 1):
2495             direction_hint = 'bprop weights'
2496         else:
2497             return None
2498         rval = dnn_conv3d(inp1, inp2,
2499                           border_mode=border_mode,
2500                           subsample=subsample,
2501                           dilation=filter_dilation,
2502                           direction_hint=direction_hint,
2503     elif isinstance(op, AbstractConv3d_gradWeights):
2504         if(<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>border_mode == 'valid' and subsample == (1, 1, 1) and
2505            filter_dilation == (1, 1, 1) and num_groups == 1):
2506             img = gpu_contiguous(inp1)
2507             topgrad = gpu_contiguous(inp2)
2508             ctx_name = infer_context_name(img, topgrad)
2509             img = gpu_contiguous(img.dimshuffle(</b></font>1, 0, 2, 3, 4))
2510             topgrad = gpu_contiguous(topgrad.dimshuffle(1, 0, 2, 3, 4))
2511             ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
2512             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
2513             out_shp = get_conv_output_shape(ishape,
2514                                             tshape,
2515                                             border_mode=border_mode,
2516                                             subsample=subsample,
2517                                             filter_dilation=filter_dilation)
2518             out_shp = assert_conv_shape(out_shp)
2519             out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
2520             desc = GpuDnnConvDesc(border_mode=border_mode,
2521                                   subsample=subsample,
2522                                   dilation=filter_dilation,
2523                                   conv_mode='cross',
2524                                   num_groups=num_groups,
2525                                   precision=precision)(out.shape)
2526             conv = GpuDnnConv(algo=None, num_groups=num_groups)(
2527                 img, topgrad, out, desc)
2528             if conv_mode == 'conv':
2529                 conv = conv[:, :, ::-1, ::-1, ::-1]
2530             rval = as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)
2531         else:
2532             return None
2533     elif isinstance(op, AbstractConv3d_gradInputs):
2534         if border_mode == 'valid' and subsample == (1, 1, 1) and num_groups == 1:
2535             kerns = gpu_contiguous(inp1.dimshuffle(1, 0, 2, 3, 4))
2536             topgrad = gpu_contiguous(inp2)
2537             ctx_name = infer_context_name(kerns, topgrad)
2538             conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
2539             desc = GpuDnnConvDesc(border_mode='full',
2540                                   subsample=subsample,
2541                                   dilation=filter_dilation,
2542                                   conv_mode=conv_mode,
2543                                   num_groups=num_groups,
2544                                   precision=precision)(kerns.shape)
2545             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
2546             kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
2547             shape = get_conv_output_shape(tshape,
2548                                           kshape,
2549                                           border_mode='full',
2550                                           subsample=subsample,
2551                                           filter_dilation=filter_dilation)
2552             shape = assert_conv_shape(shape)
2553             out = GpuAllocEmpty(dtype=topgrad.dtype, context_name=ctx_name)(*shape)
2554             rval = GpuDnnConv(algo=None, num_groups=num_groups)(
2555                 topgrad, kerns, out, desc)
2556         else:
2557             return None
2558     return [rval]
2559 @local_optimizer([AbstractConv2d_gradWeights, AbstractConv3d_gradWeights])
2560 def local_abstractconv_gw_cudnn(node):
2561     ctx = infer_context_name(*node.inputs)
2562     if not isinstance(node.inputs[0].type, GpuArrayType):
2563         return
2564     if node.op.unshared:
2565         return None
2566     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
2567         return None
2568     if isinstance(node.op, AbstractConv2d_gradWeights):
2569         with inherit_stack_trace(node.outputs):
2570             return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2571     elif isinstance(node.op, AbstractConv3d_gradWeights):
2572         with inherit_stack_trace(node.outputs):
2573             return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2574 @local_optimizer([AbstractConv2d_gradInputs, AbstractConv3d_gradInputs])
2575 def local_abstractconv_gi_cudnn(node):
2576     ctx = infer_context_name(*node.inputs)
2577     if not isinstance(node.inputs[0].type, GpuArrayType):
2578         return
2579     if node.op.unshared:
2580         return None
2581     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
2582         return None
2583     if isinstance(node.op, AbstractConv2d_gradInputs):
2584         with inherit_stack_trace(node.outputs):
2585             return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2586     elif isinstance(node.op, AbstractConv3d_gradInputs):
2587         with inherit_stack_trace(node.outputs):
2588             return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2589 @inplace_allocempty(GpuDnnConv, 2)
2590 def local_dnn_conv_inplace(node, inputs):
2591     return [GpuDnnConv(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
2592 @inplace_allocempty(GpuDnnConvGradW, 2)
2593 def local_dnn_convgw_inplace(node, inputs):
2594     return [GpuDnnConvGradW(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
2595 @inplace_allocempty(GpuDnnConvGradI, 2)
2596 def local_dnn_convgi_inplace(node, inputs):
2597     return [GpuDnnConvGradI(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
2598 optdb.register('local_dnna_conv_inplace',
2599                tensor.opt.in2out(local_dnn_conv_inplace,
2600                                  local_dnn_convgw_inplace,
2601                                  local_dnn_convgi_inplace,
2602                                  name="local_dnna_conv_inplace"),
2603                70.0, 'fast_run', 'inplace', 'gpuarray', 'cudnn')
2604 @register_opt('cudnn')
2605 @alpha_merge(GpuDnnConv, alpha_in=4, beta_in=5)
2606 def local_dnn_conv_alpha_merge(node, *inputs):
2607     return [GpuDnnConv(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2608 @register_opt('cudnn')
2609 @alpha_merge(GpuDnnConvGradW, alpha_in=4, beta_in=5)
2610 def local_dnn_convw_alpha_merge(node, *inputs):
2611     return [GpuDnnConvGradW(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2612 @register_opt('cudnn')
2613 @alpha_merge(GpuDnnConvGradI, alpha_in=4, beta_in=5)
2614 def local_dnn_convi_alpha_merge(node, *inputs):
2615     return [GpuDnnConvGradI(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2616 @register_opt('cudnn')
2617 @output_merge(GpuDnnConv, alpha_in=4, beta_in=5, out_in=2)
2618 def local_dnn_conv_output_merge(node, *inputs):
2619     inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
2620     return [GpuDnnConv(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2621 @register_opt('cudnn')
2622 @output_merge(GpuDnnConvGradW, alpha_in=4, beta_in=5, out_in=2)
2623 def local_dnn_convw_output_merge(node, *inputs):
2624     inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
2625     return [GpuDnnConvGradW(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2626 @register_opt('cudnn')
2627 @output_merge(GpuDnnConvGradI, alpha_in=4, beta_in=5, out_in=2)
2628 def local_dnn_convi_output_merge(node, *inputs):
2629     inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
2630     return [GpuDnnConvGradI(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2631 def local_gpua_pool_dnn_alternative(op, ctx_name, inputs, outputs):
2632     if not dnn_available(ctx_name):
2633         return
2634     if not op.ignore_border:
2635         return
2636     img, ws, stride, pad = inputs
2637     nd = op.ndim
2638     if nd not in (2, 3):
2639         return
2640     img = gpu_contiguous(as_gpuarray_variable(img, ctx_name))
2641     mode = op.mode
2642     if img.ndim == nd + 2:
2643         return dnn_pool(img, ws, stride=stride, pad=pad, mode=mode)
2644     else:
2645         img_padded = pad_dims(img, 2, nd)
2646         ret_padded = dnn_pool(img_padded, ws, stride=stride, pad=pad, mode=mode)
2647         return unpad_dims(ret_padded, img, 2, nd)
2648 pool_db.register("local_gpua_pool_dnn_alternative",
2649                  op_lifter([Pool])(local_gpua_pool_dnn_alternative),
2650                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2651                  position=0)
2652 pool_db2.register("local_gpua_pool_dnn_alternative",
2653                   local_optimizer([Pool])(local_gpua_pool_dnn_alternative),
2654                   'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2655                   position=0)
2656 def local_gpua_pool_dnn_grad_stride(op, ctx_name, inputs, outputs):
2657     if not dnn_available(ctx_name):
2658         return
2659     if not op.ignore_border:
2660         return
2661     inp, out, out_grad, ws, stride, pad = inputs
2662     nd = op.ndim
2663     if nd not in (2, 3):
2664         return
2665     inp = gpu_contiguous(as_gpuarray_variable(inp, ctx_name))
2666     out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
2667     out_grad = gpu_contiguous(as_gpuarray_variable(out_grad, ctx_name))
2668     mode = op.mode
2669     if inp.ndim == nd + 2:
2670         return GpuDnnPoolGrad(mode=mode)(inp,
2671                                          out,
2672                                          out_grad,
2673                                          ws,
2674                                          stride,
2675                                          pad)
2676     else:
2677         inp_padded = pad_dims(inp, 2, nd)
2678         out_padded = pad_dims(out, 2, nd)
2679         out_grad_padded = pad_dims(out_grad, 2, nd)
2680         ret_padded = GpuDnnPoolGrad(mode=mode)(inp_padded,
2681                                                out_padded,
2682                                                out_grad_padded,
2683                                                ws,
2684                                                stride,
2685                                                pad)
2686         return unpad_dims(ret_padded, inp, 2, nd)
2687 pool_db.register("local_gpua_pool_dnn_grad_stride",
2688                  op_lifter([MaxPoolGrad])(local_gpua_pool_dnn_grad_stride),
2689                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2690                  position=0)
2691 pool_db2.register("local_gpua_pool_dnn_grad_stride",
2692                   local_optimizer([MaxPoolGrad])(local_gpua_pool_dnn_grad_stride),
2693                   'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2694                   position=0)
2695 def local_gpua_avg_pool_dnn_grad_stride(op, ctx_name, inputs, outputs):
2696     if not dnn_available(ctx_name):
2697         return
2698     if not op.ignore_border:
2699         return
2700     inp, out_grad, ws, stride, pad = inputs
2701     nd = op.ndim
2702     if nd not in (2, 3):
2703         return
2704     inp = gpu_contiguous(as_gpuarray_variable(inp, ctx_name))
2705     out_grad = gpu_contiguous(as_gpuarray_variable(out_grad, ctx_name))
2706     mode = op.mode
2707     if inp.ndim == nd + 2:
2708         return GpuDnnPoolGrad(mode=mode)(inp, out_grad, out_grad, ws, stride, pad)
2709     else:
2710         inp_padded = pad_dims(inp, 2, nd)
2711         out_grad_padded = pad_dims(out_grad, 2, nd)
2712         ret_padded = GpuDnnPoolGrad(mode=mode)(inp_padded,
2713                                                out_grad_padded,
2714                                                out_grad_padded,
2715                                                ws,
2716                                                stride,
2717                                                pad)
2718         return unpad_dims(ret_padded, inp, 2, nd)
2719 pool_db.register("local_gpua_avg_pool_dnn_grad_stride",
2720                  op_lifter([AveragePoolGrad])(local_gpua_avg_pool_dnn_grad_stride),
2721                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2722                  position=0)
2723 pool_db2.register("local_gpua_avg_pool_dnn_grad_stride",
2724                   local_optimizer([AveragePoolGrad])(local_gpua_avg_pool_dnn_grad_stride),
2725                   'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2726                   position=0)
2727 @register_opt('cudnn', 'fast_compile')
2728 @local_optimizer([GpuSoftmax])
2729 def local_softmax_dnn(node):
2730     if isinstance(node.op, GpuSoftmax):
2731         if not dnn_available(node.outputs[0].type.context_name):
2732             return
2733         ins = node.inputs[0].dimshuffle(0, 1, 'x', 'x')
2734         ins = gpu_contiguous(ins)
2735         out = GpuDnnSoftmax('accurate', 'channel')(ins)
2736         out = as_gpuarray_variable(out.dimshuffle(0, 1), out.type.context_name)
2737         return [out]
2738 @register_opt('cudnn', 'stabilize')
2739 def local_log_softmax_dnn(node):
2740     if (isinstance(node<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op, GpuElemwise) and
2741             isinstance(node.op.scalar_op, Log) and
2742             node.inputs[0].owner and
2743             isinstance(node.inputs[0].owner.</b></font>op, GpuDnnSoftmax) and
2744             len(node.inputs[0].clients) == 1):
2745         softmax_node = node.inputs[0].owner
2746         new_softmax = GpuDnnSoftmax('log', softmax_node.op.mode)
2747         return [new_softmax(softmax_node.inputs[0])]
2748 @register_opt('cudnn', 'fast_compile')
2749 @op_lifter([LogSoftmax])
2750 @register_opt2([LogSoftmax], 'fast_compile', 'cudnn')
2751 def local_gpua_logsoftmax_to_dnn(op, ctx_name, inputs, outputs):
2752     inp = inputs[0]
2753     if inp.ndim != 2:
2754         return
2755     if not dnn_available(ctx_name):
2756         return
2757     inp = inp.dimshuffle(0, 1, 'x', 'x')
2758     inp.tag.context_name = ctx_name
2759     out = GpuDnnSoftmax('log', 'channel')(gpu_contiguous(inp))
2760     return [out.dimshuffle(0, 1)]
2761 @register_opt('cudnn', 'fast_compile')
2762 @op_lifter([SoftmaxGrad])
2763 @register_opt2([SoftmaxGrad], 'cudnn', 'fast_compile')
2764 def local_gpua_softmax_dnn_grad(op, ctx_name, inputs, outputs):
2765     if not dnn_available(ctx_name):
2766         return
2767     ins = []
2768     for n in inputs:
2769         n = as_gpuarray_variable(n, ctx_name)
2770         if n.ndim != 2:
2771             return
2772         ins.append(n.dimshuffle(0, 'x', 1, 'x'))
2773     out = GpuDnnSoftmaxGrad('accurate', 'instance')(
2774         gpu_contiguous(ins[0]), gpu_contiguous(ins[1]))
2775     return [out.dimshuffle(0, 2)]
2776 @register_opt('cudnn')
2777 @local_optimizer([GpuCAReduceCuda])
2778 def local_dnn_reduction(node):
2779     if not isinstance(node.op, GpuCAReduceCuda):
2780         return
2781     if not dnn_available(node.inputs[0].type.context_name):
2782         return
2783     if version(raises=False) &lt; 6000:
2784         return
2785     if node.inputs[0].ndim &gt; 8:
2786         return
2787     acc_dtype = node.op._acc_dtype(node.inputs[0].dtype)
2788     if node.inputs[0].dtype != node.outputs[0].dtype:
2789         if (node.inputs[0].dtype == 'float64' or
2790                 node.outputs[0].dtype == 'float64'):
2791             return
2792         if acc_dtype != 'float32':
2793             return
2794     if node.inputs[0].dtype not in ['float16', 'float32', 'float64']:
2795         return
2796     if (node.inputs[0].dtype == 'float64' and acc_dtype != 'float64'):
2797         return
2798     if (node.inputs[0].dtype == 'float32' and acc_dtype != 'float32'):
2799         return
2800     if (node.inputs[0].dtype == 'float16' and acc_dtype == 'float64'):
2801         return
2802     def _identity(a):
2803         return a
2804     def _square(a):
2805         return GpuElemwise(theano.scalar.basic.sqr)(a)
2806     scal = node.op.scalar_op.name
2807     post = _identity
2808     if node.op.pre_scalar_op is not None:
2809         if isinstance(node.op.scalar_op, theano.scalar.basic.Add):
2810             if isinstance(node.op.pre_scalar_op, theano.scalar.basic.Sqr):
2811                 scal = 'norm2'
2812                 post = _square
2813             elif isinstance(node.op.pre_scalar_op, theano.scalar.basic.Abs):
2814             else:
2815                 return
2816         elif (<font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>isinstance(node.op.scalar_op, theano.scalar.basic.Maximum) and
2817                 isinstance(node.op.pre_scalar_op, theano.scalar.basic.</b></font>Abs)):
2818             scal = 'absmax'
2819         else:
2820             return
2821     if not cudnn.cudnnReduceTensorOp_t.has_alias(scal):
2822         return
2823     with inherit_stack_trace(node.outputs):
2824         ret = GpuDnnReduction(scal,
2825                               node.op.axis,
2826                               acc_dtype,
2827                               node.op.dtype,
2828                               False)(node.inputs[0])
2829         return [post(ret)]
2830 @register_opt('cudnn')
2831 @local_optimizer([GpuMaxAndArgmax])
2832 def local_cudnn_maxandargmax(node):
2833     if not isinstance(node.op, GpuMaxAndArgmax):
2834         return
2835     if not dnn_available(node.inputs[0].type.context_name):
2836         return
2837     if version(raises=False) &lt; 6000:
2838         return
2839     if node.inputs[0].ndim &gt; 8:
2840         return
2841     if node.inputs[0].dtype != node.outputs[0].dtype:
2842         return
2843     if node.inputs[0].dtype not in ['float16', 'float32', 'float64']:
2844         return
2845     if (node.op.axis is not None and
2846         return
2847     max, arg = GpuDnnReduction<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('maximum', node.op.axis, node.outputs[0].dtype,
2848                                node.outputs[0].dtype, True)(node.inputs[</b></font>0])
2849     return (max, as_gpuarray_variable(arg.astype('int64'),
2850                                       node.outputs[1].type.context_name))
2851 @register_opt('cudnn', 'fast_compile')
2852 @op_lifter([Argmax])
2853 @register_opt2([Argmax], 'fast_compile', 'cudnn')
2854 def local_dnn_argmax(op, ctx_name, inputs, outputs):
2855     if not dnn_available(ctx_name):
2856         return
2857     if version(raises=False) &lt; 6000:
2858         return
2859     if inputs[0].ndim &gt; 8:
2860         return
2861     if inputs[0].dtype not in ['float16', 'float32', 'float64']:
2862         return
2863     if op.axis is not None and tuple(sorted(op.axis)) != op.axis:
2864         return
2865     max, arg = GpuDnnReduction('maximum', op.axis, inputs[0].dtype,
2866                                inputs[0].dtype, True)(*inputs)
2867     return [as_gpuarray_variable(arg.astype('int64'), ctx_name)]
2868 class NoCuDNNRaise(Optimizer):
2869     def apply(self, fgraph):
2870         """
2871         Raise a error if cudnn can't be used.
2872         """
2873         for c in list_contexts():
2874             if not dnn_available(c):
2875                 raise AssertionError(
2876                     "cuDNN optimization was enabled, but Theano was not able "
2877                     "to use it for context " + str(c) + ". We got this error: \n" +
2878                     dnn_available.msg)
2879 gpu_seqopt.register("NoCuDNNRaise", NoCuDNNRaise(), 0, 'cudnn')
2880 def local_abstract_batch_norm_train_cudnn(op, ctx_name, inputs, outputs):
2881     x, scale, bias, epsilon, running_average_factor = inputs[:5]
2882     running_mean = inputs[5] if len(inputs) &gt; 5 else None
2883     running_var = inputs[6] if len(inputs) &gt; 6 else None
2884     axes = tuple(op.axes)
2885     if axes == (0,):
2886         mode = 'per-activation'
2887     elif axes == (0,) + tuple(range(2, x.ndim)):
2888         mode = 'spatial'
2889     else:
2890         return None
2891     try:
2892         eps = theano.tensor.get_scalar_constant_value(epsilon)
2893     except theano.tensor.NotScalarConstantError:
2894         return None
2895     if eps &lt; 1e-5:
2896         return None
2897     try:
2898         running_average_factor = theano.tensor.get_scalar_constant_value(running_average_factor)
2899     except theano.tensor.NotScalarConstantError:
2900         return None
2901     ctx = infer_context_name(*inputs)
2902     if not dnn_available(ctx):
2903         return
2904     x = as_gpuarray_variable(x, context_name=ctx)
2905     scale = as_gpuarray_variable(scale, context_name=ctx)
2906     bias = as_gpuarray_variable(bias, context_name=ctx)
2907     inputs = [x, scale, bias, mode, eps, running_average_factor]
2908     if running_mean is not None and running_var is not None:
2909         inputs.append(running_mean)
2910         inputs.append(running_var)
2911     results = list(dnn_batch_normalization_train(*inputs))
2912     return results
2913 @register_inplace()
2914 @local_optimizer([GpuDnnBatchNorm], inplace=True)
2915 def local_batch_norm_inplace_output(node):
2916     if isinstance(node.op, GpuDnnBatchNorm) and not node.op.inplace_output:
2917         return GpuDnnBatchNorm(mode=node.op.mode,
2918                                running_averages=node.op.running_averages,
2919                                inplace_running_mean=node.op.inplace_running_mean,
2920                                inplace_running_var=node.op.inplace_running_var,
2921                                inplace_output=True)(*node.inputs)
2922 @register_inplace()
2923 @local_optimizer([GpuDnnBatchNorm], inplace=True)
2924 def local_batch_norm_inplace_running_mean(node):
2925     if isinstance(node.op, GpuDnnBatchNorm) and node.op.running_averages and not node.op.inplace_running_mean:
2926         return GpuDnnBatchNorm(mode=node.op.mode,
2927                                running_averages=node.op.running_averages,
2928                                inplace_running_mean=True,
2929                                inplace_running_var=node.op.inplace_running_var,
2930                                inplace_output=node.op.inplace_output)(*node.inputs)
2931 @register_inplace()
2932 @local_optimizer([GpuDnnBatchNorm], inplace=True)
2933 def local_batch_norm_inplace_running_var(node):
2934     if isinstance(node.op, GpuDnnBatchNorm) and node.op.running_averages and not node.op.inplace_running_var:
2935         return GpuDnnBatchNorm(mode=node.op.mode,
2936                                running_averages=node.op.running_averages,
2937                                inplace_running_mean=node.op.inplace_running_mean,
2938                                inplace_running_var=True,
2939                                inplace_output=node.op.inplace_output)(*node.inputs)
2940 @register_inplace()
2941 @local_optimizer([GpuDnnBatchNormInference], inplace=True)
2942 def local_batch_norm_inference_inplace(node):
2943     if isinstance(node.op, GpuDnnBatchNormInference) and not node.op.inplace:
2944         return [GpuDnnBatchNormInference(mode=node.op.mode, inplace=True)(*node.inputs)]
2945 def local_abstract_batch_norm_train_grad_cudnn(op, ctx_name, inputs, outputs):
2946     x, dy, scale, x_mean, x_invstd, epsilon = inputs
2947     x_on_gpu = (isinstance(x.type, GpuArrayType) or
2948                 (x.owner and isinstance(x.owner.op, HostFromGpu)))
2949     dy_on_gpu = (isinstance(dy.type, GpuArrayType) or
2950                  (dy.owner and isinstance(dy.owner.op, HostFromGpu)))
2951     if not (x_on_gpu or dy_on_gpu):
2952         return None
2953     axes = tuple(op.axes)
2954     if axes == (0,):
2955         mode = 'per-activation'
2956     elif axes == (0,) + tuple(range(2, x.ndim)):
2957         mode = 'spatial'
2958     else:
2959         return None
2960     ndim = x.ndim
2961     if ndim &lt; 4:
2962         x <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.shape_padright(x, 4 - ndim)
2963         dy = theano.tensor.shape_padright(dy, 4 - ndim)
2964         scale = theano.tensor.shape_padright(scale, 4 - ndim)
2965         x_mean = theano.tensor.shape_padright(x_mean, 4 - ndim)
2966         x_invstd =</b></font> theano.tensor.shape_padright(x_invstd, 4 - ndim)
2967         x_shape = x.shape
2968         params_shape = scale.shape
2969         x <font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.flatten(x, 5)
2970         dy = theano.tensor.flatten(dy, 5)
2971         scale = theano.tensor.flatten(scale, 5)
2972         x_mean =</b></font> theano.tensor.flatten(x_mean, 5)
2973         x_invstd = theano.tensor.flatten(x_invstd, 5)
2974     try:
2975         eps = theano.tensor.get_scalar_constant_value(epsilon)
2976     except theano.tensor.NotScalarConstantError:
2977         return None
2978     if eps &lt; 1e-5:
2979         return None
2980     if not dnn_available(ctx):
2981         return
2982     x <font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= as_gpuarray_variable(x, context_name=ctx)
2983     dy = as_gpuarray_variable(dy, context_name=ctx)
2984     scale = as_gpuarray_variable(scale, context_name=ctx)
2985     x_mean = as_gpuarray_variable(x_mean, context_name=</b></font>ctx)
2986     x_invstd = as_gpuarray_variable(x_invstd, context_name=ctx)
2987     g_wrt_inputs, g_wrt_scale, g_wrt_bias = \
2988     if ndim &lt; 4:
2989         g_wrt_inputs = theano.tensor<font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.flatten(g_wrt_inputs, ndim)
2990         g_wrt_scale = theano.tensor.flatten(g_wrt_scale, ndim)
2991         g_wrt_bias = theano.tensor.flatten(g_wrt_bias, ndim)
2992     elif ndim &gt; 5:
2993         g_wrt_inputs = theano.tensor.</b></font>reshape(g_wrt_inputs, x_shape)
2994         g_wrt_scale = theano.tensor.reshape(g_wrt_scale, params_shape)
2995         g_wrt_bias = theano.tensor.reshape(g_wrt_bias, params_shape)
2996     return [g_wrt_inputs, g_wrt_scale, g_wrt_bias]
2997 def local_abstract_batch_norm_inference_cudnn(op, ctx_name, inputs, outputs):
2998     x, scale, bias, estimated_mean, estimated_variance, epsilon = inputs
2999     axes = tuple(op.axes)
3000     if axes == (0,):
3001         mode = 'per-activation'
3002     elif axes == (0,) + tuple(range(2, x.ndim)):
3003         mode = 'spatial'
3004     else:
3005         return None
3006     try:
3007         eps = theano.tensor.get_scalar_constant_value(epsilon)
3008     except theano.tensor.NotScalarConstantError:
3009         return None
3010     if eps &lt; 1e-5:
3011         return None
3012     ctx = infer_context_name(*inputs)
3013     if not dnn_available(ctx):
3014         return
3015     x = as_gpuarray_variable(x, context_name=ctx)
3016     scale = as_gpuarray_variable(scale, context_name=ctx)
3017     bias = as_gpuarray_variable(bias, context_name=ctx)
3018     estimated_mean = as_gpuarray_variable(estimated_mean, context_name=ctx)
3019     estimated_variance = as_gpuarray_variable(estimated_variance, context_name=ctx)
3020     out = dnn_batch_normalization_test(x, scale, bias, estimated_mean, estimated_variance,
3021                                        mode, eps)
3022     return [out]
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_basic_3.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from copy import copy, deepcopy
2 from functools import partial
3 import itertools
4 import logging
5 from nose.plugins.skip import SkipTest
6 from nose.tools import assert_raises
7 import operator
8 import os
9 import sys
10 from tempfile import mkstemp
11 import unittest
12 import warnings
13 from six import iteritems
14 from six.moves import StringIO, reduce
15 from six.moves import xrange
16 from six.moves.builtins import min as builtin_min
17 import numpy as np
18 from numpy.testing import dec, assert_array_equal, assert_allclose
19 import theano
20 from theano.compat import izip
21 from theano.compat import PY3, exc_message, operator_div
22 from theano import compile, config, function, gof, tensor, shared
23 from theano.compile import DeepCopyOp
24 from theano.compile.mode import get_default_mode
25 from theano.scalar import autocast_float_as, autocast_float
26 from theano.tensor import (
27     wvector, bvector,
28     argmin, max_and_argmax, cscalar, join,
29     horizontal_stack, vertical_stack, argmax, get_vector_length,
30     fscalar, sum, tensor3, vector, add, addbroadcast,
31     alloc, as_tensor_variable, tensor_from_scalar, ARange,
32     clip, constant, default, diag, dot, batched_dot,
33     dmatrix, dscalar, dvector, eq, eye, fill, flatten, inverse_permutation,
34     tensor4, permute_row_elements, fmatrix, fscalars, grad,
35     inplace, iscalar, matrix, minimum, matrices, maximum, mul, neq,
36     Reshape, row, scalar, scalars, second, smallest, stack, sub, Tensor,
37     tensor_copy, tensordot, TensorType, Tri, tri, tril, triu, unbroadcast,
38     var, Argmax, Join, shape, MaxAndArgmax, lscalar, zvector, exp,
39     get_scalar_constant_value, ivector, reshape, scalar_from_tensor, scal,
40     iscalars, arange, dscalars, fvector, imatrix, numeric_grad,
41     opt, lvector, true_div, max, min, Split, roll,
42     tile, patternbroadcast, Eye, Shape, Dot, PermuteRowElements,
43     ScalarFromTensor, TensorFromScalar, dtensor4, Rebroadcast, Alloc,
44     dtensor3, SpecifyShape, Mean,
45     itensor3, Tile, switch, ExtractDiag, AllocDiag,
46     nonzero, flatnonzero, nonzero_values,
47     stacklists, DimShuffle, hessian, ptp, power,
48     swapaxes, choose, Choose, NoneConst, AllocEmpty,
49     isclose, allclose, mgrid, ogrid, extract_constant,
50     )
51 from theano.tests import unittest_tools as utt
52 from theano.tests.unittest_tools import attr
53 from theano import change_flags
54 imported_scipy_special =</b></font> False
55 mode_no_scipy = get_default_mode()
56 try:
57     import scipy.special
58     import scipy.stats
59     imported_scipy_special = True
60 except ImportError:
61     if config.mode == "FAST_COMPILE":
62         mode_no_scipy = "FAST_RUN"
63 floatX = config.floatX
64 if config.mode == "FAST_COMPILE":
65     mode_opt = "FAST_RUN"
66 else:
67     mode_opt = get_default_mode()
68 utt.seed_rng()
69 test_rng = np.random.RandomState(seed=utt.fetch_seed())
70 if PY3:
71     def L(i):
72         return i
73 else:
74     def L(i):
75         return long(i)  # noqa for Python 3
76 def inplace_func(inputs, outputs, mode=None, allow_input_downcast=False,
77                  on_unused_input='raise', name=None):
78     if mode is None:
79         mode = get_default_mode()
80     return function(inputs, outputs,
81                     mode=mode,
82                     allow_input_downcast=allow_input_downcast,
83                     accept_inplace=True,
84                     on_unused_input=on_unused_input,
85                     name=name)
86 def eval_outputs(outputs, ops=(), mode=None):
87     f = inplace_func([], outputs, mode=mode)
88     variables = f()
89     if ops:
90         assert any(isinstance(node.op, ops) for node in f.maker.fgraph.apply_nodes)
91     if isinstance(variables, (tuple, list)) and len(variables) == 1:
92         return variables[0]
93     return variables
94 def get_numeric_subclasses(cls=np.number, ignore=None):
95     if ignore is None:
96         ignore = []
97     rval = []
98     dtype = np.dtype(cls)
99     dtype_num = dtype.num
100     if dtype_num not in ignore:
101         np.array(0, dtype=dtype)
102         rval.append(cls)
103         ignore.append(dtype_num)
104     for sub_ in cls.__subclasses__():
105         rval += [c for c in get_numeric_subclasses(sub_, ignore=ignore)]
106     return rval
107 def get_numeric_types(with_int=True, with_float=True, with_complex=False,
108                       only_theano_types=True):
109     if only_theano_types:
110         theano_types = [d.dtype for d in theano.scalar.all_types]
111     rval = []
112     def is_within(cls1, cls2):
113         return (cls1 is cls2 or
114                 issubclass(cls1, cls2) or
115                 isinstance(np.array([0], dtype=cls1)[0], cls2))
116     for cls in get_numeric_subclasses():
117         dtype = np.dtype(cls)
118         if ((not with_complex and is_within(cls, np.complexfloating)) or
119                 (not with_int and is_within(cls, np.integer)) or
120                 (not with_float and is_within(cls, np.floating)) or
121                 (only_theano_types and dtype not in theano_types)):
122             continue
123         rval.append([str(dtype), dtype, dtype.num])
124     return [x[1] for x in sorted(rval, key=str)]
125 def _numpy_checker(x, y):
126     x, y = x[0], y[0]
127     if (x.dtype != y.dtype or x.shape != y.shape or
128             np.any(np.abs(x - y) &gt; 1e-10)):
129         raise Exception("Output mismatch.", {'performlinker': x, 'clinker': y})
130 def safe_make_node(op, *inputs):
131     node = op(*inputs)
132     if isinstance(node, list):
133         return node[0].owner
134     else:
135         return node.owner
136 def upcast_float16_ufunc(fn):
137     def ret(*args, **kwargs):
138         out_dtype = np.find_common_type(
139             [a.dtype for a in args], [np.float16])
140         if out_dtype == 'float16':
141             sig = 'f' * fn.nin + '-&gt;' + 'f' * fn.nout
142             kwargs.update(sig=sig)
143         return fn(*args, **kwargs)
144     return ret
145 def upcast_int8_nfunc(fn):
146     def ret(*args, **kwargs):
147         args = list(args)
148         for i, a in enumerate(args):
149             if getattr(a, 'dtype', None) in ('int8', 'uint8'):
150                 args[i] = a.astype('float32')
151         return fn(*args, **kwargs)
152     return ret
153 def makeTester(name, op, expected, checks=None, good=None, bad_build=None,
154                bad_runtime=None, grad=None, mode=None, grad_rtol=None,
155                eps=1e-10, skip=False, test_memmap=True, check_name=True,
156                grad_eps=None):
157     if checks is None:
158         checks = {}
159     if good is None:
160         good = {}
161     if bad_build is None:
162         bad_build = {}
163     if bad_runtime is None:
164         bad_runtime = {}
165     if grad is None:
166         grad = {}
167     if grad is True:
168         grad = good
169     _op, _expected, _checks, _good = op, expected, checks, good
170     _bad_build, _bad_runtime, _grad = bad_build, bad_runtime, grad
171     _mode, _grad_rtol, _eps, skip_ = mode, grad_rtol, eps, skip
172     _test_memmap = test_memmap
173     _check_name = check_name
174     _grad_eps = grad_eps
175     class Checker(unittest.TestCase):
176         op = staticmethod(_op)
177         expected = staticmethod(_expected)
178         checks = _checks
179         check_name = _check_name
180         good = _good
181         bad_build = _bad_build
182         bad_runtime = _bad_runtime
183         grad = _grad
184         mode = _mode
185         skip = skip_
186         test_memmap = _test_memmap
187         def setUp(self):
188             if self.check_name:
189                 eval(self.__class__.__module__ + '.' + self.__class__.__name__)
190             self.tmp_files = []
191         def add_memmap_values(self, val_dict):
192             if not self.test_memmap:
193                 return val_dict
194             val_dict = val_dict.copy()
195             for k, v in sorted(val_dict.items()):
196                 new_k = '_'.join((k, 'memmap'))
197                 if new_k in val_dict:
198                     break
199                 new_v = []
200                 for inp in v:
201                     if type(inp) is np.ndarray and inp.size &gt; 0:
202                         f, fname = mkstemp()
203                         self.tmp_files.append((f, fname))
204                         new_inp = np.memmap(fname, dtype=inp.dtype,
205                                             mode='w+', shape=inp.shape)
206                         new_inp[...] = inp[...]
207                         new_v.append(new_inp)
208                     else:
209                         new_v.append(inp)
210                 val_dict[new_k] = new_v
211                 break
212             return val_dict
213         def tearDown(self):
214             import gc
215             gc.collect()
216             for f, fname in self.tmp_files:
217                 os.close(f)
218                 os.remove(fname)
219         def test_good(self):
220             if skip:
221                 raise SkipTest(skip)
222             good = self.add_memmap_values(self.good)
223             for testname, inputs in iteritems(good):
224                 inputs = [copy(input) for input in inputs]
225                 inputrs = [TensorType(
226                     dtype=input.dtype,
227                     broadcastable=[shape_elem == 1
228                                    for shape_elem in input.shape]
229                     )() for input in inputs]
230                 try:
231                     node = safe_make_node(self.op, *inputrs)
232                 except Exception as exc:
233                     err_msg = ("Test %s::%s: Error occurred while"
234                                " making a node with inputs %s") % (
235                                    self.op, testname, inputs)
236                     exc.args += (err_msg,)
237                     raise
238                 try:
239                     f = inplace_func(inputrs, node.outputs, mode=mode, name='test_good')
240                 except Exception as exc:
241                     err_msg = ("Test %s::%s: Error occurred while"
242                                " trying to make a Function") % (self.op, testname)
243                     exc.args += (err_msg,)
244                     raise
245                 if (isinstance(self.expected, dict) and
246                         testname in self.expected):
247                     expecteds = self.expected[testname]
248                     eps = 5e-9
249                 else:
250                     expecteds = self.expected(*inputs)
251                     eps = 1e-10
252                 if any([i.dtype in ('float32', 'int8', 'uint8', 'uint16')
253                         for i in inputs]):
254                     eps = 1e-6
255                 eps = np.max([eps, _eps])
256                 try:
257                     variables = f(*inputs)
258                 except Exception as exc:
259                     err_msg = ("Test %s::%s: Error occurred while calling"
260                                " the Function on the inputs %s") % (
261                                    self.op, testname, inputs)
262                     exc.args += (err_msg,)
263                     raise
264                 if not isinstance(expecteds, (list, tuple)):
265                     expecteds = (expecteds, )
266                 for i, (variable, expected) in enumerate(
267                         izip(variables, expecteds)):
268                     if (variable.dtype != expected.dtype or
269                             variable.shape != expected.shape or
270                             not np.allclose(variable, expected,
271                                             atol=eps, rtol=eps)):
272                         self.fail(("Test %s::%s: Output %s gave the wrong"
273                                    " value. With inputs %s, expected %s (dtype %s),"
274                                    " got %s (dtype %s). eps=%f"
275                                    " np.allclose returns %s %s") % (
276                             self.op,
277                             testname,
278                             i,
279                             inputs,
280                             expected,
281                             expected.dtype,
282                             variable,
283                             variable.dtype,
284                             eps,
285                             np.allclose(variable, expected,
286                                         atol=eps, rtol=eps),
287                             np.allclose(variable, expected)))
288                 for description, check in iteritems(self.checks):
289                     if not check(inputs, variables):
290                         self.fail(("Test %s::%s: Failed check: %s (inputs"
291                                    " were %s, outputs were %s)") % (
292                             self.op, testname, description,
293                             inputs, variables))
294         def test_bad_build(self):
295             if skip:
296                 raise SkipTest(skip)
297             for testname, inputs in iteritems(self.bad_build):
298                 inputs = [copy(input) for input in inputs]
299                 inputrs = [shared(input) for input in inputs]
300                 self.assertRaises(Exception,
301                                   safe_make_node, self.op, *inputrs)
302         @change_flags(compute_test_value='off')
303         def test_bad_runtime(self):
304             if skip:
305                 raise SkipTest(skip)
306             for testname, inputs in iteritems(self.bad_runtime):
307                 inputrs = [shared(input) for input in inputs]
308                 try:
309                     node = safe_make_node(self.op, *inputrs)
310                 except Exception as exc:
311                     err_msg = ("Test %s::%s: Error occurred while trying"
312                                " to make a node with inputs %s") % (
313                         self.op, testname, inputs)
314                     exc.args += (err_msg,)
315                     raise
316                 try:
317                     f = inplace_func([], node.outputs, mode=mode, name="test_bad_runtime")
318                 except Exception as exc:
319                     err_msg = ("Test %s::%s: Error occurred while trying"
320                                " to make a Function") % (self.op, testname)
321                     exc.args += (err_msg,)
322                     raise
323                 self.assertRaises(Exception, f, [])
324         def test_grad(self):
325             if skip:
326                 raise SkipTest(skip)
327             backup = config.warn.sum_div_dimshuffle_bug
328             config.warn.sum_div_dimshuffle_bug = False
329             try:
330                 for testname, inputs in iteritems(self.grad):
331                     inputs = [copy(input) for input in inputs]
332                     try:
333                         utt.verify_grad(self.op, inputs,
334                                         mode=self.mode,
335                                         rel_tol=_grad_rtol,
336                                         eps=_grad_eps)
337                     except Exception as exc:
338                         err_msg = ("Test %s::%s: Error occurred while"
339                                    " computing the gradient on the following"
340                                    " inputs: %s") % (self.op, testname, inputs)
341                         exc.args += (err_msg,)
342                         raise
343             finally:
344                 config.warn.sum_div_dimshuffle_bug = backup
345         def test_grad_none(self):
346             if skip:
347                 raise SkipTest(skip)
348             if not hasattr(self.op, 'grad'):
349                 return
350             for testname, inputs in iteritems(self.good):
351                 inputs = [copy(input) for input in inputs]
352                 inputrs = [TensorType(
353                     dtype=input.dtype,
354                     broadcastable=[shape_elem == 1
355                                    for shape_elem in input.shape]
356                     )() for input in inputs]
357                 if (isinstance(self.expected, dict) and
358                         testname in self.expected):
359                     expecteds = self.expected[testname]
360                 else:
361                     expecteds = self.expected(*inputs)
362                 if not isinstance(expecteds, (list, tuple)):
363                     expecteds = (expecteds, )
364                 out_grad_vars = []
365                 for out in expecteds:
366                     if str(out.dtype) in tensor.discrete_dtypes:
367                         dtype = floatX
368                     else:
369                         dtype = str(out.dtype)
370                     bcast = [shape_elem == 1 for shape_elem in out.shape]
371                     var = TensorType(dtype=dtype, broadcastable=bcast)()
372                     out_grad_vars.append(var)
373                 try:
374                     in_grad_vars = self.op.grad(inputrs, out_grad_vars)
375                 except (gof.utils.MethodNotDefined, NotImplementedError):
376                     pass
377                 else:
378                     assert None not in in_grad_vars
379     Checker.__name__ = name
380     if hasattr(Checker, '__qualname__'):
381         Checker.__qualname__ = name
382     return Checker
383 def rand(*shape):
384     r = test_rng.rand(*shape) * 2 - 1
385     return np.asarray(r, dtype=config.floatX)
386 def rand_nonzero(shape, eps=3e-4):
387     r = np.asarray(test_rng.rand(*shape), dtype=config.floatX)
388     r = r * (1 - eps) + eps * (r &gt;= 0.5)
389     r = r * 2 - 1
390     return r
391 def randint(*shape):
392     return test_rng.randint(-5, 6, shape)
393 def randuint32(*shape):
394     return np.array(test_rng.randint(5, size=shape), dtype=np.uint32)
395 def randuint16(*shape):
396     return np.array(test_rng.randint(5, size=shape), dtype=np.uint16)
397 def randcomplex(*shape):
398     r = np.asarray(test_rng.rand(*shape), dtype=config.floatX)
399     return np.complex128(2 * r - 1)
400 def randcomplex_nonzero(shape, eps=1e-4):
401     return np.complex128(rand_nonzero(shape, eps))
402 def randint_nonzero(*shape):
403     r = test_rng.randint(-5, 5, shape)
404     return r + (r == 0) * 5
405 def rand_ranged(min, max, shape):
406     return np.asarray(test_rng.rand(*shape) * (max - min) + min,
407                       dtype=config.floatX)
408 def randint_ranged(min, max, shape):
409     return test_rng.randint(min, max + 1, shape)
410 def randc128_ranged(min, max, shape):
411     return np.asarray(test_rng.rand(*shape) * (max - min) + min,
412                       dtype='complex128')
413 def rand_of_dtype(shape, dtype):
414     if dtype in tensor.discrete_dtypes:
415         return randint(*shape).astype(dtype)
416     elif dtype in tensor.float_dtypes:
417         return rand(*shape).astype(dtype)
418     elif dtype in tensor.complex_dtypes:
419         return randcomplex(*shape).astype(dtype)
420     else:
421         raise TypeError()
422 _eps = 1e-2
423 def makeBroadcastTester(op, expected, checks=None, name=None, **kwargs):
424     if checks is None:
425         checks = {}
426     if name is None:
427         name = str(op)
428     capitalize = False
429     if name.startswith('Elemwise{') and name.endswith(',no_inplace}'):
430         name = name[9:-12]
431         capitalize = True
432     elif name.endswith('_inplace'):
433         capitalize = True
434     if capitalize:
435         name = ''.join([x.capitalize() for x in name.split('_')])
436     if not name.endswith('Tester'):
437         name += "Tester"
438     if 'inplace' in kwargs:
439         if kwargs['inplace']:
440             _expected = expected
441             if not isinstance(_expected, dict):
442                 def expected(*inputs):
443                     return np.array(_expected(*inputs), dtype=inputs[0].dtype)
444             def inplace_check(inputs, outputs):
445                 return np.all(inputs[0] == outputs[0])
446             checks = dict(checks, inplace_check=inplace_check)
447         del kwargs['inplace']
448     return makeTester(name, op, expected, checks, **kwargs)
449 _good_broadcast_binary_normal = dict(
450     same_shapes=(rand(2, 3), rand(2, 3)),
451     not_same_dimensions=(rand(2, 2), rand(2)),
452     scalar=(rand(2, 3), rand(1, 1)),
453     row=(rand(2, 3), rand(1, 3)),
454     column=(rand(2, 3), rand(2, 1)),
455     integers=(randint(2, 3), randint(2, 3)),
456     uint32=(randuint32(2, 3), randuint32(2, 3)),
457     uint16=(randuint16(2, 3), randuint16(2, 3)),
458     dtype_mixup_1=(rand(2, 3), randint(2, 3)),
459     dtype_mixup_2=(randint(2, 3), rand(2, 3)),
460     complex1=(randcomplex(2, 3), randcomplex(2, 3)),
461     complex2=(randcomplex(2, 3), rand(2, 3)),
462     empty=(np.asarray([], dtype=config.floatX),
463            np.asarray([1], dtype=config.floatX)),
464     )
465 _bad_runtime_broadcast_binary_normal = dict(
466     bad_shapes=(rand(2, 3), rand<font color="#ae694a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, 2)),
467     bad_row=(rand(2, 3), rand(1, 2)))
468 _grad_broadcast_binary_normal = dict(
469     same_shapes=(rand(2, 3), rand(2, 3)),
470     scalar=</b></font>(rand(2, 3), rand(1, 1)),
471     row=(rand(2, 3), rand(1, 3)),
472     column=(rand(2, 3), rand(2, 1)),
473     )
474 def check_floatX(inputs, rval):
475     if (isinstance(rval, np.ndarray) and
476             rval.dtype == 'float64' and
477             config.cast_policy == 'numpy+floatX' and
478             config.floatX == 'float32' and
479             all(x.dtype != 'float64' for x in inputs)):
480         return rval.astype('float32')
481     else:
482         return rval
483 AddTester = makeBroadcastTester(
484     op=add,
485     expected=lambda *inputs: check_floatX(
486         inputs, reduce(lambda x, y: x + y, inputs)),
487     good=dict(
488         three_inputs_same_shapes=(rand(2, 3),
489                                   rand(2, 3),
490                                   rand(2, 3)),
491         three_inputs_same_shapes_uint=(randuint32(2, 3),
492                                        randuint32(2, 3),
493                                        randuint32(2, 3)),
494         four_inputs_broadcast=(rand(2, 3),
495                                rand(1, 3),
496                                rand(2, 1),
497                                rand(1, 1)),
498         **_good_broadcast_binary_normal),
499     bad_build=_bad_build_broadcast_binary_normal,
500     bad_runtime=_bad_runtime_broadcast_binary_normal)
501 AddInplaceTester = makeBroadcastTester(
502     op=inplace.add_inplace,
503     expected=lambda x, y: x + y,
504     good=_good_broadcast_binary_normal,
505     bad_build=_bad_build_broadcast_binary_normal,
506     bad_runtime=_bad_runtime_broadcast_binary_normal,
507     inplace=True)
508 SubTester = makeBroadcastTester(
509     op=sub,
510     expected=lambda x, y: check_floatX((x, y), x - y),
511     good=_good_broadcast_binary_normal,
512     bad_build=_bad_build_broadcast_binary_normal,
513     bad_runtime=_bad_runtime_broadcast_binary_normal,
514     grad=_grad_broadcast_binary_normal)
515 SubInplaceTester = makeBroadcastTester(op=inplace.sub_inplace,
516                                        expected=lambda x, y: x - y,
517                                        good=_good_broadcast_binary_normal,
518                                        bad_build=_bad_build_broadcast_binary_normal,
519                                        bad_runtime=_bad_runtime_broadcast_binary_normal,
520                                        inplace=True)
521 SwitchTester = makeBroadcastTester(
522     op=switch,
523     expected=np.where,
524     good=dict(all_true=(np.asarray(1, dtype=config.floatX),
525                         rand(4, 5), rand(4, 5)),
526               false_true=(np.asarray(0, dtype=config.floatX),
527                           rand(4, 5), rand(4, 5)),
528               mixed=(randint_ranged(0, 1, (4, 5)),
529                      rand(4, 5), rand(4, 5))
530               ),
531     bad_build=dict(all_true=(np.asarray(1, dtype=config.floatX),
532                              rand(4, 5))),
533     bad_runtime=dict(all_true=(np.asarray(1, dtype=config.floatX),
534                                rand(3, 5), rand(4, 5)),
535                      false_true=(np.asarray(0, dtype=config.floatX),
536                                  rand(4, 6), rand(4, 5)),
537                      ),
538     grad=dict(all_true=(np.asarray(1, dtype=config.floatX),
539                         rand(4, 5), rand(4, 5)),
540               ),
541 )
542 MaximumTester = makeBroadcastTester(
543     op=maximum,
544     expected=lambda *inputs: check_floatX(inputs, np.maximum(*inputs)),
545     good=_good_broadcast_binary_normal,
546     bad_build=_bad_build_broadcast_binary_normal,
547     bad_runtime=_bad_runtime_broadcast_binary_normal,
548     grad=_grad_broadcast_binary_normal)
549 MaximumInplaceTester = makeBroadcastTester(
550     op=inplace.maximum_inplace,
551     expected=np.maximum,
552     good=_good_broadcast_binary_normal,
553     bad_build=_bad_build_broadcast_binary_normal,
554     bad_runtime=_bad_runtime_broadcast_binary_normal,
555     inplace=True)
556 def test_maximum_minimum_grad():
557     x, y = tensor.vectors('xy')
558     for op in [tensor.maximum, tensor.minimum]:
559         o = op(x, y)
560         g = theano.grad(o.sum(), [x, y])
561         f = theano.function([x, y], g)
562         assert np.allclose(f([1], [1]), [[1], [0]])
563 MinimumTester = makeBroadcastTester(
564     op=minimum,
565     expected=lambda *inputs: check_floatX(inputs, np.minimum(*inputs)),
566     good=_good_broadcast_binary_normal,
567     bad_build=_bad_build_broadcast_binary_normal,
568     bad_runtime=_bad_runtime_broadcast_binary_normal,
569     grad=_grad_broadcast_binary_normal)
570 MinimumInplaceTester = makeBroadcastTester(
571     op=inplace.minimum_inplace,
572     expected=np.minimum,
573     good=_good_broadcast_binary_normal,
574     bad_build=_bad_build_broadcast_binary_normal,
575     bad_runtime=_bad_runtime_broadcast_binary_normal,
576     inplace=True)
577 MulTester = makeBroadcastTester(
578     op=mul,
579     expected=lambda *inputs: check_floatX(inputs, reduce(lambda x, y: x * y, inputs)),
580     good=dict(three_inputs_same_shapes=(rand(2, 3), rand(2, 3), rand(2, 3)),
581               four_inputs_broadcast=(rand(2, 3), rand(1, 3), rand(2, 1), rand(1, 1)),
582               **_good_broadcast_binary_normal),
583     bad_build=_bad_build_broadcast_binary_normal,
584     bad_runtime=_bad_runtime_broadcast_binary_normal,
585     grad=dict(three_inputs_same_shapes=(rand(2, 3), rand(2, 3), rand(2, 3)),
586               four_inputs_broadcast=(rand(2, 3), rand(1, 3), rand(2, 1), rand(1, 1)),
587               **_grad_broadcast_binary_normal))
588 MulInplaceTester = makeBroadcastTester(
589     op=inplace.mul_inplace,
590     expected=lambda x, y: x * y,
591     good=_good_broadcast_binary_normal,
592     bad_build=_bad_build_broadcast_binary_normal,
593     bad_runtime=_bad_runtime_broadcast_binary_normal,
594     inplace=True)
595 def copymod(dct, without=None, **kwargs):
596     if without is None:
597         without = []
598     rval = copy(dct)
599     for a in without:
600         if a in rval:
601             del rval[a]
602     for kw, val in iteritems(kwargs):
603     return rval
604 _good_broadcast_div_mod_normal_float_no_complex = dict<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(
605     same_shapes=(rand(2, 3), rand_nonzero((2, 3))),
606     scalar=(rand(2, 3), rand_nonzero((1, 1))),
607     row=(rand(2, 3), rand_nonzero((1</b></font>, 3))),
608     column=(rand(2, 3), rand_nonzero((2, 1))),
609     dtype_mixup_1=(rand(2, 3), randint_nonzero(2, 3)),
610     dtype_mixup_2=(randint_nonzero(2, 3), rand_nonzero((2, 3))),
611     integer=(randint(2, 3), randint_nonzero(2, 3)),
612     uint8=(randint(2, 3).astype("uint8"),
613            randint_nonzero(2, 3).astype("uint8")),
614     uint16=(randint(2, 3).astype("uint16"),
615             randint_nonzero(2, 3).astype("uint16")),
616     int8=[np.tile(np.arange(-127, 128, dtype='int8'), [254, 1]).T,
617           np.tile(np.array(list(range(-127, 0)) + list(range(1, 128)),
618                            dtype='int8'),
619                   [255, 1])],
620     )
621 if PY3:
622     _good_broadcast_div_mod_normal_float_inplace = copymod(
623         _good_broadcast_div_mod_normal_float_no_complex,
624         empty1=(np.asarray([]), np.asarray([1])),
625         )
626 else:
627     _good_broadcast_div_mod_normal_float_inplace = copymod(
628         _good_broadcast_div_mod_normal_float_no_complex,
629         empty1=(np.asarray([], dtype=config.floatX),
630                 np.asarray([1], dtype=config.floatX)),
631         complex1=(randcomplex(2, 3), randcomplex_nonzero((2, 3))),
632         complex2=(randcomplex(2, 3), rand_nonzero((2, 3))),
633         )
634 _good_broadcast_div_mod_normal_float = copymod(
635     _good_broadcast_div_mod_normal_float_inplace,
636     empty2=(np.asarray([0], dtype=config.floatX),
637             np.asarray([], dtype=config.floatX))
638     )
639 _grad_broadcast_div_mod_normal = dict(
640     same_shapes=(rand(2, 3), rand_nonzero((2, 3))),
641     scalar=(rand(2, 3), rand_nonzero((1, 1))),
642     row=(rand(2, 3), rand_nonzero((1, 3))),
643     column=(rand(2, 3), rand_nonzero((2, 1))),
644     )
645 div_grad_rtol = None
646 if config.floatX == 'float32':
647     div_grad_rtol = 0.025
648 def _numpy_true_div(x, y):
649     out = np.true_divide(x, y)
650     if x.dtype in tensor.discrete_dtypes and y.dtype in tensor.discrete_dtypes:
651         out = theano._asarray(out, dtype=config.floatX)
652     return out
653 TrueDivTester = makeBroadcastTester(
654     op=tensor.true_div,
655     expected=_numpy_true_div,
656     good=_good_broadcast_div_mod_normal_float_no_complex,
657     grad=_grad_broadcast_div_mod_normal,
658     grad_rtol=div_grad_rtol,
659     )
660 TrueDivInplaceTester = makeBroadcastTester(
661     op=inplace.true_div_inplace,
662     expected=_numpy_true_div,
663     good=copymod(
664         _good_broadcast_div_mod_normal_float_inplace,
665         without=['integer', 'uint8', 'uint16', 'int8']),
666     grad_rtol=div_grad_rtol,
667     inplace=True)
668 _good_inv = dict(
669     normal=[5 * rand_nonzero((2, 3))],
670     integers=[randint_nonzero(2, 3)],
671     int8=[np.array(list(range(-127, 0)) + list(range(1, 127)), dtype='int8')],
672     uint8=[np.array(list(range(0, 255)), dtype='uint8')],
673     uint16=[np.array(list(range(0, 65535)), dtype='uint16')],
674     complex=[randcomplex_nonzero((2, 3))],
675     empty=[np.asarray([], dtype=config.floatX)])
676 _good_inv_inplace = copymod(_good_inv, without=['integers', 'int8', 'uint8', 'uint16', 'complex'])
677 _grad_inv = copymod(_good_inv,
678                     without=['integers', 'int8', 'uint8', 'uint16', 'complex', 'empty'])
679 _bad_runtime_inv = dict(
680     float=[np.zeros((2, 3))],
681     integers=[np.zeros((2, 3), dtype='int64')],
682     int8=[np.zeros((2, 3), dtype='int8')],
683     complex=[np.zeros((2, 3), dtype='complex128')])
684 InvTester = makeBroadcastTester(
685     op=tensor.inv,
686     expected=lambda x: upcast_int8_nfunc(np.true_divide)(np.int8(1), x),
687     good=_good_inv,
688     bad_runtime=_bad_runtime_inv,
689     grad=_grad_inv,
690     grad_rtol=div_grad_rtol)
691 InvInplaceTester = makeBroadcastTester(
692     op=inplace.inv_inplace,
693     expected=lambda x: _numpy_true_div(np.int8(1), x),
694     good=_good_inv_inplace,
695     bad_runtime=_bad_runtime_inv,
696     grad_rtol=div_grad_rtol,
697     inplace=True)
698 CeilIntDivTester = makeBroadcastTester(
699     op=tensor.ceil_intdiv,
700     expected=lambda x, y: check_floatX((x, y), (x // y) + ((x % y) != 0)),
701     good=_good_broadcast_div_mod_normal_float_no_complex,
702     name='CeilIntDiv',
703     )
704 ModTester = makeBroadcastTester(
705     op=tensor.mod,
706     expected=lambda x, y: np.asarray(
707         x % y, dtype=theano.scalar.basic.upcast(x.dtype, y.dtype)),
708     good=copymod(_good_broadcast_div_mod_normal_float,
709                  ['complex1', 'complex2']),
710     grad=_grad_broadcast_div_mod_normal,
711     grad_eps=1e-5,
712     )
713 ModInplaceTester = makeBroadcastTester(
714     op=inplace.mod_inplace,
715     expected=lambda x, y: np.asarray(
716         x % y, dtype=theano.scalar.basic.upcast(x.dtype, y.dtype)),
717     good=copymod(_good_broadcast_div_mod_normal_float_inplace,
718                  ["complex1", "complex2"]),
719     grad_eps=1e-5,
720     inplace=True)
721 _good_broadcast_pow_normal_float = dict(
722     same_shapes=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 3))),
723     scalar=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 1))),
724     row=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 3))),
725     dtype_mixup=(rand_ranged(-3, 3, (2, 3)), randint_ranged(-3, 3, (2, 3))),
726     complex1=(randcomplex(2, 3), randcomplex(2, 3)),
727     complex2=(randcomplex(2, 3), rand<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(2, 3)),
728     empty1=(np.asarray([], dtype=config.floatX),
729             np.asarray([1], dtype=config.floatX)),
730     empty2=(np.asarray([0], dtype=config.floatX),
731             np.asarray([], dtype=config.</b></font>floatX)),
732     empty3=(np.asarray([], dtype=config.floatX),
733             np.asarray([], dtype=config.floatX)),
734     )
735 _grad_broadcast_pow_normal = dict(
736     same_shapes=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 3))),
737     scalar=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 1))),
738     row=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 3))),
739     column=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 1))),
740     x_eq_zero=(
741         np.asarray([0.], dtype=config.floatX),
742         np.asarray([2.], dtype=config.floatX)
743         ),  # Test for issue 1780
744     )
745 _good_broadcast_pow_normal_float_pow = copy(_good_broadcast_pow_normal_float)
746 del _good_broadcast_pow_normal_float_pow["empty2"]
747 m = copy(theano.compile.get_default_mode())
748 m.check_isfinite = False
749 PowTester = makeBroadcastTester(
750     op=pow,
751     expected=lambda x, y: check_floatX((x, y), x ** y),
752     good=_good_broadcast_pow_normal_float,
753     grad=_grad_broadcast_pow_normal,
754     name='Pow',
755     mode=m
756 )
757 PowInplaceTester = makeBroadcastTester(
758     op=inplace.pow_inplace,
759     expected=lambda x, y: x ** y,
760     good=_good_broadcast_pow_normal_float_pow,
761     inplace=True,
762     mode=m
763 )
764 corner_case = np.asarray(
765     [-2.5, -2., -1.5, -1., -0.5, -.51, -.49, 0,
766      0.49, 0.5, 0.9, 1, 1.5, 2, 2.5],
767     dtype=floatX)
768 corner_case_grad = np.asarray(
769     [-2.5, -2., -1.5, -1., -0.5, -.51, -.49,
770      0.49, 0.5, 0.9, 1, 1.5, 2, 2.5],
771     dtype=floatX)
772 _good_broadcast_unary_normal_float = dict(
773     normal=[rand_ranged(-5, 5, (2, 3))],
774     corner_case=[corner_case],
775     empty=[np.asarray([], dtype=config.floatX)])
776 _good_broadcast_unary_normal_float_no_empty <font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= copymod(
777     _good_broadcast_unary_normal_float,
778     without=['empty'])
779 _good_broadcast_unary_normal_float_no_empty_no_complex = copymod(
780     _good_broadcast_unary_normal_float_no_empty,
781     without=['complex'])
782 _good_broadcast_unary_normal_float_no_complex = copymod(
783     _good_broadcast_unary_normal_float,
784     without=['complex'])
785 _good_broadcast_unary_normal_float_no_complex_small_neg_range = dict(
786     normal=</b></font>[rand_ranged(-2, 5, (2, 3))],
787     corner_case=[corner_case],
788     empty=[np.asarray([], dtype=config.floatX)])
789 _good_broadcast_unary_normal = dict(
790     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)),
791                        dtype=config.floatX)],
792     integers=[randint_ranged(-5, 5, (2, 3))],
793     int8=[np.arange(-127, 128, dtype='int8')],
794     uint8=[np.arange(0, 255, dtype='uint8')],
795     uint16=[np.arange(0, 65535, dtype='uint16')],
796     corner_case=[corner_case],
797     complex=[randcomplex(2, 3)],
798     empty=[np.asarray([], dtype=config.floatX)],
799     )
800 _good_broadcast_unary_normal_no_complex = dict(
801     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
802     integers=[randint_ranged(-5, 5, (2, 3))],
803     int8=[np.arange(-127, 128, dtype='int8')],
804     uint8=[np.arange(0, 89, dtype='uint8')],
805     uint16=[np.arange(0, 89, dtype='uint16')],
806     corner_case=[corner_case],
807     empty=[np.asarray([], dtype=config.floatX)],
808     )
809 _grad_broadcast_unary_normal_no_complex = dict(
810     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
811     corner_case=[corner_case_grad])
812 _grad_broadcast_unary_normal = dict(
813     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
814     corner_case=[corner_case_grad],
815     )
816 _grad_broadcast_unary_normal_noint = dict(
817     normal=[(rand_ranged(_eps, 1 - _eps, (2, 3)) + randint(2, 3))
818             .astype(floatX)])
819 _grad_broadcast_unary_normal_small_neg_range = dict(
820     normal=[np.asarray(rand_ranged(-2, 5, (2, 3)), dtype=floatX)],
821     corner_case=[corner_case_grad])
822 _grad_broadcast_unary_normal_no_complex_no_corner_case = copymod(
823     _grad_broadcast_unary_normal_no_complex,
824     without=['corner_case'])
825 _grad_broadcast_unary_abs1_no_complex = dict(
826     normal=[np.asarray(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)), dtype=floatX)],
827     )
828 _grad_broadcast_unary_0_2_no_complex = dict(
829     normal=[np.asarray(rand_ranged(_eps, 1 - _eps, (2, 3)), dtype=floatX)],
830     )
831 AbsTester = makeBroadcastTester(
832     op=tensor.abs_,
833     expected=lambda x: abs(x),
834     good=_good_broadcast_unary_normal,
835     grad=_grad_broadcast_unary_normal)
836 _good_broadcast_unary_normal_abs = copy(_good_broadcast_unary_normal)
837 del _good_broadcast_unary_normal_abs['complex']
838 AbsInplaceTester = makeBroadcastTester(
839     op=inplace.abs__inplace,
840     expected=lambda x: np.abs(x),
841     good=_good_broadcast_unary_normal_abs,
842     inplace=True)
843 NegTester = makeBroadcastTester(
844     op=tensor.neg,
845     expected=lambda x: -x,
846     good=_good_broadcast_unary_normal,
847     grad=_grad_broadcast_unary_normal)
848 NegInplaceTester = makeBroadcastTester(
849     op=inplace.neg_inplace,
850     expected=lambda x: -x,
851     good=_good_broadcast_unary_normal,
852     inplace=True)
853 SgnTester = makeBroadcastTester(
854     op=tensor.sgn,
855     expected=np.sign,
856     good=_good_broadcast_unary_normal_no_complex,
857     grad=_grad_broadcast_unary_normal,)
858 SgnInplaceTester = makeBroadcastTester(
859     op=inplace.sgn_inplace,
860     expected=np.sign,
861     good=_good_broadcast_unary_normal_no_complex,
862     inplace=True)
863 IntDivTester = makeBroadcastTester(
864     op=tensor.int_div,
865     expected=lambda x, y: check_floatX((x, y), x // y),
866     good=_good_broadcast_div_mod_normal_float,
867     )
868 IntDivInplaceTester = makeBroadcastTester(
869     op=inplace.int_div_inplace,
870     expected=lambda x, y: check_floatX((x, y), x // y),
871     good=_good_broadcast_div_mod_normal_float_inplace,
872     inplace=True
873     )
874 CeilTester = makeBroadcastTester(
875     op=tensor.ceil,
876     expected=upcast_float16_ufunc(np.ceil),
877     good=_good_broadcast_unary_normal_no_complex,
878     grad=copymod(_grad_broadcast_unary_normal_noint,
879                  extra=[np.asarray([-2.5, -1.5, -1.51, 0.49, .98, 1.02],
880                                    dtype=floatX)]))
881 CeilInplaceTester = makeBroadcastTester(
882     op=inplace.ceil_inplace,
883     expected=upcast_float16_ufunc(np.ceil),
884     good=copymod(_good_broadcast_unary_normal_no_complex,
885                  without=['integers', 'int8', 'uint8', 'uint16']),
886     inplace=True)
887 FloorTester = makeBroadcastTester(
888     op=tensor.floor,
889     expected=upcast_float16_ufunc(np.floor),
890     good=_good_broadcast_unary_normal_no_complex,
891     grad=_grad_broadcast_unary_normal_noint)
892 FloorInplaceTester = makeBroadcastTester(
893     op=inplace.floor_inplace,
894     expected=upcast_float16_ufunc(np.floor),
895     good=copymod(_good_broadcast_unary_normal_no_complex,
896                  without=["integers", "int8", "uint8", "uint16"]),
897     inplace=True)
898 TruncInplaceTester = makeBroadcastTester(
899     op=inplace.trunc_inplace,
900     expected=upcast_float16_ufunc(np.trunc),
901     good=_good_broadcast_unary_normal_no_complex,
902     inplace=True)
903 TruncTester = makeBroadcastTester(
904     op=tensor.trunc,
905     expected=upcast_float16_ufunc(np.trunc),
906     good=_good_broadcast_unary_normal_no_complex)
907 RoundHalfToEvenTester = makeBroadcastTester(
908     op=tensor.round_half_to_even,
909     expected=np.round,
910     good=_good_broadcast_unary_normal_float_no_complex,
911     grad=_grad_broadcast_unary_normal_no_complex_no_corner_case)
912 RoundHalfToEvenInplaceTester = makeBroadcastTester(
913     op=inplace.round_half_to_even_inplace,
914     expected=np.round,
915     good=_good_broadcast_unary_normal_float_no_complex,
916     inplace=True)
917 RoundHalfAwayFromZeroTester = makeBroadcastTester(
918     op=tensor.round_half_away_from_zero,
919     expected=lambda a: theano.scalar.basic.round_half_away_from_zero_vec(a),
920     good=_good_broadcast_unary_normal_float_no_empty_no_complex,
921     grad=_grad_broadcast_unary_normal_no_complex_no_corner_case)
922 RoundHalfAwayFromZeroInplaceTester = makeBroadcastTester(
923     op=inplace.round_half_away_from_zero_inplace,
924     expected=lambda a: theano.scalar.basic.round_half_away_from_zero_vec(a),
925     good=_good_broadcast_unary_normal_float_no_empty_no_complex,
926     inplace=True)
927 SqrTester = makeBroadcastTester(
928     op=tensor.sqr,
929     expected=np.square,
930     good=_good_broadcast_unary_normal,
931     grad=_grad_broadcast_unary_normal)
932 SqrInplaceTester = makeBroadcastTester(
933     op=inplace.sqr_inplace,
934     expected=np.square,
935     good=_good_broadcast_unary_normal,
936     inplace=True)
937 ExpTester = makeBroadcastTester(
938     op=tensor.exp,
939     expected=upcast_float16_ufunc(np.exp),
940     good=dict(_good_broadcast_unary_normal,
941               int8=[np.arange(-127, 89, dtype='int8')],
942               uint8=[np.arange(0, 89, dtype='uint8')],
943               uint16=[np.arange(0, 89, dtype='uint16')]),
944     grad=_grad_broadcast_unary_normal)
945 ExpInplaceTester = makeBroadcastTester(
946     op=inplace.exp_inplace,
947     expected=np.exp,
948     good=_good_broadcast_unary_normal_float,
949     inplace=True)
950 Exp2Tester = makeBroadcastTester(
951     op=tensor.exp2,
952     expected=upcast_float16_ufunc(np.exp2),
953     good=_good_broadcast_unary_normal,
954     grad=_grad_broadcast_unary_normal)
955 Exp2InplaceTester = makeBroadcastTester(
956     op=inplace.exp2_inplace,
957     expected=np.exp2,
958     good=_good_broadcast_unary_normal_float,
959     inplace=True)
960 Expm1Tester = makeBroadcastTester(
961     op=tensor.expm1,
962     expected=upcast_float16_ufunc(np.expm1),
963     good=dict(_good_broadcast_unary_normal,
964               int8=[np.arange(-127, 89, dtype='int8')],
965               uint8=[np.arange(0, 89, dtype='uint8')],
966               uint16=[np.arange(0, 89, dtype='uint16')]),
967     grad=_grad_broadcast_unary_normal)
968 Expm1InplaceTester = makeBroadcastTester(
969     op=inplace.expm1_inplace,
970     expected=np.expm1,
971     good=_good_broadcast_unary_normal_float,
972     inplace=True)
973 _good_broadcast_unary_positive = dict(
974     normal=(rand_ranged(0.001, 5, (2, 3)),),
975     integers=(randint_ranged(1, 5, (2, 3)),),
976     uint8=[np.arange(1, 256, dtype='uint8')],
977     complex=(randc128_ranged(1, 5, (2, 3)),),
978     empty=(np.asarray([], dtype=config.floatX),),
979     )
980 _good_broadcast_unary_positive_float = copymod(
981     _good_broadcast_unary_positive,
982     without=['integers', 'uint8'])
983 _grad_broadcast_unary_positive = dict(normal=(rand_ranged(_eps, 5, (2, 3)),),)
984 LogTester = makeBroadcastTester(
985     op=tensor.log,
986     expected=upcast_float16_ufunc(np.log),
987     good=_good_broadcast_unary_positive,
988     grad=_grad_broadcast_unary_positive)
989 LogInplaceTester = makeBroadcastTester(
990     op=inplace.log_inplace,
991     expected=np.log,
992     good=_good_broadcast_unary_positive_float,
993     inplace=True)
994 Log2Tester = makeBroadcastTester(
995     op=tensor.log2,
996     expected=upcast_float16_ufunc(np.log2),
997     good=_good_broadcast_unary_positive,
998     grad=_grad_broadcast_unary_positive)
999 Log2InplaceTester = makeBroadcastTester(
1000     op=inplace.log2_inplace,
1001     expected=np.log2,
1002     good=_good_broadcast_unary_positive_float,
1003     inplace=True)
1004 Log10Tester = makeBroadcastTester(
1005     op=tensor.log10,
1006     expected=upcast_float16_ufunc(np.log10),
1007     good=_good_broadcast_unary_positive,
1008     grad=_grad_broadcast_unary_positive)
1009 Log10InplaceTester = makeBroadcastTester(
1010     op=inplace.log10_inplace,
1011     expected=np.log10,
1012     good=_good_broadcast_unary_positive_float,
1013     inplace=True)
1014 Log1pTester = makeBroadcastTester(
1015     op=tensor.log1p,
1016     expected=upcast_float16_ufunc(np.log1p),
1017     good=_good_broadcast_unary_positive,
1018     grad=_grad_broadcast_unary_positive)
1019 Log1pInplaceTester = makeBroadcastTester(
1020     op=inplace.log1p_inplace,
1021     expected=np.log1p,
1022     good=_good_broadcast_unary_positive_float,
1023     inplace=True)
1024 SqrtTester = makeBroadcastTester(
1025     op=tensor.sqrt,
1026     expected=upcast_float16_ufunc(np.sqrt),
1027     good=_good_broadcast_unary_positive,
1028     grad=_grad_broadcast_unary_positive)
1029 SqrtInplaceTester = makeBroadcastTester(
1030     op=inplace.sqrt_inplace,
1031     expected=np.sqrt,
1032     good=_good_broadcast_unary_positive_float,
1033     inplace=True)
1034 _good_broadcast_unary_wide = dict(
1035     normal=(rand_ranged(-1000, 1000, (2, 3)),),
1036     integers=(randint_ranged(-1000, 1000, (2, 3)),),
1037     int8=[np.arange(-127, 128, dtype='int8')],
1038     uint8=[np.arange(0, 255, dtype='uint8')],
1039     uint16=[np.arange(0, 65535, dtype='uint16')],
1040     complex=(randc128_ranged(-1000, 1000, (2, 3)),),
1041     empty=(np.asarray([], dtype=config.floatX),),)
1042 _good_broadcast_unary_wide_float = copymod(
1043     _good_broadcast_unary_wide,
1044     without=['integers', 'int8', 'uint8', 'uint16'])
1045 _grad_broadcast_unary_wide = dict(normal=(rand_ranged(-1000, 1000, (2, 3)),),)
1046 if theano.config.floatX == 'float32':
1047     angle_eps = 1e-4
1048 else:
1049     angle_eps = 1e-10
1050 Deg2radTester = makeBroadcastTester(
1051     op=tensor.deg2rad,
1052     expected=upcast_float16_ufunc(np.deg2rad),
1053     good=_good_broadcast_unary_normal_no_complex,
1054     grad=_grad_broadcast_unary_normal_no_complex,
1055     eps=angle_eps)
1056 Deg2radInplaceTester = makeBroadcastTester(
1057     op=inplace.deg2rad_inplace,
1058     expected=np.deg2rad,
1059     good=_good_broadcast_unary_normal_float_no_complex,
1060     inplace=True,
1061     eps=angle_eps)
1062 Rad2degTester = makeBroadcastTester(
1063     op=tensor.rad2deg,
1064     expected=upcast_float16_ufunc(np.rad2deg),
1065     good=_good_broadcast_unary_normal_no_complex,
1066     grad=_grad_broadcast_unary_normal_no_complex,
1067     eps=angle_eps)
1068 Rad2degInplaceTester = makeBroadcastTester(
1069     op=inplace.rad2deg_inplace,
1070     expected=np.rad2deg,
1071     good=_good_broadcast_unary_normal_float_no_complex,
1072     inplace=True,
1073     eps=angle_eps)
1074 SinTester = makeBroadcastTester(
1075     op=tensor.sin,
1076     expected=upcast_float16_ufunc(np.sin),
1077     good=_good_broadcast_unary_wide,
1078     grad=_grad_broadcast_unary_wide)
1079 SinInplaceTester = makeBroadcastTester(
1080     op=inplace.sin_inplace,
1081     expected=np.sin,
1082     good=_good_broadcast_unary_wide_float,
1083     inplace=True)
1084 _good_broadcast_unary_arcsin = dict(
1085     normal=(rand_ranged(-1, 1, (2, 3)),),
1086     integers=(randint_ranged(-1, 1, (2, 3)),),
1087     int8=[np.arange(-1, 2, dtype='int8')],
1088     uint8=[np.arange(0, 2, dtype='uint8')],
1089     uint16=[np.arange(0, 2, dtype='uint16')],
1090     complex=(randc128_ranged(-1, 1, (2, 3)),),
1091     empty=(np.asarray([], dtype=config.floatX),),)
1092 _good_broadcast_unary_arcsin_float = copymod(
1093     _good_broadcast_unary_arcsin,
1094     without=['integers', 'int8', 'uint8', 'uint16'])
1095 _grad_broadcast_unary_arcsin = dict(normal=(rand_ranged(-0.9, 0.9, (2, 3)),),)
1096 ArcsinTester = makeBroadcastTester(
1097     op=tensor.arcsin,
1098     expected=upcast_float16_ufunc(np.arcsin),
1099     good=_good_broadcast_unary_arcsin,
1100     grad=_grad_broadcast_unary_arcsin)
1101 ArcsinInplaceTester = makeBroadcastTester(
1102     op=inplace.arcsin_inplace,
1103     expected=np.arcsin,
1104     good=_good_broadcast_unary_arcsin_float,
1105     inplace=True)
1106 CosTester = makeBroadcastTester(
1107     op=tensor.cos,
1108     expected=upcast_float16_ufunc(np.cos),
1109     good=_good_broadcast_unary_wide,
1110     grad=_grad_broadcast_unary_wide)
1111 CosInplaceTester = makeBroadcastTester(
1112     op=inplace.cos_inplace,
1113     expected=np.cos,
1114     good=_good_broadcast_unary_wide_float,
1115     inplace=True)
1116 def test_py_c_match():
1117     a = tensor.TensorType(dtype='int8', broadcastable=(False,))()
1118     f = theano.function([a], tensor.arccos(a), mode='DebugMode')
1119     f(np.asarray([1, 0, -1], dtype='int8'))
1120 ArccosTester = makeBroadcastTester(
1121     op=tensor.arccos,
1122     expected=upcast_float16_ufunc(np.arccos),
1123     good=_good_broadcast_unary_arcsin,
1124     grad=_grad_broadcast_unary_arcsin)
1125 ArccosInplaceTester = makeBroadcastTester(
1126     op=inplace.arccos_inplace,
1127     expected=np.arccos,
1128     good=_good_broadcast_unary_arcsin_float,
1129     inplace=True)
1130 _good_broadcast_unary_tan = dict(
1131     normal=(rand_ranged(-3.14, 3.14, (2, 3)),),
1132     shifted=(rand_ranged(3.15, 6.28, (2, 3)),),
1133     integers=(randint_ranged(-3, 3, (2, 3)),),
1134     int8=[np.arange(-3, 4, dtype='int8')],
1135     uint8=[np.arange(0, 4, dtype='uint8')],
1136     uint16=[np.arange(0, 4, dtype='uint16')],
1137     complex=(randc128_ranged(-3.14, 3.14, (2, 3)),),
1138     empty=(np.asarray([], dtype=config.floatX),),)
1139 _grad_broadcast_unary_tan = dict(normal=(rand_ranged(-1.5, 1.5, (2, 3)),),
1140                                  shifted=(rand_ranged(1.6, 4.6, (2, 3)),))
1141 TanTester = makeBroadcastTester(
1142     op=tensor.tan,
1143     expected=upcast_float16_ufunc(np.tan),
1144     good=_good_broadcast_unary_tan,
1145     grad=_grad_broadcast_unary_tan)
1146 TanInplaceTester = makeBroadcastTester(
1147     op=inplace.tan_inplace,
1148     expected=np.tan,
1149     good=copymod(_good_broadcast_unary_tan, without=['integers', 'int8', 'uint8', 'uint16']),
1150     inplace=True)
1151 ArctanTester = makeBroadcastTester(
1152     op=tensor.arctan,
1153     expected=upcast_float16_ufunc(np.arctan),
1154     good=_good_broadcast_unary_wide,
1155     grad=_grad_broadcast_unary_wide)
1156 ArctanInplaceTester = makeBroadcastTester(
1157     op=inplace.arctan_inplace,
1158     expected=np.arctan,
1159     good=_good_broadcast_unary_wide_float,
1160     inplace=True)
1161 _good_broadcast_binary_arctan2 = dict(
1162     same_shapes=(rand(2, 3), rand(2, 3)),
1163     not_same_dimensions=(rand(2, 2), rand(2)),
1164     scalar=(rand(2, 3), rand(1, 1)),
1165     row=(rand(2, 3), rand(1, 3)),
1166     column=(rand(2, 3), rand(2, 1)),
1167     integers=(randint(2, 3), randint(2, 3)),
1168     int8=[np.arange(-127, 128, dtype='int8'),
1169           np.arange(-127, 128, dtype='int8')[:, np.newaxis]],
1170     uint8=[np.arange(0, 128, dtype='uint8'),
1171            np.arange(0, 128, dtype='uint8')[:, np.newaxis]],
1172     uint16=[np.arange(0, 128, dtype='uint16'),
1173             np.arange(0, 128, dtype='uint16')[:, np.newaxis]],
1174     dtype_mixup_1=(rand(2, 3), randint(2, 3)),
1175     dtype_mixup_2=(randint(2, 3), rand(2, 3)),
1176     empty=(np.asarray([], dtype=config.floatX),
1177            np.asarray([1], dtype=config.floatX)),
1178     )
1179 _grad_broadcast_binary_arctan2 = dict(
1180     same_shapes=(rand(2, 3), rand(2, 3)),
1181     scalar=(rand(2, 3), rand(1, 1)),
1182     row=(rand(2, 3), rand(1, 3)),
1183     column=(rand(2, 3), rand(2, 1)),
1184     )
1185 Arctan2Tester = makeBroadcastTester(
1186     op=tensor.arctan2,
1187     expected=upcast_float16_ufunc(np.arctan2),
1188     good=_good_broadcast_binary_arctan2,
1189     grad=_grad_broadcast_binary_arctan2)
1190 Arctan2InplaceTester = makeBroadcastTester(
1191     op=inplace.arctan2_inplace,
1192     expected=np.arctan2,
1193     good=copymod(_good_broadcast_binary_arctan2,
1194                  without=['integers', 'int8', 'uint8',
1195                           'uint16', 'dtype_mixup_2']),
1196     inplace=True)
1197 CoshTester = makeBroadcastTester(
1198     op=tensor.cosh,
1199     expected=upcast_float16_ufunc(np.cosh),
1200     good=dict(_good_broadcast_unary_normal,
1201               int8=[np.arange(-89, 90, dtype='int8')],
1202               uint8=[np.arange(0, 90, dtype='uint8')],
1203               uint16=[np.arange(0, 90, dtype='uint16')]),
1204     grad=_grad_broadcast_unary_normal)
1205 CoshInplaceTester = makeBroadcastTester(
1206     op=inplace.cosh_inplace,
1207     expected=np.cosh,
1208     good=_good_broadcast_unary_normal_float,
1209     inplace=True)
1210 _good_broadcast_unary_arccosh = dict(
1211     normal=(rand_ranged(1, 1000, (2, 3)),),
1212     integers=(randint_ranged(1, 1000, (2, 3)),),
1213     uint8=[np.arange(1, 256, dtype='uint8')],
1214     complex=(randc128_ranged(1, 1000, (2, 3)),),
1215     empty=(np.asarray([], dtype=config.floatX),),)
1216 _grad_broadcast_unary_arccosh = dict(normal=(rand_ranged(1 + _eps, 1000, (2, 3)),),)
1217 ArccoshTester = makeBroadcastTester(
1218     op=tensor.arccosh,
1219     expected=upcast_float16_ufunc(np.arccosh),
1220     good=_good_broadcast_unary_arccosh,
1221     grad=_grad_broadcast_unary_arccosh)
1222 ArccoshInplaceTester = makeBroadcastTester(
1223     op=inplace.arccosh_inplace,
1224     expected=np.arccosh,
1225     good=copymod(_good_broadcast_unary_arccosh, without=['integers', 'uint8']),
1226     inplace=True)
1227 SinhTester = makeBroadcastTester(
1228     op=tensor.sinh,
1229     expected=upcast_float16_ufunc(np.sinh),
1230     good=dict(_good_broadcast_unary_normal,
1231               int8=[np.arange(-89, 90, dtype='int8')],
1232               uint8=[np.arange(0, 90, dtype='uint8')],
1233               uint16=[np.arange(0, 90, dtype='uint16')]),
1234     grad=_grad_broadcast_unary_normal)
1235 SinhInplaceTester = makeBroadcastTester(
1236     op=inplace.sinh_inplace,
1237     expected=np.sinh,
1238     good=_good_broadcast_unary_normal_float,
1239     inplace=True)
1240 ArcsinhTester = makeBroadcastTester(
1241     op=tensor.arcsinh,
1242     expected=upcast_float16_ufunc(np.arcsinh),
1243     good=_good_broadcast_unary_normal,
1244     grad=_grad_broadcast_unary_normal)
1245 ArcsinhInplaceTester = makeBroadcastTester(
1246     op=inplace.arcsinh_inplace,
1247     expected=np.arcsinh,
1248     good=_good_broadcast_unary_normal_float,
1249     inplace=True)
1250 TanhTester = makeBroadcastTester(
1251     op=tensor.tanh,
1252     expected=upcast_float16_ufunc(np.tanh),
1253     good=_good_broadcast_unary_normal,
1254     grad=_grad_broadcast_unary_normal)
1255 TanhInplaceTester = makeBroadcastTester(
1256     op=inplace.tanh_inplace,
1257     expected=np.tanh,
1258     good=_good_broadcast_unary_normal_float,
1259     inplace=True)
1260 _good_broadcast_unary_arctanh = dict(
1261     normal=(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
1262     integers=(randint_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
1263     int8=[np.arange(0, 1, dtype='int8')],
1264     uint8=[np.arange(0, 1, dtype='uint8')],
1265     uint16=[np.arange(0, 1, dtype='uint16')],
1266     complex=(randc128_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
1267     empty=(np.asarray([], dtype=config.floatX),),)
1268 _grad_broadcast_unary_arctanh = dict(
1269     normal=(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)),),)
1270 ArctanhTester = makeBroadcastTester(
1271     op=tensor.arctanh,
1272     expected=upcast_float16_ufunc(np.arctanh),
1273     good=_good_broadcast_unary_arctanh,
1274     grad=_grad_broadcast_unary_arctanh)
1275 ArctanhInplaceTester = makeBroadcastTester(
1276     op=inplace.arctanh_inplace,
1277     expected=np.arctanh,
1278     good=copymod(_good_broadcast_unary_arctanh, without=['integers', 'int8', 'uint8', 'uint16']),
1279     inplace=True)
1280     expected_erf = scipy.special.erf
1281     expected_erfc = scipy.special.erfc
1282     expected_erfinv = scipy.special<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.erfinv
1283     expected_erfcinv = scipy.special.erfcinv
1284     expected_gamma = scipy.special.gamma
1285     expected_psi = scipy.special.psi
1286     expected_tri_gamma = partial(scipy.</b></font>special.polygamma, 1)
1287     expected_chi2sf = scipy.stats.chi2<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.sf
1288     expected_j0 = scipy.special.j0
1289     expected_j1 = scipy.special.j1
1290     expected_jv = scipy.special.jv
1291     expected_i0 = scipy.special.i0
1292     expected_i1 =</b></font> scipy.special.i1
1293     expected_iv = scipy.special.iv
1294     skip_scipy = False
1295     expected_erfcx = scipy.special.erfcx
1296 else:
1297     expected_erf = []
1298     expected_erfc = []
1299     expected_erfcx = []
1300     expected_erfinv = []
1301     expected_erfcinv = []
1302     expected_gamma = []
1303     expected_gammaln = []
1304     expected_psi = []
1305     expected_tri_gamma = []
1306     expected_chi2sf = []
1307     expected_j0 = []
1308     expected_j1 = []
1309     expected_jv = []
1310     expected_i0 = []
1311     expected_i1 = []
1312     expected_iv = []
1313     skip_scipy = "scipy is not present"
1314 ErfTester = makeBroadcastTester(
1315     op=tensor.erf,
1316     expected=expected_erf,
1317     good=_good_broadcast_unary_normal,
1318     grad=_grad_broadcast_unary_normal,
1319     eps=2e-10,
1320     mode=mode_no_scipy,
1321     skip=skip_scipy)
1322 ErfInplaceTester = makeBroadcastTester(
1323     op=inplace.erf_inplace,
1324     expected=expected_erf,
1325     good=_good_broadcast_unary_normal_float,
1326     mode=mode_no_scipy,
1327     eps=2e-10,
1328     inplace=True,
1329     skip=skip_scipy)
1330 ErfcTester = makeBroadcastTester(
1331     op=tensor.erfc,
1332     expected=expected_erfc,
1333     good=_good_broadcast_unary_normal_float_no_complex,
1334     grad=_grad_broadcast_unary_normal,
1335     eps=2e-10,
1336     mode=mode_no_scipy,
1337     skip=skip_scipy)
1338 ErfcInplaceTester = makeBroadcastTester(
1339     op=inplace.erfc_inplace,
1340     expected=expected_erfc,
1341     good=_good_broadcast_unary_normal_float_no_complex,
1342     eps=2e-10,
1343     mode=mode_no_scipy,
1344     inplace=True,
1345     skip=skip_scipy)
1346 ErfcxTester = makeBroadcastTester(
1347     op=tensor.erfcx,
1348     expected=expected_erfcx,
1349     good=_good_broadcast_unary_normal_float_no_complex_small_neg_range,
1350     grad=_grad_broadcast_unary_normal_small_neg_range,
1351     eps=2e-10,
1352     mode=mode_no_scipy,
1353     skip=skip_scipy)
1354 ErfcxInplaceTester = makeBroadcastTester(
1355     op=inplace.erfcx_inplace,
1356     expected=expected_erfcx,
1357     good=_good_broadcast_unary_normal_float_no_complex_small_neg_range,
1358     eps=2e-10,
1359     mode=mode_no_scipy,
1360     inplace=True,
1361     skip=skip_scipy)
1362 ErfinvTester = makeBroadcastTester(
1363     op=tensor.erfinv,
1364     expected=expected_erfinv,
1365     good={'normal': [rand_ranged(-.9, .9, (2, 3))],
1366           'empty': [np.asarray([], dtype=config.floatX)]},
1367     grad=_grad_broadcast_unary_abs1_no_complex,
1368     eps=2e-10,
1369     mode=mode_no_scipy,
1370     skip=skip_scipy)
1371 ErfcinvTester = makeBroadcastTester(
1372     op=tensor.erfcinv,
1373     expected=expected_erfcinv,
1374     good={'normal': [rand_ranged(0.001, 1.9, (2, 3))],
1375           'empty': [np.asarray([], dtype=config.floatX)]},
1376     grad=_grad_broadcast_unary_0_2_no_complex,
1377     eps=2e-10,
1378     mode=mode_no_scipy,
1379     skip=skip_scipy)
1380 _good_broadcast_unary_gammaln = dict(
1381     normal=(rand_ranged(-1 + 1e-2, 10, (2, 3)),),
1382     empty=(np.asarray([], dtype=config.floatX),),
1383     int=(randint_ranged(1, 10, (2, 3)),),
1384     uint8=(randint_ranged(1, 6, (2, 3)).astype('uint8'),),
1385     uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),),
1386     uint64=(randint_ranged(1, 10, (2, 3)).astype('uint64'),))
1387 _grad_broadcast_unary_gammaln = dict(
1388     normal=(rand_ranged(1e-1, 8, (2, 3)),),)
1389 GammaTester = makeBroadcastTester(
1390     op=tensor.gamma,
1391     expected=expected_gamma,
1392     good=_good_broadcast_unary_gammaln,
1393     grad=_grad_broadcast_unary_gammaln,
1394     mode=mode_no_scipy,
1395     eps=1e-5,
1396     skip=skip_scipy)
1397 GammaInplaceTester = makeBroadcastTester(
1398     op=inplace.gamma_inplace,
1399     expected=expected_gamma,
1400     good=_good_broadcast_unary_gammaln,
1401     mode=mode_no_scipy,
1402     eps=1e-5,
1403     inplace=True,
1404     skip=skip_scipy)
1405 GammalnTester = makeBroadcastTester(
1406     op=tensor.gammaln,
1407     expected=expected_gammaln,
1408     good=_good_broadcast_unary_gammaln,
1409     grad=_grad_broadcast_unary_gammaln,
1410     eps=2e-10,
1411     mode=mode_no_scipy,
1412     skip=skip_scipy)
1413 GammalnInplaceTester = makeBroadcastTester(
1414     op=inplace.gammaln_inplace,
1415     expected=expected_gammaln,
1416     good=_good_broadcast_unary_gammaln,
1417     eps=2e-10,
1418     mode=mode_no_scipy,
1419     inplace=True,
1420     skip=skip_scipy)
1421 _good_broadcast_unary_psi = dict(
1422     normal=(rand_ranged(1, 10, (2, 3)),),
1423     empty=(np.asarray([], dtype=config.floatX),),
1424     int=(randint_ranged(1, 10, (2, 3)),),
1425     uint8=(randint_ranged(1, 10, (2, 3)).astype('uint8'),),
1426     uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),))
1427 PsiTester = makeBroadcastTester(
1428     op=tensor.psi,
1429     expected=expected_psi,
1430     good=_good_broadcast_unary_psi,
1431     eps=2e-10,
1432     mode=mode_no_scipy,
1433     skip=skip_scipy)
1434 PsiInplaceTester = makeBroadcastTester(
1435     op=inplace.psi_inplace,
1436     expected=expected_psi,
1437     good=_good_broadcast_unary_psi,
1438     eps=2e-10,
1439     mode=mode_no_scipy,
1440     inplace=True,
1441     skip=skip_scipy)
1442 _good_broadcast_unary_tri_gamma = _good_broadcast_unary_psi
1443 TriGammaTester = makeBroadcastTester(
1444     op=tensor.tri_gamma,
1445     expected=expected_tri_gamma,
1446     good=_good_broadcast_unary_psi,
1447     eps=2e-8,
1448     mode=mode_no_scipy,
1449     skip=skip_scipy)
1450 TriGammaInplaceTester = makeBroadcastTester(
1451     op=inplace.tri_gamma_inplace,
1452     expected=expected_tri_gamma,
1453     good=_good_broadcast_unary_tri_gamma,
1454     eps=2e-8,
1455     mode=mode_no_scipy,
1456     inplace=True,
1457     skip=skip_scipy)
1458 _good_broadcast_unary_chi2sf = dict(
1459     normal=(rand_ranged(1, 10, (2, 3)),
1460             np.asarray(1, dtype<font color="#e77471"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=config.floatX)),
1461     empty=(np.asarray([], dtype=config.floatX),
1462            np.asarray(1, dtype=config.</b></font>floatX)),
1463     integers=(randint_ranged(1, 10, (2, 3)),
1464               np.asarray(1, dtype=config.floatX)),
1465     uint8=(randint_ranged(1, 10, (2, 3)).astype('uint8'),
1466            np.asarray(1, dtype=config.floatX)),
1467     uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),
1468             np.asarray(1, dtype=config.floatX)))
1469 Chi2SFTester = makeBroadcastTester(
1470     op=tensor.chi2sf,
1471     expected=expected_chi2sf,
1472     good=_good_broadcast_unary_chi2sf,
1473     eps=2e-10,
1474     mode=mode_no_scipy,
1475     skip=skip_scipy,
1476     name='Chi2SF')
1477 Chi2SFInplaceTester = makeBroadcastTester(
1478     op=inplace.chi2sf_inplace,
1479     expected=expected_chi2sf,
1480     good=_good_broadcast_unary_chi2sf,
1481     eps=2e-10,
1482     mode=mode_no_scipy,
1483     inplace=True,
1484     skip=skip_scipy,
1485     name='Chi2SF')
1486 _good_broadcast_unary_bessel = dict(
1487     normal=(rand_ranged(-10, 10, (2, 3)),),
1488     empty=(np.asarray([], dtype=config.floatX),),
1489     int=(randint_ranged(-10, 10, (2, 3)),),
1490     uint8=(randint_ranged(0, 10, (2, 3)).astype('uint8'),),
1491     uint16=(randint_ranged(0, 10, (2, 3)).astype('uint16'),))
1492 _grad_broadcast_unary_bessel = dict(
1493     normal=(rand_ranged(-10., 10., (2, 3)),),)
1494 _good_broadcast_binary_bessel = dict(
1495     normal=(rand_ranged(-5, 5, (2, 3)),
1496             rand_ranged(0, 10, (2, 3))),
1497     empty=(np.asarray([], dtype=config.floatX),
1498            np.asarray([], dtype=config.floatX)),
1499     integers=(randint_ranged(-5, 5, (2, 3)),
1500               randint_ranged(-10, 10, (2, 3))),
1501     uint8=(randint_ranged(0, 5, (2, 3)).astype('uint8'),
1502            randint_ranged(0, 10, (2, 3)).astype('uint8')),
1503     uint16=(randint_ranged(0, 5, (2, 3)).astype('uint16'),
1504             randint_ranged(0, 10, (2, 3)).astype('uint16')))
1505 _grad_broadcast_binary_bessel = dict(
1506     normal=(rand_ranged(1, 5, (2, 3)),
1507             rand_ranged(0, 10, (2, 3))))
1508 J0Tester = makeBroadcastTester(
1509     op=tensor.j0,
1510     expected=expected_j0,
1511     good=_good_broadcast_unary_bessel,
1512     grad=_grad_broadcast_unary_bessel,
1513     eps=2e-10,
1514     mode=mode_no_scipy,
1515     skip=skip_scipy)
1516 J0InplaceTester = makeBroadcastTester(
1517     op=inplace.j0_inplace,
1518     expected=expected_j0,
1519     good=_good_broadcast_unary_bessel,
1520     eps=2e-10,
1521     mode=mode_no_scipy,
1522     inplace=True,
1523     skip=skip_scipy)
1524 J1Tester = makeBroadcastTester(
1525     op=tensor.j1,
1526     expected=expected_j1,
1527     good=_good_broadcast_unary_bessel,
1528     grad=_grad_broadcast_unary_bessel,
1529     eps=2e-10,
1530     mode=mode_no_scipy,
1531     skip=skip_scipy)
1532 J1InplaceTester = makeBroadcastTester(
1533     op=inplace.j1_inplace,
1534     expected=expected_j1,
1535     good=_good_broadcast_unary_bessel,
1536     eps=2e-10,
1537     mode=mode_no_scipy,
1538     inplace=True,
1539     skip=skip_scipy)
1540 JvTester = makeBroadcastTester(
1541     op=tensor.jv,
1542     expected=expected_jv,
1543     eps=2e-10,
1544     mode=mode_no_scipy,
1545     skip<font color="#3ea99f"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=skip_scipy)
1546 JvInplaceTester = makeBroadcastTester(
1547     op=inplace.jv_inplace,
1548     expected=expected_jv,
1549     good=_good_broadcast_binary_bessel,
1550     eps=2e-10,
1551     mode=mode_no_scipy,
1552     inplace=True,
1553     skip=skip_scipy)
1554 def</b></font> test_verify_jv_grad():
1555     if skip_scipy:
1556         raise SkipTest("SciPy needed")
1557     v_val, x_val = _grad_broadcast_binary_bessel['normal']
1558     def fixed_first_input_jv(x):
1559         return tensor.jv(v_val, x)
1560     utt.verify_grad(fixed_first_input_jv, [x_val])
1561 I0Tester = makeBroadcastTester(
1562     op=tensor.i0,
1563     expected=expected_i0,
1564     good=_good_broadcast_unary_bessel,
1565     grad=_grad_broadcast_unary_bessel,
1566     eps=2e-10,
1567     mode=mode_no_scipy,
1568     skip=skip_scipy)
1569 I0InplaceTester = makeBroadcastTester(
1570     op=inplace.i0_inplace,
1571     expected=expected_i0,
1572     good=_good_broadcast_unary_bessel,
1573     eps=2e-10,
1574     mode=mode_no_scipy,
1575     inplace=True,
1576     skip=skip_scipy)
1577 I1Tester = makeBroadcastTester(
1578     op=tensor.i1,
1579     expected=expected_i1,
1580     good=_good_broadcast_unary_bessel,
1581     grad=_grad_broadcast_unary_bessel,
1582     eps=2e-10,
1583     mode=mode_no_scipy,
1584     skip=skip_scipy)
1585 I1InplaceTester = makeBroadcastTester(
1586     op=inplace.i1_inplace,
1587     expected=expected_i1,
1588     good=_good_broadcast_unary_bessel,
1589     eps=2e-10,
1590     mode=mode_no_scipy,
1591     inplace=True,
1592     skip=skip_scipy)
1593 IvTester = makeBroadcastTester(
1594     op=tensor.iv,
1595     expected=expected_iv,
1596     eps=2e-10,
1597     mode=mode_no_scipy,
1598     skip<font color="#af7a82"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=skip_scipy)
1599 IvInplaceTester = makeBroadcastTester(
1600     op=inplace.iv_inplace,
1601     expected=expected_iv,
1602     good=_good_broadcast_binary_bessel,
1603     eps=2e-10,
1604     mode=mode_no_scipy,
1605     inplace=True,
1606     skip=skip_scipy)
1607 def</b></font> test_verify_iv_grad():
1608     if skip_scipy:
1609         raise SkipTest("SciPy needed")
1610     v_val, x_val = _grad_broadcast_binary_bessel['normal']
1611     def fixed_first_input_iv(x):
1612         return tensor.iv(v_val, x)
1613     utt.verify_grad(fixed_first_input_iv, [x_val])
1614 ZerosLikeTester = makeBroadcastTester(
1615     op=tensor.zeros_like,
1616     expected=np.zeros_like,
1617     good=_good_broadcast_unary_normal,
1618     grad=_grad_broadcast_unary_normal,
1619     name='ZerosLike')
1620 OnesLikeTester = makeBroadcastTester(
1621     op=tensor.ones_like,
1622     expected=np.ones_like,
1623     good=_good_broadcast_unary_normal,
1624     grad=_grad_broadcast_unary_normal,
1625     name='OnesLike')
1626 _good_complex_from_polar = dict(
1627     same_shapes=(abs(rand(2, 3)), rand(2, 3)),
1628     not_same_dimensions=(abs(rand(2, 2)), rand(2)),
1629     scalar=(abs(rand(2, 3)), rand(1, 1)),
1630     row=(abs(rand(2, 3)), rand(1, 3)),
1631     column=(abs(rand(2, 3)), rand(2, 1)),
1632     integers=(abs(randint(2, 3)), randint(2, 3)),
1633     empty=(np.asarray([], dtype=config.floatX),
1634            np.asarray([1], dtype=config.floatX)),)
1635 _grad_complex_from_polar = dict(
1636     same_shapes=(abs(rand(2, 3)), rand(2, 3)),
1637     scalar=(abs(rand(2, 3)), rand(1, 1)),
1638     row=(abs(rand(2, 3)), rand(1, 3)),
1639     column=(abs(rand(2, 3)), rand(2, 1)))
1640 ComplexFromPolarTester = makeBroadcastTester(
1641     op=tensor.complex_from_polar,
1642     expected=lambda r, theta: r * np.cos(theta) + 1j * r * np.sin(theta),
1643     good=_good_complex_from_polar)
1644 ConjTester = makeBroadcastTester(
1645     op=tensor.conj,
1646     expected=np.conj,
1647     good=_good_broadcast_unary_normal)
1648 ConjInplaceTester = makeBroadcastTester(
1649     op=inplace.conj_inplace,
1650     expected=np.conj,
1651     good=_good_broadcast_unary_normal,
1652     inplace=True)
1653 DotTester = makeTester(
1654     name='DotTester',
1655     op=dot,
1656     expected=lambda x, y: np.dot(x, y),
1657     checks={},
1658     good=dict(correct1=(rand(5, 7), rand(7, 5)),
1659               correct2=(rand(5, 7), rand(7, 9)),
1660               correct3=(rand(5, 7), rand(7)),
1661               correct4=(rand(5), rand(5, 7)),
1662               mixed1=(rand(5).astype('float32'), rand(5, 7)),
1663               mixed2=(rand(5).astype('float64'), rand(5, 7)),
1664               complex1=(randcomplex(5, 7), randcomplex(7)),
1665               complex2=(rand(5, 7), randcomplex(7)),
1666               complex3=(randcomplex(5, 7), rand(7)),
1667               empty1=(np.asarray([], dtype=config.floatX),
1668                       np.asarray([], dtype=config.floatX)),
1669               empty2=(rand(5, 0), rand(0, 2)),
1670               empty3=(rand(0, 5), rand(5, 0)),
1671               ),
1672     bad_build=dict(),
1673     bad_runtime=dict(bad1=(rand(5, 7), rand(5, 7)),
1674                      bad2=(rand(5, 7), rand(8, 3))))
1675 BatchedDotTester = makeTester(
1676     name='BatchedDotTester',
1677     op=batched_dot,
1678     expected=(lambda xs, ys:
1679               np.asarray(
1680                   list(x * y if x.ndim == 0 or y.ndim == 0 else np.dot(x, y)
1681                        for x, y in zip(xs, ys)),
1682                   dtype=theano.scalar.upcast(xs.dtype, ys.dtype))),
1683     checks={},
1684     grad=dict(correct1=(rand(3, 5, 7), rand(3, 7, 5)),
1685               correct2=(rand(3, 5, 7), rand(3, 7, 9)),
1686               correct3=(rand(3, 5, 7), rand(3, 7)),
1687               correct4=(rand(3, 5), rand(3, 5, 7)),
1688               correct5=(rand(3), rand(3, 5, 7)),
1689               correct6=(rand(3, 5), rand(3)),
1690               correct7=(rand(3, 5), rand(3, 5)),
1691               correct8=(rand(3), rand(3)),
1692               correct9=(rand(3, 5, 7, 11), rand(3)),
1693               correct10=(rand(3, 2, 6, 5), rand(3, 5)),
1694               correct11=(rand(3, 2, 6, 5), rand(3, 5, 7)),
1695               correct12=(rand(3, 2, 6, 5), rand(3, 7, 5, 8)),
1696               mixed1=(rand(3, 5).astype('float32'),
1697                       rand(3, 5, 7)),
1698               mixed2=(rand(3, 5).astype('float64'),
1699                       rand(3, 5, 7))),
1700     good=dict(correct1=(rand(3, 5, 7), rand(3, 7, 5)),
1701               correct2=(rand(3, 5, 7), rand(3, 7, 9)),
1702               correct3=(rand(3, 5, 7), rand(3, 7)),
1703               correct4=(rand(3, 5), rand(3, 5, 7)),
1704               correct5=(rand(3), rand(3, 5, 7)),
1705               correct6=(rand(3, 5), rand(3)),
1706               correct7=(rand(3, 5), rand(3, 5)),
1707               correct8=(rand(3), rand(3)),
1708               correct9=(rand(3, 5, 7, 11), rand(3)),
1709               correct10=(rand(3, 7, 11, 5), rand(3, 5)),
1710               correct11=(rand(3, 7, 11, 5), rand(3, 5, 13)),
1711               correct12=(rand(3, 7, 11, 5), rand(3, 13, 5, 17)),
1712               mixed1=(rand(3, 5).astype('float32'),
1713               mixed2=(rand(3, 5).astype('float64'),
1714                       rand(3, 5, 7))),
1715     bad_build=dict(no_batch_axis2=(rand(), rand<font color="#717d7d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, 5)),
1716                    no_batch_axis3=(rand(3, 5), rand())),
1717     bad_runtime=dict(batch_dim_mismatch1=(rand(2, 5, 7), rand(3, 7, 9)),
1718                      batch_dim_mismatch2=</b></font>(rand(3, 5, 7), rand(2, 7, 9)),
1719                      batch_dim_mismatch3=(rand(3), rand(5)),
1720                      bad_dim1=(rand(3, 5, 7), rand(3, 5, 7)),
1721                      bad_dim2=(rand(3, 5, 7), rand(3, 8, 3)),
1722                      bad_dim3=(rand(3, 5), rand(3, 7)),
1723                      bad_dim4=(rand(3, 5, 7, 11), rand(3, 5)),
1724                      bad_dim5=(rand(3, 5, 7, 11), rand(3, 5, 13)),
1725                      bad_dim6=(rand(3, 5, 7, 11), rand(3, 13, 5, 17))))
1726 def _numpy_second(x, y):
1727     return np.broadcast_arrays(x, y)[1]
1728 ALL_DTYPES = ('int8', 'int16', 'int32', 'int64', 'float32', 'float64',
1729               'uint8', 'uint16',
1730               'complex64', 'complex128')
1731 REAL_DTYPES = ALL_DTYPES[:6]
1732 COMPLEX_DTYPES = ALL_DTYPES[-2:]
1733 def multi_dtype_checks(shape1, shape2, dtypes=ALL_DTYPES, nameprefix=''):
1734     for dtype1, dtype2 in itertools.combinations(dtypes, 2):
1735         name1 = '%s_%s_%s' % (nameprefix, dtype1, dtype2)
1736         name2 = '%s_%s_%s' % (nameprefix, dtype2, dtype1)
1737         obj1 = rand_of_dtype(shape1, dtype1)
1738         obj2 = rand_of_dtype(shape2, dtype2)
1739         yield (name1, (obj1, obj2))
1740         yield (name2, (obj2, obj1))
1741 def multi_dtype_cast_checks(shape, dtypes=ALL_DTYPES, nameprefix=''):
1742     for dtype1, dtype2 in itertools.combinations(dtypes, 2):
1743         name1 = '%s_%s_%s' % (nameprefix, dtype1, dtype2)
1744         name2 = '%s_%s_%s' % (nameprefix, dtype2, dtype1)
1745         obj1 = rand_of_dtype(shape, dtype1)
1746         obj2 = rand_of_dtype(shape, dtype2)
1747         yield (name1, (obj1, dtype2))
1748         yield (name2, (obj2, dtype1))
1749 SecondBroadcastTester = makeTester(
1750     name='SecondBroadcastTester',
1751     op=second,
1752     expected=_numpy_second,
1753     good=dict(itertools.chain(
1754         multi_dtype_checks((4, 5), (5,)),
1755         multi_dtype_checks((2, 3, 2), (3, 2)),
1756         multi_dtype_checks((2, 3, 2), (2,)),
1757         )),
1758     bad_runtime=dict(
1759         fail1=(rand(5, 4), rand(5)),
1760         fail2=(rand(3, 2, 3), rand(6, 9)),
1761         fail3=(randint(6, 2, 9), rand(3, 2)),
1762         )
1763     )
1764 SecondSameRankTester = makeTester(
1765     name='SecondSameRankTester',
1766     op=second,
1767     expected=_numpy_second,
1768     good=dict(itertools.chain(
1769         multi_dtype_checks((4, 5), (4, 5)),
1770         multi_dtype_checks((1, 2), (3, 2)),
1771         multi_dtype_checks((3, 2), (1, 2)),
1772         )),
1773     bad_runtime=dict(itertools.chain(
1774         multi_dtype_checks((4, 5), (5, 4)),
1775         multi_dtype_checks((1, 5), (5, 4)),
1776         )),
1777     mode=get_default_mode().excluding(
1778         'local_fill_to_alloc',
1779         'local_useless_fill')
1780     )
1781 AllocTester = makeBroadcastTester(
1782     name='AllocTester',
1783     op=alloc,
1784     expected=(lambda x, *shp: np.zeros(shp, dtype=x.dtype) + x),
1785     good=dict(
1786         correct01=(rand(), np.int32(7)),
1787         correct01_bcast=(rand(1), np.int32(7)),
1788         correct02=(rand(), np.int32(4), np.int32(7)),
1789         correct12=(rand(7), np.int32(4), np.int32(7)),
1790         correct13=(rand(7), np.int32(2), np.int32(4), np.int32(7)),
1791         correct23=(rand(4, 7), np.int32(2), np.int32(4), np.int32(7)),
1792         correctb1=(rand(1, 7), np.int32(4), np.int32(7)),
1793         correctb2=(rand(1, 7), np.int32(2), np.int32(4), np.int32(7)),
1794         correctb3=(rand(7, 1), np.int32(7), np.int32(4)),
1795         correctb4=(rand(7, 1), np.int32(2), np.int32(7), np.int32(4)),
1796         ),
1797     bad_runtime=dict(
1798         bad_shape12=(rand(7), np.int32(7), np.int32(5)),
1799         ),
1800     bad_build=dict(
1801         vec=(rand(1), [np.int32(2)]),
1802         too_big32=(rand(6, 2, 4), np.int32(6), np.int32(2)),
1803         too_big32b=(rand(6, 2, 4), np.int32(6), np.int32(4)),
1804         too_big32c=(rand(6, 2, 4), np.int32(2), np.int32(4)),
1805         too_big32d=(rand(6, 2, 4), np.int32(2), np.int32(6)),
1806         too_big32e=(rand(6, 2, 4), np.int32(4), np.int32(6)),
1807         too_big32f=(rand(6, 2, 4), np.int32(4), np.int32(2)),
1808         ),
1809     )
1810 s1, s2, s3 = randint_ranged(1, 13, (3,))
1811 Alloc01GradTester = makeBroadcastTester(
1812     name='Alloc01GradTester',
1813     op=(lambda x: alloc(x, s1)),
1814     expected=(lambda x: np.zeros((s1,), dtype=x.dtype) + x),
1815     grad=dict(
1816         x1=(rand(),),
1817         x2=(rand(),),
1818         x3=(rand(),),
1819         ),
1820     )
1821 Alloc13GradTester = makeBroadcastTester(
1822     name='Alloc13GradTester',
1823     op=(lambda x: alloc(x, s1, s2, s3)),
1824     expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
1825     grad=dict(
1826         x1=(rand(s3),),
1827         x2=(rand(s3),),
1828         x3=(rand(s3),),
1829         ),
1830     )
1831 Allocb1GradTester = makeBroadcastTester(
1832     name='Allocb1GradTester',
1833     op=lambda x: alloc(x, s1, s2),
1834     expected=(lambda x: np.zeros((s1, s2), dtype=x.dtype) + x),
1835     grad=dict(
1836         x1=(rand(1, s2),),
1837         x2=(rand(1, s2),),
1838         x3=(rand(1, s2),),
1839     ),
1840 )
1841 Allocb2GradTester = makeBroadcastTester(
1842     name='Allocb2GradTester',
1843     op=lambda x: alloc(x, s1, s2, s3),
1844     expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
1845     grad=dict(
1846         x1=(rand(1, s3),),
1847         x2=(rand(1, s3),),
1848         x3=(rand(1, s3),),
1849     ),
1850 )
1851 Allocb3GradTester = makeBroadcastTester(
1852     name='Allocb3GradTester',
1853     op=lambda x: alloc(x, s1, s2),
1854     expected=(lambda x: np.zeros((s1, s2), dtype=x.dtype) + x),
1855     grad=dict(
1856         x1=(rand(s1, 1),),
1857         x2=(rand(s1, 1),),
1858         x3=(rand(s1, 1),),
1859     ),
1860 )
1861 Allocb4GradTester = makeBroadcastTester(
1862     name='Allocb4GradTester',
1863     op=lambda x: alloc(x, s1, s2, s3),
1864     expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
1865     grad=dict(
1866         x1=(rand(s2, 1),),
1867         x2=(rand(s2, 1),),
1868         x3=(rand(s2, 1),),
1869     ),
1870 )
1871 AllocDimshuffleGradTester = makeBroadcastTester(
1872     name='Allocb4GradTester',
1873     op=lambda x: alloc(x.dimshuffle('x', 'x', 0), 1, s2, s3),
1874     expected=(lambda x: np.zeros((1, s2, s3), dtype=x.dtype) + x),
1875     grad=dict(
1876         x1=(rand(s3),),
1877         x2=(rand(s3),),
1878         x3=(rand(s3),),
1879     ),
1880 )
1881 AllocDimshuffleGradTester2 = makeBroadcastTester(
1882     name='Allocb4GradTester',
1883     op=lambda x: alloc(x.dimshuffle('x', 0), 1, s2, s3),
1884     expected=(lambda x: np.zeros((1, s2, s3), dtype=x.dtype) + x),
1885     grad=dict(
1886         x1=(rand(s3),),
1887         x2=(rand(s3),),
1888         x3=(rand(s3),),
1889     ),
1890 )
1891 class ApplyDefaultTestOp(theano.Op):
1892     def __init__(self, id):
1893         self.default_output = id
1894     def make_node(self, x):
1895         x = theano.tensor.as_tensor_variable(x)
1896         return theano.Apply(self, [x], [x.type()])
1897 class TestAsTensorVariable(unittest.TestCase):
1898     def setUp(self):
1899         self.x = tensor.scalar('x')
1900     def test_one_output(self):
1901         good_apply_var = ApplyDefaultTestOp(0).make_node(self.x)
1902         as_tensor_variable(good_apply_var)
1903     def test_below_zero_output(self):
1904         bad_apply_var = ApplyDefaultTestOp(-1).make_node(self.x)
1905         self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
1906     def test_above_output_len(self):
1907         bad_apply_var = ApplyDefaultTestOp(2).make_node(self.x)
1908         self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
1909     def test_list(self):
1910         bad_apply_var = ApplyDefaultTestOp([0, 1]).make_node(self.x)
1911         self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
1912     def test_strip_leading_broadcastable(self):
1913         x = tensor.TensorType(config.floatX, (True, False))('x')
1914         x = as_tensor_variable(x, ndim=1)
1915         assert(x.ndim == 1)
1916         x = tensor.matrix('x', dtype=config.floatX)
1917         self.assertRaises(ValueError, as_tensor_variable, x, ndim=1)
1918 class TestAlloc(unittest.TestCase):
1919     dtype = config.floatX
1920     mode = mode_opt
1921     shared = staticmethod(theano.shared)
1922     allocs = [tensor.Alloc()] * 3
1923     def setUp(self):
1924         self.rng = np.random.RandomState(seed=utt.fetch_seed())
1925     def test_alloc_constant_folding(self):
1926         test_params = np.asarray(self.rng.randn(50 * 60),
1927                                  self.dtype)
1928         some_vector = vector('some_vector', dtype=self.dtype)
1929         some_matrix = some_vector.reshape((60, 50))
1930         variables = self.shared(np.ones((50,), dtype=self.dtype))
1931         idx = tensor.constant(np.arange(50))
1932         for alloc_, (subtensor, n_alloc) in zip(self.allocs, [
1933                 (some_matrix[:60], 2),
1934                 (some_matrix[arange(60)], 2),
1935                 (some_matrix[idx, idx], 1)
1936         ]):
1937             derp = sum(dot(subtensor, variables))
1938             fobj = theano.function([some_vector], derp, mode=self.mode)
1939             grad_derp = theano.grad(derp, some_vector)
1940             fgrad = theano.function([some_vector], grad_derp,
1941                                     mode=self.mode)
1942             topo_obj = fobj.maker.fgraph.toposort()
1943             assert np.sum([isinstance(node.op, type(alloc_))
1944                            for node in topo_obj]) == 0
1945             topo_grad = fgrad.maker.fgraph.toposort()
1946             assert np.sum([isinstance(node.op, type(alloc_))
1947                            for node in topo_grad]) == n_alloc, (
1948                                alloc_, subtensor, n_alloc, topo_grad)
1949             fobj(test_params)
1950             fgrad(test_params)
1951     def test_alloc_output(self):
1952         val = tensor.constant(self.rng.randn(1, 1), dtype=self.dtype)
1953         for alloc_ in self.allocs:
1954             out = alloc_(val, 50, 60)
1955             f = theano.function([], out, mode=self.mode)
1956             topo = f.maker.fgraph.toposort()
1957             assert np.sum([isinstance(node.op, type(alloc_))
1958                            for node in topo]) == 1
1959             assert not isinstance(topo[0].op, DeepCopyOp)
1960     def test_ones(self):
1961         for shp in [[], 1, [1], [1, 2], [1, 2, 3]]:
1962             ones = theano.function([], [tensor.ones(shp)], mode=self.mode)
1963             assert np.allclose(ones(), np.ones(shp))
1964         x = scalar()
1965         shp = []
1966         ones_scalar = theano.function([], [tensor.ones(x.shape)],
1967                                       mode=self.mode)
1968         assert np.allclose(ones_scalar(), np.ones(shp))
1969         for (typ, shp) in [(vector, [3]), (matrix, [3, 4])]:
1970             x = typ()
1971             ones_tensor = theano.function([x], [tensor.ones(x.shape)],
1972                                           mode=self.mode)
1973             inp = np.zeros(shp, dtype=config.floatX)
1974             assert np.allclose(ones_tensor(inp),
1975                                np.ones(shp))
1976     def test_zeros(self):
1977         for shp in [[], 1, [1], [1, 2], [1, 2, 3]]:
1978             zeros = theano.function([], [tensor.zeros(shp)],
1979                                     mode=self.mode)
1980             assert np.allclose(zeros(), np.zeros(shp))
1981         x = scalar()
1982         shp = []
1983         zeros_scalar = theano.function([], [tensor.zeros(x.shape)],
1984                                        mode=self.mode)
1985         assert np.allclose(zeros_scalar(), np.zeros(shp))
1986         for (typ, shp) in [(vector, [3]), (matrix, [3, 4])]:
1987             x = typ()
1988             zeros_tensor = theano.function([x], [tensor.zeros(x.shape)],
1989                                            mode=self.mode)
1990             inp = np.zeros(shp, dtype=config.floatX)
1991             assert np.allclose(zeros_tensor(inp),
1992                                np.zeros(shp))
1993 def test_eye():
1994     def check(dtype, N, M_=None, k=0):
1995         M = M_
1996         if M is None and theano.config.mode in ['DebugMode', 'DEBUG_MODE']:
1997             M = N
1998         N_symb = tensor.iscalar()
1999         M_symb = tensor.iscalar()
2000         k_symb = tensor.iscalar()
2001         f = function([N_symb, M_symb, k_symb],
2002                      eye(N_symb, M_symb, k_symb, dtype=dtype))
2003         result = f(N, M, k)
2004         assert np.allclose(result, np.eye(N, M_, k, dtype=dtype))
2005         assert result.dtype == np.dtype(dtype)
2006     for dtype in ALL_DTYPES:
2007         yield check, dtype, 3
2008         yield check, dtype, 3, 5
2009         yield check, dtype, 5, 3
2010         yield check, dtype, 3, 3, 1
2011         yield check, dtype, 3, 3, -1
2012         yield check, dtype, 3, 5, 1
2013         yield check, dtype, 3, 5, -1
2014         yield check, dtype, 5, 3, 1
2015         yield check, dtype, 5, 3, -1
2016 class test_triangle(unittest.TestCase):
2017     def test_tri(self):
2018         def check(dtype, N, M_=None, k=0):
2019             M = M_
2020             if M is None and theano.config.mode in ['DebugMode', 'DEBUG_MODE']:
2021                 M = N
2022             N_symb = tensor.iscalar()
2023             M_symb = tensor.iscalar()
2024             k_symb = tensor.iscalar()
2025             f = function([N_symb, M_symb, k_symb],
2026                          tri(N_symb, M_symb, k_symb, dtype=dtype))
2027             result = f(N, M, k)
2028             self.assertTrue(
2029                 np.allclose(result, np.tri(N, M_, k, dtype=dtype)))
2030             self.assertTrue(result.dtype == np.dtype(dtype))
2031         for dtype in ALL_DTYPES:
2032             yield check, dtype, 3
2033             yield check, dtype, 3, 5
2034             yield check, dtype, 5, 3
2035             yield check, dtype, 3, 3, 1
2036             yield check, dtype, 3, 3, -1
2037             yield check, dtype, 3, 5, 1
2038             yield check, dtype, 3, 5, -1
2039             yield check, dtype, 5, 3, 1
2040             yield check, dtype, 5, 3, -1
2041     def test_tril_triu(self):
2042         def check_l(m, k=0):
2043             m_symb = matrix(dtype=m.dtype)
2044             k_symb = iscalar()
2045             f = function([m_symb, k_symb], tril(m_symb, k_symb))
2046             result = f(m, k)
2047             self.assertTrue(np.allclose(result, np.tril(m, k)))
2048             self.assertTrue(result.dtype == np.dtype(dtype))
2049         def check_u(m, k=0):
2050             m_symb = matrix(dtype=m.dtype)
2051             k_symb = iscalar()
2052             f = function([m_symb, k_symb], triu(m_symb, k_symb))
2053             result = f(m, k)
2054             self.assertTrue(np.allclose(result, np.triu(m, k)))
2055             self.assertTrue(result.dtype == np.dtype(dtype))
2056         for dtype in ALL_DTYPES:
2057             m = rand_of_dtype((10, 10), dtype)
2058             yield check_l, m, 0
2059             yield check_l, m, 1
2060             yield check_l, m, -1
2061             yield check_u, m, 0
2062             yield check_u, m, 1
2063             yield check_u, m, -1
2064             m = rand_of_dtype((10, 5), dtype)
2065             yield check_l, m, 0
2066             yield check_l, m, 1
2067             yield check_l, m, -1
2068             yield check_u, m, 0
2069             yield check_u, m, 1
2070             yield check_u, m, -1
2071 class test_nonzero(unittest.TestCase):
2072     def test_nonzero(self):
2073         def check(m):
2074             m_symb = theano.tensor.tensor(dtype=m.dtype,
2075                                           broadcastable=(False,) * m.ndim)
2076             f_tuple = function([m_symb], nonzero(m_symb, return_matrix=False))
2077             f_matrix = function([m_symb], nonzero(m_symb, return_matrix=True))
2078             self.assertTrue(np.allclose(f_matrix(m),
2079                                         np.vstack(np.nonzero(m))))
2080             for i, j in zip(f_tuple(m), np.nonzero(m)):
2081                 self.assertTrue(np.allclose(i, j))
2082         rand0d = np.array(rand())
2083         self.assertRaises(ValueError, check, rand0d)
2084         rand1d = rand(8)
2085         rand1d[:4] = 0
2086         check(rand1d)
2087         rand2d = rand(8, 9)
2088         rand2d[:4] = 0
2089         check(rand2d)
2090         rand3d = rand(8, 9, 10)
2091         rand3d[:4] = 0
2092         check(rand3d)
2093         rand4d = rand(8, 9, 10, 11)
2094         rand4d[:4] = 0
2095         check(rand4d)
2096     def test_flatnonzero(self):
2097         def check(m):
2098             m_symb = theano.tensor.tensor(dtype=m.dtype,
2099                                           broadcastable=(False,) * m.ndim)
2100             f = function([m_symb], flatnonzero(m_symb))
2101             result = f(m)
2102             assert np.allclose(result, np.flatnonzero(m))
2103         rand0d = np.array(rand())
2104         self.assertRaises(ValueError, check, rand0d)
2105         rand1d = rand(8)
2106         rand1d[:4] = 0
2107         check(rand1d)
2108         rand2d = rand(8, 9)
2109         rand2d[:4] = 0
2110         check(rand2d)
2111         rand3d = rand(8, 9, 10)
2112         rand3d[:4] = 0
2113         check(rand3d)
2114         rand4d = rand(8, 9, 10, 11)
2115         rand4d[:4] = 0
2116         check(rand4d)
2117     def test_nonzero_values(self):
2118         def check(m):
2119             m_symb = theano.tensor.tensor(dtype=m.dtype,
2120                                           broadcastable=(False,) * m.ndim)
2121             f = function([m_symb], nonzero_values(m_symb))
2122             result = f(m)
2123             assert np.allclose(result, m[np.nonzero(m)])
2124         rand0d = rand()
2125         self.assertRaises(ValueError, check, rand0d)
2126         rand1d = rand(8)
2127         rand1d[:4] = 0
2128         check(rand1d)
2129         rand2d = rand(8, 9)
2130         rand2d[:4] = 0
2131         check(rand2d)
2132         rand3d = rand(8, 9, 10)
2133         rand3d[:4] = 0
2134         check(rand3d)
2135         rand4d = rand(8, 9, 10, 11)
2136         rand4d[:4] = 0
2137         check(rand4d)
2138 def test_identity():
2139     def check(dtype):
2140         obj = rand_of_dtype((2,), dtype)
2141         sym = tensor.vector(dtype=dtype)
2142         f = function([sym], tensor_copy(sym))
2143         assert np.all(obj == f(obj))
2144         assert obj.dtype == f(obj).dtype
2145         topo = f.maker.fgraph.toposort()
2146         assert len(topo) == 1
2147         if theano.config.mode != 'FAST_COMPILE':
2148             assert isinstance(topo[0].op, DeepCopyOp)
2149     for dtype in ALL_DTYPES:
2150         yield check, dtype
2151 class CastTester(unittest.TestCase):
2152     def test_good_between_real_types(self):
2153         good = itertools.chain(
2154             multi_dtype_cast_checks((2,), dtypes=REAL_DTYPES),
2155             [('%s_%s' % (rand_of_dtype((2,), dtype), dtype),
2156               (rand_of_dtype((2,), dtype), dtype))
2157              for dtype in ALL_DTYPES])
2158         for testname, (obj, dtype) in good:
2159             inp = tensor.vector(dtype=obj.dtype)
2160             out = tensor.cast(inp, dtype=dtype)
2161             f = function([inp], out)
2162             assert f(obj).dtype == np.dtype(dtype)
2163             out2 = inp.astype(dtype=dtype)
2164             assert out2.type == out.type
2165     def test_cast_from_real_to_complex(self):
2166         for real_dtype in REAL_DTYPES:
2167             for complex_dtype in COMPLEX_DTYPES:
2168                 inp = tensor.vector(dtype=real_dtype)
2169                 out = tensor.cast(inp, dtype=complex_dtype)
2170                 f = function([inp], out)
2171                 obj = rand_of_dtype((2, ), real_dtype)
2172                 assert f(obj).dtype == np.dtype(complex_dtype)
2173     def test_cast_from_complex_to_real_raises_error(self):
2174         for real_dtype in REAL_DTYPES:
2175             for complex_dtype in COMPLEX_DTYPES:
2176                 inp = tensor.vector(dtype=real_dtype)
2177                 self.assertRaises(TypeError, tensor.cast(
2178                     inp, dtype=complex_dtype))
2179 ClipTester = makeTester(
2180     name='ClipTester',
2181     op=clip,
2182     expected=lambda x, y, z: np.clip(x, y, z),
2183     good=dict(correct1=((5 * rand(5, 5)).astype('float32'),
2184                         np.array(-1, dtype='float32'),
2185                         np.array(1, dtype='float32')),
2186               correct2=((5 * rand(5, 5)).astype('float64'),
2187                         np.array(-1, dtype='float64'),
2188                         np.array(1, dtype='float64')),
2189               correct3=(randint(5, 5).astype('int8'),
2190                         np.array(-1, dtype='int8'),
2191                         np.array(1, dtype='int8')),
2192               correct4=(randint(5, 5).astype('int16'),
2193                         np.array(-1, dtype='int16'),
2194                         np.array(1, dtype='int16')),
2195               correct5=(randint(5, 5).astype('int32'),
2196                         np.array(-1, dtype='int32'),
2197                         np.array(1, dtype='int32')),
2198               correct6=(randint(5, 5).astype('int64'),
2199                         np.array(-1, dtype='int64'),
2200                         np.array(1, dtype='int64')),
2201               correct8=(randint(0, 5).astype('uint8'),
2202                         np.array(2, dtype='uint8'),
2203                         np.array(4, dtype='uint8')),
2204               correct9=(randint(0, 5).astype('uint16'),
2205                         np.array(2, dtype='uint16'),
2206                         np.array(4, dtype='uint16')),)
2207     )
2208 BackwardsClipTester = makeTester(
2209     name='BackwardsClipTester',
2210     op=clip,
2211     expected=lambda x, y, z: np.where(x &lt; y, y, np.minimum(x, z)),
2212     good=dict(correct7=((5 * rand(5, 5)).astype('float64'),
2213                         np.array(1, dtype='float64'),
2214                         np.array(-1, dtype='float64')),)
2215     )
2216 class T_Clip(unittest.TestCase):
2217     def test_complex_value(self):
2218         for dtype in ['complex64', 'complex128']:
2219             a = tensor.vector(dtype=dtype)
2220             b = tensor.scalar()
2221             c = tensor.scalar()
2222             self.assertRaises(TypeError, clip, a, b, c)
2223     def test_clip_repeat_grad(self):
2224         x, y = tensor.vectors('xy')
2225         a = clip(x, y, x)
2226         g = theano.gradient.grad(a.sum(), x)
2227         fn = theano.function([x, y], [g])
2228         a2 = clip(x, x, y)
2229         g2 = theano.gradient.grad(a2.sum(), x)
2230         fn2 = theano.function([x, y], [g2])
2231         a3 = theano.tensor.clip(x, x, x)
2232         g3 = theano.gradient.grad(a3.sum(), x)
2233         fn3 = theano.function([x], [g3])
2234         rng = np.random.RandomState(utt.fetch_seed())
2235         nvals = 50
2236         xval = rng.rand(nvals).astype(config.floatX)
2237         yval_mn = rng.rand(nvals).astype(config.floatX) - 1.0
2238         yval_mx = rng.rand(nvals).astype(config.floatX) + 1.0
2239         aval, = fn(xval, yval_mn)
2240         aval2, = fn2(xval, yval_mx)
2241         aval3, = fn3(xval)
2242         self.assertTrue(np.all(aval == 1.))
2243         self.assertTrue(np.all(aval2 == 1.))
2244         self.assertTrue(np.all(aval3 == 1.))
2245     def test_clip_repeat_verify_grad(self):
2246         utt.verify_grad(
2247             op=lambda x: clip(x, 0, x),
2248             pt=[rand_nonzero((3, 7))])
2249         utt.verify_grad(
2250             op=lambda x: clip(x, x, 0),
2251             pt=[rand_nonzero((3, 7))])
2252         utt.verify_grad(
2253             op=lambda x: clip(0, x, x),
2254             pt=[rand_nonzero((3, 7))])
2255         utt.verify_grad(
2256             op=lambda x: clip(x, x, x),
2257             pt=[rand_nonzero((3, 7))])
2258 def test_batched_dot():
2259     first = theano.tensor.tensor3("first")
2260     second = theano.tensor.tensor3("second")
2261     output = theano.tensor.basic.batched_dot(first, second)
2262     first_val = np.random.rand(10, 10, 20).astype(config.floatX)
2263     second_val = np.random.rand(10, 20, 5).astype(config.floatX)
2264     result_fn = theano.function([first, second], output)
2265     result = result_fn(first_val, second_val)
2266     assert result.shape[0] == first_val.shape[0]
2267     assert result.shape[1] == first_val.shape[1]
2268     assert result.shape[2] == second_val.shape[2]
2269     first_mat = theano.tensor.dmatrix("first")
2270     second_mat = theano.tensor.dmatrix("second")
2271     output = theano.tensor.basic.batched_dot(first_mat, second_mat)
2272     first_mat_val = np.random.rand(10, 10).astype(config.floatX)
2273     second_mat_val = np.random.rand(10, 10).astype(config.floatX)
2274     result_fn = theano.function([first_mat, second_mat], output)
2275     result = result_fn(first_mat_val, second_mat_val)
2276     assert result.shape[0] == first_mat_val.shape[0]
2277 def test_batched_dot_not_contiguous():
2278     def np_genarray(*_shape):
2279         size = 1
2280         for dimsize in _shape:
2281             size *= dimsize
2282         return np.arange(size, dtype=floatX).reshape(_shape)
2283     X = tensor3()
2284     W = tensor3()
2285     Z = batched_dot(X, W)
2286     f = function([X, W], Z)
2287     w = np_genarray(30, 10, 5)
2288     reversed_x_container = np_genarray(20, 40, 30)
2289     x_container = reversed_x_container.T
2290     def check_first_dim(inverted):
2291         direction = -1 if inverted else 1
2292         x = x_container[::direction, ::2, ::2]
2293         assert x.shape == (30, 20, 10)
2294         assert x.strides[0] == direction * np.dtype(floatX).itemsize
2295         assert not (x.flags['C_CONTIGUOUS'] or x.flags['F_CONTIGUOUS'])
2296         result = f(x, w)
2297         ref_result = np.asarray(list(np.dot(u, v) for u, v in zip(x, w)))
2298         utt.assert_allclose(ref_result, result)
2299     for inverted in (0, 1):
2300         yield (check_first_dim, inverted)
2301 def test_batched_tensordot():
2302     first = theano.tensor.tensor4("first")
2303     second = theano.tensor.tensor4("second")
2304     axes = [[1, 2], [3, 1]]
2305     output = theano.tensor.basic.batched_tensordot(first, second, axes)
2306     first_val = np.random.rand(8, 10, 20, 3).astype(config.floatX)
2307     second_val = np.random.rand(8, 20, 5, 10).astype(config.floatX)
2308     result_fn = theano.function([first, second], output)
2309     result = result_fn(first_val, second_val)
2310     assert result.shape[0] == first_val.shape[0]
2311     assert result.shape[1] == first_val.shape[3]
2312     assert result.shape[2] == second_val.shape[2]
2313     first_mat = theano.tensor.dmatrix("first")
2314     second_mat = theano.tensor.dmatrix("second")
2315     axes = 1
2316     output = theano.tensor.basic.batched_tensordot(first_mat, second_mat, axes)
2317     first_mat_val = np.random.rand(10, 4).astype(config.floatX)
2318     second_mat_val = np.random.rand(10, 4).astype(config.floatX)
2319     result_fn = theano.function([first_mat, second_mat], output)
2320     result = result_fn(first_mat_val, second_mat_val)
2321     assert result.shape[0] == first_mat_val.shape[0]
2322     assert len(result.shape) == 1
2323 def test_tensor_values_eq_approx():
2324     a = np.asarray([-np.inf, -1, 0, 1, np.inf, np.nan])
2325     assert TensorType.values_eq_approx(a, a)
2326     b = np.asarray([np.inf, -1, 0, 1, np.inf, np.nan])
2327     assert not TensorType.values_eq_approx(a, b)
2328     b = np.asarray([-np.inf, -1, 0, 1, -np.inf, np.nan])
2329     assert not TensorType.values_eq_approx(a, b)
2330     b = np.asarray([np.inf, -1, 0, 1, 5, np.nan])
2331     assert TensorType.values_eq_approx(a, b, allow_remove_inf=True)
2332     b = np.asarray([np.inf, -1, 0, 1, 5, 6])
2333     assert not TensorType.values_eq_approx(a, b, allow_remove_inf=True)
2334     b = np.asarray([np.inf, -1, 0, 1, 5, np.nan])
2335     assert not TensorType.values_eq_approx(a, b, allow_remove_nan=False)
2336     b = np.asarray([-np.inf, -1, 0, 1, np.inf, 6])
2337     assert not TensorType.values_eq_approx(a, b, allow_remove_nan=False)
2338     test_constants <font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [
2339         [np.nan, np.inf, 0, 1],
2340         [np.nan, np.inf, -np.inf, 1],
2341         [0, np.inf, -np.inf, 1],
2342         [0, 3, -np.inf, 1],
2343         [0, 3, np.</b></font>inf, 1],
2344         [np.nan, 3, 4, 1],
2345         [0, 3, 4, 1],
2346         np.nan,
2347         np.inf,
2348         -np.inf,
2349         0,
2350         1,
2351         ]
2352     n = len(test_constants)
2353     for i in xrange(n):
2354         for j in xrange(n):
2355             x = constant(test_constants[i])
2356             y = constant(test_constants[j])
2357             assert (x.signature() == y.signature()) == (i == j)
2358     x = tensor.scalar()
2359     mode = get_default_mode()
2360     if isinstance(mode, theano.compile.debugmode.DebugMode):
2361         mode = copy(mode)
2362         mode.check_isfinite = False
2363     f = theano.function([x], eq(x, np.nan), mode=mode)
2364     assert f(0) == 0
2365     assert f(np.nan) == 0
2366 def test_isnan():
2367     for x in [tensor.matrix(), tensor.imatrix(), tensor.matrix(dtype='bool')]:
2368         y = tensor.isnan(x)
2369         assert isinstance(y.owner.op, tensor.Elemwise) == (
2370             x.dtype not in tensor.discrete_dtypes)
2371         assert y.dtype == 'bool'
2372         y = tensor.isnan_(x)
2373         assert isinstance(y.owner.op, tensor.Elemwise)
2374         assert y.dtype == 'bool'
2375         f = theano.function([x], y, allow_input_downcast=True)
2376         f([[0, 1, 2]])
2377 class T_Shape(unittest.TestCase):
2378     def test_basic0(self):
2379         s = shape(np.ones((5, 3)))
2380         self.assertTrue((eval_outputs([s]) == [5, 3]).all())
2381     def test_basic1(self):
2382         s = shape(np.ones((2)))
2383         self.assertTrue((eval_outputs([s]) == [2]).all())
2384     def test_basic2(self):
2385         s = shape(np.ones((5, 3, 10)))
2386         self.assertTrue((eval_outputs([s]) == [5, 3, 10]).all())
2387 class T_max_and_argmax(unittest.TestCase):
2388     def setUp(self):
2389         utt.seed_rng()
2390         MaxAndArgmax.debug = 0
2391     def test0(self):
2392         n = as_tensor_variable(5.0)
2393         v, i = eval_outputs(max_and_argmax(n))
2394         self.assertTrue(v == 5.0)
2395         self.assertTrue(i == 0)
2396         assert i.dtype == 'int64'
2397         v = eval_outputs(max_and_argmax(n)[0].shape)
2398         assert len(v) == 0
2399         v = eval_outputs(max_and_argmax(n)[1].shape)
2400         assert len(v) == 0
2401     def test1(self):
2402         n = as_tensor_variable([1, 2, 3, 2, -6])
2403         v, i = eval_outputs(max_and_argmax(n))
2404         self.assertTrue(v == 3)
2405         self.assertTrue(i == 2)
2406         assert i.dtype == 'int64'
2407         v = eval_outputs(max_and_argmax(n)[0].shape)
2408         assert len(v) == 0
2409     def test2(self):
2410         data = rand(2, 3)
2411         n = as_tensor_variable(data)
2412         for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2413                                 ([0, 1], None), ([1, 0], None),
2414                                 (NoneConst.clone(), None),
2415                                 (constant(0), 0)]:
2416             v, i = eval_outputs(max_and_argmax(n, axis))
2417             assert i.dtype == 'int64'
2418             self.assertTrue(np.all(v == np.max(data, np_axis)))
2419             self.assertTrue(np.all(i == np.argmax(data, np_axis)))
2420             v_shape = eval_outputs(max_and_argmax(n, axis)[0].shape)
2421             assert tuple(v_shape) == np.max(data, np_axis).shape
2422     def test2_float16(self):
2423         data = (rand(20, 30).astype("float16") - 0.5) * 20
2424         n = shared(data)
2425         for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2426                                 ([0, 1], None), ([1, 0], None),
2427                                 (NoneConst.clone(), None),
2428                                 (constant(0), 0)]:
2429             v, i = eval_outputs(max_and_argmax(n, axis), (MaxAndArgmax,))
2430             assert i.dtype == 'int64'
2431             self.assertTrue(np.all(v == np.max(data, np_axis)))
2432             self.assertTrue(np.all(i == np.argmax(data, np_axis)))
2433             v_shape = eval_outputs(max_and_argmax(n, axis)[0].shape)
2434             assert tuple(v_shape) == np.max(data, np_axis).shape
2435     def test2_invalid(self):
2436         n = as_tensor_variable(rand(2, 3))
2437         _logger = logging.getLogger('theano.gof.opt')
2438         oldlevel = _logger.level
2439         _logger.setLevel(logging.CRITICAL)
2440         try:
2441             try:
2442                 eval_outputs(max_and_argmax(n, 3))
2443                 assert False
2444             except ValueError:
2445                 pass
2446         finally:
2447             _logger.setLevel(oldlevel)
2448     def test2_invalid_neg(self):
2449         n = as_tensor_variable(rand(2, 3))
2450         old_stderr = sys.stderr
2451         sys.stderr = StringIO()
2452         try:
2453             try:
2454                 eval_outputs(max_and_argmax(n, -3))
2455                 assert False
2456             except ValueError:
2457                 pass
2458         finally:
2459             sys.stderr = old_stderr
2460     def test2_valid_neg(self):
2461         n = as_tensor_variable(rand(2, 3))
2462         v, i = eval_outputs(max_and_argmax(n, -1))
2463         assert i.dtype == 'int64'
2464         self.assertTrue(v.shape == (2,))
2465         self.assertTrue(i.shape == (2,))
2466         self.assertTrue(np.all(v == np.max(n.value, -1)))
2467         self.assertTrue(np.all(i == np.argmax(n.value, -1)))
2468         v, i = eval_outputs(max_and_argmax(n, -2))
2469         assert i.dtype == 'int64'
2470         self.assertTrue(v.shape == (3,))
2471         self.assertTrue(i.shape == (3,))
2472         self.assertTrue(np.all(v == np.max(n.value, -2)))
2473         self.assertTrue(np.all(i == np.argmax(n.value, -2)))
2474         v = eval_outputs(max_and_argmax(n, -1)[0].shape)
2475         assert v == (2)
2476         v = eval_outputs(max_and_argmax(n, -2)[0].shape)
2477         assert v == (3)
2478         data = rand(2, 3, 4)
2479         n = as_tensor_variable(data)
2480         for (<font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2481                                 ([0, 1, 2], None), ([1, 2, 0], None)]:
2482             v, i = eval_outputs(</b></font>max_and_argmax(n, axis))
2483             assert i.dtype == 'int64'
2484             self.assertTrue(np.all(v == np.max(data, np_axis)))
2485             self.assertTrue(np.all(i == np.argmax(data, np_axis)))
2486             v = eval_outputs(max_and_argmax(n, axis)[0].shape)
2487             assert tuple(v) == np.max(data, np_axis).shape
2488     def test_arg_grad(self):
2489         x = matrix()
2490         cost = argmax(x, axis=0).sum()
2491         gx = grad(cost, x)
2492         val = tensor.get_scalar_constant_value(gx)
2493         assert val == 0.0
2494     def test_grad(self):
2495         data = rand(2, 3)
2496         n = as_tensor_variable(data)
2497         def safe_verify_grad(func, data):
2498             data_tensor, = data
2499             data_vector = data_tensor.flatten()
2500             diff = np.abs(data_vector.reshape((-1, 1)) - data_vector)
2501             for i in xrange(len(diff)):
2502                 diff[i, i] = 1
2503             eps = builtin_min(numeric_grad.type_eps[config.floatX],
2504                               diff.min() / 2)
2505             utt.verify_grad(func, data, eps=eps)
2506         def check_grad_max(data, max_grad_data, axis=None):
2507             assert axis in [0, None]
2508             z = np.zeros_like(data)
2509             z = z.flatten()
2510             argmax = np.argmax(data, axis=axis)
2511             if argmax.ndim == 0:
2512                 z[argmax] += 1
2513             else:
2514                 for id, v in enumerate(argmax):
2515                     z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
2516                       id] += 1
2517             z = z.reshape(data.shape)
2518             assert np.all(max_grad_data == z)
2519         for axis in (-1, 0, 1, None):
2520             for j in xrange(2):
2521                 safe_verify_grad(lambda v: max_and_argmax(v, axis=axis)[j],
2522                                  [data])
2523                 if axis != 1:
2524                     safe_verify_grad(lambda v: max_and_argmax(v.flatten(),
2525                                                               axis=axis)[j],
2526                                      [data])
2527             if axis in (0, None):
2528                 check_grad_max(data, eval_outputs(grad(
2529                     max_and_argmax(n, axis=axis)[0].sum(), n)), axis=axis)
2530             check_grad_max(data, eval_outputs(grad(
2531                 max_and_argmax(n.flatten())[0], n)))
2532         data = rand(3, 4, 5)
2533         for i in [0, 1, 2]:
2534             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[0], [data])
2535             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[1], [data])
2536         data = rand(2, 3, 4, 5)
2537         for i in [0, 1, 2, 3]:
2538             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[0], [data])
2539             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[1], [data])
2540         for i in [[0, 1], [0, 0]]:
2541             safe_verify_grad(lambda v: max_and_argmax(v, axis=i)[0], [data])
2542             safe_verify_grad(lambda v: max_and_argmax(v, axis=i)[1], [data])
2543     def test_preserve_broadcastable(self):
2544         x = tensor.matrix().dimshuffle('x', 0, 'x', 1, 'x')
2545         y = x.max(axis=1)
2546         assert y.type.broadcastable == (True, True, False, True)
2547     def test_multiple_axes(self):
2548         data = np.arange(24).reshape(3, 2, 4)
2549         x = as_tensor_variable(data)
2550         v, i = eval_outputs(max_and_argmax(x, [1, -1]))
2551         assert np.all(v == np.array([7, 15, 23]))
2552         assert np.all(i == np.array([7, 7, 7]))
2553         v = eval_outputs(max_and_argmax(x, [1, -1])[0].shape)
2554         assert tuple(v) == np.max(data, (1, -1)).shape
2555     def test_zero_shape(self):
2556         x = tensor.matrix()
2557         m, i = max_and_argmax(x, axis=1)
2558         f = theano.function([x], [m, i])
2559         xv = np.zeros((0, 4), dtype=floatX)
2560         mv, iv = f(xv)
2561         assert mv.shape == (0,)
2562         assert iv.shape == (0,)
2563     def test_numpy_input(self):
2564         ar = np.array([1, 2, 3])
2565         max, argmax = max_and_argmax(ar, axis=None)
2566         self.assertEqual(max.eval(), 3)
2567         self.assertEqual(argmax.eval(), 2)
2568 class T_argmin_argmax(unittest.TestCase):
2569     def setUp(self):
2570         utt.seed_rng()
2571         MaxAndArgmax.debug = 0
2572     def test_scalar(self):
2573         for fct in [argmin, argmax]:
2574             n = as_tensor_variable(5.0)
2575             i = eval_outputs(fct(n))
2576             self.assertTrue(i == 0)
2577             v = eval_outputs(fct(n).shape)
2578             assert len(v) == 0
2579     def test_list(self):
2580         n = as_tensor_variable([1, 2, 3, 2, -6])
2581         i = eval_outputs(argmin(n))
2582         self.assertTrue(i == 4)
2583         v = eval_outputs(argmin(n).shape)
2584         assert len(v) == 0
2585         n = as_tensor_variable([1, 2, 3, 2, -6])
2586         i = eval_outputs(argmax(n))
2587         self.assertTrue(i == 2)
2588         v = eval_outputs(argmax(n).shape)
2589         assert len(v) == 0
2590     def test2(self):
2591         data = rand(2, 3)
2592         n = as_tensor_variable(data)
2593         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2594             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2595                                     ([0, 1], None), ([1, 0], None)]:
2596                 v = eval_outputs(fct(n, axis))
2597                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2598                 v_shape = eval_outputs(fct(n, axis).shape)
2599                 assert tuple(v_shape) == nfct(data, np_axis).shape
2600     def test2_float16(self):
2601         data = (rand(20, 30).astype("float16") - 0.5) * 20
2602         n = shared(data)
2603         mode = get_default_mode().including("local_max_and_argmax", "uncanonicalize")
2604         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2605             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2606                                     ([0, 1], None), ([1, 0], None)]:
2607                 v = eval_outputs(fct(n, axis), (Argmax,), mode=mode)
2608                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2609                 v_shape = eval_outputs(fct(n, axis).shape, mode=mode)
2610                 assert tuple(v_shape) == nfct(data, np_axis).shape
2611     def test2_invalid(self):
2612         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2613             n = as_tensor_variable(rand(2, 3))
2614             _logger = logging.getLogger('theano.gof.opt')
2615             oldlevel = _logger.level
2616             _logger.setLevel(logging.CRITICAL)
2617             try:
2618                 try:
2619                     eval_outputs(fct(n, 3))
2620                     assert False
2621                 except ValueError:
2622                     pass
2623             finally:
2624                 _logger.setLevel(oldlevel)
2625     def test2_invalid_neg(self):
2626         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2627             n = as_tensor_variable(rand(2, 3))
2628             old_stderr = sys.stderr
2629             sys.stderr = StringIO()
2630             try:
2631                 try:
2632                     eval_outputs(fct(n, -3))
2633                     assert False
2634                 except ValueError:
2635                     pass
2636             finally:
2637                 sys.stderr = old_stderr
2638     def test2_valid_neg(self):
2639         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2640             n = as_tensor_variable(rand(2, 3))
2641             i = eval_outputs(fct(n, -1))
2642             self.assertTrue(i.shape == (2,))
2643             self.assertTrue(np.all(i == nfct(n.value, -1)))
2644             i = eval_outputs(fct(n, -2))
2645             self.assertTrue(i.shape == (3,))
2646             self.assertTrue(np.all(i == nfct(n.value, -2)))
2647             v = eval_outputs(fct(n, -1).shape)
2648             assert v == (2)
2649             v = eval_outputs(fct(n, -2).shape)
2650             assert v == (3)
2651     def test3(self):
2652         data = rand(2, 3, 4)
2653         n = as_tensor_variable(data)
2654         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2655             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (2, 2),
2656                                     (None, None), ([0, 1, 2], None),
2657                                     ([1, 0, 2], None)]:
2658                 v = eval_outputs(fct(n, axis))
2659                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2660                 v_shape = eval_outputs(fct(n, axis).shape)
2661                 assert tuple(v_shape) == nfct(data, np_axis).shape
2662     def test_grad_argmin(self):
2663         data = rand(2, 3)
2664         n = as_tensor_variable(data)
2665         n.name = 'n'
2666         utt.verify_grad(lambda v: argmin(v, axis=-1), [data])
2667         utt.verify_grad(lambda v: argmin(v, axis=[0]), [data])
2668         utt.verify_grad(lambda v: argmin(v, axis=[1]), [data])
2669         utt.verify_grad(lambda v: argmin(v.flatten()), [data])
2670         try:
2671             cost = argmin(n, axis=-1)
2672             cost.name = None
2673             grad(cost, n)
2674             raise Exception('Expected an error')
2675         except TypeError:
2676             pass
2677     def test_grad_argmax(self):
2678         data = rand(2, 3)
2679         n = as_tensor_variable(data)
2680         utt.verify_grad(lambda v: argmax(v, axis=-1), [data])
2681         utt.verify_grad(lambda v: argmax(v, axis=[0]), [data])
2682         utt.verify_grad(lambda v: argmax(v, axis=[1]), [data])
2683         utt.verify_grad(lambda v: argmax(v.flatten()), [data])
2684         try:
2685             grad(argmax(n, axis=-1), n)
2686             raise Exception('Expected an error')
2687         except TypeError:
2688             pass
2689     def test_uint(self):
2690         for dtype in ('uint8', 'uint16', 'uint32', 'uint64'):
2691             itype = np.iinfo(dtype)
2692             data = np.array([itype.min + 3, itype.min, itype.max - 5, itype.max], dtype)
2693             n = as_tensor_variable(data)
2694             i = eval_outputs(argmin(n))
2695             self.assertEqual(i, 1)
2696             i = eval_outputs(argmax(n))
2697             self.assertEqual(i, 3)
2698     def test_bool(self):
2699         data = np.array([True, False], 'bool')
2700         n = as_tensor_variable(data)
2701         i = eval_outputs(argmin(n))
2702         self.assertEqual(i, 1)
2703         i = eval_outputs(argmax(n))
2704         self.assertEqual(i, 0)
2705 class T_min_max(unittest.TestCase):
2706     def setUp(self):
2707         utt.seed_rng()
2708         MaxAndArgmax.debug = 0
2709     def test_scalar(self):
2710         for fct in [max, min]:
2711             n = as_tensor_variable(5.0)
2712             v = eval_outputs(fct(n))
2713             self.assertTrue(v == 5.0)
2714             v = eval_outputs(fct(n).shape)
2715             assert len(v) == 0
2716     def test_list(self):
2717         for fct, nfct in [(max, np.max), (min, np.min)]:
2718             n = as_tensor_variable([1, 2, 3, 2, -6])
2719             v = eval_outputs([fct(n)])
2720             self.assertTrue(v == nfct(n.value))
2721             v = eval_outputs(fct(n).shape)
2722             assert len(v) == 0
2723     def test2(self):
2724         data = rand(2, 3)
2725         n = as_tensor_variable(data)
2726         for fct, nfct in [(max, np.max), (min, np.min)]:
2727             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2728                                     ([0, 1], None), ([1, 0], None)]:
2729                 v = eval_outputs(fct(n, axis))
2730                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2731                 v_shape = eval_outputs(fct(n, axis).shape)
2732                 assert tuple(v_shape) == nfct(data, np_axis).shape
2733     def test2_invalid(self):
2734         for fct in [max, min]:
2735             n = as_tensor_variable(rand(2, 3))
2736             _logger = logging.getLogger('theano.gof.opt')
2737             oldlevel = _logger.level
2738             _logger.setLevel(logging.CRITICAL)
2739             try:
2740                 try:
2741                     eval_outputs(fct(n, 3))
2742                     assert False
2743                 except ValueError:
2744                     pass
2745             finally:
2746                 _logger.setLevel(oldlevel)
2747     def test2_invalid_neg(self):
2748         for fct in [max, min]:
2749             n = as_tensor_variable(rand(2, 3))
2750             old_stderr = sys.stderr
2751             sys.stderr = StringIO()
2752             try:
2753                 try:
2754                     eval_outputs(fct(n, -3))
2755                     assert False
2756                 except ValueError:
2757                     pass
2758             finally:
2759                 sys.stderr = old_stderr
2760     def test2_valid_neg(self):
2761         for fct, nfct in [(max, np.max), (min, np.min)]:
2762             n = as_tensor_variable(rand(2, 3))
2763             v = eval_outputs(fct(n, -1))
2764             self.assertTrue(v.shape == (2,))
2765             self.assertTrue(np.all(v == nfct(n.value, -1)))
2766             v = eval_outputs(fct(n, -2))
2767             self.assertTrue(v.shape == (3,))
2768             self.assertTrue(np.all(v == nfct(n.value, -2)))
2769             v = eval_outputs(fct(n, -1).shape)
2770             assert v == (2)
2771             v = eval_outputs(fct(n, -2).shape)
2772             assert v == (3)
2773     def test3(self):
2774         data = rand(2, 3, 4)
2775         n = as_tensor_variable(data)
2776         for fct, nfct in [(max, np.max), (min, np.min)]:
2777             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (2, 2),
2778                                     (None, None), ([0, 1, 2], None),
2779                                     ([1, 0, 2], None)]:
2780                 v = eval_outputs(fct(n, axis))
2781                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2782                 v_shape = eval_outputs(fct(n, axis).shape)
2783                 assert tuple(v_shape) == nfct(data, np_axis).shape
2784     def test3b(self):
2785         data = rand(2, 3, 4)
2786         n = as_tensor_variable(data)
2787         for fct, nfct in [(max, np.max), (min, np.min)]:
2788             for axis in [[0, 1], [1, 2], [0, 2]]:
2789                 v = eval_outputs(fct(n, axis))
2790                 np_v = nfct(nfct(data, axis[1]), axis[0])
2791                 self.assertTrue(np.all(v == np_v))
2792                 v_shape = eval_outputs(fct(n, axis).shape)
2793                 assert tuple(v_shape) == np_v.shape
2794     def test_grad_max(self):
2795         data = rand(2, 3)
2796         n = as_tensor_variable(data)
2797         def check_grad_max(data, max_grad_data, axis=None):
2798             assert axis in [0, None]
2799             z = np.zeros_like(data)
2800             z = z.flatten()
2801             argmax = np.argmax(data, axis=axis)
2802             if argmax.ndim == 0:
2803                 z[np.argmax(data, axis=axis)] += 1
2804             else:
2805                 for id, v in enumerate(argmax):
2806                     z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
2807                       id] += 1
2808             z = z.reshape(data.shape)
2809             assert np.all(max_grad_data == z)
2810         utt.verify_grad(lambda v: max(v, axis=-1), [data])
2811         utt.verify_grad(lambda v: max(v, axis=[0]), [data])
2812         check_grad_max(data, eval_outputs(grad(max(n, axis=0).sum(), n)),
2813                        axis=0)
2814         utt.verify_grad(lambda v: max(v, axis=[1]), [data])
2815         utt.verify_grad(lambda v: max(v.flatten()), [data])
2816         check_grad_max(data, eval_outputs(grad(max(n.flatten()), n)))
2817     def test_grad_min(self):
2818         data = rand(2, 3)
2819         n = as_tensor_variable(data)
2820         def check_grad_min(data, min_grad_data, axis=None):
2821             assert axis in [0, None]
2822             z = np.zeros_like(data)
2823             z = z.flatten()
2824             argmin = np.argmin(data, axis=axis)
2825             if argmin.ndim == 0:
2826                 z[np.argmin(data, axis=axis)] += 1
2827             else:
2828                 for id, v in enumerate(argmin):
2829                     z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
2830                       id] += 1
2831             z = z.reshape(data.shape)
2832             assert np.all(min_grad_data == z)
2833         utt.verify_grad(lambda v: min(v, axis=-1), [data])
2834         utt.verify_grad(lambda v: min(v, axis=[0]), [data])
2835         check_grad_min(data, eval_outputs(grad(min(n, axis=0).sum(), n)),
2836                        axis=0)
2837         utt.verify_grad(lambda v: min(v, axis=[1]), [data])
2838         utt.verify_grad(lambda v: min(v.flatten()), [data])
2839         check_grad_min(data, eval_outputs(grad(min(n.flatten()), n)))
2840     def _grad_list(self):
2841         data = rand(2, 3)
2842         for fct in [max_and_argmax, max, min]:
2843             utt.verify_grad(lambda v: fct(v, axis=[0, 1]), [data])
2844     def test_uint(self):
2845         for dtype in ('uint8', 'uint16', 'uint32', 'uint64'):
2846             itype = np.iinfo(dtype)
2847             data = np.array([itype.min + 3, itype.min, itype.max - 5, itype.max], dtype)
2848             n = as_tensor_variable(data)
2849             self.assertEqual(min(n).dtype, dtype)
2850             i = eval_outputs(min(n))
2851             self.assertEqual(i, itype.min)
2852             self.assertEqual(max(n).dtype, dtype)
2853             i = eval_outputs(max(n))
2854             self.assertEqual(i, itype.max)
2855     def test_bool(self):
2856         data = np.array([True, False], 'bool')
2857         n = as_tensor_variable(data)
2858         self.assertEqual(min(n).dtype, 'bool')
2859         i = eval_outputs(min(n))
2860         self.assertEqual(i, False)
2861         self.assertEqual(max(n).dtype, 'bool')
2862         i = eval_outputs(max(n))
2863         self.assertEqual(i, True)
2864 def test_basic_allclose():
2865     assert tensor.basic._allclose(-0.311023883434, -0.311022856884)
2866 class T_outer(unittest.TestCase):
2867     def test_outer(self):
2868         for m in range(4):
2869             for n in range(4):
2870                 x = tensor.tensor(dtype='floatX', broadcastable=(False,) * m)
2871                 y = tensor.tensor(dtype='floatX', broadcastable=(False,) * n)
2872                 s1 = np.random.randint(1, 10, m)
2873                 s2 = np.random.randint(1, 10, n)
2874                 v1 = np.asarray(np.random.rand(*s1)).astype(floatX)
2875                 v2 = np.asarray(np.random.rand(*s2)).astype(floatX)
2876                 o = tensor.outer(x, y).eval({x: v1, y: v2})
2877                 assert_allclose(o, np.outer(v1, v2))
2878     def test_grad(self):
2879         for shp0, shp1 in [((1,), (2,)),
2880                            ((3,), (1,)),
2881                            ((1,), (1,)),
2882                            ((3,), (2,)),
2883                            ((3, 2), (1, 1)),
2884                            ((3, 2), (1, 4)),
2885                            ((3, 2), (4, 1)),
2886                            ((3, 2), (4, 5)),
2887                            ((1, 2), (4, 5)),
2888                            ((3, 1), (4, 5)),
2889                            ((1, 1), (4, 5)),
2890                            ((1, 1), (1, 1)),
2891                            ]:
2892             data0 = np.random.rand(*shp0).astype(floatX)
2893             data1 = np.random.rand(*shp1).astype(floatX)
2894             utt.verify_grad(tensor.outer, [data0, data1])
2895 class T_GetVectorLength(unittest.TestCase):
2896     def test_get_vector_length(self):
2897         x = theano.shared(np.zeros((2, 3, 4, 5)))
2898         assert len(list(x.shape)) == 4
2899         assert len(list(x.shape[2:4])) == 2
2900         assert len(list(x.shape[2:])) == 2
2901         assert len(list(x.shape[1:4])) == 3
2902         assert len(list(x.shape[2:2])) == 0
2903         assert len(list(x.shape[1:5])) == 3
2904         assert len(list(x.shape[1:10])) == 3
2905         assert len(list(x.shape[1:10:2])) == 2
2906         assert len(list(x.shape[-1:4])) == 1
2907         assert len(list(x.shape[-6:4])) == 4
2908         assert len(list(x.shape[1:-2])) == 1
2909         assert len(list(x.shape[1:-1])) == 2
2910 class T_Join_and_Split(unittest.TestCase):
2911     def setUp(self):
2912         Join.debug = False
2913         utt.seed_rng()
2914         self.mode = theano.compile.get_default_mode().excluding(
2915             'constant_folding')
2916         self.join_op = Join()
2917         self.split_op_class = Split
2918         self.make_vector_op = opt.MakeVector()
2919         self.floatX = config.floatX
2920         self.hide_error = theano.config.mode not in [
2921             'DebugMode', 'DEBUG_MODE', 'FAST_COMPILE']
2922         self.shared = shared
2923     def eval_outputs_and_check_join(self, outputs):
2924         f = theano.function([], outputs, self.mode)
2925         topo = f.maker.fgraph.toposort()
2926         assert [True for node in topo
2927                 if isinstance(node.op, type(self.join_op))]
2928         variables = f()
2929         if isinstance(variables, (tuple, list)) and len(variables) == 1:
2930             return variables[0]
2931         return variables
2932     def eval_outputs_and_check_vector(self, outputs,
2933                                       make_vector_op=None):
2934         if make_vector_op is None:
2935             make_vector_op = self.make_vector_op
2936         f = theano.function([], outputs, self.mode)
2937         topo = f.maker.fgraph.toposort()
2938         assert [True for node in topo
2939                 if isinstance(node.op, type(make_vector_op))]
2940         variables = f()
2941         if isinstance(variables, (tuple, list)) and len(variables) == 1:
2942             return variables[0]
2943         return variables
2944     def test_join_scalar(self):
2945         a = as_tensor_variable(1)
2946         b = as_tensor_variable(2)
2947         self.assertRaises(TypeError, join, 0, a, b)
2948     def test_stack_mixed_type_constants(self):
2949         a = as_tensor_variable(1)
2950         b = as_tensor_variable(2.0)
2951         c = tensor._shared(np.asarray(3.0, dtype=self.floatX))
2952         s = stack([a, b, c])
2953         want = np.array([1, 2, 3])
2954         out = self.eval_outputs_and_check_vector([s], opt.MakeVector())
2955         self.assertTrue((out == want).all())
2956     def test_stack_scalar(self):
2957         a = self.shared(np.asarray(1., dtype=self.floatX))
2958         b = as_tensor_variable(2.)
2959         c = as_tensor_variable(3.)
2960         s = stack([a, b, c])
2961         want = np.array([1, 2, 3])
2962         out = self.eval_outputs_and_check_vector([s])
2963         self.assertTrue((out == want).all())
2964     def test_stack_scalar_make_vector(self):
2965         a = tensor.scalar('a', dtype=self.floatX)
2966         b = tensor.scalar('b', dtype=self.floatX)
2967         s = stack([a, b, a, b])
2968         f = function([a, b], s, mode=self.mode)
2969         val = f(1, 2)
2970         self.assertTrue(np.all(val == [1, 2, 1, 2]))
2971         topo = f.maker.fgraph.toposort()
2972         assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
2973         assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
2974         assert f.maker.fgraph.outputs[0].dtype == self.floatX
2975     def test_stack_scalar_make_vector_dtype(self):
2976         a = tensor.iscalar('a')
2977         b = tensor.lscalar('b')
2978         s = stack([a, b, a, b])
2979         f = function([a, b], s, mode=self.mode)
2980         val = f(1, 2)
2981         self.assertTrue(np.all(val == [1, 2, 1, 2]))
2982         topo = f.maker.fgraph.toposort()
2983         assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
2984         assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
2985         assert f.maker.fgraph.outputs[0].dtype == 'int64'
2986     def test_stack_scalar_make_vector_constant(self):
2987         a = tensor.iscalar('a')
2988         b = tensor.lscalar('b')
2989         s = stack([10, a, b, np.int8(3)])
2990         f = function([a, b], s, mode=self.mode)
2991         val = f(1, 2)
2992         self.assertTrue(np.all(val == [10, 1, 2, 3]))
2993         topo = f.maker.fgraph.toposort()
2994         assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
2995         assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
2996         assert f.maker.fgraph.outputs[0].dtype == 'int64'
2997     def test_stack_new_interface(self):
2998         warnings.simplefilter('always', DeprecationWarning)
2999         a = tensor.imatrix('a')
3000         b = tensor.imatrix('b')
3001         s1 = stack(a, b)
3002         s2 = stack([a, b])
3003         f = function([a, b], [s1, s2], mode=self.mode)
3004         v1, v2 = f([[1, 2]], [[3, 4]])
3005         self.assertTrue(v1.shape == v2.shape)
3006         self.assertTrue(np.all(v1 == v2))
3007         s3 = stack([a, b], 1)
3008         f = function([a, b], s3, mode=self.mode)
3009         v3 = f([[1, 2]], [[3, 4]])
3010         v4 = np.array([[[1, 2], [3, 4]]])
3011         self.assertTrue(v3.shape == v4.shape)
3012         self.assertTrue(np.all(v3 == v4))
3013         v1 = [[1, 2, 3], [4, 5, 6]]
3014         v2 = [[7, 8, 9], [10, 11, 12]]
3015         s = stack([a, b], axis=-1)
3016         f = function([a, b], s, mode=self.mode)
3017         v = np.zeros((2, 3, 2))
3018         v[:, :, 0] = v1
3019         v[:, :, 1] = v2
3020         out = f(v1, v2)
3021         self.assertTrue(v.shape == out.shape)
3022         self.assertTrue(np.all(v == out))
3023         s = stack([a, b], axis=-2)
3024         f = function([a, b], s, mode=self.mode)
3025         v = np.zeros((2, 2, 3))
3026         v[:, 0, :] = v1
3027         v[:, 1, :] = v2
3028         out = f(v1, v2)
3029         self.assertTrue(v.shape == out.shape)
3030         self.assertTrue(np.all(v == out))
3031         self.assertRaises(IndexError, stack, [a, b], 4)
3032         self.assertRaises(IndexError, stack, [a, b], -4)
3033         with warnings.catch_warnings(record=True) as w:
3034             s = stack(a, b)
3035             assert len(w) == 1
3036             assert issubclass(w[-1].category, DeprecationWarning)
3037         with warnings.catch_warnings(record=True) as w:
3038             s = stack([a, b])
3039             s = stack([a, b], 1)
3040             s = stack([a, b], axis=1)
3041             s = stack(tensors=[a, b])
3042             s = stack(tensors=[a, b], axis=1)
3043             assert not w
3044     def test_stack_hessian(self):
3045         a = tensor.dvector('a')
3046         b = tensor.dvector('b')
3047         A = stack([a, b])
3048         B = A.T.dot(A)
3049         Ha, Hb = hessian(B.sum(), [a, b])
3050         a_v = np.random.rand(4)
3051         b_v = np.random.rand(4)
3052         f = theano.function([a, b], [Ha, Hb])
3053         Ha_v, Hb_v = f(a_v, b_v)
3054         assert Ha_v.shape == (4, 4)
3055         assert Hb_v.shape == (4, 4)
3056         assert np.allclose(Ha_v, 2.)
3057         assert np.allclose(Hb_v, 2.)
3058     def test_stack_hessian2(self):
3059         a = tensor.dvector('a')
3060         b = tensor.dvector('b')
3061         A = stack([a, b])
3062         Ha, Hb = hessian(A.sum(), [a, b])
3063         a_v = np.random.rand(4)
3064         b_v = np.random.rand(4)
3065         f = theano.function([a, b], [Ha, Hb])
3066         Ha_v, Hb_v = f(a_v, b_v)
3067         assert Ha_v.shape == (4, 4)
3068         assert Hb_v.shape == (4, 4)
3069         assert np.allclose(Ha_v, 0.)
3070         assert np.allclose(Hb_v, 0.)
3071     def test_join_concatenate_one_element(self):
3072         m = tensor.fmatrix()
3073         c = tensor.concatenate([m])
3074         f = theano.function(inputs=[m], outputs=[c],
3075                             mode=self.mode.including('local_join_1'))
3076         topo = f.maker.fgraph.toposort()
3077         assert len(topo) == 1
3078         assert isinstance(topo[0].op, DeepCopyOp)
3079     def test_join_vector(self):
3080         a = self.shared(np.array([1, 2, 3], dtype=self.floatX))
3081         b = as_tensor_variable(np.array([7, 8, 9], dtype=self.floatX))
3082         s = join(0, a, b)
3083         want = np.array([1, 2, 3, 7, 8, 9])
3084         out = self.eval_outputs_and_check_join([s])
3085         self.assertTrue((out == want).all())
3086     def test_roll(self):
3087         for get_shift in [lambda a:a, lambda x:theano.shared(x)]:
3088             a = self.shared(np.array([1, 2, 3, 4, 5, 6], dtype=self.floatX))
3089             b = roll(a, get_shift(2))
3090             want = np.array([5, 6, 1, 2, 3, 4])
3091             out = theano.function([], b)()
3092             assert (out == want).all()
3093             b = roll(a, get_shift(-1), 0)
3094             want = np.array([2, 3, 4, 5, 6, 1])
3095             out = theano.function([], b)()
3096             assert (out == want).all()
3097             a = self.shared(np.arange(21).reshape((3, 7)).astype(self.floatX))
3098             b = roll(a, get_shift(-2), 1)
3099             want = np.roll(a.get_value(borrow=True), -2, 1)
3100             out = theano.function([], b)()
3101             assert (out == want).all()
3102             a = self.shared(np.arange(24).reshape((3, 2, 4)).astype(self.floatX))
3103             b = roll(a, get_shift(-2), -2)
3104             want = np.roll(a.get_value(borrow=True), -2, -2)
3105             out = theano.function([], b)()
3106             assert (out == want).all()
3107             want = np.roll(a.get_value(borrow=True), -2, 0)
3108             b = roll(a, get_shift(-2), 0)
3109             out = theano.function([], b)()
3110             assert (out == want).all()
3111             want = np.roll(a.get_value(borrow=True), 2)
3112             b = roll(a, get_shift(2))
3113             out = theano.function([], b)()
3114             assert (out == want).all()
3115             want = np.roll(a.get_value(borrow=True), 4, 0)
3116             b = roll(a, get_shift(4), 0)
3117             out = theano.function([], b)()
3118             assert (out == want).all()
3119             want = np.roll(a.get_value(borrow=True), -4, 0)
3120             b = roll(a, get_shift(-4), 0)
3121             out = theano.function([], b)()
3122             assert (out == want).all()
3123     def test_stack_vector(self):
3124         a = self.shared(np.array([1, 2, 3], dtype=self.floatX))
3125         b = as_tensor_variable(np.array([7, 8, 9], dtype=self.floatX))
3126         s = stack([a, b])
3127         want = np.array([[1, 2, 3], [7, 8, 9]])
3128         out = self.eval_outputs_and_check_join([s])
3129         self.assertTrue((out == want).all())
3130     def test_join_matrix0(self):
3131         a = self.shared(np.array([[1, 2, 3], [4, 5, 6]],
3132                                  dtype=self.floatX))
3133         b = as_tensor_variable(np.array([[7, 8, 9]], dtype=self.floatX))
3134         s = join(0, a, b)
3135         want = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
3136         out = self.eval_outputs_and_check_join([s])
3137         self.assertTrue((out == want).all())
3138     def test_join_matrix1(self):
3139         av = np.array([[.1, .2, .3], [.4, .5, .6]], dtype='float32')
3140         bv = np.array([[.7], [.8]], dtype='float32')
3141         a = self.shared(av)
3142         b = as_tensor_variable(bv)
3143         s = join(1, a, b)
3144         want = np.array([[.1, .2, .3, .7], [.4, .5, .6, .8]],
3145                         dtype='float32')
3146         out = self.eval_outputs_and_check_join([s])
3147         self.assertTrue((out == want).all())
3148         utt.verify_grad(lambda a, b: join(1, a, b), [av, bv],
3149                         mode=self.mode)
3150     def test_join_matrix_dtypes(self):
3151         if "float32" in self.shared.__name__:
3152             raise SkipTest(
3153                 "The shared variable constructor"
3154                 " need to support other dtype then float32")
3155         av = np.array([[1, 2, 3], [4, 5, 6]], dtype='int8')
3156         bv = np.array([[7], [8]], dtype='float32')
3157         a = self.shared(av)
3158         b = as_tensor_variable(bv)
3159         s = join(1, a, b)
3160         want = np.array([[1, 2, 3, 7], [4, 5, 6, 8]], dtype='float32')
3161         out = self.eval_outputs_and_check_join([s])
3162         self.assertTrue((out == want).all())
3163         grad(s.sum(), b)
3164         grad(s.sum(), a)
3165         utt.verify_grad(lambda b: join(1, a, b), [bv],
3166                         eps=1.0e-2, mode=self.mode)
3167     def test_join_matrix_ints(self):
3168         if "float32" in self.shared.__name__:
3169             raise SkipTest(
3170                 "The shared variable constructor"
3171                 " need to support other dtype then float32")
3172         av = np.array([[1, 2, 3], [4, 5, 6]], dtype='int8')
3173         bv = np.array([[7], [8]], dtype='int32')
3174         a = self.shared(av)
3175         b = as_tensor_variable(bv)
3176         s = join(1, a, b)
3177         want = np.array([[1, 2, 3, 7], [4, 5, 6, 8]], dtype='float32')
3178         out = self.eval_outputs_and_check_join([s])
3179         self.assertTrue((out == want).all())
3180         assert (np.asarray(grad(s.sum(), b).eval()) == 0).all()
3181         assert (np.asarray(grad(s.sum(), a).eval()) == 0).all()
3182     def test_join_matrix1_using_vertical_stack(self):
3183         a = self.shared(np.array([[1, 2, 3], [4, 5, 6]], dtype=self.floatX))
3184         b = as_tensor_variable(np.array([[7, 8, 9]], dtype=self.floatX))
3185         c = as_tensor_variable(np.array([[9, 8, 7]], dtype=self.floatX))
3186         s = vertical_stack(a, b, c)
3187         want = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [9, 8, 7]])
3188         out = self.eval_outputs_and_check_join([s])
3189         self.assertTrue((out == want).all())
3190     def test_join_matrix1_using_horizontal_stack(self):
3191         av = np.array([[.1, .2, .3], [.4, .5, .6]], dtype='float32')
3192         bv = np.array([[.7], [.8]], dtype='float32')
3193         cv = np.array([[.3, .2, .1], [.6, .5, .4]], dtype='float32')
3194         a = self.shared(av)
3195         b = as_tensor_variable(bv)
3196         c = as_tensor_variable(cv)
3197         s = horizontal_stack(a, b, c)
3198         want = np.array([[.1, .2, .3, .7, .3, .2, .1],
3199                          [.4, .5, .6, .8, .6, .5, .4]],
3200                         dtype='float32')
3201         out = self.eval_outputs_and_check_join([s])
3202         self.assertTrue((out == want).all())
3203         utt.verify_grad(lambda a, b: join(1, a, b), [av, bv],
3204                         mode=self.mode)
3205     def test_join_matrixV(self):
3206         v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
3207         a = self.shared(v)
3208         b = as_tensor_variable(v)
3209         ax = lscalar()
3210         s = join(ax, a, b)
3211         f = inplace_func([ax], [s], mode=self.mode)
3212         topo = f.maker.fgraph.toposort()
3213         assert [True for node in topo
3214                 if isinstance(node.op, type(self.join_op))]
3215         want = np.array([[.1, .2, .3], [.4, .5, .6],
3216                          [.1, .2, .3], [.4, .5, .6]])
3217         got = f(0)
3218         assert np.allclose(got, want)
3219         want = np.array([[.1, .2, .3, .1, .2, .3],
3220                          [.4, .5, .6, .4, .5, .6]])
3221         got = f(1)
3222         assert np.allclose(got, want)
3223         utt.verify_grad(lambda a, b: join(0, a, b), [v, 2 * v], mode=self.mode)
3224         utt.verify_grad(lambda a, b: join(1, a, b), [v, 2 * v], mode=self.mode)
3225     def test_join_matrixV_negative_axis(self):
3226         v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
3227         a = self.shared(v)
3228         b = as_tensor_variable(v)
3229         ax = lscalar()
3230         s = join(ax, a, b)
3231         f = inplace_func([ax], [s], mode=self.mode)
3232         topo = f.maker.fgraph.toposort()
3233         assert [True for node in topo
3234                 if isinstance(node.op, type(self.join_op))]
3235         want = np.array([[.1, .2, .3, .1, .2, .3],
3236                          [.4, .5, .6, .4, .5, .6]])
3237         got = f(-1)
3238         assert np.allclose(got, want)
3239         want = np.array([[.1, .2, .3], [.4, .5, .6],
3240                          [.1, .2, .3], [.4, .5, .6]])
3241         got = f(-2)
3242         assert np.allclose(got, want)
3243         self.assertRaises(IndexError, f, -3)
3244     def test_join_matrixC_negative_axis(self):
3245         v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
3246         a = self.shared(v)
3247         b = as_tensor_variable(v)
3248         s = join(-1, a, b)
3249         f = theano.function([], [s], mode=self.mode)
3250         topo = f.maker.fgraph.toposort()
3251         assert [True for node in topo
3252                 if isinstance(node.op, type(self.join_op))]
3253         want = np.array([[.1, .2, .3, .1, .2, .3],
3254                          [.4, .5, .6, .4, .5, .6]])
3255         got = f()
3256         assert np.allclose(got, want)
3257         s = join(-2, a, b)
3258         f = theano.function([], [s], mode=self.mode)
3259         topo = f.maker.fgraph.toposort()
3260         assert [True for node in topo
3261                 if isinstance(node.op, type(self.join_op))]
3262         want = np.array([[.1, .2, .3], [.4, .5, .6],
3263                          [.1, .2, .3], [.4, .5, .6]])
3264         got = f()
3265         assert np.allclose(got, want)
3266         self.assertRaises(IndexError, join, -3, a, b)
3267         utt.verify_grad(lambda a, b: join(-1, a, b), [v, 2 * v],
3268                         mode=self.mode)
3269     def test_vector_len(self):
3270         x = lscalar('x')
3271         y = dscalar('y')
3272         triple = as_tensor_variable((x, y, 9.0))
3273         assert 3 == get_vector_length(triple)
3274         a, b, c = triple
3275         f = function([x, y], [b, c, a], mode=self.mode)
3276         topo = f.maker.fgraph.toposort()
3277         assert [True for node in topo if isinstance(node.op, opt.MakeVector)]
3278         assert np.allclose(f(4, 5), [5, 9, 4])
3279     def test_broadcastable_flag_assignment_mixed_otheraxes(self):
3280         rng = np.random.RandomState(seed=utt.fetch_seed())
3281         a_val = rng.rand(1, 4, 1).astype(self.floatX)
3282         b_val = rng.rand(1, 3, 1).astype(self.floatX)
3283         a = self.shared(a_val, broadcastable=(False, False, True))
3284         b = self.shared(b_val, broadcastable=(True, False, True))
3285         c = self.join_op(1, a, b)
3286         assert c.type.broadcastable[0] and c.type.broadcastable[2]
3287         assert not c.type.broadcastable[1]
3288         c = self.join_op(theano.tensor.constant(1), a, b)
3289         assert c.type.broadcastable[0] and c.type.broadcastable[2]
3290         assert not c.type.broadcastable[1]
3291         c = self.join_op(theano.tensor.cast(theano.tensor.constant(1),
3292                                             dtype="int32"),
3293                          a, b)
3294         assert c.type.broadcastable[0] and c.type.broadcastable[2]
3295         assert not c.type.broadcastable[1]
3296         f = function([], c, mode=self.mode)
3297         topo = f.maker.fgraph.toposort()
3298         assert [True for node in topo
3299                 if isinstance(node.op, type(self.join_op))]
3300         f()
3301         utt.verify_grad((lambda a, b: join(1, a, b)), [a_val, b_val], rng=rng,
3302                         mode=self.mode)
3303         a.set_value(rng.rand(2, 4, 1).astype(self.floatX))
3304         self.assertRaises(ValueError, f)
3305     def test_broadcastable_flag_assignment_mixed_thisaxes(self):
3306         rng = np.random.RandomState(seed=utt.fetch_seed())
3307         a_val = rng.rand(2, 4, 1).astype(self.floatX)
3308         b_val = rng.rand(1, 4, 1).astype(self.floatX)
3309         a = self.shared(a_val, broadcastable=(False, False, True))
3310         b = self.shared(b_val, broadcastable=(True, False, True))
3311         c = self.join_op(0, a, b)
3312         assert not c.type.broadcastable[0]
3313         f = function([], c, mode=self.mode)
3314         topo = f.maker.fgraph.toposort()
3315         assert [True for node in topo
3316                 if isinstance(node.op, type(self.join_op))]
3317         f()
3318         utt.verify_grad((lambda a, b: join(0, a, b)), [a_val, b_val], rng=rng,
3319                         mode=self.mode)
3320         self.assertRaises(TypeError, b.set_value,
3321                           rng.rand(3, 4, 1).astype(self.floatX))
3322         a = TensorType(dtype=self.floatX, broadcastable=[0, 0, 1])()
3323         b = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1])()
3324         c = self.join_op(0, a, b)
3325         f = function([a, b], c, mode=self.mode)
3326         bad_b_val = rng.rand(3, 4, 1).astype(self.floatX)
3327         self.assertRaises(TypeError, f, a_val, bad_b_val)
3328     def test_broadcastable_flags_all_broadcastable_on_joinaxis(self):
3329         rng = np.random.RandomState(seed=utt.fetch_seed())
3330         a_val = rng.rand(1, 4, 1).astype(self.floatX)
3331         b_val = rng.rand(1, 4, 1).astype(self.floatX)
3332         a = self.shared(a_val, broadcastable=(True, False, True))
3333         b = self.shared(b_val, broadcastable=(True, False, True))
3334         c = self.join_op(0, a, b)
3335         assert not c.type.broadcastable[0]
3336         f = function([], c, mode=self.mode)
3337         topo = f.maker.fgraph.toposort()
3338         assert [True for node in topo
3339                 if isinstance(node.op, type(self.join_op))]
3340         f()
3341         utt.verify_grad((lambda a, b: join(0, a, b)), [a_val, b_val], rng=rng,
3342                         mode=self.mode)
3343     def test_broadcastable_single_input_broadcastable_dimension(self):
3344         rng = np.random.RandomState(seed=utt.fetch_seed())
3345         a_val = rng.rand(1, 4, 1).astype(self.floatX)
3346         a = self.shared(a_val, broadcastable=(True, False, True))
3347         b = self.join_op(0, a)
3348         assert b.type.broadcastable[0]
3349         assert b.type.broadcastable[2]
3350         assert not b.type.broadcastable[1]
3351         f = function([], b, mode=self.mode)
3352         topo = f.maker.fgraph.toposort()
3353         if theano.config.mode != 'FAST_COMPILE':
3354             assert not [True for node in topo
3355                         if isinstance(node.op, type(self.join_op))]
3356         f()
3357         utt.verify_grad((lambda a: join(0, a)), [a_val], rng=rng,
3358                         mode=self.mode)
3359         self.assertRaises(TypeError, a.set_value,
3360                           rng.rand(2, 4, 1).astype(self.floatX))
3361     def test_broadcastable_flags_many_dims_and_inputs(self):
3362         a = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 0, 0, 0])()
3363         b = TensorType(dtype=self.floatX, broadcastable=[1, 1, 1, 0, 0, 0])()
3364         c = TensorType(dtype=self.floatX, broadcastable=[1, 0, 0, 0, 0, 0])()
3365         d = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 1, 0, 1])()
3366         e = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 0, 0, 1])()
3367         f = self.join_op(0, a, b, c, d, e)
3368         fb = f.type.broadcastable
3369         assert not fb[0] and fb[1] and fb[2] and fb[3] and not fb[4] and fb[5]
3370         g = self.join_op(1, a, b, c, d, e)
3371         gb = g.type.broadcastable
3372         assert gb[0] and not gb[1] and gb[2] and gb[3] and not gb[4] and gb[5]
3373         h = self.join_op(4, a, b, c, d, e)
3374         hb = h.type.broadcastable
3375         assert hb[0] and hb[1] and hb[2] and hb[3] and not hb[4] and hb[5]
3376         f = function([a, b, c, d, e], f, mode=self.mode)
3377         topo = f.maker.fgraph.toposort()
3378         assert [True for node in topo
3379                 if isinstance(node.op, type(self.join_op))]
3380         rng = np.random.RandomState(seed=utt.fetch_seed())
3381         a_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3382         b_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3383         c_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3384         d_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3385         e_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3386         f(a_val, b_val, c_val, d_val, e_val)
3387         utt.verify_grad((lambda a, b, c, d, e: join(0, a, b, c, d, e)),
3388                         [a_val, b_val, c_val, d_val, e_val], rng=rng,
3389                         mode=self.mode)
3390         bad_val = rng.rand(2, 1, 1, 1, 2, 1).astype(self.floatX)
3391         self.assertRaises(TypeError, f, bad_val, b_val, c_val, d_val, e_val)
3392         self.assertRaises(TypeError, f, a_val, bad_val, c_val, d_val, e_val)
3393         self.assertRaises(TypeError, f, a_val, b_val, bad_val, d_val, e_val)
3394         self.assertRaises(TypeError, f, a_val, b_val, c_val, bad_val, e_val)
3395         self.assertRaises(TypeError, f, a_val, b_val, c_val, d_val, bad_val)
3396         bad_a_val = rng.rand(1, 2, 1, 1, 2, 1).astype(self.floatX)
3397         bad_b_val = rng.rand(1, 1, 1, 1, 2, 2).astype(self.floatX)
3398         bad_c_val = rng.rand(1, 1, 2, 1, 2, 1).astype(self.floatX)
3399         bad_d_val = rng.rand(1, 2, 1, 1, 2, 1).astype(self.floatX)
3400         bad_e_val = rng.rand(1, 1, 1, 2, 2, 1).astype(self.floatX)
3401         self.assertRaises(ValueError, f, bad_a_val, b_val, c_val, d_val, e_val)
3402         self.assertRaises(ValueError, f, a_val, bad_b_val, c_val, d_val, e_val)
3403         self.assertRaises(ValueError, f, a_val, b_val, bad_c_val, d_val, e_val)
3404         self.assertRaises(ValueError, f, a_val, b_val, c_val, bad_d_val, e_val)
3405         self.assertRaises(ValueError, f, a_val, b_val, c_val, d_val, bad_e_val)
3406     def test_infer_shape_join(self):
3407         def get_mat(s1, s2):
3408             return np.asarray(np.random.uniform(size=(s1, s2)),
3409                               dtype=self.floatX)
3410         x1 = self.shared(get_mat(3, 4))
3411         x2 = self.shared(get_mat(2, 4))
3412         x3 = self.shared(get_mat(1, 4))
3413         z = self.join_op(0, x1, x2, x3)
3414         f = theano.function([], z.shape, mode=self.mode)
3415         topo = f.maker.fgraph.toposort()
3416         out = f()
3417         assert (out == [6, 4]).all()
3418         if theano.config.mode != 'FAST_COMPILE':
3419             for node in f.maker.fgraph.toposort():
3420                 assert not isinstance(node.op, type(self.join_op))
3421         x1.set_value(get_mat(3, 4))
3422         x2.set_value(get_mat(3, 4))
3423         x3.set_value(get_mat(3, 5))
3424         z = self.join_op(1, x1, x2, x3)
3425         f = theano.function([], z.shape, mode=self.mode)
3426         topo = f.maker.fgraph.toposort()
3427         out = f()
3428         assert (out == [3, 13]).all()
3429         if theano.config.mode != 'FAST_COMPILE':
3430             for node in topo:
3431                 assert not isinstance(node.op, type(self.join_op))
3432         with change_flags(compute_test_value='off'):
3433             x1.set_value(get_mat(3, 4))
3434             x2.set_value(get_mat(3, 4))
3435             x3.set_value(get_mat(2, 5))
3436             if not self.hide_error:
3437                 self.assertRaises(ValueError, f)
3438             else:
3439                 f()
3440     def test_rebroadcast(self):
3441         x = tensor.TensorType(self.floatX, [False, False, True])()
3442         u = tensor.TensorType(self.floatX, [False, False, True])()
3443         tensor.concatenate([x, -u], axis=2)
3444     def test_concatenate_same(self):
3445         rng = np.random.RandomState(seed=utt.fetch_seed())
3446         T_shared = self.shared(rng.rand(3, 4).astype(self.floatX))
3447         Tout = tensor.concatenate([T_shared, T_shared])
3448         f = function([], Tout, mode=self.mode)
3449         out = f()
3450         if theano.config.mode != 'FAST_COMPILE':
3451             assert [True for node in f.maker.fgraph.toposort()
3452                     if isinstance(node.op, type(self.join_op))]
3453         assert np.allclose(out,
3454                            np.concatenate([T_shared.get_value(),
3455                                            T_shared.get_value()]))
3456     def test_mixed_ndim_error(self):
3457         rng = np.random.RandomState(seed=utt.fetch_seed())
3458         v = self.shared(rng.rand(4).astype(self.floatX))
3459         m = self.shared(rng.rand(4, 4).astype(self.floatX))
3460         self.assertRaises(TypeError, self.join_op, 0, v, m)
3461     def test_split_0elem(self):
3462         rng = np.random.RandomState(seed=utt.fetch_seed())
3463         m = self.shared(rng.rand(4, 6).astype(self.floatX))
3464         o = self.split_op_class(2)(m, 0, [4, 0])
3465         f = function([], o, mode=self.mode)
3466         assert any([isinstance(node.op, self.split_op_class)
3467                     for node in f.maker.fgraph.toposort()])
3468         o1, o2 = f()
3469         assert np.allclose(o1, m.get_value(borrow=True))
3470         assert np.allclose(o2, m.get_value(borrow=True)[4:])
3471     @change_flags(compute_test_value='off')
3472     def test_split_neg(self):
3473         rng = np.random.RandomState(seed=utt.fetch_seed())
3474         m = self.shared(rng.rand(4, 6).astype(self.floatX))
3475         o = self.split_op_class(2)(m, 0, [5, -1])
3476         f = function([], o, mode=self.mode)
3477         assert any([isinstance(node.op, self.split_op_class)
3478                     for node in f.maker.fgraph.toposort()])
3479         self.assertRaises(ValueError, f)
3480 def test_join_inplace():
3481     s = tensor.lscalar()
3482     x = tensor.vector('x')
3483     z = tensor.zeros((s,))
3484     join = Join(view=0)
3485     c = join(0, x, z, z)
3486     f = theano.function([theano.In(x, borrow=True), s], theano.Out(c, borrow=True))
3487     data = np.array([3, 4, 5], dtype=theano.config.floatX)
3488     print(f(data, 0))
3489     if theano.config.mode not in ["DebugMode", "DEBUG_MODE"]:
3490         assert f(data, 0) is data
3491     assert np.allclose(f(data, 0), [3, 4, 5])
3492 def test_join_oneInput():
3493     x_0 <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.fmatrix()
3494     x_1 = theano.tensor.fmatrix()
3495     x_2 = theano.tensor.fvector()
3496     join_0 = theano.tensor.concatenate([x_0], axis=</b></font>1)
3497     join_1 = theano.tensor.concatenate([x_0, x_1, theano.tensor.shape_padright(x_2)],
3498                                        axis=1)
3499     assert join_0 is x_0
3500     assert join_1 is not x_0
3501 class test_comparison(unittest.TestCase):
3502     def setUp(self):
3503         utt.seed_rng()
3504         self.mode = None
3505         self.shared = shared
3506         self.dtypes = ['float64', 'float32', 'complex64', 'complex128']
3507     def inplace_func(self, inputs, outputs, check_isfinite=None):
3508         mode = self.mode
3509         if check_isfinite is False:
3510             if mode is None:
3511                 mode = get_default_mode()
3512             mode.check_isfinite = False
3513         f = inplace_func(inputs, outputs, mode=mode)
3514         return f
3515     def test_gt(self):
3516         for dtype in self.dtypes:
3517             l = np.asarray([0., -1., 1.], dtype=dtype)
3518             r = np.asarray([0., 1., -1.], dtype=dtype)
3519             for x, y, err in [
3520                 (self.shared(l.astype(dtype)),
3521                  self.shared(r.astype(dtype)), False),
3522                 (l, self.shared(r.astype(dtype)), True),
3523                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3524                 (self.shared(l.astype(dtype)), r, False),
3525                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3526             ]:
3527                 try:
3528                     fn = self.inplace_func([], x &gt; y)
3529                     v = fn()
3530                     self.assertTrue(np.all(v == (l &gt; r)), (v, (l &gt; r)))
3531                 except TypeError:
3532                     assert err
3533     def test_lt(self):
3534         for dtype in self.dtypes:
3535             l = np.asarray([0., -1., 1.], dtype=dtype)
3536             r = np.asarray([0., 1., -1.], dtype=dtype)
3537             for x, y, err in [
3538                 (self.shared(l.astype(dtype)), self.shared(r.astype(dtype)), False),
3539                 (l, self.shared(r.astype(dtype)), True),
3540                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3541                 (self.shared(l.astype(dtype)), r, False),
3542                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3543             ]:
3544                 try:
3545                     fn = self.inplace_func([], x &lt; y)
3546                     v = fn()
3547                     self.assertTrue(np.all(v == (l &lt; r)), (v, (l &lt; r)))
3548                 except TypeError:
3549                     assert err
3550     def test_le(self):
3551         for dtype in self.dtypes:
3552             l = np.asarray([0., -1., 1.], dtype=dtype)
3553             r = np.asarray([0., 1., -1.], dtype=dtype)
3554             for x, y, err in [
3555                 (self.shared(l.astype(dtype)),
3556                  self.shared(r.astype(dtype)), False),
3557                 (l, self.shared(r.astype(dtype)), True),
3558                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3559                 (self.shared(l.astype(dtype)), r, False),
3560                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3561             ]:
3562                 try:
3563                     fn = self.inplace_func([], x &lt;= y)
3564                     v = fn()
3565                     self.assertTrue(np.all(v == (l &lt;= r)), (v, (l &lt;= r)))
3566                 except TypeError:
3567                     assert err
3568     def test_ge(self):
3569         for dtype in self.dtypes:
3570             l = np.asarray([0., -1., 1.], dtype=dtype)
3571             r = np.asarray([0., 1., -1.], dtype=dtype)
3572             for x, y, err in [
3573                 (self.shared(l.astype(dtype)),
3574                  self.shared(r.astype(dtype)), False),
3575                 (l, self.shared(r.astype(dtype)), True),
3576                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3577                 (self.shared(l.astype(dtype)), r, False),
3578                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3579             ]:
3580                 try:
3581                     fn = self.inplace_func([], x &gt;= y)
3582                     v = fn()
3583                     self.assertTrue(np.all(v == (l &gt;= r)), (v, (l &gt;= r)))
3584                 except TypeError:
3585                     assert err
3586     def test_eq(self):
3587         for dtype in self.dtypes:
3588             l = np.asarray([0., -1., 1.], dtype=dtype)
3589             r = np.asarray([0., 1., -1.], dtype=dtype)
3590             for x, y, err in [
3591                 (self.shared(l.astype(dtype)),
3592                  self.shared(r.astype(dtype)), False),
3593                 (l, self.shared(r.astype(dtype)), True),
3594                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3595                 (self.shared(l.astype(dtype)), r, False),
3596                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3597             ]:
3598                 try:
3599                     fn = self.inplace_func([], eq(x, y))
3600                     v = fn()
3601                     self.assertTrue(np.all(v == (l == r)), (v, (l == r)))
3602                 except TypeError:
3603                     assert err
3604     def test_neq(self):
3605         for dtype in self.dtypes:
3606             l = np.asarray([0., -1., 1.], dtype=dtype)
3607             r = np.asarray([0., 1., -1.], dtype=dtype)
3608             for x, y, err in [
3609                 (self.shared(l.astype(dtype)),
3610                  self.shared(r.astype(dtype)), False),
3611                 (l, self.shared(r.astype(dtype)), True),
3612                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3613                 (self.shared(l.astype(dtype)), r, False),
3614                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3615             ]:
3616                 try:
3617                     fn = self.inplace_func([], neq(x, y))
3618                     v = fn()
3619                     self.assertTrue(np.all(v == (l != r)), (v, (l != r)))
3620                 except TypeError:
3621                     assert err
3622     def test_isclose(self):
3623         for dtype in self.dtypes:
3624             l = np.asarray(
3625                 [0., 1., -1., 0.,
3626                  np.nan, np.inf, -np.inf, np.inf],
3627                 dtype=dtype)
3628             r = np.asarray(
3629                 [0., 1.0001, -1.000000000001, np.nan,
3630                  np.nan, np.inf, np.inf, 0.],
3631                 dtype=dtype)
3632             for x, y, err in [
3633                 (self.shared(l.astype(dtype)),
3634                  self.shared(r.astype(dtype)), False),
3635                 (l, self.shared(r.astype(dtype)), True),
3636                 (constant(l), self.shared(r.astype(dtype)), False),
3637                 (self.shared(l.astype(dtype)), r, False),
3638                 (self.shared(l.astype(dtype)), constant(r), False),
3639             ]:
3640                 try:
3641                     o1 = isclose(x, y, equal_nan=False)
3642                     fn1 = self.inplace_func([], o1, check_isfinite=False)
3643                     o2 = isclose(x, y, equal_nan=True)
3644                     fn2 = self.inplace_func([], o2, check_isfinite=False)
3645                     v1 = fn1()
3646                     v2 = fn2()
3647                     self.assertTrue(
3648                         np.all(
3649                             v1 == np.asarray(
3650                                 [True, False, True, False,
3651                                  False, True, False, False],
3652                                 dtype="bool"
3653                             )
3654                         ),
3655                         np.all(
3656                             v2 == np.asarray(
3657                                 [True, False, True, False,
3658                                  True, True, False, False],
3659                                 dtype="bool"
3660                             )
3661                         )
3662                     )
3663                 except TypeError:
3664                     if not dtype.startswith('complex'):
3665                         raise
3666                         assert err
3667     def test_allclose(self):
3668         for dtype in self.dtypes:
3669             l = np.asarray(
3670                 [0., 1., -1., 0.,
3671                  np.nan, np.inf, -np.inf, np.inf],
3672                 dtype=dtype)
3673             r = np.asarray(
3674                 [0., 1.0001, -1.000000000001, np.nan,
3675                  np.nan, np.inf, np.inf, 0.],
3676                 dtype=dtype)
3677             for x, y, err in [
3678                 (self.shared(l.astype(dtype)),
3679                  self.shared(r.astype(dtype)), False),
3680                 (l, self.shared(r.astype(dtype)), True),
3681                 (constant(l), self.shared(r.astype(dtype)), False),
3682                 (self.shared(l.astype(dtype)), r, False),
3683                 (self.shared(l.astype(dtype)), constant(r), False),
3684             ]:
3685                 try:
3686                     fn = self.inplace_func([], allclose(x, y, equal_nan=False),
3687                                            check_isfinite=False)
3688                     v = fn()
3689                     self.assertTrue(np.all(v == np.allclose(l, r)))
3690                 except TypeError:
3691                     if not dtype.startswith('complex'):
3692                         assert err
3693 class test_bitwise(unittest.TestCase):
3694     dtype = ['int8', 'int16', 'int32', 'int64', ]
3695     def test_or(self):
3696         for dtype in self.dtype:
3697             x, y = vector(dtype=dtype), vector(dtype=dtype)
3698             fn = inplace_func([x, y], x | y)
3699             l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3700             r = theano._asarray([0, 1, 0, 1], dtype=dtype)
3701             v = fn(l, r)
3702             self.assertTrue(np.all(v == (operator.or_(l, r))), (l, r, v))
3703     def test_xor(self):
3704         for dtype in self.dtype:
3705             x, y = vector(dtype=dtype), vector(dtype=dtype)
3706             fn = inplace_func([x, y], x ^ y)
3707             ix = x
3708             ix = inplace.xor_inplace(ix, y)
3709             gn = inplace_func([x, y], ix)
3710             l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3711             r = theano._asarray([0, 1, 0, 1], dtype=dtype)
3712             v = fn(l, r)
3713             self.assertTrue(np.all(v == (operator.xor(l, r))), (l, r, v))
3714             v = gn(l, r)
3715             self.assertTrue(np.all(l == np.asarray([0, 1, 1, 0])), l)
3716     def test_and(self):
3717         for dtype in self.dtype:
3718             x, y = vector(dtype=dtype), vector(dtype=dtype)
3719             fn = inplace_func([x, y], x &amp; y)
3720             l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3721             r = theano._asarray([0, 1, 0, 1], dtype=dtype)
3722             v = fn(l, r)
3723             self.assertTrue(np.all(v == (operator.and_(l, r))), (l, r, v))
3724     def test_inv(self):
3725         for dtype in self.dtype:
3726             x = vector(dtype=dtype)
3727             fn = inplace_func([x], ~x)
3728             for l in [[0, 0, 1, 1], [0, 1, 0, 1],
3729                       [0, 0, 1, 1], [0, 1, 0, 1],
3730                       [-1, 2 ** 16, 2 ** 16 - 1]
3731                       ]:
3732                 l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3733                 v = fn(l)
3734                 self.assertTrue(np.all(v == (~l)), (l, v))
3735     def test_eye(self):
3736         n = iscalar()
3737         m = iscalar()
3738         k = iscalar()
3739         fn = theano.function([m, n, k], eye(m, n, k))
3740         self.assertTrue(np.all(fn(5, 6, 1) == np.eye(5, 6, 1)))
3741 class T_add(unittest.TestCase):
3742     def setUp(self):
3743         utt.seed_rng()
3744     def test_complex_all_ops(self):
3745         for nbits in (64, 128):
3746             a = shared(np.ones(3, dtype='complex%i' % nbits) + 0.5j)
3747             b = shared(np.ones(3, dtype='complex%i' % nbits) + 1.5j)
3748             tests = (("+", lambda x, y: x + y),
3749                      ("-", lambda x, y: x - y),
3750                      ("*", lambda x, y: x * y),
3751                      ("/", lambda x, y: x / y))
3752             for s, fn in tests:
3753                 f = inplace_func([], fn(a, b))
3754                 self.assertTrue(a.type.values_eq_approx(fn(
3755                     a.get_value(), b.get_value()), f()))
3756     def test_grad_scalar_l(self):
3757         utt.verify_grad(add, [np.asarray([3.0]), rand(3)])
3758     def test_grad_scalar_r(self):
3759         utt.verify_grad(add, [rand(3), np.asarray([3.0])])
3760     def test_grad_row(self):
3761         utt.verify_grad(add, [rand(3, 5), rand(1, 5)])
3762     def test_grad_col(self):
3763         utt.verify_grad(add, [rand(3, 5), rand(3, 1)])
3764 class T_ceil(unittest.TestCase):
3765     def test_complex(self):
3766         self.assertRaises(TypeError, tensor.ceil, tensor.zvector())
3767 class T_exp(unittest.TestCase):
3768     def test_grad_0(self):
3769         utt.verify_grad(exp, [
3770             np.asarray([[1.5089518, 1.48439076, -4.7820262],
3771                         [2.04832468, 0.50791564, -1.58892269]])])
3772     def test_grad_1(self):
3773         utt.verify_grad(inplace.exp_inplace, [
3774             np.asarray([[1.5089518, 1.48439076, -4.7820262],
3775                         [2.04832468, 0.50791564, -1.58892269]])])
3776     if theano.config.cycle_detection == 'fast' and theano.config.mode != 'FAST_COMPILE':
3777         test_grad_1 = unittest.expectedFailure(test_grad_1)
3778     def test_int(self):
3779         x = ivector()
3780         f = function([x], exp(x))
3781         exp_3 = f([3])
3782         assert exp_3.dtype == 'float64'
3783     def test_complex(self):
3784         x = zvector()
3785         assert exp(x).dtype == 'complex128'
3786         f = function([x], exp(x))
3787         exp_3 = f([3 + 2j])
3788         assert np.allclose(exp_3, np.exp(3 + 2j))
3789 class T_divimpl(unittest.TestCase):
3790     def test_impls(self):
3791         i = iscalar()
3792         ii = lscalar()
3793         d = dscalar()
3794         f = fscalar()
3795         c = cscalar()
3796         assert np.allclose(function([i, d], i / d)(5, 7.0), (5.0 / 7.0))
3797         assert np.allclose(function([i, d], d / i)(5, 7.0), (7.0 / 5.0))
3798         assert np.allclose(function([i, f], i / f)(5, 11.0), (5.0 / 11.0))
3799         assert np.allclose(function([i, f], f / i)(5, 11.0), (11.0 / 5.0))
3800         assert np.allclose(function([i, ii], i // ii)(5, 3), (5 // 3))
3801         assert np.allclose(function([i, ii], ii // i)(5, 3), (3 // 5))
3802         assert np.allclose(function([i, ii], true_div(i, ii))(5, 3),
3803                            (5. / 3.))
3804         assert np.allclose(function([i, ii], true_div(ii, i))(5, 3),
3805                            (3. / 5.))
3806         assert np.allclose(function([i, c], i / c)(5, np.complex(5, 3)),
3807                            (5. / (5 + 3j)))
3808         assert np.allclose(function([i, c], c / i)(5, np.complex(5, 3)),
3809                            ((5 + 3j) / 5.))
3810 class T_mean(unittest.TestCase):
3811     def test_regression_mean_of_ndarray_failure(self):
3812         try:
3813             tensor.mean(np.zeros(1))
3814         except AttributeError:
3815             self.fail()
3816     def test_mean_f16(self):
3817         x = tensor.vector(dtype='float16')
3818         y = x.mean()
3819         f = theano.function([x], y)
3820         utt.assert_allclose(f(np.ones((100000,), dtype='float16')), 1.0)
3821     def test0(self):
3822         x = tensor.vector()
3823         f = theano.function([x], tensor.mean(x))
3824         data = rand(50)
3825         assert np.allclose(f(data), np.mean(data))
3826     def test_list(self):
3827         ll = [theano.shared(0.), theano.shared(2.)]
3828         tensor.mean(ll).eval() == 1
3829 class test_matinv(unittest.TestCase):
3830     def mat_reciprocal(self, dim):
3831         rng = np.random.RandomState(seed=utt.fetch_seed())
3832         a, b = matrices('ab')
3833         ab = a * b
3834         diff = ab - as_tensor_variable(np.ones((dim, dim),
3835                                                dtype=config.floatX))
3836         ssdiff = sum((diff ** 2.0))
3837         g_b = grad(ssdiff, b)
3838         fn = inplace_func([a, b], [ssdiff, g_b])
3839         x = rng.rand(dim, dim) + 0.1      # Initialized s.t. x is not too tiny
3840         w = rng.rand(dim, dim)
3841         x = np.asarray(x, dtype=config.floatX)
3842         w = np.asarray(w, dtype=config.floatX)
3843         for i in xrange(100):
3844             ssd, gw = fn(x, w)
3845             if i == 0:
3846                 ssd0 = ssd
3847             w -= 0.4 * gw
3848         return ssd0, ssd
3849     def test_reciprocal(self):
3850         ssd0, ssd = self.mat_reciprocal(3)
3851         rng = np.random.RandomState(seed=utt.fetch_seed())
3852         x = rng.rand(3, 3) + 0.1
3853         w = rng.rand(3, 3)
3854         x = np.asarray(x, dtype=config.floatX)
3855         w = np.asarray(w, dtype=config.floatX)
3856         ones = np.ones((3, 3), dtype=config.floatX)
3857         myssd0 = np.sum((x * w - ones) ** 2.0)
3858         for i in xrange(100):
3859             gw = 2 * (x * w - ones) * x  # derivative of dMSE/dw
3860             myssd = np.sum((x * w - ones) ** 2)
3861             w -= 0.4 * gw
3862         self.assertAlmostEqual(ssd0, myssd0)
3863         self.assertAlmostEqual(ssd, myssd)
3864 class t_dot(unittest.TestCase):
3865     def setUp(self):
3866         utt.seed_rng()
3867     def cmp_dot(self, x, y):
3868         def spec(x):
3869             x = np.asarray(x)
3870             return type(x), x.dtype, x.shape
3871         tz = eval_outputs([dot(as_tensor_variable(x), as_tensor_variable(y))])
3872         self.assertTrue(tz.dtype == nz.dtype,
3873                         (tz<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.dtype, tz.dtype.num, nz.dtype, nz.dtype.num))
3874         self.assertTrue(tz.shape == nz.shape, (tz.</b></font>shape, nz.shape))
3875         utt.assert_allclose(nz, tz, rtol=1e-4, atol=1e-4)
3876     def test_Op_dims(self):
3877         _dot = theano<font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.tensor.basic._dot
3878         d0 = scalar()
3879         d1 = vector()
3880         d2 = matrix()
3881         d3 = tensor3()
3882         self.assertRaises(</b></font>TypeError, _dot, d0, d0)
3883         self.assertRaises(TypeError, _dot, d0, d1)
3884         self.assertRaises(TypeError, _dot, d0, d2)
3885         self.assertRaises(TypeError, _dot, d0, d3)
3886         self.assertRaises(TypeError, _dot, d1, d0)
3887         _dot(d1, d2)
3888         self.assertRaises(TypeError, _dot, d1, d3)
3889         self<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertRaises(TypeError, _dot, d2, d0)
3890         _dot(d2, d1)
3891         _dot(d2, d2)
3892         self.assertRaises(TypeError, _dot, d2, d3)
3893         self.assertRaises(TypeError, _dot, d3, d0)
3894         self.assertRaises(TypeError, _dot, d3, d1)
3895         self.assertRaises(TypeError, _dot, d3, d2)
3896         self.assertRaises(</b></font>TypeError, _dot, d3, d3)
3897     def test_dot_0d_0d(self):
3898         self.cmp_dot(rand(), rand())
3899     def test_dot_0d_1d(self):
3900         self.cmp_dot(rand(), rand(5))
3901     def test_dot_0d_2d(self):
3902         self.cmp_dot(rand(), rand(6, 7))
3903     def test_dot_0d_3d(self):
3904         self.cmp_dot(rand(), rand(8, 6, 7))
3905     def test_dot_1d_0d(self):
3906         self.cmp_dot(rand(5), rand())
3907     def test_dot_1d_1d(self):
3908         self.cmp_dot(rand(5), rand(5))
3909     def test_dot_1d0_1d0(self):
3910         self.cmp_dot(rand(0), rand(0))
3911     def test_dot_1d_1d0(self):
3912         self.assertRaises(ValueError, self.cmp_dot, rand(5), rand(0))
3913     def test_dot_1d0_1d(self):
3914         self.assertRaises(ValueError, self.cmp_dot, rand(0), rand(5))
3915     def test_dot_1d_2d(self):
3916         self.cmp_dot(rand(6), rand(6, 7))
3917     def test_dot_1d0_2d(self):
3918         self.cmp_dot(rand(0), rand(0, 7))
3919     def test_dot_1d_2d0(self):
3920         self.cmp_dot(rand(6), rand(6, 0))
3921     def test_dot_1d0_2d0(self):
3922         self.cmp_dot(rand(0), rand(0, 0))
3923     def test_dot_1d_3d(self):
3924         self.cmp_dot(rand(6), rand(8, 6, 7))
3925     def test_dot_2d_0d(self):
3926         self.cmp_dot(rand(5, 6), rand())
3927     def test_dot_2d_1d(self):
3928         self.cmp_dot(rand(5, 6), rand(6))
3929     def test_dot_2d0_1d(self):
3930         self.cmp_dot(rand(0, 6), rand(6))
3931     def test_dot_2d_1d0(self):
3932         self.cmp_dot(rand(5, 0), rand(0))
3933     def test_dot_2d0_1d0(self):
3934         self.cmp_dot(rand(0, 0), rand(0))
3935     def test_dot_2d_2d(self):
3936         self.cmp_dot(rand(5, 6), rand(6, 7))
3937     def test_dot_2d0_2d(self):
3938         self.cmp_dot(rand(0, 6), rand(6, 7))
3939     def test_dot_2d_2d0(self):
3940         self.cmp_dot(rand(5, 6), rand(6, 0))
3941     def test_dot_2d0_2d0(self):
3942         self.cmp_dot(rand(0, 6), rand(6, 0))
3943     def test_dot_2d_0_2d(self):
3944         self.cmp_dot(rand(5, 0), rand(0, 7))
3945     def test_dot_2d0_0_2d0(self):
3946         self.cmp_dot(rand(0, 6), rand(6, 0))
3947     def test_dot_2d_3d(self):
3948         self.cmp_dot(rand(5, 6), rand(8, 6, 7))
3949     def test_dot_3d_0d(self):
3950         self.cmp_dot(rand(4, 5, 6), rand())
3951     def test_dot_3d_1d(self):
3952         self.cmp_dot(rand(4, 5, 6), rand(6))
3953     def test_dot_3d_2d(self):
3954         self.cmp_dot(rand(4, 5, 6), rand(6, 7))
3955     def test_dot_3d_3d(self):
3956         self.cmp_dot(rand(4, 5, 6), rand(8, 6, 7))
3957     def not_aligned(self, x, y):
3958         ctv_backup = config.compute_test_value
3959         config.compute_test_value = 'off'
3960         try:
3961             z = dot(x, y)
3962         finally:
3963             config.compute_test_value = ctv_backup
3964         _logger = logging.getLogger('theano.gof.opt')
3965         oldlevel = _logger.level
3966         _logger.setLevel(logging.CRITICAL)
3967         try:
3968             try:
3969                 eval_outputs([z])
3970                 assert False    # should have raised exception
3971             except ValueError as e:
3972                 e0 = exc_message(e)
3973                 self.assertTrue(
3974                     e0.split()[1:4] == ['are', 'not', 'aligned'] or
3975                     e0.split()[0:2] == ['Shape', 'mismatch:'] or
3976                     (e0.split()[0:4] ==
3977                         ['Incompatible', 'shapes', 'for', 'gemv']) or
3978                     e)
3979         finally:
3980             _logger.setLevel(oldlevel)
3981     def test_align_1_1(self):
3982         self.not_aligned(rand(5), rand(6))
3983     def test_align_1_2(self):
3984         self.not_aligned(rand(5), rand(6, 4))
3985     def test_align_1_3(self):
3986         self.not_aligned(rand(5), rand(6, 4, 7))
3987     def test_align_2_1(self):
3988         self.not_aligned(rand(5, 4), rand(6))
3989     def test_align_2_2(self):
3990         self.not_aligned(rand(5, 4), rand(6, 7))
3991     def test_align_2_3(self):
3992         self.not_aligned(rand(5, 4), rand(6, 7, 8))
3993     def test_align_3_1(self):
3994         self.not_aligned(rand(5, 4, 3), rand(6))
3995     def test_align_3_2(self):
3996         self.not_aligned(rand(5, 4, 3), rand(6, 7))
3997     def test_align_3_3(self):
3998     def test_grad(self):
3999         utt<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.verify_grad(dot, [rand(2, 3), rand(3, 2)])
4000         utt.verify_grad(dot, [rand(2), rand(2, 3)])
4001         utt.verify_grad(dot, [rand(3, 2), rand(2)])
4002         utt.verify_grad(dot, [rand(2), rand(2)])
4003         utt.verify_grad(dot, [rand(), rand(2)])
4004         utt.verify_grad(dot, [rand(2), rand()])
4005         utt.verify_grad(dot, [rand(2, 5), rand()])
4006         utt.</b></font>verify_grad<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(dot, [rand(2, 3, 4), rand(4)])
4007         utt.verify_grad(dot, [rand(3), rand(2, 3, 4)])
4008         utt.verify_grad(dot, [rand(4, 3), rand(2, 3, 4)])
4009         utt.verify_grad(dot, [rand(2, 3, 4), rand(4, 5)])
4010         utt.verify_grad(dot, [rand(2, 3, 4), rand(</b></font>3, 4, 5)])
4011     @attr('slow')
4012     def test_broadcastable_patterns(self):
4013         def val_for(r):
4014             if r.dtype.startswith('complex'):
4015                 if r.ndim == 0:
4016                     return np.asarray(np.complex(1.1, 2.1),
4017                                       dtype=r.dtype)
4018                 if r.ndim == 1:
4019                     if r.dtype == 'complex64':
4020                         return np.complex64([np.complex(1.2, 2.2)])
4021                     elif r.dtype == 'complex128':
4022                         return np.complex128([np.complex(1.2, 2.2)])
4023                 elif r.ndim == 2:
4024                     if r.dtype == 'complex64':
4025                         return np.complex64([[np.complex(1.3, 2.3)]])
4026                     elif r.dtype == 'complex128':
4027                         return np.complex128([[np.complex(1.3, 2.3)]])
4028             if r.ndim == 0:
4029                 return np.asarray(1.1, dtype=r.dtype)
4030             if r.ndim == 1:
4031                 return np.asarray([1.2], dtype=r.dtype)
4032             elif r.ndim == 2:
4033                 return np.asarray([[1.3]], dtype=r.dtype)
4034             raise ValueError()
4035         for dtype0 in ('float32', 'float64', 'complex64'):
4036             for dtype1 in ('float32', 'complex64', 'complex128'):
4037                 for bc0 in ((True,), (False,), (True, True),
4038                             (True, False), (False, True),
4039                             (False, False)):
4040                     x = TensorType(dtype=dtype0, broadcastable=bc0)()
4041                     for bc1 in ((True,), (False,), (True, True),
4042                                 (True, False), (False, True),
4043                                 (False, False)):
4044                         y = TensorType(dtype=dtype1, broadcastable=bc1)()
4045                         z = dot(x, y)
4046                         t = TensorType(dtype=dtype0,
4047                                        broadcastable=z.broadcastable)()
4048                         rval = z * 3 + 2 * t
4049                         f = function([x, y, t], rval)
4050                         xval = val_for(x)
4051                         yval = val_for(y)
4052                         tval = val_for(t)
4053                         f(xval, yval, tval)  # debugmode checks result
4054                         if (dtype0.startswith('float') and
4055                                 dtype1.startswith('float')):
4056                             g = grad(z.sum(), x)
4057                             assert g.broadcastable == x.broadcastable
4058                             g = grad(z.sum(), y)
4059                             assert g.broadcastable == y.broadcastable
4060 class T_tensorfromscalar(unittest.TestCase):
4061     def test0(self):
4062         t = tensor_from_scalar(s)
4063         self.assertTrue(t.owner.op is tensor_from_scalar)
4064         self<font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
4065         self.assertTrue(t.type.ndim == 0, t.type.ndim)
4066         self.</b></font>assertTrue(t.type.dtype == s.type.dtype)
4067         v = eval_outputs([t])
4068         self.assertTrue(v == 56, v)
4069         self.assertTrue(isinstance(v, np.ndarray))
4070         self.assertTrue(v.shape == (), v.shape)
4071     def test1(self):
4072         t = as_tensor_variable(s)
4073         self.assertTrue(t.owner.op is tensor_from_scalar)
4074         self<font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
4075         self.assertTrue(t.type.ndim == 0, t.type.</b></font>ndim)
4076         self.assertTrue(t.type.dtype == s.type.dtype)
4077         v = eval_outputs([t])
4078         self.assertTrue(v == 56, v)
4079         self.assertTrue(isinstance(v, np.ndarray))
4080         self.assertTrue(v.shape == (), v.shape)
4081         g = grad(t, s)
4082         self.assertTrue(eval_outputs([g]) == 0.)
4083     def test2(self):
4084         s = scal.constant(56.)
4085         t = as_tensor_variable(s)
4086         self.assertTrue(t.owner.op is tensor_from_scalar)
4087         self.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
4088         self.assertTrue(t.type.ndim == 0, t.type.ndim)
4089         self.assertTrue(t.type.dtype == s.type.dtype)
4090         v = eval_outputs([t])
4091         self.assertTrue(v == 56., v)
4092         self.assertTrue(isinstance(v, np.ndarray))
4093         self.assertTrue(v.shape == (), v.shape)
4094         g = grad(t, s)
4095         self.assertTrue(eval_outputs([g]) == 1.)
4096 class T_scalarfromtensor(unittest.TestCase):
4097     def test0(self):
4098         tt = constant(56)  # scal.constant(56)
4099         ss = scalar_from_tensor(tt)
4100         self.assertTrue(ss.owner.op is scalar_from_tensor)
4101         self.assertTrue(ss.type.dtype == tt.type.dtype)
4102         v = eval_outputs([ss])
4103         self.assertTrue(v == 56, v)
4104         if config.cast_policy == 'custom':
4105             self.assertTrue(isinstance(v, np.int8))
4106         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4107             self.assertTrue(isinstance(
4108                 v, getattr(np, str(np.asarray(56).dtype))))
4109         else:
4110             raise NotImplementedError(config.cast_policy)
4111         self.assertTrue(v.shape == (), v.shape)
4112         tt = lscalar()
4113         ss = scalar_from_tensor(tt)
4114         ss.owner.op.grad([tt], [ss])
4115         fff = function([tt], ss)
4116         v = fff(np.asarray(5))
4117         self.assertTrue(v == 5, v)
4118         self.assertTrue(isinstance(v, np.int64))
4119         self.assertTrue(v.shape == (), v.shape)
4120 class test_grad(unittest.TestCase):
4121     class Obj1(gof.op.Op):
4122         def __init__(self):
4123             self.gval0 = scalar('e')
4124             self.gval1 = scalar('f')
4125         def make_node(self):
4126             inputs = [scalar('a'), scalar('c')]
4127             outputs = [scalar('b'), scalar('d')]
4128             return gof.Apply(self, inputs, outputs)
4129         def grad(self, inp, grads):
4130             x0, x1 = inp
4131             gz0, gz1 = grads
4132             return self.gval0, self.gval1
4133     def test_1param(self):
4134         o = test_grad.Obj1()
4135         a1 = o.make_node()
4136         self.assertTrue(o.gval0 is tensor.grad(a1.outputs[0], a1.inputs[0]))
4137     def test_Nparam(self):
4138         o = test_grad.Obj1()
4139         a1 = o.make_node()
4140         g0, g1 = grad(a1.outputs[0], a1.inputs)
4141         g0.name = None
4142         self.assertTrue(o.gval0 is g0)
4143         self.assertTrue(o.gval1 is g1)
4144     def test_grad_keep_type(self):
4145         X = tensor.matrix()
4146         y = X.sum()
4147         G = tensor.grad(y, [X])
4148         assert isinstance(G, list)
4149         G = tensor.grad(y, X)
4150         assert not isinstance(G, list)
4151     def test_1None_rval(self):
4152         o = test_grad.Obj1()
4153         a1 = o.make_node()
4154         g = grad(a1.outputs[0], a1.outputs[1],
4155                  disconnected_inputs='ignore')
4156         self.assertTrue(g.owner.op == fill)
4157         self.assertTrue(g.owner.inputs[1].data == 0)
4158         self.assertRaises(TypeError, grad, a1.outputs[0], 'wtf')
4159     def test_NNone_rval(self):
4160         o = test_grad.Obj1()
4161         a1 = o.make_node()
4162         g0, g1, g2 = grad(a1.outputs[0], a1.inputs + [scalar('z')],
4163                           disconnected_inputs='ignore')
4164         self.assertTrue(o.gval0 is g0)
4165         self.assertTrue(o.gval1 is g1)
4166         self.assertTrue(g2.owner.op == fill)
4167         self.assertTrue(g2.owner.inputs[1].data == 0)
4168     def test_zero_gradient_shape(self):
4169         x = dmatrix()
4170         f = theano.function([x], grad(dscalar(), x,
4171                                       disconnected_inputs='ignore'))
4172         a = np.ones((3, 7))
4173         self.assertTrue((f(a) == 0).all())  # Zero gradient.
4174         self.assertTrue(a.shape == f(a).shape)  # With proper shape.
4175     def test_cost_is_scalar(self):
4176         v = vector()
4177         m = matrix()
4178         self.assertRaises(TypeError, grad, v, v)
4179         self.assertRaises(TypeError, grad, m, m)
4180 class T_op_cache(unittest.TestCase):
4181     def setUp(self):
4182         utt.seed_rng()
4183     def test0(self):
4184         v = matrix()
4185         v.name = 'v'
4186         gv = fill(v / v, 1.0) / v - (fill(v / v, 1.0) * v) / (v * v)
4187         fn_py = inplace_func([v], gv)
4188         fn_c_or_py = inplace_func([v], gv)
4189         a = rand(5, 2).astype(config.floatX)
4190         self.assertTrue(np.all(fn_py(a) == fn_c_or_py(a)))
4191 class T_reshape(utt.InferShapeTester, utt.TestOptimizationMixin):
4192     def __init__(self, name, shared=tensor._shared, op=Reshape, mode=None,
4193                  ignore_topo=(DeepCopyOp, opt.MakeVector,
4194                               opt.Shape_i, DimShuffle, theano.tensor.Elemwise)):
4195         self.shared = shared
4196         self.op = op
4197         self.mode = mode
4198         self.ignore_topo = ignore_topo
4199         super(T_reshape, self).__init__(name)
4200     def function(self, inputs, outputs, ignore_empty=False):
4201         f = function(inputs, outputs, mode=self.mode)
4202         if self.mode is not None or theano.config.mode != "FAST_COMPILE":
4203             topo = f.maker.fgraph.toposort()
4204             topo_ = [node for node in topo if not isinstance(node.op,
4205                                                              self.ignore_topo)]
4206             if ignore_empty:
4207                 assert len(topo_) &lt;= 1, topo_
4208             else:
4209                 assert len(topo_) == 1, topo_
4210             if len(topo_) &gt; 0:
4211                 assert type(topo_[0].op) is self.op
4212         return f
4213     def test_reshape(self):
4214         a = dvector()
4215         b = dmatrix()
4216         d = dmatrix()
4217         c = reshape(b, as_tensor_variable(6), ndim=1)
4218         f = self.function([b], c)
4219         b_val1 = np.asarray([[0, 1, 2], [3, 4, 5]])
4220         c_val1 = np.asarray([0, 1, 2, 3, 4, 5])
4221         b_val2 = b_val1.T
4222         c_val2 = np.asarray([0, 3, 1, 4, 2, 5])
4223         f_out1 = f(b_val1)
4224         f_out2 = f(b_val2)
4225         assert np.all(f_out1 == c_val1), (f_out1, c_val1)
4226         assert np.all(f_out2 == c_val2), (f_out2, c_val2)
4227         c = reshape(b, (as_tensor_variable(6),), ndim=1)
4228         f = self.function([b], c)
4229         assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
4230                       np.asarray([0, 1, 2, 3, 4, 5]))
4231         c = reshape(b, d.shape)
4232         f = self.function([b, d], c)
4233         assert np.all(f(np.asarray<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>([[0, 1, 2], [3, 4, 5]]),
4234                         [[0, 1], [2, 3], [4, 5]]) ==
4235                       np.asarray([[0</b></font>, 1], [2, 3], [4, 5]]))
4236         c = reshape(a, [2, 3])
4237         f = self.function([a], c)
4238         assert np.all(f(np.asarray([0, 1, 2, 3, 4, 5])) ==
4239                       np.asarray([[0, 1, 2], [3, 4, 5]]))
4240         a_val = np.asarray([0, 1, 2, 3, 4, 5])
4241         a_val_copy = np.asarray([0, 1, 2, 3, 4, 5])
4242         b_val = np.asarray([[0, 1, 2], [3, 4, 5]])
4243         f_sub = self.function([a, b], c - b)
4244         assert np.all(f_sub(a_val, b_val) == 0.0)
4245         assert np.all(a_val == a_val_copy)
4246         a_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4247         a_val_copy = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4248         b_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4249         f_sub = self.function([a, b], c - b)
4250         assert np.all(f_sub(a_val, b_val) == 0.0)
4251         assert np.all(a_val == a_val_copy)
4252         def just_vals(v):
4253             return Reshape(2)(v, theano._asarray([2, 3], dtype='int32'))
4254         utt.verify_grad(just_vals, [a_val], mode=self.mode)
4255         self._compile_and_check([a], [c], (a_val,), self.op)
4256         c = reshape(b, (b.shape[0], b.shape[1], 1))
4257         f = self.function([b], c, ignore_empty=True)
4258         assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
4259                       np.asarray([[[0], [1], [2]], [[3], [4], [5]]]))
4260         assert (f.maker.fgraph.toposort()[-1].outputs[0].type.broadcastable ==
4261                 (False, False, True))
4262         c = reshape(b, (b.shape[1], b.shape[0], 1))
4263         f = self.function([b], c, ignore_empty=True)
4264         assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
4265                       np.asarray([[[0], [1]], [[2], [3]], [[4], [5]]]))
4266         assert (f.maker.fgraph.toposort()[-1].outputs[0].type.broadcastable ==
4267                 (False, False, True))
4268     def test_m1(self):
4269         t = tensor3()
4270         rng = np.random.RandomState(seed=utt.fetch_seed())
4271         val = rng.uniform(size=(3, 4, 5)).astype(config.floatX)
4272         for out in [t.reshape([-1]), t.reshape([-1, 5]),
4273                     t.reshape([5, -1]), t.reshape([5, -1, 3])]:
4274             self._compile_and_check([t], [out], [val], self.op)
4275     def test_reshape_long_in_shape(self):
4276         v = dvector('v')
4277         r = v.reshape((v.shape[0], L(1)))
4278         print(r.eval({v: np.arange(5.)}))
4279         assert np.allclose(r.eval({v: np.arange(5.)}).T,
4280                            np.arange(5.))
4281     def test_bad_shape(self):
4282         a = matrix('a')
4283         shapes = ivector('shapes')
4284         rng = np.random.RandomState(seed=utt.fetch_seed())
4285         a_val = rng.uniform(size=(3, 4)).astype(config.floatX)
4286         r = a.reshape(shapes, ndim=1)
4287         f = self.function([a, shapes], r)
4288         self.assertRaises(ValueError, f, a_val, [13])
4289         r = a.reshape(shapes, ndim=2)
4290         f = self.function([a, shapes], r)
4291         self.assertRaises(ValueError, f, a_val, [-1, 5])
4292         self.assertRaises(ValueError, f, a_val, [7, -1])
4293         self.assertRaises(ValueError, f, a_val, [7, 5])
4294         self.assertRaises(ValueError, f, a_val, [-1, -1])
4295     def test_0(self):
4296         x = fvector('x')
4297         f = self.function([x], x.reshape((0, 100)))
4298         assert f(np.ndarray((0,), dtype='float32')).shape == (0, 100)
4299     def test_empty_shp(self):
4300         const = theano.tensor.constant([1]).reshape(())
4301         f = function([], const)
4302         assert f().shape == ()
4303 def test_make_column_matrix_broadcastable():
4304     a = tensor.dmatrix()
4305     b = a.reshape((a.shape[0], )).dimshuffle(0, 'x')
4306     f = function([a], b)
4307     assert (f(np.zeros((3, 1))) + np.ones(2) == np.ones((3, 2))).all()
4308 def test_flatten_outdimNone():
4309     a = dmatrix()
4310     c = flatten(a)
4311     f = inplace_func([a], c)
4312     a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4313     c_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4314     assert np.all(f(a_val) == c_val)
4315     f = inplace_func([a], c)
4316     assert np.all(f(a_val) == c_val)
4317     utt.verify_grad(flatten, [a_val])
4318 def test_flatten_scalar():
4319     a = dscalar()
4320     c = flatten(a)
4321     f = inplace_func([a], c)
4322     a_val = theano._asarray(3.0, dtype='float64')
4323     c_val = theano._asarray([3.0], dtype='float64')
4324     assert np.all(f(a_val) == c_val)
4325     f = inplace_func([a], c)
4326     assert np.all(f(a_val) == c_val)
4327 def test_flatten_ndim1():
4328     a = dmatrix()
4329     c = flatten(a, 1)
4330     f = inplace_func([a], c)
4331     a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4332     c_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4333     assert np.all(f(a_val) == c_val)
4334     f = inplace_func([a], c)
4335     assert np.all(f(a_val) == c_val)
4336     utt.verify_grad(flatten, [a_val])
4337 def test_flatten_ndim2():
4338     a = dmatrix()
4339     c = flatten(a, 2)
4340     f = inplace_func([a], c)
4341     a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4342     assert np.all(f(a_val) == a_val)
4343     f = inplace_func([a], c)
4344     assert np.all(f(a_val) == a_val)
4345     flatten_2 = partial(flatten, ndim=2)
4346     utt.verify_grad(flatten_2, [a_val])
4347 def test_flatten_ndim2_of_3():
4348     a = TensorType('float64', (False, False, False))()
4349     c = flatten(a, 2)
4350     f = inplace_func([a], c)
4351     a_val = theano._asarray([[[0, 1], [2, 3]], [[4, 5], [6, 7]]],
4352                             dtype='float64')
4353     c_val = theano._asarray([[0, 1, 2, 3], [4, 5, 6, 7]], dtype='float64')
4354     assert np.all(f(a_val) == c_val)
4355     f = inplace_func([a], c)
4356     assert np.all(f(a_val) == c_val)
4357     flatten_2 = partial(flatten, ndim=2)
4358     utt.verify_grad(flatten_2, [a_val])
4359     flatten_2 = partial(flatten, outdim=2)
4360     utt.verify_grad(flatten_2, [a_val])
4361 def test_flatten_broadcastable():
4362     inp = TensorType('float64', (False, False, False, False))()
4363     out = flatten(inp, ndim=2)
4364     assert out.broadcastable == (False, False)
4365     inp = TensorType('float64', (False, False, False, True))()
4366     out = flatten(inp, ndim=2)
4367     assert out.broadcastable == (False, False)
4368     inp = TensorType('float64', (False, True, False, True))()
4369     out = flatten(inp, ndim=2)
4370     assert out.broadcastable == (False, False)
4371     inp = TensorType('float64', (False, True, True, True))()
4372     out = flatten(inp, ndim=2)
4373     assert out.broadcastable == (False, True)
4374     inp = TensorType('float64', (True, False, True, True))()
4375     out = flatten(inp, ndim=3)
4376     assert out.broadcastable == (True, False, True)
4377 def test_flatten_ndim_invalid():
4378     a = dmatrix()
4379     assert_raises(ValueError, flatten, a, 3)
4380     assert_raises(ValueError, flatten, a, 0)
4381 def test_is_flat():
4382     assert tensor.is_flat(tensor.as_tensor_variable(np.zeros((10))))
4383     assert tensor.is_flat(tensor.as_tensor_variable(np.zeros((10, 10, 10))),
4384                           ndim=3)
4385     assert not tensor.is_flat(
4386         tensor.as_tensor_variable(np.zeros((10, 10, 10))))
4387     assert tensor.is_flat(tensor.vector())
4388     assert tensor.is_flat(tensor.tensor3(), ndim=3)
4389     assert not tensor.is_flat(tensor.tensor3())
4390     X = tensor.tensor4()
4391     assert tensor.is_flat(X.reshape((-1, )))
4392     assert tensor.is_flat(X.reshape((10, 10, -1)), ndim=3)
4393     assert not tensor.is_flat(X.reshape((10, 10, -1)))
4394     X = tensor.tensor4()
4395     assert tensor.is_flat(X.reshape((tensor.iscalar(), )))
4396     assert tensor.is_flat(X.reshape((tensor.iscalar(), ) * 3), ndim=3)
4397     assert not tensor.is_flat(X.reshape((tensor.iscalar(), ) * 3))
4398 def test_tile():
4399     def run_tile(x, x_, reps, use_symbolic_reps):
4400         if use_symbolic_reps:
4401             rep_symbols = [iscalar() for _ in range(len(reps))]
4402             f = function([x] + rep_symbols, tile(x, rep_symbols))
4403             return f(*([x_] + list(reps)))
4404         else:
4405             f = function([x], tile(x, reps))
4406             return f(x_)
4407     rng = np.random.RandomState(utt.fetch_seed())
4408     for use_symbolic_reps in [False, True]:
4409         x = vector()
4410         x_ = rng.randn(5).astype(config.floatX)
4411         assert np.all(run_tile(x, x_, (2,), use_symbolic_reps) ==
4412                       np.tile(x_, (2,)))
4413         x = matrix()
4414         x_ = rng.randn(2, 4).astype(config.floatX)
4415         assert np.all(run_tile(x, x_, (2, 3), use_symbolic_reps) ==
4416                       np.tile(x_, (2, 3)))
4417         x = tensor3()
4418         x_ = rng.randn(2, 4, 3).astype(config.floatX)
4419         assert np.all(run_tile(x, x_, (2, 3, 4), use_symbolic_reps) ==
4420                       np.tile(x_, (2, 3, 4)))
4421         x = tensor4()
4422         x_ = rng.randn(2, 4, 3, 5).astype(config.floatX)
4423         assert np.all(run_tile(x, x_, (2, 3, 4, 6), use_symbolic_reps) ==
4424                       np.tile(x_, (2, 3, 4, 6)))
4425     test_shape = [2, 4, 3, 5]
4426     k = 0
4427     for xtype in [vector(), matrix(), tensor3(), tensor4()]:
4428         x = xtype
4429         k = k + 1
4430         x_ = rng.randn(*test_shape[0:k]).astype(config.floatX)
4431         reps_ = 2
4432         f = function([x], tile(x, reps_))
4433         assert np.all(f(x_) == np.tile(x_, reps_))
4434         reps = iscalar()
4435         reps_ = 2
4436         f = function([x, reps], tile(x, reps))
4437         assert np.all(f(x_, reps_) == np.tile(x_, reps_))
4438         reps = ivector()
4439         reps_ = [2] if k == 1 or k == 2 else [2, 3]
4440         ndim_ = k
4441         f = function([x, reps], tile(x, reps, ndim_))
4442         assert np.all(f(x_, reps_) == np.tile(x_, reps_))
4443         reps_ = [2, 3, 4]
4444         f = function([x], tile(x, reps_))
4445         assert np.all(f(x_) == np.tile(x_, reps_))
4446         d = iscalar()
4447         reps = [2, d, 4]
4448         f = function([x, d], tile(x, reps))
4449         reps_ = [2, 3, 4]
4450         assert np.all(f(x_, 3) == np.tile(x_, reps_))
4451         r = [2, 3, 4, 5, 6]
4452         reps_ = r[:k + 1]  # len(reps_) = x.ndim+1
4453         f = function([x], tile(x, reps_))
4454         assert np.all(f(x_) == np.tile(x_, reps_))
4455         ndim_ = len(reps_)
4456         f = function([x], tile(x, reps_, ndim_))
4457         assert np.all(f(x_) == np.tile(x_, reps_))
4458         ndim_ = len(reps_) + 1
4459         f = function([x], tile(x, reps_, ndim_))
4460         assert np.all(f(x_) == np.tile(x_, [1] + reps_))
4461         r = [2, 3, 4, 5]
4462         if k &gt; 1:
4463             ndim_ = k + 1
4464             reps_ = r[:k - 1]
4465             f = function([x], tile(x, reps_, ndim_))
4466             assert np.all(f(x_) == np.tile(x_, [1, 1] + reps_))
4467         reps = ivector()
4468         np.testing.assert_raises(ValueError, tile, x, reps)
4469         for reps in [2.5, fscalar(), fvector()]:
4470             np.testing.assert_raises(ValueError, tile, x, reps)
4471         reps = imatrix()
4472         np.testing.assert_raises(ValueError, tile, x, reps)
4473         for reps in [[2, 3, 4], iscalar(), ivector()]:
4474             if k &gt; 1:
4475                 ndim = k - 1
4476                 np.testing.assert_raises(ValueError, tile, x, reps, ndim)
4477         r = [2, 3, 4, 5, 6]
4478         reps = r[:k + 1]
4479         ndim = k
4480         np.testing.assert_raises(ValueError, tile, x, reps, ndim)
4481         reps = ivector()
4482         r = [2, 3, 4, 5, 6, 7]
4483         reps_ = r[:k + 2]
4484         ndim_ = k + 1
4485         f = function([x, reps], tile(x, reps, ndim_))
4486         np.testing.assert_raises(AssertionError, f, x_, reps_)
4487 def test_tile_grad():
4488     def grad_tile(x, reps, np_x):
4489         y = tile(x, reps)
4490         z = y.sum()
4491         g = theano.function([x], grad(z, x))
4492         grad_res = g(np_x)
4493         assert np.all(grad_res == np.prod(reps))
4494     rng = np.random.RandomState(utt.fetch_seed())
4495     grad_tile(vector('x'), [3], rng.randn(5).astype(config.floatX))
4496     grad_tile(matrix('x'), [3, 4], rng.randn(2, 3).astype(config.floatX))
4497     grad_tile(tensor3('x'), [3, 4, 5],
4498               rng.randn(2, 4, 3).astype(config.floatX))
4499     grad_tile(tensor4('x'), [3, 4, 5, 6],
4500               rng.randn(2, 4, 3, 5).astype(config.floatX))
4501 class TestARange(unittest.TestCase):
4502     def setUp(self):
4503         utt.seed_rng()
4504     def test_Op_integers(self):
4505         start, stop, step = iscalars('start', 'stop', 'step')
4506         out = ARange(start.type.dtype)(start, stop, step)
4507         f = function([start, stop, step], out)
4508         assert np.all(f(0, 5, 1) == np.arange(0, 5, 1))
4509         assert np.all(f(2, 11, 4) == np.arange(2, 11, 4))
4510         assert np.all(f(-5, 1, 1) == np.arange(-5, 1, 1))
4511         assert np.all(f(10, 2, -2) == np.arange(10, 2, -2))
4512         assert np.all(f(10, 2, 2) == np.arange(10, 2, 2))
4513         assert np.all(f(0, 0, 1) == np.arange(0, 0, 1))
4514     def test_grads(self):
4515         def f(start, stop, step):
4516             return ARange(start.type.dtype)(start, stop, step)
4517         rng = np.random.RandomState(utt.fetch_seed())
4518         for start, stop, step in [(0, 4.9, 1),
4519                                   (5.1, 0, -0.5),
4520                                   (1, 5.1, 0.5)]:
4521             utt.verify_grad(f, [np.asarray(start).astype(config.floatX),
4522                                 np.asarray(stop).astype(config.floatX),
4523                                 np.asarray(step).astype(config.floatX)],
4524                             rng=rng)
4525     def test_integers(self):
4526         start, stop, step = iscalars('start', 'stop', 'step')
4527         out = arange(start, stop, step)
4528         f = function([start, stop, step], out)
4529         if config.cast_policy == 'custom':
4530             assert out.dtype == 'int64'
4531         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4532             numpy_dtype = np.arange(np.array(1, dtype='int32')).dtype
4533             assert out.dtype == numpy_dtype
4534         else:
4535             raise NotImplementedError(config.cast_policy)
4536         assert np.all(f(0, 5, 1) == np.arange(0, 5, 1))
4537         assert np.all(f(2, 11, 4) == np.arange(2, 11, 4))
4538         assert np.all(f(-5, 1, 1) == np.arange(-5, 1, 1))
4539         assert np.all(f(10, 2, -2) == np.arange(10, 2, -2))
4540         assert np.all(f(10, 2, 2) == np.arange(10, 2, 2))
4541         assert np.all(f(0, 0, 1) == np.arange(0, 0, 1))
4542     def test_float32(self):
4543         start, stop, step = fscalars('start', 'stop', 'step')
4544         out = arange(start, stop, step)
4545         f = function([start, stop, step], out)
4546             assert out.dtype == start.type.dtype
4547         elif config.cast_policy == 'numpy':
4548             numpy_dtype = np.arange(np<font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.array(0, dtype=start.dtype),
4549                                     np.array(1, dtype=stop.dtype),
4550                                     np.array(1, dtype=step.dtype)).</b></font>dtype
4551             assert out.dtype == numpy_dtype
4552         elif config.cast_policy == 'numpy+floatX':
4553             assert out.dtype == config.floatX
4554         else:
4555             raise NotImplementedError(config.cast_policy)
4556         arg_vals = [(0, 5, 1), (2, 11, 4), (-5, 1.1, 1.2), (1.3, 2, -2.1),
4557                     (10, 2, 2)]
4558         for arg_v in arg_vals:
4559             start_v, stop_v, step_v = arg_v
4560             start_v_, stop_v_, step_v_ = np.asarray(arg_v,
4561                                                     dtype=start.type.dtype)
4562             f_val = f(start_v_, stop_v_, step_v_)
4563             if config.cast_policy == 'custom':
4564                 expected_val = np.arange(start_v, stop_v, step_v,
4565                                          dtype=start.type.dtype)
4566             elif config.cast_policy in ('numpy', 'numpy+floatX'):
4567                 expected_val = np.arange(start_v_, stop_v_, step_v_,
4568                                          dtype=out.dtype)
4569             else:
4570                 raise NotImplementedError(config.cast_policy)
4571             assert np.all(f_val == expected_val)
4572     def test_float64(self):
4573         start, stop, step = dscalars('start', 'stop', 'step')
4574         out = arange(start, stop, step)
4575         f = function([start, stop, step], out)
4576         assert out.dtype == start.type.dtype
4577         arg_vals = [(0, 5, 1), (2, 11, 4), (-5, 1.1, 1.2),
4578                     (1.3, 2, -2.1), (10, 2, 2)]
4579         for arg_v in arg_vals:
4580             start_v, stop_v, step_v = arg_v
4581             start_v_, stop_v_, step_v_ = np.asarray(arg_v,
4582                                                     dtype=start.type.dtype)
4583             f_val = f(start_v_, stop_v_, step_v_)
4584             if config.cast_policy == 'custom':
4585                 expected_val = np.arange(start_v, stop_v, step_v,
4586                                          dtype=start.type.dtype)
4587             elif config.cast_policy in ('numpy', 'numpy+floatX'):
4588                 expected_val = np.arange(start_v_, stop_v_, step_v_)
4589             else:
4590                 raise NotImplementedError(config.cast_policy)
4591             assert np.all(f_val == expected_val)
4592     def test_default_step(self):
4593         start, stop = iscalars('start', 'stop')
4594         out = arange(start, stop)
4595         f = function([start, stop], out)
4596         if config.cast_policy == 'custom':
4597             assert out.dtype == 'int64'
4598         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4599             assert out.dtype == np.arange(np.int32(0),
4600                                           np.int32(1)).dtype
4601         else:
4602             raise NotImplementedError(config.cast_policy)
4603         assert np.all(f(0, 5) == np.arange(0, 5))
4604         assert np.all(f(-5, 1) == np.arange(-5, 1))
4605         assert np.all(f(0, 0) == np.arange(0, 0))
4606         dstart, dstop = dscalars('start', 'stop')
4607         dout = arange(dstart, dstop)
4608         df = function([dstart, dstop], dout)
4609         assert dout.dtype == dstart.type.dtype
4610         assert np.all(df(0.2, 5.3) == np.arange(0.2, 5.3))
4611         assert np.all(df(0.8, 5.3) == np.arange(0.8, 5.3))
4612         assert np.all(df(-0.7, 5.3) == np.arange(-0.7, 5.3))
4613     def test_default_start(self):
4614         stop = iscalar('stop')
4615         out = arange(stop)
4616         f = function([stop], out)
4617         if config.cast_policy == 'custom':
4618             assert out.dtype == 'int64'
4619         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4620             assert out.dtype == np.arange(np.int32(1)).dtype
4621         else:
4622             raise NotImplementedError(config.cast_policy)
4623         assert np.all(f(8) == np.arange(8))
4624         assert np.all(f(-2) == np.arange(-2))
4625         fstop = fscalar('stop')
4626         fout = arange(fstop)
4627         ff = function([fstop], fout)
4628         if config.cast_policy == 'custom':
4629             assert fout.dtype == fstop.type.dtype
4630         elif config.cast_policy == 'numpy':
4631             assert fout.dtype == np.arange(np.float32(1)).dtype
4632         elif config.cast_policy == 'numpy+floatX':
4633             if config.floatX == 'float32':
4634                 assert fout.dtype == 'float32'
4635             else:
4636                 assert fout.dtype == np.arange(np.float32(1)).dtype
4637         else:
4638             raise NotImplementedError(config.cast_policy)
4639         fstop_values = [0.2, -0.7, 8.5]
4640         for fstop_v in fstop_values:
4641             fstop_v32 = np.float32(fstop_v)
4642             assert np.all(ff(fstop_v32) == np.arange(fstop_v))
4643     def test_upcast(self):
4644         if config.cast_policy == 'custom':
4645             assert arange(iscalar()).dtype == 'int64'
4646             assert arange(fscalar()).dtype == fscalar().dtype
4647             assert arange(dscalar()).dtype == dscalar().dtype
4648             assert arange(iscalar(), fscalar()).dtype == dscalar().dtype
4649             assert arange(iscalar(), dscalar()).dtype == dscalar().dtype
4650             assert arange(fscalar(), dscalar()).dtype == dscalar().dtype
4651             assert arange(iscalar(), fscalar(), dscalar()).dtype == \
4652                 dscalar().dtype
4653         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4654             for dtype in get_numeric_types():
4655                 arange_dtype = arange(scalar(dtype=str(dtype))).dtype
4656                 numpy_dtype = np.arange(np.array(1, dtype=dtype)).dtype
4657                 if (dtype != 'float64' and
4658                         numpy_dtype == 'float64' and
4659                         config.cast_policy == 'numpy+floatX' and
4660                         config.floatX == 'float32'):
4661                     assert arange_dtype == 'float32'
4662                 else:
4663                     assert arange_dtype == numpy_dtype
4664                 for stop_dtype in get_numeric_types():
4665                     arange_dtype = arange(
4666                         start=scalar(dtype=str(dtype)),
4667                         stop=scalar(dtype=str(stop_dtype))).dtype
4668                     numpy_dtype = np.arange(
4669                         start=np.array(0, dtype=dtype),
4670                         stop=np.array(1, dtype=stop_dtype)).dtype
4671                     if (dtype != 'float64' and
4672                             stop_dtype != 'float64' and
4673                             numpy_dtype == 'float64' and
4674                             config.cast_policy == 'numpy+floatX' and
4675                             config.floatX == 'float32'):
4676                         assert arange_dtype == 'float32'
4677                     else:
4678                         assert arange_dtype == numpy_dtype
4679                     for step_dtype in get_numeric_types():
4680                         arange_dtype <font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= arange(
4681                             start=scalar(dtype=str(dtype)),
4682                             stop=scalar(dtype=str(stop_dtype)),
4683                             step=scalar(dtype=str(step_dtype))).</b></font>dtype
4684                         numpy_dtype = np.arange(
4685                             start=np.array(0, dtype=dtype),
4686                             stop=np.array(1, dtype=stop_dtype),
4687                             step=np.array(1, dtype=step_dtype)).dtype
4688                         if (dtype != 'float64' and
4689                                 stop_dtype != 'float64' and
4690                                 step_dtype != 'float64' and
4691                                 numpy_dtype == 'float64' and
4692                                 config.cast_policy == 'numpy+floatX' and
4693                                 config.floatX == 'float32'):
4694                             assert arange_dtype == 'float32'
4695                         else:
4696                             assert arange_dtype == numpy_dtype
4697         else:
4698             raise NotImplementedError(config.cast_policy)
4699     def test_dtype_cache(self):
4700         start, stop, step = iscalars('start', 'stop', 'step')
4701         out1 = arange(start, stop, step)
4702         out2 = arange(start, stop, step, dtype=out1.dtype)
4703         out3 = arange(start, stop, 2., dtype=out1.dtype)
4704         out4 = arange(start, stop, 2.)
4705         assert out1.owner.op is out2.owner.op
4706         assert out2.owner.op is out3.owner.op
4707         assert out3.owner.op is not out4.owner.op
4708     def test_infer_shape(self):
4709         start, stop, step = iscalars('start', 'stop', 'step')
4710         out = arange(start, stop, step)
4711         mode = theano.config.mode
4712         if mode == 'FAST_COMPILE':
4713             mode = 'FAST_RUN'
4714         mode = compile.mode.get_mode(mode).excluding('fusion')
4715         f = function([start, stop, step], out.shape, mode=mode)
4716         assert len(f.maker.fgraph.toposort()) == 9
4717         if config.cast_policy == 'custom':
4718             assert out.dtype == 'int64'
4719         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4720             numpy_dtype = np.arange(np.array(0, dtype=start.dtype),
4721                                     np.array(1, dtype=stop.dtype),
4722                                     np.array(1, dtype=step.dtype)).dtype
4723             assert out.dtype == numpy_dtype
4724         else:
4725             raise NotImplementedError(config.cast_policy)
4726         assert np.all(f(0, 5, 1) == len(np.arange(0, 5, 1)))
4727         assert np.all(f(2, 11, 4) == len(np.arange(2, 11, 4)))
4728         assert np.all(f(-5, 1, 1) == len(np.arange(-5, 1, 1)))
4729         assert np.all(f(10, 2, -2) == len(np.arange(10, 2, -2)))
4730         assert np.all(f(10, 2, 2) == len(np.arange(10, 2, 2)))
4731         assert np.all(f(0, 0, 1) == len(np.arange(0, 0, 1)))
4732         out = arange(start, stop, 1)
4733         f = function([start, stop], out.shape, mode=mode)
4734         assert len(f.maker.fgraph.toposort()) == 5
4735         if config.cast_policy == 'custom':
4736             assert out.dtype == 'int64'
4737         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4738             assert out.dtype == np.arange(
4739                 np.int32(0), np.int32(1), np.int32(1)).dtype
4740         else:
4741             raise NotImplementedError(config.cast_policy)
4742         assert np.all(f(0, 5) == len(np.arange(0, 5)))
4743         assert np.all(f(2, 11) == len(np.arange(2, 11)))
4744         assert np.all(f(-5, 1) == len(np.arange(-5, 1)))
4745         assert np.all(f(10, 2) == len(np.arange(10, 2)))
4746         assert np.all(f(10, 2) == len(np.arange(10, 2)))
4747         assert np.all(f(0, 0) == len(np.arange(0, 0)))
4748         assert np.all(f(-64, 64) == len(np.arange(-64, 64)))
4749         assert arange(-64, 64).shape.eval() == [128]
4750         assert arange(-64, 64, 2).shape.eval() == [64]
4751         out = arange(0, stop, 1)
4752         f = function([stop], out.shape, mode=mode)
4753         assert len(f.maker.fgraph.toposort()) == 2
4754         if config.cast_policy == 'custom':
4755             assert out.dtype == 'int64'
4756         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4757             numpy_dtype = np.arange(0,
4758                                     np.array(1, dtype=stop.dtype),
4759                                     1).dtype
4760             assert out.dtype == numpy_dtype
4761         else:
4762             raise NotImplementedError(config.cast_policy)
4763         assert np.all(f(5) == len(np.arange(0, 5)))
4764         assert np.all(f(11) == len(np.arange(0, 11)))
4765         assert np.all(f(1) == len(np.arange(0, 1)))
4766         assert np.all(f(2) == len(np.arange(0, 2)))
4767         assert np.all(f(2) == len(np.arange(0, 2)))
4768         assert np.all(f(0) == len(np.arange(0, 0)))
4769 class TestNdGrid(unittest.TestCase):
4770     def setUp(self):
4771         pass
4772     def test_mgrid_numpy_equiv(self):
4773         nmgrid = (np.mgrid[0:1:.1, 1:10:1., 10:100:10.],
4774                   np.mgrid[0:2:1, 1:10:1, 10:100:10])
4775         tmgrid = (mgrid[0:1:.1, 1:10:1., 10:100:10.],
4776                   mgrid[0:2:1, 1:10:1, 10:100:10])
4777         for n, t in zip(nmgrid, tmgrid):
4778             for ng, tg in zip(n, t):
4779                 utt.assert_allclose(ng, tg.eval())
4780     def test_ogrid_numpy_equiv(self):
4781         nogrid = (np.ogrid[0:1:.1, 1:10:1., 10:100:10.],
4782                   np.ogrid[0:2:1, 1:10:1, 10:100:10])
4783         togrid = (ogrid[0:1:.1, 1:10:1., 10:100:10.],
4784                   ogrid[0:2:1, 1:10:1, 10:100:10])
4785         for n, t in zip(nogrid, togrid):
4786             for ng, tg in zip(n, t):
4787                 utt.assert_allclose(ng, tg.eval())
4788     def test_mgrid_theano_variable_numpy_equiv(self):
4789         nfmgrid = np.mgrid[0:1:.1, 1:10:1., 10:100:10.]
4790         nimgrid = np.mgrid[0:2:1, 1:10:1, 10:100:10]
4791         i, j, k = dscalars('i', 'j', 'k')
4792         l, m, n = iscalars('l', 'm', 'n')
4793         tfmgrid = mgrid[i:1:.1, 1:j:1., 10:100:k]
4794         timgrid = mgrid[l:2:1, 1:m:1, 10:100:n]
4795         ff = theano.function([i, j, k], tfmgrid)
4796         fi = theano.function([l, m, n], timgrid)
4797         for n, t in zip((nfmgrid, nimgrid), (ff(0, 10, 10.), fi(0, 10, 10))):
4798             for ng, tg in zip(n, t):
4799                 utt.assert_allclose(ng, tg)
4800     def test_ogrid_theano_variable_numpy_equiv(self):
4801         nfogrid = np.ogrid[0:1:.1, 1:10:1., 10:100:10.]
4802         niogrid = np.ogrid[0:2:1, 1:10:1, 10:100:10]
4803         i, j, k = dscalars('i', 'j', 'k')
4804         l, m, n = iscalars('l', 'm', 'n')
4805         tfogrid = ogrid[i:1:.1, 1:j:1., 10:100:k]
4806         tiogrid = ogrid[l:2:1, 1:m:1, 10:100:n]
4807         ff = theano.function([i, j, k], tfogrid)
4808         fi = theano.function([l, m, n], tiogrid)
4809         for n, t in zip((nfogrid, niogrid), (ff(0, 10, 10.), fi(0, 10, 10))):
4810             for ng, tg in zip(n, t):
4811                 utt.assert_allclose(ng, tg)
4812 class TestInversePermutation(unittest.TestCase):
4813     <font color="#68818b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def setUp(self):
4814         utt.seed_rng()
4815     def test_dim1(self):
4816         p = ivector()
4817         inv = inverse_permutation(p)
4818         assert inv.dtype == p.</b></font>dtype
4819         f_inverse = function([p], inv)
4820         rng = np.random.RandomState(utt.fetch_seed())
4821         p_val = rng.permutation(10).astype('int32')
4822         inv_val = f_inverse(p_val)
4823         assert np.all(f_inverse(inv_val) == p_val)
4824         assert np.all(p_val[inv_val] == np.arange(10))
4825         assert np.all(inv_val[p_val] == np.arange(10))
4826     def test_dim2(self):
4827         p = imatrix()
4828         inv = inverse_permutation(p)
4829         f_inverse = function([p], inv)
4830         rng = np.random.RandomState(utt.fetch_seed())
4831         p_val = np.asarray([rng.permutation(10) for i in range(7)],
4832                            dtype='int32')
4833         inv_val = f_inverse(p_val)
4834         assert np.all(f_inverse(inv_val) == p_val)
4835         for p_row, i_row in zip(p_val, inv_val):
4836             assert np.all(p_row[i_row] == np.arange(10))
4837             assert np.all(i_row[p_row] == np.arange(10))
4838 class TestPermuteRowElements(unittest.TestCase):
4839     def setUp(self):
4840         utt.seed_rng()
4841     def test_1_1(self):
4842         input = dvector()
4843         p = ivector()
4844         out = permute_row_elements(input, p)
4845         permute = function([input, p], out)
4846         rng = np.random.RandomState(utt.fetch_seed())
4847         input_val = rng.uniform(size=(5,))
4848         p_val = rng.permutation(5).astype('int32')
4849         out_val = permute(input_val, p_val)
4850         out_bis = input_val[p_val]
4851         assert np.all(out_val == out_bis)
4852         def permute_fixed(s_input):
4853             return permute_row_elements(s_input, p_val)
4854         utt.verify_grad(permute_fixed, [input_val])
4855     def test_2_1(self):
4856         input = matrix()
4857         p = ivector()
4858         out = permute_row_elements(input, p)
4859         permute = function([input, p], out)
4860         rng = np.random.RandomState(utt.fetch_seed())
4861         input_val = rng.uniform(size=(3, 5)).astype(config.floatX)
4862         p_val = rng.permutation(5).astype('int32')
4863         out_val = permute(input_val, p_val)
4864         out_bis = np.asarray([r[p_val] for r in input_val])
4865         assert np.all(out_val == out_bis)
4866         def permute_fixed(s_input):
4867             return permute_row_elements(s_input, p_val)
4868         utt.verify_grad(permute_fixed, [input_val])
4869     def test_2_2(self):
4870         input = matrix()
4871         p = imatrix()
4872         out = permute_row_elements(input, p)
4873         permute = function([input, p], out)
4874         rng = np.random.RandomState(utt.fetch_seed())
4875         input_val = rng.uniform(size=(3, 5)).astype(config.floatX)
4876         p_val = np.asarray([rng.permutation(5) for i in range(3)],
4877                            dtype='int32')
4878         out_val = permute(input_val, p_val)
4879         out_bis = np.asarray([i_row[p_row]
4880                               for i_row, p_row in zip(input_val, p_val)])
4881         assert np.all(out_val == out_bis)
4882         def permute_fixed(s_input):
4883             return permute_row_elements(s_input, p_val)
4884         utt.verify_grad(permute_fixed, [input_val])
4885     def test_1_2(self):
4886         input = vector()
4887         p = imatrix()
4888         out = permute_row_elements(input, p)
4889         permute = function([input, p], out)
4890         rng = np.random.RandomState(utt.fetch_seed())
4891         input_val = rng.uniform(size=(5,)).astype(config.floatX)
4892         p_val = np.asarray([rng.permutation(5)
4893                             for i in range(3)], dtype='int32')
4894         out_val = permute(input_val, p_val)
4895         out_bis = np.asarray([input_val[p_row] for p_row in p_val])
4896         assert np.all(out_val == out_bis)
4897         def permute_fixed(s_input):
4898             return permute_row_elements(s_input, p_val)
4899         utt.verify_grad(permute_fixed, [input_val])
4900     def test_3b_2(self):
4901         input = TensorType('floatX', (False, True, False))()
4902         p = imatrix()
4903         out = permute_row_elements(input, p)
4904         permute = function([input, p], out)
4905         rng = np.random.RandomState(utt.fetch_seed())
4906         input_val = rng.uniform(size=(4, 1, 5)).astype(config.floatX)
4907         p_val = np.asarray([rng.permutation(5) for i in range(3)],
4908                            dtype='int32')
4909         out_val = permute(input_val, p_val)
4910         out_bis = np.asarray([[in_mat[0, p_row] for p_row in p_val]
4911                               for in_mat in input_val])
4912         assert np.all(out_val == out_bis)
4913         def permute_fixed(s_input):
4914             return permute_row_elements(s_input, p_val)
4915         utt.verify_grad(permute_fixed, [input_val])
4916 class test_tensordot(unittest.TestCase):
4917     def TensorDot(self, axes):
4918         return lambda a, b: tensordot(a, b, axes)
4919     def setUp(self):
4920         utt.seed_rng()
4921     def test0(self):
4922         avec = vector()
4923         bvec = vector()
4924         axes = ((0, ), (0, ))
4925         c = tensordot(avec, bvec, axes)
4926         f1 = inplace_func([avec, bvec], c)
4927         aval = rand(5)
4928         bval = rand(5)
4929         out0 = np.tensordot(aval, bval, axes)
4930         out1 = f1(aval, bval)
4931         utt.assert_allclose(out0, out1)
4932         utt.verify_grad(self.TensorDot(axes), [aval, bval])
4933         bmat = matrix()
4934         axes = ((0, ), (1, ))
4935         c = tensordot(avec, bmat, axes)
4936         f2 = inplace_func([avec, bmat], c)
4937         aval = rand(5)
4938         bval = rand(8, 5)
4939         utt.assert_allclose(np.tensordot(aval, bval, axes),
4940                             f2(aval, bval))
4941         utt.verify_grad(self.TensorDot(axes), [aval, bval])
4942         amat = matrix()
4943                            [((0,), (1,)), [(4, 7), (9, 4)]],
4944                            [((1,), (0,)), [(4, 7), (7, 9)]],
4945                            [((<font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>1,), (1,)), [(4, 7), (9, 7)]],
4946                            [((0, 1), (0, 1)), [(4, 7), (4, 7)]],
4947                            ]:
4948             c = tensordot(amat, bmat, axes)
4949             f3 =</b></font> inplace_func([amat, bmat], c)
4950             aval = rand(*shps[0])
4951             bval = rand(*shps[1])
4952             utt.assert_allclose(np.tensordot(aval, bval, axes),
4953                                 f3(aval, bval))
4954             utt.verify_grad(self.TensorDot(axes), [aval, bval])
4955         for axes, shps in [[((2,), (1,)), [(1, 2, 3, 4), (2, 3)]],
4956                            [((0,), (1,)), [(1, 2, 3, 4), (3, 1)]],
4957                            [((<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0,), (0,)), [(1, 2, 3, 4), (1, 3)]],
4958                            [((3,), (0,)), [(1, 2, 3, 4), (4, 1)]],
4959                            ]:
4960             atens = tensor4()
4961             c =</b></font> tensordot(atens, bmat, axes)
4962             f4 = inplace_func([atens, bmat], c)
4963             aval = rand(*shps[0])
4964             bval = rand(*shps[1])
4965             utt.assert_allclose(np.tensordot(aval, bval, axes),
4966                                 f4(aval, bval))
4967             utt.verify_grad(self.TensorDot(axes), [aval, bval])
4968         atens = tensor4()
4969         btens = tensor3()
4970         axes = ((1, 3), (0, 2))
4971         c = tensordot(atens, btens, axes)
4972         f5 = inplace_func([atens, btens], c)
4973         aval = rand(4, 3, 5, 2)
4974         bval = rand(3, 4, 2)
4975         utt.assert_allclose(np.tensordot(aval, bval, axes),
4976                             f5(aval, bval))
4977         utt.verify_grad(self.TensorDot(axes), [aval, bval])
4978         axes = (axes[1], axes[0])
4979         c = tensordot(btens, atens, axes)
4980         f6 = inplace_func([btens, atens], c)
4981         utt.assert_allclose(np.tensordot(bval, aval, axes),
4982                             f6(bval, aval))
4983         utt.verify_grad(self.TensorDot(axes), [bval, aval])
4984     def test_raise_error(self):
4985         amat = matrix()
4986         bmat = matrix()
4987         bvec = vector()
4988         self.assertRaises(ValueError, tensordot, amat, bmat, (0, 1, 2))
4989         self.assertRaises(ValueError, tensordot, amat, bmat, ((0, 1), (0)))
4990         self.assertRaises(ValueError, tensordot, amat, bmat, ((0, 1, 2), (0, 1, 2)))
4991         self.assertRaises(ValueError, tensordot, amat, bvec, (0, 1))
4992         self.assertRaises(ValueError, tensordot, amat, bvec, 2)
4993     def test_weird_valid_axes(self):
4994         amat = matrix()
4995         bmat = matrix()
4996         for axes in [0,
4997                      (1, 0),
4998                      [1, 0],
4999                      (1, (0, )),
5000                      ((1, ), 0),
5001                      ([1], [0]),
5002                      ([], [])]:
5003             c = tensordot(amat, bmat, axes)
5004             f3 = inplace_func([amat, bmat], c)
5005             aval = rand(4, 7)
5006             bval = rand(7, 9)
5007             utt.assert_allclose(np.tensordot(aval, bval, axes),
5008                                 f3(aval, bval))
5009             utt.verify_grad(self.TensorDot(axes), [aval, bval])
5010     def test_scalar_axes(self):
5011         amat = fmatrix()
5012         bmat = dmatrix()
5013         axes = 1
5014         aval = rand(4, 5).astype('float32')
5015         bval = rand(5, 3)
5016         c = tensordot(amat, bmat, axes)
5017         f3 = inplace_func([amat, bmat], c)
5018         self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
5019                                     f3(aval, bval)))
5020         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5021         amat = tensor3()
5022         bmat = tensor3()
5023         axes = 2
5024         aval = rand(3, 4, 5)
5025         bval = rand(4, 5, 3)
5026         c = tensordot(amat, bmat, axes)
5027         f3 = inplace_func([amat, bmat], c)
5028         self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
5029                                     f3(aval, bval)))
5030         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5031     def test_scalar0(self):
5032         amat = matrix()
5033         bmat = matrix()
5034         axes = 0
5035         aval = rand(4, 5)
5036         bval = rand(5, 4)
5037         c = tensordot(amat, bmat, axes)
5038         f3 = inplace_func([amat, bmat], c)
5039         self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
5040                                     f3(aval, bval)))
5041         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5042     def test_broadcastable1(self):
5043         x = TensorType(dtype=floatX, broadcastable=(True, False, False))('x')
5044         y = tensor3('y')
5045         z = tensordot(x, y)
5046         assert z.broadcastable == (True, False)
5047         f = inplace_func([x, y], z)
5048         xv = rand(1, 3, 4)
5049         yv = rand(3, 4, 5)
5050         zv = f(xv, yv)
5051         self.assertTrue(np.allclose(np.tensordot(xv, yv), zv))
5052     def test_broadcastable2(self):
5053         x = TensorType(dtype=floatX, broadcastable=(True, False, False))('x')
5054         y = tensor3('y')
5055         axes = [[2, 1], [0, 1]]
5056         z = tensordot(x, y, axes=axes)
5057         assert z.broadcastable == (True, False)
5058         f = inplace_func([x, y], z)
5059         xv = rand(1, 3, 4)
5060         yv = rand(4, 3, 5)
5061         zv = f(xv, yv)
5062         self.assertTrue(np.allclose(np.tensordot(xv, yv, axes=axes), zv))
5063 def test_smallest_stack():
5064     sx, sy = dscalar(), dscalar()
5065     rval = inplace_func([sx, sy], stack([sx, sy]))(-4.0, -2.0)
5066     assert type(rval) == np.ndarray
5067     assert [-4, -2] == list(rval)
5068 def test_smallest():
5069     x = dvector()
5070     y = dvector()
5071     z = dvector()
5072     f1 = inplace_func([x], smallest(x))
5073     assert np.all([1, 2, 3] == f1([1, 2, 3]))
5074     f3 = inplace_func([x, y, z], smallest(x, y, z))
5075     assert np.all([1, 2, 3] == f3([1, 3, 9], [7, 7, 7], [8, 2, 3]))
5076     sx, sy = dscalar(), dscalar()
5077     assert -4 == inplace_func([sx, sy], smallest(sx, sy))(-4.0, -2.0)
5078 def test_reshape_member_fn():
5079     x = dmatrix()
5080     y = x.reshape((4, 5, 6))
5081     assert y.owner.op == Reshape(3)
5082 def test_var():
5083     a = Tensor(dtype='float64', broadcastable=[False, False, False])()
5084     f = function([a], var(a))
5085     a_val = np.arange(60).reshape(3, 4, 5)
5086     assert np.allclose(np.var(a_val), f(a_val))
5087     f = function([a], var(a, axis=0))
5088     assert np.allclose(np.var(a_val, axis=0), f(a_val))
5089     f = function([a], var(a, axis=1))
5090     assert np.allclose(np.var(a_val, axis=1), f(a_val))
5091     f = function([a], var(a, axis=2))
5092     assert np.allclose(np.var(a_val, axis=2), f(a_val))
5093     f = function([a], var(a, axis=0, ddof=0))
5094     assert np.allclose(np.var(a_val, axis=0, ddof=0), f(a_val))
5095     f = function([a], var(a, axis=1, ddof=1))
5096     assert np.allclose(np.var(a_val, axis=1, ddof=1), f(a_val))
5097     f = function([a], var(a, axis=2, ddof=1))
5098     assert np.allclose(np.var(a_val, axis=2, ddof=1), f(a_val))
5099     f = function([a], var(a, ddof=0, corrected=True))
5100     mean_a = np.mean(a_val)
5101     centered_a = a_val - mean_a
5102     v = np.mean(centered_a ** 2)
5103     error = (np.mean(centered_a)) ** 2
5104     v = v - error
5105     assert np.allclose(v, f(a_val))
5106     f = function([a], var(a, axis=2, ddof=1, corrected=True))
5107     mean_a = np.mean(a_val, axis=2, keepdims=True)
5108     centered_a = a_val - mean_a
5109     v = np.var(a_val, axis=2, ddof=1)
5110     shp_inp = np.shape(a_val)
5111     shp = shp_inp - np.array(1)
5112     error = (np.sum(centered_a, axis=2)) ** 2
5113     error = np.true_divide(error, shp[1] * shp_inp[1])
5114     v = v - error
5115     assert np.allclose(v, f(a_val))
5116     assert theano.tensor.vector(dtype='float16').var().dtype == 'float16'
5117 class T_sum(unittest.TestCase):
5118     def test_sum_overflow(self):
5119         a = Tensor(dtype='int8', broadcastable=[False])()
5120         f = function([a], sum(a))
5121         assert f([1] * 300) == 300
5122     def test_list(self):
5123         ll = [theano.shared(0.), theano.shared(2.)]
5124         tensor.sum(ll).eval() == 2
5125 @dec.skipif(
5126     isinstance(get_default_mode(), theano.compile.debugmode.DebugMode),
5127     ("This test fails in DEBUG_MODE, but the generated code is OK. "
5128      "It is actually a problem of DEBUG_MODE, see #626."))
5129 def test_default():
5130     x, y = scalars('xy')
5131     z = default(x, y)
5132     f = function([x, y], z)
5133     assert f(1, 2) == 1
5134     assert f(None, 2) == 2
5135     assert f(1, None) == 1
5136 @dec.skipif(
5137     isinstance(get_default_mode(), theano.compile.debugmode.DebugMode),
5138     ("This test fails in DEBUG_MODE, but the generated code is OK. "
5139      "It is actually a problem of DEBUG_MODE, see #626."))
5140 def test_default_state():
5141     x, y = scalars('xy')
5142     z = default(x, 3.8)
5143     new_x = y + z
5144     f = function([y, compile.In(x, update=new_x, value=12.0)], new_x)
5145     assert f(3) == 15
5146     f['x'] = None
5147     assert np.allclose(f(1), 4.8)
5148     assert np.allclose(f(np.asarray(2.2, dtype=config.floatX)), 7)
5149 def test_autocast():
5150     backup_config = config.cast_policy
5151     for autocast_cfg in (
5152             'custom',
5153             'numpy+floatX',
5154             ):
5155         config.cast_policy = autocast_cfg
5156         try:
5157             eval('_test_autocast_' + autocast_cfg.replace('+', '_'))()
5158         finally:
5159             config.cast_policy = backup_config
5160 def _test_autocast_custom():
5161     assert config.cast_policy == 'custom'
5162     orig_autocast = autocast_float.dtypes
5163     with autocast_float_as('float32'):
5164         assert autocast_float.dtypes == ('float32',)
5165     assert autocast_float.dtypes == orig_autocast
5166     with autocast_float_as('float64'):
5167         assert autocast_float.dtypes == ('float64',)
5168     assert autocast_float.dtypes == orig_autocast
5169     with autocast_float_as('float32'):
5170         assert autocast_float.dtypes == ('float32',)
5171         with autocast_float_as('float64'):
5172             assert autocast_float.dtypes == ('float64',)
5173         assert autocast_float.dtypes == ('float32',)
5174     assert autocast_float.dtypes == orig_autocast
5175     with autocast_float_as('float32'):
5176         assert (dvector() + 1.1).dtype == 'float64'
5177         assert (fvector() + 1.1).dtype == 'float32'
5178         assert ((fvector() + theano._asarray(1.1, dtype='float64')).dtype ==
5179                 'float64')
5180         assert ((fvector() + theano._asarray(1.1, dtype='float32')).dtype ==
5181                 'float32')
5182         assert (dvector() + 1).dtype == 'float64'
5183         assert (fvector() + 1).dtype == 'float32'
5184     with autocast_float_as('float64'):
5185         assert (dvector() + 1.1).dtype == 'float64'
5186         assert (fvector() + 1.1).dtype == 'float64'
5187         assert (fvector() + 1.0).dtype == 'float64'
5188         assert ((fvector() + theano._asarray(1.1, dtype='float64')).dtype ==
5189                 'float64')
5190         assert ((fvector() + theano._asarray(1.1, dtype='float32')).dtype ==
5191                 'float32')
5192         assert (dvector() + 1).dtype == 'float64'
5193         assert (fvector() + 1).dtype == 'float32'
5194     with autocast_float_as('float32', 'float64'):
5195         assert (dvector() + 1.1).dtype == 'float64'
5196         assert (fvector() + 1.1).dtype == theano.config.floatX
5197         assert (fvector() + 1.0).dtype == 'float32'
5198         assert (dvector() + np.float32(1.1)).dtype == 'float64'
5199         assert (dvector() + np.float64(1.1)).dtype == 'float64'
5200         assert (dvector() + np.float(1.1)).dtype == 'float64'
5201         assert (fvector() + np.float32(1.1)).dtype == 'float32'
5202         assert (fvector() + np.float64(1.1)).dtype == 'float64'
5203         assert (fvector() + np.float(1.1)).dtype == theano.config.floatX
5204         assert (lvector() + np.int64(1)).dtype == 'int64'
5205         assert (lvector() + np.int32(1)).dtype == 'int64'
5206         assert (lvector() + np.int16(1)).dtype == 'int64'
5207         assert (lvector() + np.int8(1)).dtype == 'int64'
5208         assert (ivector() + np.int8(1)).dtype == 'int32'
5209         assert (wvector() + np.int8(1)).dtype == 'int16'
5210         assert (bvector() + np.int8(1)).dtype == 'int8'
5211         with autocast_float_as('float64'):
5212             assert (fvector() + 1.0).dtype == 'float64'
5213 def _test_autocast_numpy():
5214     assert config.cast_policy == 'numpy'
5215     def ok(z):
5216         assert tensor.constant(z).dtype == np.asarray(z).dtype
5217     for x in ([2 ** i for i in xrange(63)] +
5218               [0, L(0), L(1), L(2 ** 63 - 1)] +
5219               [0., 1., 1.1, 1.5]):
5220         n_x = np.asarray(x)
5221         ok(x)
5222         ok(-x)
5223         ok(x - 1)
5224         ok(-x + 1)
5225         ok(n_x)
5226 def _test_autocast_numpy_floatX():
5227     assert config.cast_policy == 'numpy+floatX'
5228     backup_floatX = config.floatX
5229     def ok(z, floatX):
5230         if (isinstance(z, float) and
5231                 floatX == 'float32' and
5232                 not hasattr(z, 'dtype')):
5233             assert tensor.constant(z).dtype == 'float32'
5234         else:
5235             assert tensor.constant(z).dtype == np.asarray(z).dtype
5236     try:
5237         for floatX in ('float32', 'float64'):
5238             config.floatX = floatX
5239             for x in ([2 ** i - 1 for i in xrange(64)] +
5240                       [0, L(0), L(1), L(2 ** 63 - 1)] +
5241                       [0., 1., 1.1, 1.5]):
5242                 ok(x, floatX)
5243                 ok(-x, floatX)
5244                 ok(x - 1, floatX)
5245                 ok(-x + 1, floatX)
5246                 ok(np.asarray(x), floatX)
5247                 ok(np.float64(x), floatX)
5248     finally:
5249         config.floatX = backup_floatX
5250 class test_arithmetic_cast(unittest.TestCase):
5251     def test_arithmetic_cast(self):
5252         backup_config = config.cast_policy
5253         dtypes = get_numeric_types(with_complex=True)
5254         def theano_scalar(dtype):
5255             return tensor.scalar(dtype=str(dtype))
5256         def numpy_scalar(dtype):
5257             return np.array(1, dtype=dtype)
5258         def theano_array(dtype):
5259             return tensor.vector(dtype=str(dtype))
5260         def numpy_array(dtype):
5261             return np.array([1], dtype=dtype)
5262         def theano_i_scalar(dtype):
5263             return theano.scalar.Scalar(str(dtype))()
5264         def numpy_i_scalar(dtype):
5265             return numpy_scalar(dtype)
5266         if config.int_division == 'int':
5267             warnings.filterwarnings('ignore', message='Division of two integer',
5268                                     category=DeprecationWarning)
5269         try:
5270             for cfg in ('numpy+floatX', ):  # Used to test 'numpy' as well.
5271                 config.cast_policy = cfg
5272                 for op in (operator.add, operator.sub, operator.mul,
5273                            operator_div, operator.floordiv):
5274                     for a_type in dtypes:
5275                         for b_type in dtypes:
5276                             is_int_division = (
5277                                 op is operator_div and
5278                                 a_type in tensor.discrete_dtypes and
5279                                 b_type in tensor.discrete_dtypes)
5280                             for combo in (
5281                                     ('scalar', 'scalar'),
5282                                     ('array', 'array'),
5283                                     ('scalar', 'array'),
5284                                     ('array', 'scalar'),
5285                                     ('i_scalar', 'i_scalar'),
5286                                     ):
5287                                 theano_args = list(
5288                                     map(eval, ['theano_%s' % c for c in combo]))
5289                                 numpy_args = list(
5290                                     map(eval, ['numpy_%s' % c for c in combo]))
5291                                 try:
5292                                     theano_dtype = op(
5293                                         theano_args[0](a_type),
5294                                         theano_args[1](b_type)).type.dtype
5295                                     assert not (is_int_division and
5296                                                 config.int_division == 'raise')
5297                                 except theano.scalar.IntegerDivisionError:
5298                                     assert (is_int_division and
5299                                             config.int_division == 'raise')
5300                                     continue
5301                                 numpy_dtypes = [
5302                                     op(numpy_args[0](a_type),
5303                                        numpy_args[1](b_type)).dtype,
5304                                     op(numpy_args[1](b_type),
5305                                        numpy_args[0](a_type)).dtype]
5306                                 numpy_dtype = theano.scalar.upcast(
5307                                     *list(map(str, numpy_dtypes)))
5308                                 if numpy_dtype == theano_dtype:
5309                                     continue
5310                                 if (cfg == 'numpy+floatX' and
5311                                         config.floatX == 'float32' and
5312                                         a_type != 'float64' and
5313                                         b_type != 'float64' and
5314                                         numpy_dtype == 'float64'):
5315                                     assert theano_dtype == 'float32'
5316                                     continue
5317                                 if 'array' in combo and 'scalar' in combo:
5318                                     array_type, scalar_type = (
5319                                         (a_type, b_type)[list(combo).index(arg)]
5320                                         for arg in ('array', 'scalar'))
5321                                     up_type = theano.scalar.upcast(array_type,
5322                                                                    scalar_type)
5323                                     if (
5324                                             scalar_type != array_type and
5325                                             array_type != up_type and
5326                                             theano_dtype == up_type and
5327                                             array_type == numpy_dtype):
5328                                         continue
5329                                 if (is_int_division and
5330                                         config.int_division == 'floatX'):
5331                                     assert theano_dtype == config.floatX
5332                                     continue
5333                                 if (cfg == 'numpy+floatX' and
5334                                         a_type == 'complex128' and
5335                                         (b_type == 'float32' or
5336                                          b_type == 'float16') and
5337                                         combo == ('scalar', 'array') and
5338                                         theano_dtype == 'complex128' and
5339                                         numpy_dtype == 'complex64'):
5340                                     raise SkipTest("Known issue with"
5341                                                    "numpy see #761")
5342                                 assert False
5343         finally:
5344             config.cast_policy = backup_config
5345             if config.int_division == 'int':
5346                 warnings.filterwarnings(
5347                     'default',
5348                     message='Division of two integer',
5349                     category=DeprecationWarning)
5350 class T_long_tensor(unittest.TestCase):
5351     def test_fit_int64(self):
5352         bitwidth = theano.configdefaults.python_int_bitwidth()
5353         for exponent in xrange(bitwidth):
5354             val = L(2 ** exponent - 1)
5355             scalar_ct = constant(val)
5356             assert scalar_ct.dtype in tensor.int_dtypes, (exponent, val, scalar_ct.dtype)
5357             assert scalar_ct.value == val
5358             vector_ct = constant([val, val])
5359             if PY3 and bitwidth == 32:
5360                 assert vector_ct.dtype == 'int32'
5361             else:
5362                 assert vector_ct.dtype == 'int64'
5363             assert np.all(vector_ct.value == val)
5364             matrix_ct = constant([[val, val]])
5365             if PY3 and bitwidth == 32:
5366                 assert matrix_ct.dtype == 'int32'
5367             else:
5368                 assert matrix_ct.dtype == 'int64'
5369             assert np.all(matrix_ct.value == val)
5370     def test_too_big(self):
5371         val = L(2 ** 64)
5372         self.assertRaises(Exception, constant, val)
5373         self.assertRaises(Exception, constant, [val, val])
5374         self.assertRaises(Exception, constant, [[val, val]])
5375 class test_broadcast(unittest.TestCase):
5376     def test_broadcast_bigdim(self):
5377         def f():
5378             x = matrix()
5379             addbroadcast(x, 2)
5380         self.assertRaises(ValueError, f)
5381     def test_unbroadcast_addbroadcast(self):
5382         x = matrix()
5383         assert unbroadcast(x, 0) is x
5384         assert unbroadcast(x, 1) is x
5385         assert unbroadcast(x, 1, 0) is x
5386         assert unbroadcast(x, 0, 1) is x
5387         assert addbroadcast(x, 0) is not x
5388         assert addbroadcast(x, 1) is not x
5389         assert addbroadcast(x, 1, 0).owner.inputs[0] is x
5390         assert unbroadcast(addbroadcast(x, 0), 0) is x
5391         assert addbroadcast(unbroadcast(x, 0), 0) is not x
5392         x = row()
5393         assert unbroadcast(x, 0) is not x
5394         assert unbroadcast(x, 1) is x
5395         assert unbroadcast(x, 1, 0) is not x
5396         assert unbroadcast(x, 0, 1) is not x
5397         assert addbroadcast(x, 0) is x
5398         assert addbroadcast(x, 1).owner.inputs[0] is x
5399         assert addbroadcast(x, 1, 0).owner.inputs[0] is x
5400         assert addbroadcast(x, 0, 1).owner.inputs[0] is x
5401         assert unbroadcast(addbroadcast(x, 1), 1) is x
5402         assert addbroadcast(unbroadcast(x, 1), 1) is not x
5403         assert unbroadcast(unbroadcast(x, 0), 0).owner.inputs[0] is x
5404         x = TensorType(dtype='float64', broadcastable=(True, True))()
5405         assert unbroadcast(unbroadcast(x, 1), 0).owner.inputs[0] is x
5406         assert addbroadcast(unbroadcast(x, 1), 0).owner.inputs[0] is x
5407         assert addbroadcast(unbroadcast(x, 0), 0) is x
5408     def test_patternbroadcast(self):
5409         x = scalar('x')
5410         m = tensor.matrix('m')
5411         s = patternbroadcast(m, x.broadcastable)
5412         assert s is m
5413         x2 = patternbroadcast(x, x.broadcastable)
5414         assert x2 is x
5415     def test_infer_shape(self):
5416         x = matrix()
5417         y = addbroadcast(x, 0)
5418         f = theano.function([x], y.shape)
5419         assert (f(np.zeros((1, 5), dtype=config.floatX)) == [1, 5]).all()
5420         topo = f.maker.fgraph.toposort()
5421         if theano.config.mode != 'FAST_COMPILE':
5422             assert len(topo) == 2
5423             assert isinstance(topo[0].op, opt.Shape_i)
5424             assert isinstance(topo[1].op, opt.MakeVector)
5425         x = matrix()
5426         y = unbroadcast(x, 0)
5427         f = theano.function([x], y.shape)
5428         assert (f(np.zeros((2, 5), dtype=config.floatX)) == [2, 5]).all()
5429         topo = f.maker.fgraph.toposort()
5430         if theano.config.mode != 'FAST_COMPILE':
5431             assert len(topo) == 3
5432             assert isinstance(topo[0].op, opt.Shape_i)
5433             assert isinstance(topo[1].op, opt.Shape_i)
5434             assert isinstance(topo[2].op, opt.MakeVector)
5435         x = row()
5436         y = unbroadcast(x, 0)
5437         f = theano.function([x], y.shape)
5438         assert (f(np.zeros((1, 5), dtype=config.floatX)) == [1, 5]).all()
5439         topo = f.maker.fgraph.toposort()
5440         if theano.config.mode != 'FAST_COMPILE':
5441             assert len(topo) == 2
5442             assert isinstance(topo[0].op, opt.Shape_i)
5443             assert isinstance(topo[1].op, opt.MakeVector)
5444 def test_len():
5445     for shape_ in [(5,), (3, 4), (7, 4, 6)]:
5446         x = tensor.tensor(dtype='floatX', broadcastable=(False,) * len(shape_))
5447         assert_raises(TypeError, len, x)
5448 def test_mod():
5449     x, y = fscalars('xy')
5450     fn = gof.DualLinker().accept(
5451         gof.FunctionGraph([x, y], [x % y])).make_function()
5452     for a, b in ((0, 1), (1, 1), (0, -1), (1, -1), (-1, -1),
5453                  (1, 2), (-1, 2), (1, -2), (-1, -2),
5454                  (5, 3), (-5, 3), (5, -3), (-5, -3)
5455                  ):
5456         assert fn(a, b) == a % b, (a,)
5457 def test_divmod():
5458     x, y = fscalars('xy')
5459     fn = gof.DualLinker().accept(
5460         gof.FunctionGraph([x, y], [d, r])).make_function()
5461     for a, b in ((0, 1), (1, 1), (0, -1), (<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>1, -1), (-1, -1),
5462                  (1, 2), (-1, 2), (1, -2), (-1, -2),
5463                  (5, 3), (-5, 3), (5, -3), (-5, -3)
5464                  ):
5465         d_v, r_v = fn(a, b)
5466         d_vp, r_vp =</b></font> divmod(a, b)
5467         assert d_v == d_vp and r_v == r_vp, (a,)
5468 def test_mod_compile():
5469     x = tensor.vector()
5470     y = tensor.vector()
5471     out = tensor.switch(tensor.eq(3 % x.shape[0], 0), y, y[:-1])
5472     theano.function([x, y], out)
5473 def test_unalign():
5474     if config.floatX == 'float64':
5475         dtype = "b1,f8"
5476     else:
5477         dtype = "b1,f4"
5478     a = np.empty(10000, dtype=dtype)['f1']
5479     b = np.empty(10000, dtype=dtype)['f1']
5480     assert not a.flags.aligned
5481     assert not b.flags.aligned
5482     a[:] = rand(len(a))
5483     b[:] = rand(len(b))
5484     out_numpy = 2 * a + 3 * b
5485     av, bv = tensor.vectors('ab')
5486     f = theano.function([av, bv], 2 * av + 3 * bv)
5487     f.maker.fgraph.toposort()
5488     try:
5489         out_theano = f(a, b)
5490         assert not a.flags.aligned
5491         assert not b.flags.aligned
5492         assert np.allclose(out_numpy, out_theano)
5493         assert False
5494     except TypeError:
5495         pass
5496     a = np.empty((), dtype=dtype)['f1']
5497     b = np.empty((), dtype=dtype)['f1']
5498     assert not a.flags.aligned
5499     assert not b.flags.aligned
5500     out_numpy = 2 * a + 3 * b
5501     av, bv = tensor.scalars('ab')
5502     f = theano.function([av, bv], 2 * av + 3 * bv)
5503     f.maker.fgraph.toposort()
5504     try:
5505         out_theano = f(a, b)
5506         assert not a.flags.aligned
5507         assert not b.flags.aligned
5508         assert np.allclose(out_numpy, out_theano)
5509         assert False
5510     except TypeError:
5511         pass
5512 def test_dimshuffle_duplicate():
5513     x = tensor.vector()
5514     success = False
5515     try:
5516         tensor.DimShuffle((False, ), (0, 0))(x)
5517     except ValueError as e:
5518         assert str(e).find("may not appear twice") != -1
5519         success = True
5520     assert success
5521 class T_get_scalar_constant_value(unittest.TestCase):
5522     def test_get_scalar_constant_value(self):
5523         a = tensor.stack([1, 2, 3])
5524         assert get_scalar_constant_value(a[0]) == 1
5525         assert get_scalar_constant_value(a[1]) == 2
5526         assert get_scalar_constant_value(a[2]) == 3
5527         b = tensor.iscalar()
5528         a = tensor.stack([b, 2, 3])
5529         self.assertRaises(tensor.basic.NotScalarConstantError, get_scalar_constant_value, a[0])
5530         assert get_scalar_constant_value(a[1]) == 2
5531         assert get_scalar_constant_value(a[2]) == 3
5532         v = tensor.ivector()
5533         a = tensor.stack([v, [2], [3]])
5534         self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[0])
5535         self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[1])
5536         self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[2])
5537         v = tensor.row()
5538         assert get_scalar_constant_value(v.shape[0]) == 1
5539     def test_subtensor_of_constant(self):
5540         c = constant(rand(5))
5541         for i in range(c.value.shape[0]):
5542             assert get_scalar_constant_value(c[i]) == c.value[i]
5543         c = constant(rand(5, 5))
5544         for i in range(c.value.shape[0]):
5545             for j in range(c.value.shape[1]):
5546                 assert get_scalar_constant_value(c[i, j]) == c.value[i, j]
5547     def test_numpy_array(self):
5548         assert get_scalar_constant_value(np.array(3)) == 3
5549         self.assertRaises(
5550             tensor.NotScalarConstantError,
5551             get_scalar_constant_value,
5552             np.array([0, 1]))
5553         self.assertRaises(
5554             tensor.EmptyConstantError,
5555             get_scalar_constant_value,
5556             np.array([]))
5557     def test_make_vector(self):
5558         mv = opt.make_vector(1, 2, 3)
5559         self.assertRaises(
5560             tensor.NotScalarConstantError,
5561             get_scalar_constant_value,
5562             mv)
5563         assert get_scalar_constant_value(mv[0]) == 1
5564         assert get_scalar_constant_value(mv[1]) == 2
5565         assert get_scalar_constant_value(mv[2]) == 3
5566         assert get_scalar_constant_value(mv[np.int32(0)]) == 1
5567         assert get_scalar_constant_value(mv[np.int64(1)]) == 2
5568         assert get_scalar_constant_value(mv[np.uint(2)]) == 3
5569         t = theano.scalar.Scalar('int64')
5570         self.assertRaises(
5571             tensor.NotScalarConstantError,
5572             get_scalar_constant_value,
5573             mv[t()])
5574     def test_shape_i(self):
5575         c = theano.tensor.constant(np.random.rand(3, 4))
5576         s = opt.Shape_i(0)(c)
5577         assert get_scalar_constant_value(s) == 3
5578         s = opt.Shape_i(1)(c)
5579         assert get_scalar_constant_value(s) == 4
5580         d = theano.shared(np.random.randn(1, 1), broadcastable=(True, True))
5581         f = theano.tensor.basic.ScalarFromTensor()(opt.Shape_i(0)(d))
5582         assert get_scalar_constant_value(f) == 1
5583     def test_elemwise(self):
5584         c = theano.tensor.constant(np.random.rand())
5585         s = c + 1
5586         assert np.allclose(get_scalar_constant_value(s), c.data + 1)
5587         s = c - 1
5588         assert np.allclose(get_scalar_constant_value(s), c.data - 1)
5589         s = c * 1.2
5590         assert np.allclose(get_scalar_constant_value(s), c.data * 1.2)
5591         s = c &lt; 0.5
5592         assert np.allclose(get_scalar_constant_value(s), int(c.data &lt; 0.5))
5593         s = tensor.second(c, .4)
5594         assert np.allclose(get_scalar_constant_value(s), .4)
5595     def test_assert(self):
5596         c = theano.tensor.constant(2)
5597         x = theano.tensor.scalar()
5598         a = opt.Assert()(c, c &gt; 1)
5599         assert get_scalar_constant_value(a) == 2
5600         with change_flags(compute_test_value='off'):
5601             a = opt.Assert()(c, c &gt; 2)
5602             self.assertRaises(
5603                 tensor.NotScalarConstantError,
5604                 get_scalar_constant_value, a)
5605         a = opt.Assert()(c, c &gt; x)
5606         self.assertRaises(
5607             tensor.NotScalarConstantError,
5608             get_scalar_constant_value, a)
5609     def test_second(self):
5610         c = theano.tensor.constant(np.random.rand())
5611         shp = theano.tensor.vector()
5612         s = theano.tensor.second(shp, c)
5613         assert get_scalar_constant_value(s) == c.data
5614     def test_copy(self):
5615         c = theano.tensor.constant(3)
5616         d = extract_constant(c)
5617         d += 1
5618         e = extract_constant(c)
5619         self.assertTrue(e == 3, (c, d, e))
5620 class T_as_tensor_variable(unittest.TestCase):
5621     def test_bool(self):
5622         self.assertRaises(TypeError, as_tensor_variable, True)
5623         self.assertRaises(TypeError, as_tensor_variable, False)
5624     def test_ndarray_bool(self):
5625         ten = as_tensor_variable(np.array([True, False, False, True, True]))
5626         assert ten.type.dtype == 'bool'
5627     def test_memmap(self):
5628         inp = np.random.rand(4, 3)
5629         f, fname = mkstemp()
5630         new_inp = np.memmap(fname, dtype=inp.dtype,
5631                             mode='w+', shape=inp.shape)
5632         new_inp[...] = inp
5633         as_tensor_variable(new_inp)
5634     def test_empty_dtype(self):
5635         old = theano.config.floatX
5636         for dtype in ['float16', 'float32', 'float64']:
5637             try:
5638                 theano.config.floatX = dtype
5639                 assert theano.tensor.as_tensor_variable(()).dtype == dtype
5640                 assert theano.tensor.as_tensor_variable([]).dtype == dtype
5641             finally:
5642                 theano.config.floatX = old
5643 class test_complex_mod(unittest.TestCase):
5644     def test_fail(self):
5645         x = vector(dtype='complex64')
5646         try:
5647             x % 5
5648             assert False
5649         except theano.scalar.ComplexError:
5650             pass
5651 class test_size(unittest.TestCase):
5652     def test_matrix(self):
5653         x = tensor.matrix()
5654         y = np.zeros((5, 7), dtype=config.floatX)
5655         assert y.size == function([x], x.size)(y)
5656     def test_vector(self):
5657         x = tensor.vector()
5658         y = np.zeros(7, dtype=config.floatX)
5659         assert y.size == function([x], x.size)(y)
5660     def test_scalar(self):
5661         x = tensor.scalar()
5662         y = np.array(7, dtype=config.floatX)
5663         assert y.size == function([x], x.size)(y)
5664     def test_shared(self):
5665         y = np.zeros((1, 2, 3, 4), dtype=config.floatX)
5666         x = theano.shared(y)
5667         assert y.size == function([], x.size)()
5668 class test_diag(unittest.TestCase):
5669     def __init__(self, name, mode=None, shared=tensor._shared,
5670                  floatX=None, type=tensor.TensorType):
5671         self.mode = mode
5672         self.shared = shared
5673         if floatX is None:
5674             floatX = config.floatX
5675         self.floatX = floatX
5676         self.type = type
5677         super(test_diag, self).__init__(name)
5678     def test_diag(self):
5679         rng = np.random.RandomState(utt.fetch_seed())
5680         x = theano.tensor.vector()
5681         g = diag(x)
5682         assert isinstance(g.owner.op, AllocDiag)
5683         f = theano.function([x], g)
5684         for shp in [5, 0, 1]:
5685             m = rng.rand(shp).astype(self.floatX)
5686             v = np.diag(m)
5687             r = f(m)
5688             assert (r == v).all()
5689         xx = self.shared(rng.rand(3, 5))
5690         g = diag(xx)
5691         assert isinstance(g.owner.op, ExtractDiag)
5692         f = theano.function([], g)
5693         for shp in [(5, 3), (3, 5), (5, 1), (1, 5), (5, 0), (0, 5),
5694                     (1, 0), (0, 1)]:
5695             m = rng.rand(*shp).astype(self.floatX)
5696             xx.set_value(m)
5697             v = np.diag(m)
5698             r = f()
5699             assert (r == v).all()
5700         xx = theano.tensor.scalar()
5701         np.testing.assert_raises(ValueError, diag, xx)
5702     def test_infer_shape(self):
5703         rng = np.random.RandomState(utt.fetch_seed())
5704         x = theano.tensor.vector()
5705         g = diag(x)
5706         f = theano.function([x], g.shape)
5707         topo = f.maker.fgraph.toposort()
5708         if config.mode != 'FAST_COMPILE':
5709             assert np.sum(
5710                 [isinstance(node.op, AllocDiag) for node in topo]) == 0
5711         for shp in [5, 0, 1]:
5712             m = rng.rand(shp).astype(self.floatX)
5713             assert (f(m) == np.diag(m).shape).all()
5714         x = theano.tensor.matrix()
5715         g = diag(x)
5716         f = theano.function([x], g.shape)
5717         topo = f.maker.fgraph.toposort()
5718         if config.mode != 'FAST_COMPILE':
5719             assert np.sum(
5720                 [isinstance(node.op, ExtractDiag) for node in topo]) == 0
5721         for shp in [(5, 3), (3, 5), (5, 1), (1, 5), (5, 0), (0, 5),
5722                     (1, 0), (0, 1)]:
5723             m = rng.rand(*shp).astype(self.floatX)
5724             assert (f(m) == np.diag(m).shape).all()
5725     def test_diag_grad(self):
5726         rng = np.random.RandomState(utt.fetch_seed())
5727         x = rng.rand(5)
5728         tensor.verify_grad(diag, [x], rng=rng)
5729         x = rng.rand(5, 3)
5730         tensor.verify_grad(diag, [x], rng=rng)
5731 class TestAllocDiag(unittest.TestCase):
5732     def __init__(self, name, alloc_diag=AllocDiag, mode=None):
5733         self.alloc_diag = alloc_diag
5734         if mode is None:
5735             mode = theano.compile.mode.get_default_mode()
5736         self.mode = mode
5737         super(TestAllocDiag, self).__init__(name)
5738     def _generator(self):
5739         dims = 4
5740         shape = (5,) * dims
5741         xv = np.random.randn(*shape).astype(config.floatX)
5742         for d in xrange(1, dims + 1):
5743             x = TensorType(dtype=config.floatX, broadcastable=(False,) * d)('x')
5744             test_val = xv[((0,) * (dims - d))]
5745             yield x, test_val
5746     def test_alloc_diag_values(self):
5747         for x, test_val in self._generator():
5748             for offset, axis1, axis2 in [(0, 0, 1), (0, 1, 2), (1, 0, 1),
5749                                          (0, 1, 3), (0, 2, 3), (1, 2, 3),
5750                                          (-1, 0, 1), (-2, 0, 1), (-1, 1, 2)]:
5751                 if np.maximum(axis1, axis2) &gt; len(test_val.shape):
5752                     continue
5753                 adiag_op = self.alloc_diag(offset=offset,
5754                                            axis1=axis1,
5755                                            axis2=axis2)
5756                 f = theano.function([x], adiag_op(x))
5757                 diag_arr = f(test_val)
5758                 rediag = np.diagonal(
5759                     diag_arr,
5760                     offset=offset,
5761                     axis1=axis1,
5762                     axis2=axis2
5763                 )
5764                 assert np.all(rediag == test_val)
5765                 f_shape = theano.function([x], adiag_op(x).shape, mode='FAST_RUN')
5766                 theano.printing.debugprint(f_shape.maker.fgraph.outputs[0])
5767                 output_shape = f_shape(test_val)
5768                 assert not any(isinstance(node.op, self.alloc_diag)
5769                                for node in f_shape.maker.fgraph.toposort())
5770                 rediag_shape = np.diagonal(
5771                     np.ones(output_shape),
5772                     offset=offset,
5773                     axis1=axis1,
5774                     axis2=axis2
5775                 ).shape
5776                 assert np.all(rediag_shape == test_val.shape)
5777                 diag_x = adiag_op(x)
5778                 sum_diag_x = tensor.sum(diag_x)
5779                 grad_x = tensor.grad(sum_diag_x, x)
5780                 grad_diag_x = tensor.grad(sum_diag_x, diag_x)
5781                 f_grad_x = theano.function([x], grad_x, mode=self.mode)
5782                 f_grad_diag_x = theano.function([x], grad_diag_x, mode=self.mode)
5783                 grad_input = f_grad_x(test_val)
5784                 grad_diag_input = f_grad_diag_x(test_val)
5785                 true_grad_input = np.diagonal(
5786                     grad_diag_input,
5787                     offset=offset,
5788                     axis1=axis1,
5789                     axis2=axis2
5790                 )
5791                 assert np.all(true_grad_input == grad_input)
5792 class test_numpy_assumptions(unittest.TestCase):
5793     def test_ndarray_copy(self):
5794         assert copy(np.ndarray) is np.ndarray
5795         assert deepcopy(np.ndarray) is np.ndarray
5796     def test_dtype_equality(self):
5797         dtypes = get_numeric_types(with_complex=True)
5798         for dtype1_idx, dtype1 in enumerate(dtypes):
5799             for dtype2 in dtypes[dtype1_idx + 1:]:
5800                 assert (dtype1 == dtype2) == (str(dtype1) == str(dtype2))
5801 def test_transpose():
5802     x1 = tensor.dvector('x1')
5803     x2 = tensor.dmatrix('x2')
5804     x3 = tensor.dtensor3('x3')
5805     x1v = np.arange(24)
5806     x2v = np.arange(24).reshape(2, 12)
5807     x3v = np.arange(24).reshape(2, 3, 4)
5808     f = theano.function([x1, x2, x3], [
5809         tensor.transpose(x1),
5810         tensor.transpose(x2),
5811         tensor.transpose(x3),
5812         x1.transpose(),
5813         x2.transpose(),
5814         x3.transpose(),
5815         x2.transpose(0, 1),
5816         x3.transpose((0, 2, 1)),
5817         tensor.transpose(x2, [0, 1]),
5818         tensor.transpose(x3, [0, 2, 1]),
5819         ])
5820     t1, t2, t3, t1b, t2b, t3b, t2c, t3c, t2d, t3d = f(x1v, x2v, x3v)
5821     assert t1.shape == np.transpose(x1v).shape
5822     assert t2.shape == np.transpose(x2v).shape
5823     assert t3.shape == np.transpose(x3v).shape
5824     assert np.all(t1 == np.transpose(x1v))
5825     assert np.all(t2 == np.transpose(x2v))
5826     assert np.all(t3 == np.transpose(x3v))
5827     assert np.all(t1b == x1v.transpose())
5828     assert np.all(t2b == x2v.transpose())
5829     assert np.all(t3b == x3v.transpose())
5830     assert t2c.shape == (2, 12)
5831     assert t3c.shape == (2, 4, 3)
5832     assert np.all(t2c == x2v.transpose([0, 1]))
5833     assert np.all(t3c == x3v.transpose([0, 2, 1]))
5834     assert t2d.shape == (2, 12)
5835     assert t3d.shape == (2, 4, 3)
5836     assert np.all(t2d == np.transpose(x2v, [0, 1]))
5837     assert np.all(t3d == np.transpose(x3v, [0, 2, 1]))
5838     assert tensor.transpose(x1).name == 'x1.T'
5839     assert tensor.transpose(x2).name == 'x2.T'
5840     assert tensor.transpose(x3).name == 'x3.T'
5841     assert tensor.transpose(tensor.dmatrix()).name is None
5842 def test_stacklists():
5843     a, b, c, d = map(scalar, 'abcd')
5844     X = stacklists([[a, b],
5845                     [c, d]])
5846     f = function([a, b, c, d], X)
5847     result = f(1, 2, 3, 4)
5848     assert result.shape == (2, 2)
5849     assert np.allclose(f(1, 2, 3, 4), np.asarray([[1, 2], [3, 4]]))
5850     X = stacklists([a, b, c, d])
5851     f = function([a, b, c, d], X)
5852     result = f(1, 2, 3, 4)
5853     assert result.shape == (4,)
5854     assert np.allclose(f(1, 2, 3, 4), np.asarray([[1, 2, 3, 4]]))
5855     X = stacklists([[[a], [b]], [[c], [d]]])
5856     f = function([a, b, c, d], X)
5857     result = f(1, 2, 3, 4)
5858     assert result.shape == (2, 2, 1)
5859     a, b, c, d = [matrix(x) for x in 'abcd']
5860     X = stacklists([[a, b],
5861                     [c, d]])
5862     f = function([a, b, c, d], X)
5863     x = np.ones((4, 4), 'float32')
5864     assert f(x, x, x, x).shape == (2, 2, 4, 4)
5865 class TestSpecifyShape(unittest.TestCase):
5866     mode = None
5867     input_type = TensorType
5868     def shortDescription(self):
5869         return None
5870     def test_bad_shape(self):
5871         specify_shape = SpecifyShape()
5872         x = vector()
5873         xval = np.random.rand(2).astype(floatX)
5874         f = theano.function([x], specify_shape(x, [2]), mode=self.mode)
5875         f(xval)
5876         xval = np.random.rand(3).astype(floatX)
5877         self.assertRaises(AssertionError, f, xval)
5878         assert isinstance([n for n in f.maker.fgraph.toposort()
5879                            if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5880                           self.input_type)
5881         x = matrix()
5882         xval = np.random.rand(2, 3).astype(floatX)
5883         f = theano.function([x], specify_shape(x, [2, 3]), mode=self.mode)
5884         assert isinstance([n for n in f.maker.fgraph.toposort()
5885                            if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5886                           self.input_type)
5887         f(xval)
5888         for shape_ in [(1, 3), (2, 2), (5, 5)]:
5889             xval = np.random.rand(*shape_).astype(floatX)
5890             self.assertRaises(AssertionError, f, xval)
5891     def test_bad_number_of_shape(self):
5892         specify_shape = SpecifyShape()
5893         x = vector()
5894         shape_vec = ivector()
5895         xval = np.random.rand(2).astype(floatX)
5896         self.assertRaises(AssertionError, specify_shape, x, [])
5897         self.assertRaises(AssertionError, specify_shape, x, [2, 2])
5898         f = theano.function([x, shape_vec], specify_shape(x, shape_vec),
5899                             mode=self.mode)
5900         assert isinstance([n for n in f.maker.fgraph.toposort()
5901                            if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5902                           self.input_type)
5903         self.assertRaises(AssertionError, f, xval, [])
5904         self.assertRaises(AssertionError, f, xval, [2, 2])
5905         x = matrix()
5906         xval = np.random.rand(2, 3).astype(floatX)
5907         for shape_ in [(),
5908                        (1,),
5909                        (2, 3, 4)]:
5910             self.assertRaises(AssertionError, specify_shape, x, shape_)
5911             f = theano.function([x, shape_vec], specify_shape(x, shape_vec),
5912                                 mode=self.mode)
5913             assert isinstance([n for n in f.maker.fgraph.toposort()
5914                                if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5915                               self.input_type)
5916             self.assertRaises(AssertionError, f, xval, shape_)
5917 class TestInferShape(utt.InferShapeTester):
5918     def test_infer_shape(self):
5919         atens3 = tensor3()
5920         atens3_val = rand(4, 5, 3)
5921         for outdim in (3, 2, 1):
5922             self._compile_and_check([atens3],
5923                                     [flatten(atens3, outdim)],
5924                                     [atens3_val], Reshape,
5925                                     excluding=['local_useless_reshape'])
5926         amat = matrix()
5927         amat_val = rand(4, 5)
5928         for outdim in (2, 1):
5929             self._compile_and_check([amat],
5930                                     [flatten(amat, outdim)],
5931                                     [amat_val], Reshape,
5932                                     excluding=['local_useless_reshape'])
5933         avec = vector()
5934         avec_val = rand(4)
5935         outdim = 1
5936         self._compile_and_check([avec],
5937                                 [flatten(avec, outdim)],
5938                                 [avec_val], Reshape,
5939                                 excluding=['local_useless_reshape'])
5940         aiscal = iscalar()
5941         biscal = iscalar()
5942         ciscal = iscalar()
5943         self._compile_and_check([aiscal, biscal, ciscal],
5944                                 [Eye()(aiscal, biscal, ciscal)],
5945                                 [4, 4, 0], Eye)
5946         self._compile_and_check([aiscal, biscal, ciscal],
5947                                 [Eye()(aiscal, biscal, ciscal)],
5948                                 [4, 5, 0], Eye)
5949         self._compile_and_check([aiscal, biscal, ciscal],
5950                                 [Eye()(aiscal, biscal, ciscal)],
5951                                 [3, 5, 0], Eye)
5952         aiscal = iscalar()
5953         biscal = iscalar()
5954         ciscal = iscalar()
5955         self._compile_and_check([aiscal, biscal, ciscal],
5956                                 [Tri()(aiscal, biscal, ciscal)],
5957                                 [4, 4, 0], Tri)
5958         self._compile_and_check([aiscal, biscal, ciscal],
5959                                 [Tri()(aiscal, biscal, ciscal)],
5960                                 [4, 5, 0], Tri)
5961         self._compile_and_check([aiscal, biscal, ciscal],
5962                                 [Tri()(aiscal, biscal, ciscal)],
5963                                 [3, 5, 0], Tri)
5964         atens3 = tensor3()
5965         atens3_val = rand(4, 5, 3)
5966         atens3_diag = ExtractDiag()(atens3)
5967         self._compile_and_check([atens3], [atens3_diag],
5968                                 [atens3_val], ExtractDiag)
5969         atens3_diag = ExtractDiag(1)(atens3)
5970         self._compile_and_check([atens3], [atens3_diag],
5971                                 [atens3_val], ExtractDiag)
5972         atens3_diag = ExtractDiag(-1)(atens3)
5973         self._compile_and_check([atens3], [atens3_diag],
5974                                 [atens3_val], ExtractDiag)
5975         atens3_diag = ExtractDiag(1, 0, 2)(atens3)
5976         self._compile_and_check([atens3], [atens3_diag],
5977                                 [atens3_val], ExtractDiag)
5978         atens3_diag = ExtractDiag(1, 1, 2)(atens3)
5979         self._compile_and_check([atens3], [atens3_diag],
5980                                 [atens3_val], ExtractDiag)
5981         atens3_diag = ExtractDiag(1, 2, 0)(atens3)
5982         self._compile_and_check([atens3], [atens3_diag],
5983                                 [atens3_val], ExtractDiag)
5984         advec = dvector()
5985         advec_val = rand(4)
5986         self._compile_and_check([advec], [AllocDiag()(advec)],
5987                                 [advec_val], AllocDiag)
5988         adtens = tensor3()
5989         adtens_val = rand(4, 5, 3)
5990         self._compile_and_check([adtens],
5991                                 [Shape()(adtens)],
5992                                 [adtens_val], (opt.MakeVector, Shape))
5993         advec = dvector()
5994         bdvec = dvector()
5995         advec_val = rand(4)
5996         bdvec_val = rand(4)
5997         self._compile_and_check([advec, bdvec],
5998                                 [advec_val, bdvec_val],
5999                                 (Dot, tensor.blas.Dot22,
6000                                  tensor.blas<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Gemv, tensor.blas_c.CGemv))
6001         admat = dmatrix()
6002         bdmat = dmatrix()
6003         admat_val = rand(4, 5)
6004         bdmat_val = rand(5, 3)
6005         self._compile_and_check(</b></font>[admat, bdmat],
6006                                 [Dot()(admat, bdmat)],
6007                                 [admat_val, bdmat_val],
6008                                 (Dot, tensor.blas.Dot22))
6009         bdmat_val = rand(4, 5)
6010         self._compile_and_check([advec, bdmat],
6011                                 [Dot()(advec, bdmat)],
6012                                 [advec_val, bdmat_val],
6013                                 (Dot, tensor.blas.Dot22,
6014                                  tensor.blas.Gemv, tensor.blas_c.CGemv))
6015         admat_val = rand(5, 4)
6016         self._compile_and_check([admat, bdvec],
6017                                 [Dot()(admat, bdvec)],
6018                                 [admat_val, bdvec_val],
6019                                 (Dot, tensor.blas.Dot22,
6020                                  tensor.blas.Gemv, tensor.blas_c.CGemv))
6021         aivec = ivector()
6022         adtens_val = rand(4, 10, 3)
6023         aivec_val = [2, 5, 3]
6024         for aiscal_val in [1, -2]:
6025             self._compile_and_check(
6026                 [adtens, aiscal, aivec],
6027                 [Split(3)(adtens, aiscal, aivec)[0]],
6028                 [adtens_val, aiscal_val, aivec_val], (Split))
6029         cdmat = dmatrix()
6030         admat_val = rand(1, 3)
6031         bdmat_val = rand(2, 3)
6032         cdmat_val = rand(4, 3)
6033         for aiscal_val in [0, -2]:
6034             self._compile_and_check(
6035                 [aiscal, admat, bdmat, cdmat],
6036                 [Join()(aiscal, admat, bdmat, cdmat)],
6037                 [aiscal_val, admat_val, bdmat_val, cdmat_val], Join)
6038         admat_val = rand(4, 1)
6039         bdmat_val = rand(4, 3)
6040         cdmat_val = rand(4, 2)
6041         for aiscal_val in [-1, 1]:
6042             self._compile_and_check(
6043                 [aiscal, admat, bdmat, cdmat],
6044                 [Join()(aiscal, admat, bdmat, cdmat)],
6045                 [aiscal_val, admat_val, bdmat_val, cdmat_val], Join)
6046         abool = True
6047         rng = np.random.RandomState(utt.fetch_seed())
6048         advec_val = rand(5)
6049         aivec_val = rng.permutation(5).astype('int32')
6050         self._compile_and_check([advec, aivec],
6051                                 [PermuteRowElements()(advec, aivec, abool)],
6052                                 [advec_val, aivec_val], PermuteRowElements)
6053         admat_val = rand(3, 5)
6054         self._compile_and_check([admat, aivec],
6055                                 [PermuteRowElements()(admat, aivec, abool)],
6056                                 [admat_val, aivec_val], PermuteRowElements)
6057         adtens3 = dtensor3()
6058         adtens3_val = rand(3, 2, 5)
6059         self._compile_and_check([adtens3, aivec],
6060                                 [PermuteRowElements()(adtens3, aivec, abool)],
6061                                 [adtens3_val, aivec_val], PermuteRowElements)
6062         aimat = imatrix()
6063         perma = rng.permutation(5).astype('int32')
6064         permb = rng.permutation(5).astype('int32')
6065         permc = rng.permutation(5).astype('int32')
6066         aimat_val = np.vstack((perma, permb, permc))
6067         admat_val = rand(3, 5)
6068         self._compile_and_check([admat, aimat],
6069                                 [PermuteRowElements()(admat, aimat, abool)],
6070                                 [admat_val, aimat_val], PermuteRowElements)
6071         aitens3 = itensor3()
6072         perma = rng.permutation(5).astype('int32')
6073         permb = rng.permutation(5).astype('int32')
6074         permc = rng.permutation(5).astype('int32')
6075         bimat_val = np.vstack((perma, permb, permc))
6076         aitens3_val = np.empty((2, 3, 5), 'int32')
6077         aitens3_val[0, ::, ::] = aimat_val
6078         aitens3_val[1, ::, ::] = bimat_val
6079         self._compile_and_check([admat, aitens3],
6080                                 [PermuteRowElements()(admat, aitens3, abool)],
6081                                 [admat_val, aitens3_val], PermuteRowElements)
6082         aiscal = iscalar()
6083         self._compile_and_check([aiscal],
6084                                 [TensorFromScalar()(ScalarFromTensor()(aiscal))],
6085                                 [45], ScalarFromTensor,
6086                                 excluding=["local_tensor_scalar_tensor"])
6087         aiscal = scal.float64()
6088         self._compile_and_check([aiscal],
6089                                 [TensorFromScalar()(aiscal)],
6090                                 [4.], TensorFromScalar)
6091         adtens4 = dtensor4()
6092         adict = [(0, False), (1, True), (2, False), (3, True)]
6093         adtens4_val = rand(2, 1, 3, 1)
6094         self._compile_and_check([adtens4],
6095                                 [Rebroadcast(*adict)(adtens4)],
6096                                 [adtens4_val], Rebroadcast,
6097                                 warn=False)
6098         adtens4_bro = TensorType('float64', (True, True, True, False))()
6099         bdict = [(0, True), (1, False), (2, False), (3, False)]
6100         adtens4_bro_val = rand(1, 1, 1, 3)
6101         self._compile_and_check([adtens4_bro],
6102                                 [Rebroadcast(*bdict)(adtens4_bro)],
6103                                 [adtens4_bro_val], Rebroadcast)
6104         randint = np.random.randint
6105         adscal <font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= dscalar()
6106         aiscal = lscalar()
6107         biscal = lscalar()
6108         ciscal = lscalar()
6109         discal = lscalar()
6110         adscal_val = rand()
6111         aiscal_val = randint(</b></font>3, 6, size=())
6112         biscal_val = randint(3, 6, size=())
6113         ciscal_val = randint(3, 6, size=())
6114         discal_val = randint(3, 6, size=())
6115         self._compile_and_check(
6116             [adscal, aiscal, biscal, ciscal, discal],
6117             [Alloc()(adscal, aiscal, biscal, ciscal, discal)],
6118             [adscal_val, aiscal_val, biscal_val, ciscal_val, discal_val],
6119             Alloc)
6120         adtens3_val = rand(4, 5, 3)
6121         self._compile_and_check([adtens3],
6122                                 max_and_argmax(adtens3, None),
6123                                 [adtens3_val], MaxAndArgmax)
6124         self._compile_and_check([adtens3],
6125                                 max_and_argmax(adtens3, 0),
6126                                 [adtens3_val], MaxAndArgmax)
6127         self._compile_and_check([adtens3],
6128                                 max_and_argmax(adtens3, 1),
6129                                 [adtens3_val], MaxAndArgmax)
6130         self._compile_and_check([adtens3],
6131                                 max_and_argmax(adtens3, 2),
6132                                 [adtens3_val], MaxAndArgmax)
6133         self._compile_and_check([adtens3],
6134                                 max_and_argmax(adtens3, [0, 1, 2]),
6135                                 [adtens3_val], MaxAndArgmax)
6136         self._compile_and_check([aiscal, biscal, ciscal],
6137                                 [ARange('int64')(aiscal, biscal, ciscal)],
6138                                 [0, 5, 1], ARange)
6139         self._compile_and_check([aiscal, biscal, ciscal],
6140                                 [ARange('int64')(aiscal, biscal, ciscal)],
6141                                 [2, 11, 4], ARange)
6142         self._compile_and_check([aiscal, biscal, ciscal],
6143                                 [ARange('int64')(aiscal, biscal, ciscal)],
6144                                 [-5, 1, 1], ARange)
6145         self._compile_and_check([aiscal, biscal, ciscal],
6146                                 [ARange('int64')(aiscal, biscal, ciscal)],
6147                                 [10, 2, -2], ARange)
6148         self._compile_and_check([aiscal, biscal, ciscal],
6149                                 [ARange('int64')(aiscal, biscal, ciscal)],
6150                                 [10, 2, 2], ARange)
6151         self._compile_and_check([aiscal, biscal, ciscal],
6152                                 [ARange('int64')(aiscal, biscal, ciscal)],
6153                                 [0, 0, 1], ARange)
6154         aivec_val = [3, 4, 2, 5]
6155         adtens4_val = rand(*aivec_val)
6156         self._compile_and_check([adtens4, aivec],
6157                                 [SpecifyShape()(adtens4, aivec)],
6158                                 [adtens4_val, aivec_val], SpecifyShape)
6159         adtens3_val = rand(3, 4, 5)
6160         aiscal_val = 2
6161         self._compile_and_check([adtens3],
6162                                 [Mean(None)(adtens3)],
6163                                 [adtens3_val], Mean)
6164         self._compile_and_check([adtens3],
6165                                 [Mean(aiscal_val)(adtens3)],
6166                                 [adtens3_val], Mean)
6167         admat = dmatrix()
6168         aivec = ivector()
6169         ndim = 1
6170         admat_val = rand(3, 4)
6171         self._compile_and_check([admat],
6172                                 [Reshape(ndim)(admat, [12])],
6173                                 [admat_val], Reshape)
6174         self._compile_and_check([admat],
6175                                 [Reshape(ndim)(admat, [-1])],
6176                                 [admat_val], Reshape)
6177         ndim = 2
6178         self._compile_and_check([admat],
6179                                 [Reshape(ndim)(admat, [4, 3])],
6180                                 [admat_val], Reshape)
6181         self._compile_and_check([admat],
6182                                 [Reshape(ndim)(admat, [4, -1])],
6183                                 [admat_val], Reshape)
6184         self._compile_and_check([admat],
6185                                 [Reshape(ndim)(admat, [3, -1])],
6186                                 [admat_val], Reshape)
6187         self._compile_and_check([admat],
6188                                 [Reshape(ndim)(admat, [-1, 3])],
6189                                 [admat_val], Reshape)
6190         self._compile_and_check([admat],
6191                                 [Reshape(ndim)(admat, [-1, 4])],
6192                                 [admat_val], Reshape)
6193         adtens4 = dtensor4()
6194         ndim = 4
6195         adtens4_val = rand(2, 4, 3, 5)
6196         self._compile_and_check([adtens4],
6197                                 [Reshape(ndim)(adtens4, [1, -1, 10, 4])],
6198                                 [adtens4_val], Reshape)
6199         self._compile_and_check([adtens4],
6200                                 [Reshape(ndim)(adtens4, [1, 3, 10, 4])],
6201                                 [adtens4_val], Reshape)
6202         advec = dvector()
6203         advec_val = rand(5)
6204         aivec_val = [3]
6205         ndim = 1
6206         self._compile_and_check([advec],
6207                                 [Tile(ndim)(advec, aivec_val)],
6208                                 [advec_val], Tile)
6209         admat = dmatrix()
6210         admat_val = rand(2, 4)
6211         aivec_val = [2, 3]
6212         ndim = 2
6213         self._compile_and_check([admat],
6214                                 [Tile(ndim)(admat, aivec_val)],
6215                                 [admat_val], Tile)
6216         adtens4 = dtensor4()
6217         adtens4_val = rand(2, 4, 3, 5)
6218         aivec_val = [2, 3, 1, 4]
6219         ndim = 4
6220         self._compile_and_check([adtens4],
6221                                 [Tile(ndim)(adtens4, aivec_val)],
6222                                 [adtens4_val], Tile)
6223 class TestTensorInstanceMethods(unittest.TestCase):
6224     def setUp(self):
6225         self.vars = matrices('X', 'Y')
6226         self.vals = [m.astype(floatX) for m in [rand(2, 2), rand(2, 2)]]
6227     def test_argmin(self):
6228         X, _ = self.vars
6229         x, _ = self.vals
6230         assert_array_equal(X.argmin().eval({X: x}), x.argmin())
6231     def test_argmax(self):
6232         X, _ = self.vars
6233         x, _ = self.vals
6234         assert_array_equal(X.argmax().eval({X: x}), x.argmax())
6235     def test_argsort(self):
6236         X, _ = self.vars
6237         x, _ = self.vals
6238         assert_array_equal(X.argsort().eval({X: x}), x.argsort())
6239         assert_array_equal(X.argsort(1).eval({X: x}), x.argsort(1))
6240     def test_clip(self):
6241         X, Y = self.vars
6242         x, y = self.vals
6243         Z = X.clip(Y - 0.5, Y + 0.5)
6244         z = x.clip(y - 0.5, y + 0.5)
6245         assert_array_equal(Z.eval({X: x, Y: y}), z)
6246     def test_dot(self):
6247         X, Y = self.vars
6248         x, y = self.vals
6249         assert_allclose(x.dot(y), X.dot(Y).eval({X: x, Y: y}))
6250         Z = X.dot(Y)
6251         z = x.dot(y)
6252         assert_allclose(x.dot(z), X.dot(Z).eval({X: x, Z: z}))
6253     def test_real_imag(self):
6254         X, Y = self.vars
6255         x, y = self.vals
6256         Z = X + Y * 1j
6257         z = x + y * 1j
6258         assert_array_equal(Z.real.eval({Z: z}), x)
6259         assert_array_equal(Z.imag.eval({Z: z}), y)
6260     def test_conj(self):
6261         X, Y = self.vars
6262         x, y = self.vals
6263         Z = X + Y * 1j
6264         z = x + y * 1j
6265         assert_array_equal(Z.conj().eval({Z: z}), z.conj())
6266         assert_array_equal(Z.conjugate().eval({Z: z}), z.conj())
6267     def test_round(self):
6268         X, _ = self.vars
6269         x, _ = self.vals
6270         assert_array_equal(X.round().eval({X: x}), x.round())
6271     def test_std(self):
6272         X, _ = self.vars
6273         x, _ = self.vals
6274         assert_allclose(X.std().eval({X: x}), x.std())
6275     def test_repeat(self):
6276         X, _ = self.vars
6277         x, _ = self.vals
6278         assert_array_equal(X.repeat(2).eval({X: x}), x.repeat(2))
6279     def test_trace(self):
6280         X, _ = self.vars
6281         x, _ = self.vals
6282         assert_array_equal(X.trace().eval({X: x}), x.trace())
6283     def test_ravel(self):
6284         X, _ = self.vars
6285         x, _ = self.vals
6286         assert_array_equal(X.ravel().eval({X: x}), x.ravel())
6287     def test_diagonal(self):
6288         X, _ = self.vars
6289         x, _ = self.vals
6290         assert_array_equal(X.diagonal().eval({X: x}), x.diagonal())
6291         assert_array_equal(X.diagonal(1).eval({X: x}), x.diagonal(1))
6292         assert_array_equal(X.diagonal(-1).eval({X: x}), x.diagonal(-1))
6293         for offset, axis1, axis2 in [(1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0)]:
6294             assert_array_equal(X.diagonal(offset, axis1, axis2).eval({X: x}),
6295                                x.diagonal(offset, axis1, axis2))
6296     def test_take(self):
6297         X, _ = self.vars
6298         x, _ = self.vals
6299         indices = [1, 0, 3]
6300         assert_array_equal(X.take(indices).eval({X: x}), x.take(indices))
6301         indices = [1, 0, 1]
6302         assert_array_equal(X.take(indices, 1).eval({X: x}), x.take(indices, 1))
6303         indices = np.array([-10, 5, 12], dtype='int32')
6304         assert_array_equal(X.take(indices, 1, mode='wrap').eval({X: x}),
6305                            x.take(indices, 1, mode='wrap'))
6306         assert_array_equal(X.take(indices, -1, mode='wrap').eval({X: x}),
6307                            x.take(indices, -1, mode='wrap'))
6308         assert_array_equal(X.take(indices, 1, mode='clip').eval({X: x}),
6309                            x.take(indices, 1, mode='clip'))
6310         assert_array_equal(X.take(indices, -1, mode='clip').eval({X: x}),
6311                            x.take(indices, -1, mode='clip'))
6312         self.assertRaises(IndexError, X.take(indices).eval, {X: x})
6313         self.assertRaises(IndexError, (2 * X.take(indices)).eval, {X: x})
6314         self.assertRaises(TypeError, X.take, [0.0])
6315         indices = [[1, 0, 1], [0, 1, 1]]
6316         assert_array_equal(X.take(indices, 1).eval({X: x}), x.take(indices, 1))
6317         assert_array_equal(X[:, indices].eval({X: x}), x[:, indices])
6318     def test_cumsum(self):
6319         X, _ = self.vars
6320         x, _ = self.vals
6321         assert_array_equal(X.cumsum().eval({X: x}), x.cumsum())
6322     def test_cumprod(self):
6323         X, _ = self.vars
6324         x, _ = self.vals
6325         assert_array_equal(X.cumprod().eval({X: x}), x.cumprod())
6326 def test_norm():
6327     x = theano.tensor.vector('x')
6328     n = x.norm(2)
6329     f = theano.function([x], n)
6330     assert np.allclose(f([1, 1]), np.sqrt(2))
6331 class test_cov(unittest.TestCase):
6332     def test_core(self):
6333         x = theano.tensor.matrix('x')
6334         c = theano.tensor.cov(x)
6335         f = theano.function([x], c)
6336         data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6337         assert np.allclose(f(data), np.cov(data))
6338         data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6339         assert np.allclose(f(data), np.cov(data))
6340         data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6341         assert np.allclose(f(data), np.cov(data))
6342         data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6343         assert np.allclose(f(data), np.cov(data))
6344         data = np.asarray(np.random.rand(1, 2), dtype=config.floatX)
6345         assert np.allclose(f(data), np.cov(data))
6346     def test_rowvar(self):
6347         for rowvar in [True, False]:
6348             x = theano.tensor.matrix('x')
6349             c = theano.tensor.cov(x, rowvar=rowvar)
6350             f = theano.function([x], c)
6351             data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6352             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6353             data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6354             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6355             data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6356             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6357             data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6358             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6359         x = theano.tensor.matrix('x')
6360         c = theano.tensor.cov(x, rowvar=False)
6361         f = theano.function([x], c)
6362         data = np.asarray(np.random.rand(2, 1), dtype=config.floatX)
6363         assert np.allclose(f(data), np.cov(data, rowvar=False))
6364     def test_y(self):
6365         x <font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.matrix('x')
6366         y = theano.tensor.matrix('y')
6367         c = theano.tensor.cov(x, y=</b></font>y)
6368         f = theano.function([x, y], c)
6369         data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6370         y = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6371         assert np.allclose(f(data, y), np.cov(data, y=y))
6372         data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6373         y = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6374         assert np.allclose(f(data, y), np.cov(data, y=y))
6375         data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6376         y = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6377         assert np.allclose(f(data, y), np.cov(data, y=y))
6378         data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6379         y = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6380         assert np.allclose(f(data, y), np.cov(data, y=y))
6381     def test_ddof(self):
6382         for ddof in range(0, 5):
6383             x = theano.tensor.matrix('x')
6384             c = theano.tensor.cov(x, ddof=ddof)
6385             f = theano.function([x], c)
6386             data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6387             assert np.allclose(f(data), np.cov(data, ddof=ddof))
6388     def test_bias(self):
6389         for bias in [True, False]:
6390             x = theano.tensor.matrix('x')
6391             c = theano.tensor.cov(x, bias=bias)
6392             f = theano.function([x], c)
6393             data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6394             assert np.allclose(f(data), np.cov(data, bias=bias))
6395         for ddof in range(0, 5):
6396             for bias in [True, False]:
6397                 x = theano.tensor.matrix('x')
6398                 c = theano.tensor.cov(x, ddof=ddof, bias=bias)
6399                 f = theano.function([x], c)
6400                 data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6401                 assert np.allclose(f(data), np.cov(data, ddof=ddof, bias=bias))
6402 class test_ptp(unittest.TestCase):
6403     def test_scalar(self):
6404         x = scalar('x')
6405         p = ptp(x)
6406         f = theano.function([x], p)
6407         y = np.asarray(rand() * 2000 - 1000, dtype=config.floatX)
6408         result = f(y)
6409         numpyResult = np.ptp(y)
6410         self.assertTrue(np.array_equal(result, numpyResult))
6411     def test_vector(self):
6412         x = vector('x')
6413         p = ptp(x, 0)
6414         f = theano.function([x], p)
6415         y = rand_ranged(-1000, 1000, [100])
6416         result = f(y)
6417         numpyResult = np.ptp(y, 0)
6418         self.assertTrue(np.array_equal(result, numpyResult))
6419     def test_matrix_first_axis(self):
6420         x = matrix('x')
6421         p = ptp(x, 1)
6422         f = theano.function([x], p)
6423         y = rand_ranged(-1000, 1000, [100, 100])
6424         result = f(y)
6425         numpyResult = np.ptp(y, 1)
6426         self.assertTrue(np.array_equal(result, numpyResult))
6427     def test_matrix_second_axis(self):
6428         x = matrix('x')
6429         p = ptp(x, 0)
6430         f = theano.function([x], p)
6431         y = rand_ranged(-1000, 1000, [100, 100])
6432         result = f(y)
6433         numpyResult = np.ptp(y, 0)
6434         self.assertTrue(np.array_equal(result, numpyResult))
6435     def test_matrix_neg_axis(self):
6436         x = matrix('x')
6437         p = ptp(x, -1)
6438         f = theano.function([x], p)
6439         y = rand_ranged(-1000, 1000, [100, 100])
6440         result = f(y)
6441         numpyResult = np.ptp(y, -1)
6442         self.assertTrue(np.array_equal(result, numpyResult))
6443     def test_matrix_no_axis(self):
6444         x = matrix('x')
6445         p = ptp(x)
6446         f = theano.function([x], p)
6447         y = rand_ranged(-1000, 1000, [100, 100])
6448         result = f(y)
6449         numpyResult = np.ptp(y)
6450         self.assertTrue(np.array_equal(result, numpyResult))
6451     def test_interface(self):
6452         x = matrix('x')
6453         p = x.ptp(1)
6454         f = theano.function([x], p)
6455         y = rand_ranged(-1000, 1000, [100, 100])
6456         result = f(y)
6457         numpyResult = np.ptp(y, 1)
6458         self.assertTrue(np.array_equal(result, numpyResult))
6459 if __name__ == '__main__':
6460     t = TestInferShape('setUp')
6461     t.setUp()
6462     t.test_infer_shape()
6463 class T_swapaxes(unittest.TestCase):
6464     def test_no_dimensional_input(self):
6465         self.assertRaises(IndexError, swapaxes, 2, 0, 1)
6466     def test_unidimensional_input(self):
6467         self.assertRaises(IndexError, swapaxes, [2, 1], 0, 1)
6468     def test_not_enough_dimension(self):
6469         self.assertRaises(IndexError, swapaxes, [[2, 1], [3, 4]], 3, 4)
6470     def test_doubleswap(self):
6471         y = matrix()
6472         n = swapaxes(y, 0, 1)
6473         f = function([y], n)
6474         testMatrix = [[2, 1], [3, 4]]
6475         self.assertTrue(np.array_equal(testMatrix, f(f(testMatrix))))
6476     def test_interface(self):
6477         x = theano.tensor.matrix()
6478         x.swapaxes(0, 1)
6479     def test_numpy_compare(self):
6480         rng = np.random.RandomState(utt.fetch_seed())
6481         A = tensor.matrix("A", dtype=theano.config.floatX)
6482         Q = swapaxes(A, 0, 1)
6483         fn = function([A], [Q])
6484         a = rng.rand(4, 4).astype(theano.config.floatX)
6485         n_s = np.swapaxes(a, 0, 1)
6486         t_s = fn(a)
6487         assert np.allclose(n_s, t_s)
6488 class T_Power(unittest.TestCase):
6489     def test_numpy_compare(self):
6490         rng = np.random.RandomState(utt.fetch_seed())
6491         A = tensor.matrix("A", dtype=theano.config.floatX)
6492         Q = power(A, 3)
6493         fn = function([A], [Q])
6494         a = rng.rand(4, 4).astype(theano.config.floatX)
6495         n_p = np.power(a, 3)
6496         t_p = fn(a)
6497         assert np.allclose(n_p, t_p)
6498     def test_multiple_power(self):
6499         x = tensor.vector()
6500         y = [1, 2, 3]
6501         z = power(x, y)
6502         f = function([x], z)
6503         assert np.allclose(f([1, 2, 3]), [1, 4, 27])
6504     def test_wrong_shape(self):
6505         x = tensor.vector()
6506         y = [1, 2, 3]
6507         z = power(x, y)
6508         f = function([x], z)
6509         self.assertRaises(ValueError, f, [1, 2, 3, 4])
6510 class T_Choose(utt.InferShapeTester):
6511     op = staticmethod(choose)
6512     op_class = Choose
6513     modes = ['raise', 'wrap', 'clip']
6514     def test_numpy_compare(self):
6515         a = tensor.vector(dtype='int32')
6516         b = tensor.matrix(dtype='float32')
6517         A = np.random.randint(0, 4, 4).astype('int32')
6518         B = np.asarray(np.random.rand(4, 4), dtype='float32')
6519         for m in self.modes:
6520             f = function([a, b], choose(a, b, mode=m))
6521             t_c = f(A, B)
6522             n_c = np.choose(A, B, mode=m)
6523             assert np.allclose(t_c, n_c)
6524     def test_method(self):
6525         a = tensor.vector(dtype='int32')
6526         b = tensor.matrix(dtype='float32')
6527         A = np.random.randint(0, 4, 4).astype('int32')
6528         B = np.asarray(np.random.rand(4, 4), dtype='float32')
6529         for m in self.modes:
6530             f = function([a, b], a.choose(b, mode=m))
6531             t_c = f(A, B)
6532             n_c = A.choose(B, mode=m)
6533             assert np.allclose(t_c, n_c)
6534     def test_broadcasted(self):
6535         a = tensor.scalar(dtype='int32')
6536         b = tensor.matrix(dtype='float32')
6537         A = 3
6538         B = np.asarray(np.random.rand(4, 4), dtype='float32')
6539         for m in self.modes:
6540             f = function([a, b], choose(a, b, mode=m))
6541             t_c = f(A, B)
6542             n_c = np.choose(A, B, mode=m)
6543             assert np.allclose(t_c, n_c)
6544         b = theano.tensor.col(dtype='float32')
6545         B = np.asarray(np.random.rand(4, 1), dtype='float32')
6546         for m in self.modes:
6547             f = function([a, b], choose(a, b, mode=m))
6548             assert choose(a, b, mode=m).broadcastable[0]
6549             t_c = f(A, B)
6550             n_c = np.choose(A, B, mode=m)
6551             assert np.allclose(t_c, n_c)
6552     def test_dtype_error(self):
6553         a = tensor.scalar(dtype='float32')
6554         b = tensor.matrix(dtype='float32')
6555         self.assertRaises(TypeError, choose, a, b)
6556     def test_numpy_compare_tuple(self):
6557         a = tensor.tensor3(dtype='int32')
6558         b = tensor.tensor3(dtype='float32')
6559         c = tensor.tensor3(dtype='float32')
6560         A = np.random.randint(0, 2, (2, 1, 1)).astype('int32')
6561         B = np.asarray(np.random.rand(1, 6, 1), dtype='float32')
6562         C = np.asarray(np.random.rand(1, 1, 5), dtype='float32')
6563         for m in self.modes:
6564             f = function([a, b, c], choose(a, (b, c), mode=m))
6565             t_c = f(A, B, C)
6566             n_c = np.choose(A, (B, C), mode=m)
6567             assert np.allclose(t_c, n_c)
6568     def test_infer_shape(self):
6569         for shp1, shp2 in [
6570             ((5, 4), (7, 4)),
6571             ((1, 4), (7, 4)),
6572             ((5, 1), (7, 4)),
6573             ((5, 4), (1, 4)),
6574             ((5, 4), (7, 1)),
6575             ((5, 4), (4,)),
6576             ((1, 4), (4,)),
6577             ((5, 1), (4,)),
6578             ((5, 4), (1,)),
6579             ((4,), (5, 4)),
6580             ((1,), (5, 4)),
6581             ((4,), (1, 4)),
6582             ((4,), (3, 1)),
6583             ((4,), (4,)),
6584             ((1,), (4,)),
6585             ((4,), (1,)),
6586             ((1,), (1,)),
6587         ]:
6588             a = tensor.tensor(dtype='int32',
6589                               broadcastable=[n == 1 for n in shp1])
6590             c = tensor.tensor(dtype='float32',
6591                               broadcastable=[n == 1 for n in shp2])
6592             A = np.asarray(np.random.rand(*shp1) * shp2[0], dtype='int32')
6593             C = np.asarray(np.random.rand(*shp2) * shp2[0], dtype='float32')
6594             self._compile_and_check([a, c],  # theano.function inputs
6595                                     [self.op(a, c)],  # theano.function outputs
6596                                     [A, C],
6597                                     self.op_class)
6598     def ___test_infer_shape_tuple(self):
6599         a <font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= tensor.tensor3(dtype='int32')
6600         b = tensor.tensor3(dtype='int32')
6601         c = tensor.tensor3(dtype='int32')
6602         A = np.asarray(</b></font>[1, 0], dtype='int32').reshape((2, 1, 1))
6603         B = np.asarray(np.random.rand(1, 4, 1), dtype='int32')
6604         C = np.asarray(np.random.rand(1, 1, 7), dtype='int32')
6605         f = function([a, b, c], choose(a, (b, c)))
6606         shape = (2, 4, 7)
6607         assert np.allclose(f(A, B, C).shape, shape)
6608         self._compile_and_check([a, b, c],  # theano.function inputs
6609                                 [self.op(a, (b, c))],  # theano.function outputs
6610                                 [A, B, C],
6611                                 self.op_class)
6612 def test_allocempty():
6613     f = theano.function([], AllocEmpty("float32")(2, 3))
6614     assert len(f.maker.fgraph.apply_nodes) == 1
6615     out = f()
6616     assert out.shape == (2, 3)
6617     assert out.dtype == 'float32'
6618 def test_symbolic_slice():
6619     x = theano.tensor.tensor4('x')
6620     a, b = x.shape[:2]
6621     output = a.eval({x: np.zeros((5, 4, 3, 2), dtype=theano.config.floatX)})
6622     assert output == np.array(5)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
