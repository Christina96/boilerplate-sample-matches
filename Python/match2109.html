<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for dnn.py &amp; test_basic_3.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for dnn.py &amp; test_basic_3.py
      </h3>
<h1 align="center">
        3.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>dnn.py (7.747804%)<th>test_basic_3.py (2.481913%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(2214-2222)<td><a href="#" name="0">(5195-5203)</a><td align="center"><font color="#ff0000">41</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(5-56)<td><a href="#" name="1">(1-62)</a><td align="center"><font color="#c00000">31</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(2223-2227)<td><a href="#" name="2">(5203-5207)</a><td align="center"><font color="#950000">24</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(2893-2900)<td><a href="#" name="3">(1048-1053)</a><td align="center"><font color="#820000">21</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(4015-4019)<td><a href="#" name="4">(4547-4550)</a><td align="center"><font color="#690000">17</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(3285-3290)<td><a href="#" name="5">(1724-1729)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1470-1472)<td><a href="#" name="6">(3032-3037)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(1399-1401)<td><a href="#" name="7">(878-881)</a><td align="center"><font color="#630000">16</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(3237-3240)<td><a href="#" name="8">(6525-6532)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(3228-3231)<td><a href="#" name="9">(6508-6515)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(1973-1976)<td><a href="#" name="10">(8761-8765)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(1943-1950)<td><a href="#" name="11">(6141-6144)</a><td align="center"><font color="#5d0000">15</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(3391-3396)<td><a href="#" name="12">(1730-1735)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1999-2005)<td><a href="#" name="13">(8089-8095)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(1231-1235)<td><a href="#" name="14">(5036-5043)</a><td align="center"><font color="#570000">14</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(4049-4053)<td><a href="#" name="15">(5979-5981)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(4023-4026)<td><a href="#" name="16">(8443-8445)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(3814-3815)<td><a href="#" name="17">(5279-5281)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(3421-3426)<td><a href="#" name="18">(7957-7964)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(3316-3321)<td><a href="#" name="19">(5022-5028)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(3179-3182)<td><a href="#" name="20">(7224-7229)</a><td align="center"><font color="#500000">13</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(4039-4042)<td><a href="#" name="21">(1116-1129)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(3858-3859)<td><a href="#" name="22">(5514-5516)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(3712-3715)<td><a href="#" name="23">(5293-5294)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(3188-3190)<td><a href="#" name="24">(3202-3204)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(3133-3135)<td><a href="#" name="25">(5016-5017)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#68818b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#68818b"><font color="#68818b">-</font><td><a href="#" name="26">(3076-3083)<td><a href="#" name="26">(6299-6306)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#e77471")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#e77471"><font color="#e77471">-</font><td><a href="#" name="27">(2798-2801)<td><a href="#" name="27">(1920-1922)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#717d7d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#717d7d"><font color="#717d7d">-</font><td><a href="#" name="28">(762-764)<td><a href="#" name="28">(2222-2225)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#af7a82")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#af7a82"><font color="#af7a82">-</font><td><a href="#" name="29">(720-727)<td><a href="#" name="29">(2085-2097)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#ae694a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ae694a"><font color="#ae694a">-</font><td><a href="#" name="30">(659-661)<td><a href="#" name="30">(687-692)</a><td align="center"><font color="#4a0000">12</font>
<tr onclick='openModal("#3ea99f")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3ea99f"><font color="#3ea99f">-</font><td><a href="#" name="31">(579-586)<td><a href="#" name="31">(2017-2029)</a><td align="center"><font color="#4a0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>dnn.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 <a name="1"></a>import ctypes
3 import os
4 import sys
5 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import warnings
6 import numpy as np
7 from six import integer_types
8 from six.moves import reduce
9 import theano
10 from theano import Op, Apply, tensor, config, Variable
11 from theano.scalar import (as_scalar, constant, Log, get_scalar_type,
12                            int32 as int_t, bool as bool_t, uint32 as uint32_t)
13 from theano.tensor import as_tensor_variable, Argmax
14 from theano.tensor.extra_ops import cpu_contiguous
15 from theano.gradient import DisconnectedType, grad_not_implemented
16 from theano.gof import Optimizer, local_optimizer, COp, ParamsType, EnumList
17 from theano.gof.cmodule import GCC_compiler
18 from theano.gof.type import CDataType, Generic
19 from theano.gof.opt import inherit_stack_trace
20 from theano.tensor.opt import Assert
21 from theano.compile import optdb
22 from theano.compile.ops import shape_i, shape_i_op
23 from theano.tensor.nnet import LogSoftmax, SoftmaxGrad
24 from theano.tensor.nnet.abstract_conv import (AbstractConv2d,
25                                               AbstractConv2d_gradWeights,
26                                               AbstractConv2d_gradInputs,
27                                               AbstractConv3d,
28                                               AbstractConv3d_gradWeights,
29                                               AbstractConv3d_gradInputs,
30                                               get_conv_output_shape,
31                                               assert_conv_shape)
32 from theano.tensor.signal.pool import (
33     Pool, MaxPoolGrad, AveragePoolGrad)
34 from . import pygpu, cudnn_defs
35 from .type import (get_context, gpu_context_type, list_contexts,
36                    GpuArraySharedVariable)
37 from .basic_ops import (as_gpuarray_variable, infer_context_name, gpuarray_helper_inc_dir,
38                         gpu_contiguous, GpuAllocEmpty,
39                         empty_like, GpuArrayType, HostFromGpu)
40 from .elemwise import GpuElemwise, GpuCAReduceCuda
41 from .reduction import GpuMaxAndArgmax
42 from .nnet import GpuSoftmax
43 from .opt import (gpu_seqopt, register_opt, pool_db, pool_db2,
44                   op_lifter, register_opt2, register_inplace)
45 from .opt_util import alpha_merge, output_merge, inplace_allocempty, pad_dims, unpad_dims
46 from theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME
47 import theano.pathparse
48 DNN_CONV_ALGO_CHOOSE_ONCE =</b></font> ['guess_once', 'time_once']
49 DNN_CONV_ALGO_CHOOSE_TIME = ['time_once', 'time_on_shape_change']
50 try:
51     from pygpu import gpuarray
52 except ImportError:
53     pass
54 WIN32_CUDNN_NAMES = ['cudnn64_7.dll', 'cudnn64_6.dll', 'cudnn64_5.dll']
55 if sys.platform == 'win32':
56     theano.pathparse.PathParser(theano.config.dnn.bin_path)
57 def _load_lib(name):
58     try:
59         return ctypes.cdll.LoadLibrary(name)
60     except OSError:
61         return None
62 def _dnn_lib():
63     if _dnn_lib.handle is None:
64         import ctypes.util
65         if config.dnn.bin_path != "":
66             if sys.platform == 'darwin':
67                 dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, 'libcudnn.dylib'))
68             elif sys.platform == 'win32':
69                 for name in WIN32_CUDNN_NAMES:
70                     dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, name))
71                     if dnn_handle is not None:
72                         break
73             else:
74                 dnn_handle = _load_lib(os.path.join(config.dnn.bin_path, 'libcudnn.so'))
75         else:
76             lib_name = ctypes.util.find_library('cudnn')
77             if lib_name is None and sys.platform == 'win32':
78                 for name in WIN32_CUDNN_NAMES:
79                     lib_name = ctypes.util.find_library(name)
80                     if lib_name:
81                         break
82             if lib_name is None:
83                 raise RuntimeError(
84                     'Could not find cudnn library (looked for v5* to v7*).'
85                     ' Check your cudnn installation. Maybe using the Theano'
86                     ' flag dnn.base_path can help you. Current value "%s"' %
87                     config.dnn.base_path)
88             else:
89                 dnn_handle = ctypes.cdll.LoadLibrary(lib_name)
90         if dnn_handle is None:
91             raise RuntimeError('Could not load cudnn library. Check your cudnn'
92                                ' installation. Maybe using the Theano'
93                                ' flag dnn.base_path can help you. Current value "%s"' %
94                                config.dnn.base_path)
95         _dnn_lib.handle = dnn_handle
96         cudnn = _dnn_lib.handle
97         cudnn.cudnnCreate.argtypes = [ctypes.POINTER(ctypes.c_void_p)]
98         cudnn.cudnnCreate.restype = ctypes.c_int
99         cudnn.cudnnDestroy.argtypes = [ctypes.c_void_p]
100         cudnn.cudnnDestroy.restype = ctypes.c_int
101     return _dnn_lib.handle
102 _dnn_lib.handle = None
103 def _make_handle(ctx):
104     cudnn = _dnn_lib()
105     handle = ctypes.c_void_p()
106     with ctx:
107         err = cudnn.cudnnCreate(ctypes.byref(handle))
108     if err != 0:
109         raise RuntimeError("Error creating cudnn handle. "
110                            "This can be a sign of a too old driver.", err)
111     return handle
112 def _dnn_check_compile():
113     preambule = """
114     path_wrapper = "\"" if os.name == 'nt' else ""
115     params = ["-l", "cudnn"]
116     params.extend(['-I%s%s%s' % (path_wrapper, gpuarray_helper_inc_dir(), path_wrapper)])
117     if config.dnn.include_path:
118         params.extend(['-I%s%s%s' % (path_wrapper, config.dnn.include_path, path_wrapper)])
119     if config.cuda.include_path:
120         params.extend(['-I%s%s%s' % (path_wrapper, config.cuda.include_path, path_wrapper)])
121     if config.dnn.library_path:
122         params.extend(['-L%s%s%s' % (path_wrapper, config.dnn.library_path, path_wrapper)])
123     compiler_res = GCC_compiler.try_flags(
124         params, preambule=preambule, body=body,
125         try_run=False, output=True)
126     avail, out, err = compiler_res if isinstance(compiler_res, tuple) else (compiler_res, None, None)
127     if not avail:
128         return False, ("cannot compile with cuDNN. "
129                        "We got this error:\n" + str(err))
130     return True, None
131 def _dnn_check_version():
132     v = version()
133     if v &lt; 5000:
134         return False, "cuDNN version is too old. Update to v5* or higher, was %d." % v
135     if v &gt;= 7200:
136         warnings.warn("Your cuDNN version is more recent than "
137                       "Theano. If you encounter problems, try "
138                       "updating Theano or downgrading cuDNN to "
139                       "a version &gt;= v5 and &lt;= v7.")
140     return True, None
141 def dnn_present():
142     if dnn_present.avail is not None:
143         return dnn_present.avail
144     if config.dnn.enabled == "False":
145         dnn_present.msg = "Disabled by dnn.enabled flag"
146         dnn_present.avail = False
147         return False
148     if pygpu is None:
149         dnn_present.msg = "PyGPU not available"
150         dnn_present.avail = False
151         return False
152     if config.dnn.enabled == "no_check":
153         dnn_present.avail, dnn_present.msg = True, "presence check disabled by dnn.enabled flag"
154     else:
155         dnn_present.avail, dnn_present.msg = _dnn_check_compile()
156     if dnn_present.avail:
157         dnn_present.avail, dnn_present.msg = _dnn_check_version()
158         if not dnn_present.avail:
159             return False
160     return dnn_present.avail
161 dnn_present.avail = None
162 dnn_present.msg = None
163 def dnn_available(context_name):
164     if not dnn_present():
165         dnn_available.msg = dnn_present.msg
166         return False
167     ctx = get_context(context_name)
168     if not ctx.kind == b'cuda':
169         dnn_available.msg = "Not on a CUDA device."
170         return False
171     if int(ctx.bin_id[-2:]) &lt; 30:
172         dnn_available.msg = "Device not supported"
173         return False
174     if version() &lt; 7002:
175         if int(ctx.bin_id[-2:]) &gt;= 70:
176             dnn_available.msg = "Use cuDNN 7.0.2 or higher for Volta."
177             return False
178     return True
179 dnn_available.msg = None
180 def CUDNNDataType(name, freefunc=None):
181     cargs = []
182     if config.dnn.bin_path and sys.platform != 'win32':
183         cargs.append('-Wl,-rpath,' + config.dnn.bin_path)
184     return CDataType(name, freefunc,
185                      headers=['cudnn.h'],
186                      header_dirs=[config.dnn.include_path,
187                                   config.cuda.include_path],
188                      libraries=['cudnn'],
189                      lib_dirs=[config.dnn.library_path],
190                      compile_args=cargs,
191                      version=version(raises=False))
192 class DnnVersion(Op):
193     __props__ = ()
194     def c_headers(self):
195         return ['cudnn.h']
196     def c_header_dirs(self):
197         return [config.dnn.include_path, config.cuda.include_path]
198     def c_libraries(self):
199         return ['cudnn']
200     def c_lib_dirs(self):
201         return [config.dnn.library_path]
202     def c_compile_args(self):
203         if config.dnn.bin_path and sys.platform != 'win32':
204             return ['-Wl,-rpath,' + config.dnn.bin_path]
205         return []
206     def c_support_code(self):
207         return """
208     def do_constant_folding(self, node):
209         return False
210     def c_code_cache_version(self):
211         return None
212 def version(raises=True):
213     if not dnn_present():
214         if raises:
215             raise RuntimeError(
216                 "We can't determine the cudnn version as it is not available",
217                 dnn_available.msg)
218         else:
219             return -1
220     if version.v is None:
221         f = theano.function([], DnnVersion()(),
222                             theano.Mode(optimizer=None),
223                             profile=False)
224         v = f()
225         if v[0] != v[1]:
226             raise RuntimeError("Mixed dnn version. The header is version %s "
227                                "while the library is version %s." % v)
228         version.v = v[1]
229     return version.v
230 version.v = None
231 handle_type = CUDNNDataType('cudnnHandle_t', 'cudnnDestroy')
232 cudnn = cudnn_defs.get_definitions(version(raises=False))
233 def get_precision(precision, inputs, for_grad=False):
234     common_dtype = theano.scalar.upcast(*[i.dtype for i in inputs])
235     if not common_dtype.startswith('float'):
236         raise TypeError("cuDNN convolution only works on real numbers")
237     if precision is None:
238         precision = theano.config.dnn.conv.precision
239     if precision == 'as_input' or precision == 'as_input_f32':
240         if common_dtype == 'float16' and precision == 'as_input_f32':
241             precision = 'float32'
242         else:
243             precision = common_dtype
244     if for_grad and precision == 'float16':
245         raise TypeError("Float16 precision is disabled for cuDNN backward convolutions due to computation errors.")
246     return precision, common_dtype
247 class DnnBase(COp):
248     check_broadcast = False
249     params_type = handle_type
250     def dnn_context(self, node):
251         return node.outputs[0].type.context_name
252     def get_params(self, node):
253         ctx_name = self.dnn_context(node)
254         ctx = get_context(ctx_name)
255         if not hasattr(ctx, 'cudnn_handle_param'):
256             ptr = ctx.cudnn_handle.value
257             res = handle_type.make_value(ptr)
258             ctx.cudnn_handle_param = res
259         if isinstance(self.params_type, ParamsType):
260             if not self.params_type.has_type(handle_type):
261                 raise TypeError('DnnBase: params_type must take into account the cuDNN handle type.')
262             handle_field = self.params_type.get_field(handle_type)
263             return self.params_type.get_params(self, **{handle_field: ctx.cudnn_handle_param})
264         return ctx.cudnn_handle_param
265     def __init__(self, files=None, c_func=None):
266         if files is None:
267             files = []
268         COp.__init__(self, ["c_code/dnn_base.c"] + files, c_func)
269     def c_headers(self):
270         return ['gpuarray/types.h', 'gpuarray/array.h', 'gpuarray/kernel.h',
271                 'gpuarray/util.h', 'gpuarray/ext_cuda.h', 'gpuarray_api.h',
272                 'numpy_compat.h', 'cudnn.h', 'cudnn_helper.h',
273                 'gpuarray_helper.h']
274     def c_header_dirs(self):
275         return [gpuarray_helper_inc_dir(), pygpu.get_include(),
276                 config.dnn.include_path, config.cuda.include_path]
277     def c_libraries(self):
278         return ['cudnn', 'gpuarray']
279     def c_lib_dirs(self):
280         return [config.dnn.library_path]
281     def c_compile_args(self):
282         if config.dnn.bin_path and sys.platform != 'win32':
283             return ['-Wl,-rpath,' + config.dnn.bin_path]
284         return []
285     def c_code_cache_version(self):
286         return (super(DnnBase, self).c_code_cache_version(), version(), 4)
287 class GpuDnnConvDesc(COp):
288     __props__ = ('border_mode', 'subsample', 'dilation', 'conv_mode',
289                  'precision', 'num_groups')
290     params_type = ParamsType(pad0=int_t, pad1=int_t, pad2=int_t,
291                              sub0=int_t, sub1=int_t, sub2=int_t,
292                              dil0=int_t, dil1=int_t, dil2=int_t,
293                              nb_dims=int_t,
294                              bmode=EnumList(('BORDER_MODE_FULL', 'full'),
295                                             ('BORDER_MODE_VALID', 'valid'),
296                                             ('BORDER_MODE_HALF', 'half')),
297                              conv_mode=cudnn.cudnnConvolutionMode_t,
298                              precision=cudnn.cudnnDataType_t,
299                              num_groups=int_t)
300     def c_headers(self):
301         return ['cudnn.h', 'cudnn_helper.h']
302     def c_header_dirs(self):
303         return [gpuarray_helper_inc_dir(), config.dnn.include_path,
304                 config.cuda.include_path]
305     def c_libraries(self):
306         return ['cudnn']
307     def c_lib_dirs(self):
308         return [config.dnn.library_path]
309     def c_compile_args(self):
310         if config.dnn.bin_path and sys.platform != 'win32':
311             return ['-Wl,-rpath,' + config.dnn.bin_path]
312         return []
313     def do_constant_folding(self, node):
314         return False
315     def __init__(self, border_mode, subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
316                  precision="float32", num_groups=1):
317         COp.__init__(self, ["c_code/conv_desc.c"], "APPLY_SPECIFIC(conv_desc)")
318         if version() &lt; 6000 and any([d != 1 for d in dilation]):
319             raise RuntimeError("Dilation &gt; 1 not supported for cuDNN version &lt; 6.")
320         if isinstance(border_mode, integer_types):
321             border_mode = (border_mode,) * len(subsample)
322         if isinstance(border_mode, tuple):
323             assert len(border_mode) == len(subsample)
324             border_mode = tuple(map(int, border_mode))
325         if not ((isinstance(border_mode, tuple) and min(border_mode) &gt;= 0) or
326                 border_mode in ('valid', 'full', 'half')):
327             raise ValueError(
328                 'invalid border_mode {}, which must be either '
329                 '"valid", "full", "half", an integer or a pair of'
330                 ' integers'.format(border_mode))
331         self.border_mode = border_mode
332         assert len(subsample) in (2, 3)
333         self.subsample = subsample
334         assert cudnn.cudnnConvolutionMode_t.has_alias(conv_mode)
335         self.conv_mode = conv_mode
336         self.num_groups = num_groups
337         assert len(dilation) == len(subsample)
338         self.dilation = dilation
339         assert cudnn.cudnnDataType_t.has_alias(precision)
340         self.precision = precision
341     def make_node(self, kern_shape):
342         kern_shape = as_tensor_variable(kern_shape)
343         if kern_shape.type.ndim != 1 or kern_shape.dtype not in theano.tensor.basic.int_dtypes:
344             raise TypeError('kern must be an int64 1D shape tensor')
345         kern_shape = theano.tensor.basic.cast(kern_shape, 'int64')
346         node = Apply(self, [kern_shape],
347                      [CUDNNDataType("cudnnConvolutionDescriptor_t",
348                                     freefunc="cudnnDestroyConvolutionDescriptor")()])
349         out = node.outputs[0]
350         out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true
351         return node
352     bmode = property(lambda self: 'valid' if isinstance(self.border_mode, tuple) else self.border_mode)
353     pad0 = property(lambda self: self.border_mode[0] if isinstance(self.border_mode, tuple) else 0)
354     pad1 = property(lambda self: self.border_mode[1] if isinstance(self.border_mode, tuple) else 0)
355     pad2 = property(lambda self: self.border_mode[2] if (isinstance(self.border_mode, tuple) and
356                                                          len(self.border_mode) &gt; 2) else 0)
357     sub0 = property(lambda self: self.subsample[0])
358     sub1 = property(lambda self: self.subsample[1])
359     sub2 = property(lambda self: self.subsample[2] if len(self.subsample) &gt; 2 else 0)
360     dil0 = property(lambda self: self.dilation[0])
361     dil1 = property(lambda self: self.dilation[1])
362     dil2 = property(lambda self: self.dilation[2] if len(self.dilation) &gt; 2 else 0)
363     nb_dims = property(lambda self: len(self.subsample))
364     def c_code_cache_version(self):
365         return (super(GpuDnnConvDesc, self).c_code_cache_version(), version())
366     def __setstate__(self, d):
367         self.__dict__.update(d)
368         if not hasattr(self, "dilation"):
369             self.dilation = (1,) * len(self.subsample)
370         if not hasattr(self, "num_groups"):
371             self.num_groups = 1
372 _zero = constant(np.asarray(0.0, dtype='float64'))
373 _one = constant(np.asarray(1.0, dtype='float64'))
374 def ensure_dt(val, default, name, dtype):
375     if dtype == 'float16':
376         dtype = 'float32'
377     if val is None:
378         val = default.clone()
379     if not isinstance(val, Variable):
380         val = constant(val)
381     if hasattr(val, 'ndim') and val.ndim == 0:
382         val = as_scalar(val)
383     if not isinstance(val.type, theano.scalar.Scalar):
384         raise TypeError("%s: expected a scalar value" % (name,))
385     if not val.type.dtype == dtype:
386         val = val.astype(dtype)
387     return val
388 class GpuDnnConv(DnnBase):
389 <a name="31"></a>    _f16_ok = True
390     __props__ = ('algo', 'inplace', 'num_groups')
391     check_input <font color="#3ea99f"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= False
392     params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionFwdAlgo_t,
393                              choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
394                              inplace=bool_t,
395                              handle=handle_type,
396                              num_groups=int_t)
397     def</b></font> __init__(self, algo=None, inplace=False, num_groups=1):
398         DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_fwd.c"],
399                          "APPLY_SPECIFIC(conv_fwd)")
400         if algo is None:
401             algo = config.dnn.conv.algo_fwd
402         self.algo = algo
403         self.inplace = bool(inplace)
404         if self.inplace:
405             self.destroy_map = {0: [2]}
406         assert cudnn.cudnnConvolutionFwdAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
407         self.conv_algo = cudnn.cudnnConvolutionFwdAlgo_t.CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
408         if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
409             self.conv_algo = self.algo
410         self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
411         self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
412         self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
413         self.num_groups = num_groups
414     def __setstate__(self, d):
415         self.__dict__.update(d)
416         if not hasattr(self, 'algo'):
417             if hasattr(self, 'workmem'):
418                 self.algo = self.workmem
419             else:
420                 self.algo = config.dnn.conv.algo_fwd
421         if not hasattr(self, 'inplace'):
422             self.inplace = False
423         if not hasattr(self, 'num_groups'):
424             self.num_groups = 1
425     def make_node(self, img, kern, output, desc, alpha=None, beta=None):
426         ctx_name = infer_context_name(img, kern, output)
427         img = as_gpuarray_variable(img, ctx_name)
428         kern = as_gpuarray_variable(kern, ctx_name)
429         output = as_gpuarray_variable(output, ctx_name)
430         if img.type.ndim not in (4, 5):
431             raise TypeError('img must be 4D or 5D tensor')
432         if kern.type.ndim not in (4, 5):
433             raise TypeError('kern must be 4D or 5D tensor')
434         if output.type.ndim not in (4, 5):
435             raise TypeError('output must be a 4D or 5D tensor')
436         if (img.type.ndim != kern.type.ndim or
437                 img.type.ndim != output.type.ndim):
438             raise TypeError("The number of dimensions of "
439                             "img, kern and output must match")
440         if img.type.ndim == 5 and self.algo not in (cudnn.conv3d_fwd_algorithms +
441                                                     SUPPORTED_DNN_CONV_ALGO_RUNTIME):
442             raise ValueError("convolution algo %s can't be used for "
443                              "3d convolutions", (self.algo,))
444         if (not isinstance(desc.type, CDataType) or
445                 desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
446             raise TypeError('desc must be cudnnConvolutionDescriptor_t')
447         alpha = ensure_dt(alpha, _one, 'alpha', img.dtype)
448         beta = ensure_dt(beta, _zero, 'beta', img.dtype)
449         return Apply(self, [img, kern, output, desc, alpha, beta],
450                      [output.type()])
451     def grad(self, inp, grads):
452         img, kerns, output, desc, alpha, beta = inp
453         top, = grads
454 <a name="30"></a>
455         top = gpu_contiguous(top)
456         d_img = GpuDnnConvGradI<font color="#ae694a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(num_groups=self.num_groups)(kerns, top, empty_like(img), desc)
457         d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(img, top, empty_like(kerns), desc)
458         d_alpha =</b></font> grad_not_implemented(self, 4, alpha)
459         d_beta = grad_not_implemented(self, 5, beta)
460         return [d_img * alpha, d_kerns * alpha, top * beta,
461                 DisconnectedType()(), d_alpha, d_beta]
462     def connection_pattern(self, node):
463         return [[1], [1], [1], [0], [1], [1]]
464     @staticmethod
465     def get_out_shape(ishape, kshape, border_mode, subsample, dilation):
466         if not isinstance(ishape, (list, tuple)):
467             ishape = [ishape[i] for i in range(len(subsample) + 2)]
468         if not isinstance(kshape, (list, tuple)):
469             kshape = [kshape[i] for i in range(len(subsample) + 2)]
470         return get_conv_output_shape(
471             ishape,
472             kshape,
473             border_mode,
474             subsample,
475             dilation)
476     def infer_shape(self, node, shape):
477         return [shape[2]]
478 class GpuDnnConvGradW(DnnBase):
479 <a name="29"></a>    _f16_ok = True
480     __props__ = ('algo', 'inplace', 'num_groups')
481     check_input <font color="#af7a82"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= False
482     params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionBwdFilterAlgo_t,
483                              choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
484                              inplace=bool_t,
485                              handle=handle_type,
486                              num_groups=int_t)
487     def</b></font> __init__(self, inplace=False, algo=None, num_groups=1):
488         DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_gw.c"],
489                          "APPLY_SPECIFIC(conv_gw)")
490         self.inplace = bool(inplace)
491         if self.inplace:
492             self.destroy_map = {0: [2]}
493         if algo is None:
494             algo = config.dnn.conv.algo_bwd_filter
495         self.algo = algo
496         assert cudnn.cudnnConvolutionBwdFilterAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
497         self.conv_algo = cudnn.cudnnConvolutionBwdFilterAlgo_t.CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0
498         if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
499             self.conv_algo = self.algo
500         self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
501         self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
502         self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
503         self.num_groups = num_groups
504     def __setstate__(self, d):
505         self.__dict__.update(d)
506         if not hasattr(self, 'inplace'):
507             self.inplace = False
508         if not hasattr(self, 'algo'):
509             self.algo = config.dnn.conv.algo_bwd_filter
510         if not hasattr(self, 'num_groups'):
511             self.num_groups = 1
512     def grad(self, inp, grads):
513         img, top, output, desc, alpha, beta = inp
514         kerns, = grads
515 <a name="28"></a>
516         kerns = gpu_contiguous(kerns)
517         d_img = GpuDnnConvGradI<font color="#717d7d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(num_groups=self.num_groups)(kerns, top, empty_like(img), desc)
518         d_top = GpuDnnConv(num_groups=self.num_groups)(img, kerns, empty_like(top), desc)
519         d_alpha =</b></font> grad_not_implemented(self, 4, alpha)
520         d_beta = grad_not_implemented(self, 5, beta)
521         return (d_img * alpha, d_top * alpha, kerns * beta,
522                 DisconnectedType()(), d_alpha, d_beta)
523     def connection_pattern(self, node):
524         return [[1], [1], [1], [0], [1], [1]]
525     def op_may_fail_with_subsample(self, img, desc):
526         return (version() &lt; 6000 and
527                 img.type.dtype == 'float32' and
528                 img.type.ndim == 5 and
529                 self.algo != 'none' and
530                 desc.owner.op.subsample != (1, 1, 1))
531     def op_may_fail_with_beta(self, img, beta):
532         return (version() &lt; 6000 and
533                 img.type.dtype == 'float32' and
534                 self.algo not in ('none', 'deterministic', 'fft', 'small') and
535                 beta is not None and
536                 theano.tensor.extract_constant(beta) != 1)
537     def make_node(self, img, topgrad, output, desc, alpha=None, beta=None):
538         if self.op_may_fail_with_subsample(img, desc):
539             warnings.warn('cuDNN backward filter operation for 3D convolutions may produce bad results '
540                           'with certain cuDNN algorithms depending on the compute capability of your GPU '
541                           'if subsample is not (1, 1, 1). If you encounter problems, consider '
542                           'setting the theano flag "dnn.conv.algo_bwd_filter" to "none".')
543         if self.op_may_fail_with_beta(img, beta):
544             warnings.warn('cuDNN backward filter operation for convolutions may produce bad results '
545                           'with certain cuDNN algorithms depending on the compute capability of your GPU '
546                           'if beta != 1. If you encounter problems, consider '
547                           'setting the theano flag "dnn.conv.algo_bwd_filter" to '
548                           '"none", "deterministic", "fft", or "small".')
549         ctx_name = infer_context_name(img, topgrad, output)
550         img = as_gpuarray_variable(img, ctx_name)
551         topgrad = as_gpuarray_variable(topgrad, ctx_name)
552         output = as_gpuarray_variable(output, ctx_name)
553         if img.type.ndim not in (4, 5):
554             raise TypeError('img must be 4D or 5D tensor')
555         if topgrad.type.ndim not in (4, 5):
556             raise TypeError('topgrad must be 4D or 5D tensor')
557         if output.type.ndim not in (4, 5):
558             raise TypeError('output must be 4D or 5D tensor')
559         if (img.type.ndim != topgrad.type.ndim or
560                 img.type.ndim != output.type.ndim):
561             raise TypeError("The number of dimensions of "
562                             "img, topgrad and output must match")
563         if img.type.ndim == 5 and self.algo not in (cudnn.conv3d_bwd_filter_algorithms +
564                                                     SUPPORTED_DNN_CONV_ALGO_RUNTIME):
565             raise ValueError("convolution algo %s can't be used for "
566                              "3d convolutions", (self.algo,))
567         if (not isinstance(desc.type, CDataType) or
568                 desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
569             raise TypeError('desc must be cudnnConvolutionDescriptor_t')
570         alpha = ensure_dt(alpha, _one, 'alpha', img.dtype)
571         beta = ensure_dt(beta, _zero, 'beta', img.dtype)
572         return Apply(self, [img, topgrad, output, desc, alpha, beta],
573                      [output.type()])
574     def infer_shape(self, node, shape):
575         return [shape[2]]
576 class GpuDnnConvGradI(DnnBase):
577     _f16_ok = True
578     __props__ = ('algo', 'inplace', 'num_groups')
579     check_input = False
580     params_type = ParamsType(conv_algo=cudnn.cudnnConvolutionBwdDataAlgo_t,
581                              choose_algo=bool_t, choose_once=bool_t, choose_time=bool_t,
582                              inplace=bool_t,
583                              handle=handle_type,
584                              num_groups=int_t)
585     def __init__(self, inplace=False, algo=None, num_groups=1):
586         DnnBase.__init__(self, ["c_code/dnn_conv_base.c", "c_code/dnn_gi.c"],
587                          "APPLY_SPECIFIC(conv_gi)")
588         self.inplace = bool(inplace)
589         if self.inplace:
590             self.destroy_map = {0: [2]}
591         if algo is None:
592             algo = config.dnn.conv.algo_bwd_data
593         self.algo = algo
594         assert cudnn.cudnnConvolutionBwdDataAlgo_t.has_alias(self.algo) or self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
595         self.conv_algo = cudnn.cudnnConvolutionBwdDataAlgo_t.CUDNN_CONVOLUTION_BWD_DATA_ALGO_0
596         if self.algo not in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
597             self.conv_algo = self.algo
598         self.choose_algo = self.algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME
599         self.choose_once = self.algo in DNN_CONV_ALGO_CHOOSE_ONCE
600         self.choose_time = self.algo in DNN_CONV_ALGO_CHOOSE_TIME
601         self.num_groups = num_groups
602     def __setstate__(self, d):
603         self.__dict__.update(d)
604         if not hasattr(self, 'algo'):
605             self.algo = config.dnn.conv.algo_bwd_data
606         if not hasattr(self, 'inplace'):
607             self.inplace = False
608         if not hasattr(self, 'num_groups'):
609             self.num_groups = 1
610     def grad(self, inp, grads):
611         kerns, top, output, desc, alpha, beta = inp
612         img, = grads
613         img = gpu_contiguous(img)
614         d_kerns = GpuDnnConvGradW(num_groups=self.num_groups)(img, top, empty_like(kerns), desc)
615         d_top = GpuDnnConv(num_groups=self.num_groups)(img, kerns, empty_like(top), desc)
616         d_alpha = grad_not_implemented(self, 4, alpha)
617         d_beta = grad_not_implemented(self, 5, beta)
618         return (d_kerns * alpha, d_top * alpha, img * beta,
619                 DisconnectedType()(), d_alpha, d_beta)
620     def connection_pattern(self, node):
621         return [[1], [1], [1], [0], [1], [1]]
622     def make_node(self, kern, topgrad, output, desc, alpha=None, beta=None):
623         ctx_name = infer_context_name(kern, topgrad, output)
624         kern = as_gpuarray_variable(kern, ctx_name)
625         topgrad = as_gpuarray_variable(topgrad, ctx_name)
626         output = as_gpuarray_variable(output, ctx_name)
627         if kern.type.ndim not in (4, 5):
628             raise TypeError('kern must be 4D or 5D tensor')
629         if topgrad.type.ndim not in (4, 5):
630             raise TypeError('topgrad must be 4D or 5D tensor')
631         if output.type.ndim not in (4, 5):
632             raise TypeError('output must be 4D or 5D tensor')
633         if (kern.type.ndim != topgrad.type.ndim or
634                 kern.type.ndim != output.type.ndim):
635             raise TypeError("The number of dimensions of "
636                             "kern, topgrad and output must match")
637         if kern.type.ndim == 5 and self.algo not in (cudnn.conv3d_bwd_data_algorithms +
638                                                      SUPPORTED_DNN_CONV_ALGO_RUNTIME):
639             raise ValueError("convolution algo %s can't be used for "
640                              "3d convolutions", (self.algo,))
641         if (not isinstance(desc.type, CDataType) or
642                 desc.type.ctype != 'cudnnConvolutionDescriptor_t'):
643             raise TypeError('desc must be cudnnConvolutionDescriptor_t')
644         alpha = ensure_dt(alpha, _one, 'alpha', kern.dtype)
645         beta = ensure_dt(beta, _zero, 'beta', kern.dtype)
646         return Apply(self, [kern, topgrad, output, desc, alpha, beta],
647                      [output.type()])
648     def infer_shape(self, node, shape):
649         return [shape[2]]
650 def _dnn_conv(img, kerns, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1), dilation=(1, 1),
651               conv_mode='conv', algo=None, precision=None, num_groups=1):
652     ctx_name = infer_context_name(img, kerns)
653     img = as_gpuarray_variable(img, ctx_name)
654     kerns = as_gpuarray_variable(kerns, ctx_name)
655     precision, dt = get_precision(precision, [img, kerns])
656     img = gpu_contiguous(img.astype(dt))
657     kerns = gpu_contiguous(kerns.astype(dt))
658     desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
659                           conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns.shape)
660     desc_op = desc.owner.op
661     ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
662     kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
663     out_shp = get_conv_output_shape(ishape, kshape, desc_op.border_mode, desc_op.subsample, filter_dilation=dilation)
664     out_shp = assert_conv_shape(out_shp)
665     if beta == 0:
666         real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
667     else:
668         assert out is not None
669         out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
670         check = Assert('GpuDnnConv: given output (for beta not null) does not have expected shape')
671         real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, out_shp)))
672     return GpuDnnConv(algo=algo, num_groups=num_groups)(img, kerns, real_out, desc, alpha, beta)
673 def _dnn_gradweight(img, topgrad, kerns_shp, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1),
674                     dilation=(1, 1), conv_mode='conv', algo=None, precision=None, num_groups=1):
675     ctx_name = infer_context_name(img, topgrad)
676     img = as_gpuarray_variable(img, ctx_name)
677     topgrad = as_gpuarray_variable(topgrad, ctx_name)
678     kerns_shp = theano.tensor.as_tensor_variable(kerns_shp)
679     precision, dt = get_precision(precision, [img, topgrad], for_grad=True)
680     img = gpu_contiguous(img.astype(dt))
681     topgrad = gpu_contiguous(topgrad.astype(dt))
682     desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
683                           conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns_shp)
684     if beta == 0:
685         real_out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*kerns_shp)
686     else:
687         assert out is not None
688         out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
689         check = Assert('GpuDnnConvGradW: given output (for beta not null) does not have expected shape')
690         real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, kerns_shp)))
691     return GpuDnnConvGradW(algo=algo, num_groups=num_groups)(img, topgrad, real_out, desc, alpha, beta)
692 def _dnn_gradinput(kerns, topgrad, img_shp, alpha=1, beta=0, out=None, border_mode='valid', subsample=(1, 1),
693                    dilation=(1, 1), conv_mode='conv', algo=None, precision=None, num_groups=1):
694     ctx_name = infer_context_name(kerns, topgrad)
695     kerns = as_gpuarray_variable(kerns, ctx_name)
696     topgrad = as_gpuarray_variable(topgrad, ctx_name)
697     img_shp = theano.tensor.as_tensor_variable(img_shp)
698     precision, dt = get_precision(precision, [kerns, topgrad], for_grad=True)
699     kerns = gpu_contiguous(kerns.astype(dt))
700     topgrad = gpu_contiguous(topgrad.astype(dt))
701     desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, dilation=dilation,
702                           conv_mode=conv_mode, precision=precision, num_groups=num_groups)(kerns.shape)
703     if beta == 0:
704         real_out = GpuAllocEmpty(dtype=kerns.dtype, context_name=ctx_name)(*img_shp)
705     else:
706         assert out is not None
707         out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
708         check = Assert('GpuDnnConvGradI: given output (for beta not null) does not have expected shape')
709         real_out = check(out, theano.tensor.all(theano.tensor.eq(out.shape, img_shp)))
710     return GpuDnnConvGradI(algo=algo, num_groups=num_groups)(kerns, topgrad, real_out, desc, alpha, beta)
711 def dnn_conv(img, kerns, border_mode='valid', subsample=(1, 1), dilation=(1, 1),
712              conv_mode='conv', direction_hint=None, workmem=None,
713              algo=None, precision=None, num_groups=1):
714     if workmem is not None:
715         if algo is not None:
716             raise ValueError("You can't use both algo and workmem")
717         warnings.warn("workmem is deprecated, use algo instead", stacklevel=2)
718         algo = workmem
719     fgraph = getattr(img, 'fgraph', None) or getattr(kerns, 'fgraph', None)
720     ctx_name = infer_context_name(img, kerns)
721     if (border_mode == 'valid' and subsample == (1, 1) and dilation == (1, 1) and
722             direction_hint == 'bprop weights' and num_groups == 1):
723         img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3))
724         if conv_mode == 'conv':
725             kerns = kerns[:, :, ::-1, ::-1]
726         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))
727         out_shp = (shape_i(kerns, 1, fgraph),
728                    shape_i(img, 1, fgraph),
729                    shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,
730                    shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1)
731         out_shp = assert_conv_shape(out_shp)
732         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
733         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
734         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1), dilation=(1, 1),
735                               num_groups=num_groups,
736                               conv_mode='cross', precision=precision)(out.shape)
737         conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)
738         return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)
739     elif (border_mode == 'full' and subsample == (1, 1) and
740           direction_hint != 'forward!' and num_groups == 1):
741         img = gpu_contiguous(img)  # cudnn v2 rc3 need contiguous data
742         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3))
743         conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
744         out_shp = (shape_i(img, 0, fgraph),
745                    shape_i(kerns, 1, fgraph),
746                    shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],
747                    shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1])
748         out_shp = assert_conv_shape(out_shp)
749         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
750         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
751         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1), dilation=dilation,
752                               num_groups=num_groups,
753                               conv_mode=conv_mode, precision=precision)(kerns.shape)
754         return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)
755     return _dnn_conv(img, kerns, algo=algo, border_mode=border_mode, subsample=subsample, dilation=dilation,
756                      conv_mode=conv_mode, precision=precision, num_groups=num_groups)
757 def dnn_conv3d(img, kerns, border_mode='valid', subsample=(1, 1, 1), dilation=(1, 1, 1),
758                conv_mode='conv', direction_hint=None,
759                algo=None, precision=None, num_groups=1):
760     fgraph = getattr(img, 'fgraph', None) or getattr(kerns, 'fgraph', None)
761     ctx_name = infer_context_name(img, kerns)
762     if (border_mode == 'valid' and subsample == (1, 1, 1) and dilation == (1, 1, 1) and
763             direction_hint == 'bprop weights' and num_groups == 1):
764         img = gpu_contiguous(img.dimshuffle(1, 0, 2, 3, 4))
765         if conv_mode == 'conv':
766             kerns = kerns[:, :, ::-1, ::-1, ::-1]
767         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))
768         out_shp = (shape_i(kerns, 1, fgraph),
769                    shape_i(img, 1, fgraph),
770                    shape_i(img, 2, fgraph) - shape_i(kerns, 2, fgraph) + 1,
771                    shape_i(img, 3, fgraph) - shape_i(kerns, 3, fgraph) + 1,
772                    shape_i(img, 4, fgraph) - shape_i(kerns, 4, fgraph) + 1)
773         out_shp = assert_conv_shape(out_shp)
774         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
775         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
776         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1, 1), dilation=(1, 1, 1),
777                               num_groups=num_groups,
778                               conv_mode='cross', precision=precision)(out.shape)
779         conv = GpuDnnConvGradW(num_groups=num_groups)(img, kerns, out, desc)
780         return as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)
781     elif (border_mode == 'full' and subsample == (1, 1, 1) and
782           direction_hint != 'forward!' and num_groups == 1):
783         kerns = gpu_contiguous(kerns.dimshuffle(1, 0, 2, 3, 4))
784         conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
785         out_shp = (<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>shape_i(img, 0, fgraph),
786                    shape_i(kerns, 1, fgraph),
787                    shape_i(img, 2, fgraph) + (shape_i(kerns, 2, fgraph) - 1) * dilation[0],
788                    shape_i(img, 3, fgraph) + (shape_i(kerns, 3, fgraph) - 1) * dilation[1],
789                    shape_i(img, 4, fgraph) + (shape_i(</b></font>kerns, 4, fgraph) - 1) * dilation[2])
790         out_shp = assert_conv_shape(out_shp)
791         out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
792         precision, _ = get_precision(precision, [img, kerns], for_grad=True)
793         desc = GpuDnnConvDesc(border_mode='valid', subsample=(1, 1, 1), dilation=dilation,
794                               num_groups=num_groups,
795                               conv_mode=conv_mode, precision=precision)(kerns.shape)
796         return GpuDnnConvGradI(num_groups=num_groups)(kerns, img, out, desc)
797     return _dnn_conv(img, kerns, algo=algo, border_mode=border_mode, subsample=subsample, dilation=dilation,
798                      conv_mode=conv_mode, precision=precision, num_groups=num_groups)
799 def dnn_gradweight(img, topgrad, kerns_shp, border_mode='valid',
800                    subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
801                    precision=None, algo=None, num_groups=1):
802     return _dnn_gradweight(img, topgrad, kerns_shp, border_mode=border_mode, subsample=subsample, dilation=dilation,
803                            conv_mode=conv_mode, algo=algo, precision=precision, num_groups=num_groups)
804 def dnn_gradweight3d(img, topgrad, kerns_shp, border_mode='valid',
805                      subsample=(1, 1, 1), dilation=(1, 1, 1), conv_mode='conv',
806                      precision=None, algo=None, num_groups=1):
807     return dnn_gradweight(img, topgrad, kerns_shp, border_mode,
808                           subsample, dilation, conv_mode, precision,
809                           algo, num_groups)
810 def dnn_gradinput(kerns, topgrad, img_shp, border_mode='valid',
811                   subsample=(1, 1), dilation=(1, 1), conv_mode='conv',
812                   precision=None, algo=None, num_groups=1):
813     return _dnn_gradinput(kerns, topgrad, img_shp, border_mode=border_mode, subsample=subsample, dilation=dilation,
814                           conv_mode=conv_mode, algo=algo, precision=precision, num_groups=num_groups)
815 def dnn_gradinput3d(kerns, topgrad, img_shp, border_mode='valid',
816                     subsample=(1, 1, 1), dilation=(1, 1, 1), conv_mode='conv',
817                     precision=None, algo=None, num_groups=1):
818     return dnn_gradinput(kerns, topgrad, img_shp, border_mode, subsample,
819                          dilation, conv_mode, precision, algo,
820                          num_groups)
821 class GpuDnnPoolDesc(Op):
822     __props__ = ('ws', 'stride', 'mode', 'pad')
823     def c_headers(self):
824         return ['cudnn.h', 'cudnn_helper.h']
825     def c_header_dirs(self):
826         return [gpuarray_helper_inc_dir(), config.dnn.include_path]
827     def c_libraries(self):
828         return ['cudnn']
829     def c_lib_dirs(self):
830         return [config.dnn.library_path]
831     def do_constant_folding(self, node):
832         return False
833     def __init__(self, ws=(1, 1), stride=(1, 1), mode='max', pad=(0, 0)):
834         if mode == 'average':
835             mode = 'average_inc_pad'
836         assert mode in ('max', 'average_inc_pad', 'average_exc_pad')
837         self.mode = mode
838         assert len(ws) == len(stride) and len(stride) == len(pad)
839         assert len(ws) in (2, 3)
840         self.ws = ws
841         self.stride = stride
842         self.pad = pad
843     def get_ndim(self):
844         return len(self.ws)
845     def __setstate__(self, d):
846         self.__dict__.update(d)
847         if not hasattr(self, 'pad'):
848             self.pad = (0, 0)
849     def make_node(self):
850         node = Apply(self, [],
851                      [CUDNNDataType("cudnnPoolingDescriptor_t",
852                                     freefunc="cudnnDestroyPoolingDescriptor")()])
853         out = node.outputs[0]
854         out.tag.values_eq_approx = tensor.type.values_eq_approx_always_true
855         return node
856     def c_code(self, node, name, inputs, outputs, sub):
857         desc, = outputs
858         if self.mode == 'max':
859             mode_flag = 'CUDNN_POOLING_MAX'
860         elif self.mode == "average_inc_pad":
861             mode_flag = 'CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING'
862         elif self.mode == "average_exc_pad":
863             mode_flag = 'CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING'
864         else:
865             raise NotImplementedError("Unsupported pooling model.")
866         return """
867 {
868   cudnnStatus_t err;
869   if ((err = cudnnCreatePoolingDescriptor(&amp;%(desc)s)) != CUDNN_STATUS_SUCCESS) {
870     PyErr_Format(PyExc_MemoryError, "could not allocate pooling "
871                  "descriptor: %%s", cudnnGetErrorString(err));
872     %(fail)s
873   }
874   static const int win[%(nd)d] = {%(win)s};
875   static const int pad[%(nd)d] = {%(pad)s};
876   static const int str[%(nd)d] = {%(str)s};
877     err = cudnnSetPoolingNdDescriptor(%(desc)s, %(mode_flag)s, CUDNN_PROPAGATE_NAN, %(nd)d, win, pad, str);
878   if (err != CUDNN_STATUS_SUCCESS) {
879     PyErr_Format(PyExc_RuntimeError, "could not set op descriptor: %%s",
880                  cudnnGetErrorString(err));
881     %(fail)s
882 <a name="7"></a>  }
883 }
884            nd=self.get_ndim<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(), win=', '.join(map(str, self.ws)),
885            pad=', '.join(map(str, self.pad)),
886            str=', '.join(map(str, self.</b></font>stride)))
887     def c_code_cache_version(self):
888         return (4, version())
889 class GpuDnnPoolBase(DnnBase):
890     c_file = None
891     c_function = None
892     _f16_ok = True
893     __props__ = ('mode',)
894     check_input = False
895     params_type = ParamsType(mode=cudnn.cudnnPoolingMode_t,
896                              handle=handle_type)
897     def __init__(self, mode='max'):
898         DnnBase.__init__(self, [self.c_file], self.c_function)
899         if mode == 'average':
900             mode = 'average_inc_pad'
901         assert cudnn.cudnnPoolingMode_t.has_alias(mode)
902         self.mode = mode
903 class GpuDnnPool(GpuDnnPoolBase):
904     c_file = "c_code/dnn_pool.c"
905     c_function = "APPLY_SPECIFIC(dnn_pool)"
906     def make_node(self, img, ws, stride, pad):
907         ctx_name = infer_context_name(img)
908         img = as_gpuarray_variable(img, ctx_name)
909         ws = tensor.as_tensor_variable(ws)
910         stride = tensor.as_tensor_variable(stride)
911         pad = tensor.as_tensor_variable(pad)
912         assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim
913         assert ws.type.ndim == 1
914         return Apply(self, [img, ws, stride, pad], [img.type()])
915     def infer_shape(self, node, shape):
916         w = node.inputs[1]
917 <a name="6"></a>        s = node.inputs[2]
918         p = node.inputs[3]
919         res <font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= [shape[0][0], shape[0][1],
920                (shape[0][2] + 2 * p[0] - w[0]) // s[0] + 1,
921                (shape[0][3] + 2 * p[1] - w[1]) // s[</b></font>1] + 1
922                ]
923         if node.inputs[0].ndim == 5:
924             res.append((shape[0][4] + 2 * p[2] - w[2]) // s[2] + 1)
925         return [res]
926     def L_op(self, inp, outputs, grads):
927         img, ws, stride, pad = inp
928         grad, = grads
929         grad = gpu_contiguous(grad)
930         out, = outputs
931         g_out = GpuDnnPoolGrad(mode=self.mode)(img, out, grad, ws, stride, pad)
932         return g_out, theano.gradient.DisconnectedType()(), theano.gradient.DisconnectedType()(), theano.gradient.DisconnectedType()()
933     def connection_pattern(self, node):
934         return [[1], [0], [0], [0]]
935 class GpuDnnPoolGrad(GpuDnnPoolBase):
936     c_file = "c_code/dnn_pool_grad.c"
937     c_function = "APPLY_SPECIFIC(dnn_pool_grad)"
938     def make_node(self, inp, out, out_grad, ws, stride, pad):
939         ctx_name = infer_context_name(inp, out, out_grad)
940         inp = as_gpuarray_variable(inp, ctx_name)
941         assert (inp.ndim in [4, 5])
942         out_grad = as_gpuarray_variable(out_grad, ctx_name)
943         assert (out_grad.ndim in [4, 5])
944         out = as_gpuarray_variable(out, ctx_name)
945         assert(out.ndim in [4, 5])
946         assert (out_grad.ndim == inp.ndim)
947         assert (inp.ndim == out.ndim)
948         ws = tensor.as_tensor_variable(ws)
949         stride = tensor.as_tensor_variable(stride)
950         pad = tensor.as_tensor_variable(pad)
951         assert ws.type.ndim == stride.type.ndim and ws.type.ndim == pad.type.ndim
952         assert ws.type.ndim == 1
953         return Apply(self, [inp, out, out_grad, ws, stride, pad], [inp.type()])
954     def infer_shape(self, node, shape):
955         return [shape[0]]
956 def dnn_pool(img, ws, stride=None, mode='max', pad=None):
957     img = gpu_contiguous(img)
958     if stride is None:
959         stride = (1,) * len(ws)
960     if pad is None:
961         pad = (0,) * len(ws)
962     if mode == "sum":
963         ret = GpuDnnPool(mode="average_inc_pad")(img, ws, stride, pad)
964         context_name = ret.type.context_name
965         window_elem = theano.tensor.prod(ws).astype(ret.dtype)
966         return as_gpuarray_variable(ret * window_elem, context_name)
967     return GpuDnnPool(mode=mode)(img, ws, stride, pad)
968 class GpuDnnSoftmaxBase(DnnBase):
969     __props__ = ('mode', 'algo')
970     check_input = False
971     params_type = ParamsType(algo=cudnn.cudnnSoftmaxAlgorithm_t,
972                              mode=cudnn.cudnnSoftmaxMode_t,
973                              handle=handle_type)
974     def __init__(self, algo, mode):
975         DnnBase.__init__(self, [self.file], self.c_func)
976         assert cudnn.cudnnSoftmaxAlgorithm_t.has_alias(algo)
977         self.algo = algo
978         assert cudnn.cudnnSoftmaxMode_t.has_alias(mode)
979         self.mode = mode
980     def infer_shape(self, node, shape):
981         if self.direction == 'forward':
982             return [shape[0]]
983         else:
984             return [shape[1]]
985 class GpuDnnSoftmax(GpuDnnSoftmaxBase):
986     _f16_ok = True
987     direction = "forward"
988     file = "c_code/dnn_softmax.c"
989     c_func = "APPLY_SPECIFIC(softmax)"
990     def make_node(self, x):
991         x = as_gpuarray_variable(x, infer_context_name(x))
992         assert x.ndim == 4
993         return Apply(self, [x], [x.type()])
994     def L_op(self, inp, outputs, grads):
995         x, = inp
996         g_sm, = grads
997         sm, = outputs
998         return [GpuDnnSoftmaxGrad(
999                 self.algo,
1000                 self.mode
1001                 )(g_sm, sm)]
1002 class GpuDnnSoftmaxGrad(GpuDnnSoftmaxBase):
1003     _f16_ok = True
1004     direction = 'backward'
1005     file = "c_code/dnn_softmax_grad.c"
1006     c_func = "APPLY_SPECIFIC(softmax_grad)"
1007     def make_node(self, dy, sm):
1008         ctx_name = infer_context_name(dy, sm)
1009         dy = as_gpuarray_variable(dy, ctx_name)
1010         sm = as_gpuarray_variable(sm, ctx_name)
1011         assert dy.ndim == 4
1012         assert sm.ndim == 4
1013         return Apply(self, [dy, sm], [sm.type()])
1014 class GpuDnnReduction(DnnBase):
1015     check_input = False
1016     _f16_ok = True
1017     _cop_num_outputs = 2
1018     __props__ = ('red_op', 'axis', 'acc_dtype', 'dtype', 'return_indices')
1019     params_type = ParamsType(red_op=cudnn.cudnnReduceTensorOp_t,
1020                              acc_dtype=cudnn.cudnnDataType_t,
1021                              c_axis=uint32_t,
1022                              handle=handle_type)
1023     def __init__(self, red_op, axis, acc_dtype, dtype, return_indices):
1024         DnnBase.__init__(self, ['c_code/dnn_redux.c'], 'APPLY_SPECIFIC(dnn_redux)')
1025         assert cudnn.cudnnReduceTensorOp_t.has_alias(red_op)
1026         self.red_op = red_op
1027         assert acc_dtype in ['float16', 'float32', 'float64']
1028         self.acc_dtype = acc_dtype
1029         assert dtype in ['float16', 'float32', 'float64']
1030         self.dtype = dtype
1031         if axis is not None:
1032             if len(axis) &gt; 8:
1033                 raise ValueError('Too many axes to reduce on')
1034             if any(a &gt;= 8 for a in axis):
1035                 raise ValueError('Axes larger than 8 not supported')
1036             axis = tuple(axis)
1037         self.c_axis = self._convert_axis(axis)
1038         self.axis = axis
1039         if return_indices and (red_op != 'maximum' and red_op != 'minimum'):
1040             raise ValueError("Can't request indices for something other than"
1041                              " minimum or maximum")
1042         self.return_indices = return_indices
1043     def _convert_axis(self, axis):
1044         if axis is None:
1045             return np.uint32(-1)
1046         else:
1047             return reduce(lambda a, b: a | b, map(lambda a: 1 &lt;&lt; a, axis), 0)
1048     def make_node(self, inp):
1049         ctx_name = infer_context_name(inp)
1050         inp = as_gpuarray_variable(inp, ctx_name)
1051         inp = gpu_contiguous(inp)
1052         if inp.ndim &gt; 8:
1053             raise ValueError("cuDNN reduction doesn't support nd &gt; 8")
1054         assert inp.dtype in ['float16', 'float32', 'float64']
1055         if inp.dtype == 'float64':
1056             assert self.acc_dtype == 'float64'
1057         if inp.dtype == 'float32':
1058             assert self.acc_dtype == 'float32'
1059         if inp.dtype == 'float16':
1060             assert self.acc_dtype != 'float64'
1061         bcast = []
1062         for i in range(inp.ndim):
1063             if not (self.c_axis &amp; (1 &lt;&lt; i)):
1064                 bcast.append(inp.broadcastable[i])
1065         outs = [inp.type.clone(dtype=self.dtype, broadcastable=bcast)()]
1066         if self.return_indices:
1067             outs.append(GpuArrayType(dtype='uint32', broadcastable=bcast,
1068                                      context_name=ctx_name)())
1069         return Apply(self, [inp], outs)
1070 class GpuDnnBatchNorm(DnnBase):
1071     __props__ = ('mode', 'running_averages', 'inplace_running_mean',
1072                  'inplace_running_var', 'inplace_output')
1073     _cop_num_inputs = 7
1074     _cop_num_outputs = 5
1075     check_input = False
1076     params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
1077                              inplace_output=bool_t,
1078                              inplace_running_mean=bool_t,
1079                              inplace_running_var=bool_t,
1080                              handle=handle_type)
1081     def __init__(self, mode='per-activation', running_averages=False,
1082                  inplace_running_mean=False, inplace_running_var=False,
1083                  inplace_output=False):
1084         DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm.c'],
1085                          'dnn_batchnorm_op')
1086         assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
1087         self.mode = mode
1088         self.running_averages = running_averages
1089         self.inplace_output = inplace_output
1090         self.inplace_running_mean = inplace_running_mean
1091         self.inplace_running_var = inplace_running_var
1092         self.destroy_map = {}
1093         if self.inplace_output:
1094             self.destroy_map[0] = [0]
1095         if self.running_averages and self.inplace_running_mean:
1096             self.destroy_map[3] = [5]
1097         if self.running_averages and self.inplace_running_var:
1098             self.destroy_map[4] = [6]
1099     def __setstate__(self, d):
1100         self.__dict__.update(d)
1101         if not hasattr(self, 'running_average_factor'):
1102             self.running_average_factor = 0
1103         if not hasattr(self, 'running_averages'):
1104             self.running_averages = False
1105         if not (hasattr(self, 'inplace_running_mean') and
1106                 hasattr(self, 'inplace_running_var') and
1107                 hasattr(self, 'inplace_output')):
1108             self.inplace_running_mean = False
1109             self.inplace_running_var = False
1110             self.inplace_output = False
1111             self.destroy_map = {}
1112     def infer_shape(self, node, shape):
1113         return [shape[0]] + [shape[1]] * (len(node.outputs) - 1)
1114     def make_node(self, x, scale, bias, epsilon=1e-4,
1115                   running_average_factor=0.1,
1116                   running_mean=None, running_var=None):
1117         assert x.ndim == scale.ndim == bias.ndim
1118         assert x.ndim in (4, 5)
1119         assert self.running_averages == (running_mean is not None) == (running_var is not None)
1120         assert (running_mean is None or running_mean.ndim == x.ndim)
1121         assert (running_var is None or running_var.ndim == x.ndim)
1122         ctx_name = infer_context_name(x, scale, bias)
1123         x = as_gpuarray_variable(x, ctx_name)
1124         scale = as_gpuarray_variable(scale, ctx_name)
1125         bias = as_gpuarray_variable(bias, ctx_name)
1126         epsilon = as_scalar(epsilon).astype('float64')
1127         running_average_factor = as_scalar(running_average_factor).astype('float64')
1128         inputs = [x, scale, bias, epsilon, running_average_factor]
1129         output_types = [x.type(), scale.type(), scale.type()]
1130         if running_mean is not None and running_var is not None:
1131             inputs.append(as_gpuarray_variable(running_mean, ctx_name))
1132             inputs.append(as_gpuarray_variable(running_var, ctx_name))
1133             output_types.append(scale.type())
1134             output_types.append(scale.type())
1135         return Apply(self, inputs, output_types)
1136     def L_op(self, inputs, outputs, grads):
1137         x, scale, bias, epsilon, running_average_factor = inputs[:5]
1138         dy = grads[0]
1139         _, x_mean, x_invstd = outputs[:3]
1140         disconnected_outputs = [
1141             DisconnectedType()(),  # epsilon
1142             DisconnectedType()()]  # running_average_factor
1143         for i in range(5, len(inputs)):
1144             disconnected_outputs.append(DisconnectedType()())
1145         return GpuDnnBatchNormGrad(self.mode)(
1146             x, dy, scale, x_mean, x_invstd, epsilon) + disconnected_outputs
1147     def connection_pattern(self, node):
1148         patterns = [[True, True, True],     # x
1149                     [True, True, True],     # scale
1150                     [True, True, True],     # bias
1151                     [False, False, False],  # epsilon
1152                     [False, False, False]]  # running_average_factor
1153         for i in range(5, len(node.inputs)):
1154             patterns[0].append(True)
1155             for pattern in patterns[1:]:
1156                 pattern.append(False)
1157             patterns.append([False] * (3 + i - 5) + [True])
1158         return patterns
1159 class GpuDnnBatchNormInference(DnnBase):
1160     __props__ = ('mode', 'inplace')
1161     check_input = False
1162     params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
1163                              inplace=bool_t,
1164                              handle=handle_type)
1165     def __init__(self, mode='per-activation', inplace=False):
1166         DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm_inf.c'],
1167                          'dnn_batchnorm_op')
1168         assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
1169         self.mode = mode
1170         self.inplace = bool(inplace)
1171         if self.inplace:
1172             self.destroy_map = {0: [0]}
1173     def __setstate__(self, d):
1174         self.__dict__.update(d)
1175         if not hasattr(self, 'inplace'):
1176             self.inplace = False
1177     def infer_shape(self, node, shape):
1178 <a name="11"></a>        return [shape[0]]
1179     def make_node(self, x, scale, bias, estimated_mean, estimated_variance, epsilon=1e-4):
1180         ctx_name <font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= infer_context_name(x, scale, bias, estimated_mean,
1181                                       estimated_variance)
1182         x = as_gpuarray_variable(x, ctx_name)
1183         scale = as_gpuarray_variable(scale, ctx_name)
1184         bias = as_gpuarray_variable(bias, ctx_name)
1185         estimated_mean = as_gpuarray_variable(estimated_mean, ctx_name)
1186         estimated_variance = as_gpuarray_variable(estimated_variance, ctx_name)
1187         epsilon = as_scalar(epsilon).</b></font>astype('float64')
1188         assert x.ndim == scale.ndim == bias.ndim == estimated_mean.ndim == estimated_variance.ndim
1189         assert x.ndim in (4, 5)
1190         return Apply(self, [x, scale, bias, estimated_mean, estimated_variance, epsilon], [x.type()])
1191     def grad(self, inputs, grads):
1192         x, scale, bias, est_mean, est_var, epsilon = inputs
1193         dy = grads[0]
1194         if self.mode == "per-activation":
1195             axes = (0,)
1196         elif self.mode == "spatial":
1197             axes = (0,) + tuple(range(2, x.ndim))
1198         scale, bias, est_mean, est_var = (theano.tensor.addbroadcast(t, *axes)
1199                                           for t in (scale, bias, est_mean, est_var))
1200         est_var_eps = est_var + epsilon
1201         est_std = theano.tensor.sqrt(est_var_eps)
1202         two = theano.tensor.constant(2.)
1203 <a name="10"></a>
1204         dx = dy * (scale / est_std)
1205         dscale <font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (dy * (x - est_mean)).sum(axes, keepdims=True) / est_std
1206         dbias = dy.sum(axes, keepdims=True)
1207         dmean = -dy.sum(axes, keepdims=True) * (scale / est_std)
1208         dvar = -(dy * (x - est_mean)).sum(</b></font>axes, keepdims=True) * (scale / (two * est_var_eps * est_std))
1209         return [dx, dscale, dbias, dmean, dvar, DisconnectedType()()]
1210     def connection_pattern(self, node):
1211         return [[True], [True], [True], [True], [True], [False]]
1212 class GpuDnnBatchNormGrad(DnnBase):
1213     __props__ = ('mode',)
1214     check_input = False
1215     params_type = ParamsType(mode=cudnn.cudnnBatchNormMode_t,
1216                              handle=handle_type)
1217     def __init__(self, mode='per-activation'):
1218         DnnBase.__init__(self, ['c_code/dnn_batchnorm_base.c', 'c_code/dnn_batchnorm_grad.c'],
1219                          'dnn_batchnorm_grad')
1220         assert cudnn.cudnnBatchNormMode_t.has_alias(mode)
1221 <a name="13"></a>        self.mode = mode
1222     def make_node(self, x, dy, scale, x_mean, x_invstd, epsilon=1e-4):
1223         ctx_name <font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= infer_context_name(x, dy, scale, x_mean, x_invstd)
1224         x = as_gpuarray_variable(x, ctx_name)
1225         dy = as_gpuarray_variable(dy, ctx_name)
1226         scale = as_gpuarray_variable(scale, ctx_name)
1227         x_mean = as_gpuarray_variable(x_mean, ctx_name)
1228         x_invstd = as_gpuarray_variable(x_invstd, ctx_name)
1229         epsilon = as_scalar(</b></font>epsilon).astype('float64')
1230         assert x.ndim == dy.ndim == scale.ndim == x_mean.ndim == x_invstd.ndim
1231         assert x.ndim in (4, 5)
1232         return Apply(self, [x, dy, scale, x_mean, x_invstd, epsilon], [x.type(), scale.type(), scale.type()])
1233     def infer_shape(self, node, shape):
1234         return [shape[0], shape[2], shape[2]]
1235 gpudata_type = CDataType('gpudata *', 'gpudata_release')
1236 dropoutdesc_type = CUDNNDataType('cudnnDropoutDescriptor_t',
1237                                  'cudnnDestroyDropoutDescriptor')
1238 class GpuDnnDropoutOp(DnnBase):
1239     __props__ = ('inplace',)
1240     def __init__(self, inplace=False):
1241         DnnBase.__init__(self, ["c_code/dnn_dropout_fwd.c"], "dnn_dropout_fwd")
1242         self.inplace = inplace
1243         if self.inplace:
1244             self.destroy_map = {1: [2]}
1245     def make_node(self, inp, descriptor, state):
1246         ctx_name = infer_context_name(inp)
1247         inp = as_gpuarray_variable(inp, ctx_name)
1248         return Apply(self, [inp, descriptor, state],
1249                      [inp.type(), state.type(), gpudata_type()])
1250     def prepare_node(self, node, storage_map, compute_map, impl):
1251         assert self.inplace, "GpuDnnDropoutOp not inplace"
1252 class _DropoutDescriptor(DnnBase):
1253     __props__ = ('context_name',)
1254     def __init__(self, context_name):
1255         DnnBase.__init__(self, ["c_code/dnn_dropout_desc.c"], "dnn_dropout_desc")
1256         self.context_name = context_name
1257     def dnn_context(self, node):
1258         return self.context_name
1259     def do_constant_folding(self, node):
1260         return False
1261     def make_node(self, dropout, seed, context_name):
1262         dropout = as_scalar(dropout).astype('float32')
1263         seed = as_scalar(seed).astype('uint64')
1264         assert context_name == self.context_name
1265         context = gpu_context_type.make_constant(get_context(context_name))
1266         return Apply(self, [dropout, seed, context],
1267                      [dropoutdesc_type(),
1268                       GpuArrayType('uint8', (False,),
1269                                    context_name=context_name)()])
1270     def c_code_cache_version_apply(self, node):
1271         return None
1272 def _make_dropout_desc(dropout, seed, context_name):
1273     desc, states = theano.function(
1274         [],
1275         _DropoutDescriptor(context_name)(dropout, seed, context_name),
1276         theano.Mode(optimizer=None),
1277         profile=False)()
1278     return desc, states
1279 def dropout(x, dropout=0.0, seed=4242):
1280     desc, states = _make_dropout_desc(dropout, seed, x.type.context_name)
1281     y, odesc = GpuDnnDropoutOp()(x, desc)
1282     return y, desc, odesc, states
1283 rnndesc_type = CUDNNDataType('cudnnRNNDescriptor_t',
1284                              'cudnnDestroyRNNDescriptor')
1285 def as_i32(v):
1286     return as_scalar(v).astype('int32')
1287 class _RNNDescriptor(DnnBase):
1288     __props__ = ('context_name',)
1289     def __init__(self, context_name):
1290         if version() &lt; 5005:
1291             raise RuntimeError("cudnn RNN require cudnn v5 final or higher.")
1292         DnnBase.__init__(self, ["c_code/dnn_rnn_desc.c"], "dnn_rnn_desc")
1293         self.context_name = context_name
1294     def dnn_context(self, node):
1295         return self.context_name
1296     def do_constant_folding(self, node):
1297         return False
1298     def make_node(self, hidden_size, num_layers, ddesc, input_mode,
1299                   direction_mode, rnn_mode, dtype):
1300         hidden_size = as_i32(hidden_size)
1301         num_layers = as_i32(num_layers)
1302         if version() &lt; 5005:
1303             raise RuntimeError("cudnn RNN require cudnn v5 final or higher.")
1304         if input_mode == 'linear':
1305             input_mode = as_i32(0)
1306         elif input_mode == 'skip':
1307             input_mode = as_i32(1)
1308         else:
1309             raise ValueError("input_mode")
1310         if direction_mode == 'unidirectional':
1311             direction_mode = as_i32(0)
1312         elif direction_mode == 'bidirectional':
1313             direction_mode = as_i32(1)
1314         else:
1315             raise ValueError("direction_mode")
1316         if rnn_mode == 'rnn_relu':
1317             rnn_mode = as_i32(0)
1318         elif rnn_mode == 'rnn_tanh':
1319             rnn_mode = as_i32(1)
1320         elif rnn_mode == 'lstm':
1321             rnn_mode = as_i32(2)
1322         elif rnn_mode == 'gru':
1323             rnn_mode = as_i32(3)
1324         else:
1325             raise ValueError("rnn_mode")
1326         dtype = as_i32(gpuarray.dtype_to_typecode(dtype))
1327         return Apply(self, [hidden_size, num_layers,
1328                             dropoutdesc_type.make_constant(ddesc),
1329                             input_mode, direction_mode, rnn_mode, dtype],
1330                      [rnndesc_type()])
1331 def _make_rnn_desc(hidden_size, num_layers, ddesc, rnn_mode,
1332                    input_mode, direction_mode, dtype, context_name):
1333     desc = theano.function(
1334         [],
1335         _RNNDescriptor(context_name)(hidden_size, num_layers, ddesc,
1336                                      input_mode, direction_mode,
1337                                      rnn_mode, dtype),
1338         theano.Mode(optimizer=None),
1339         profile=False)()
1340     return desc
1341 class _RNNParamSize(DnnBase):
1342     __props__ = ('context_name',)
1343     def __init__(self, context_name):
1344         DnnBase.__init__(self, ["c_code/dnn_rnn_paramsize.c"],
1345                          "dnn_rnn_paramsize")
1346         self.context_name = context_name
1347     def dnn_context(self, node):
1348         return self.context_name
1349     def do_constant_folding(self, node):
1350         return False
1351     def make_node(self, desc, input_size, typecode):
1352         input_size = as_tensor_variable(input_size).astype('uint64')
1353         typecode = as_i32(typecode)
1354         return Apply(self, [rnndesc_type.make_constant(desc), input_size,
1355                             typecode],
1356                      [get_scalar_type('uint64')()])
1357 def _get_param_size(desc, input_size, dtype, context_name):
1358     typecode = gpuarray.dtype_to_typecode(dtype)
1359     return theano.function(
1360         [],
1361         _RNNParamSize(context_name)(desc, input_size, typecode),
1362         theano.Mode(optimizer=None),
1363         profile=False)()
1364 class _RNNSplitParams(DnnBase):
1365     __props__ = ('rnn_mode',)
1366     def __init__(self, rnn_mode):
1367         DnnBase.__init__(self)
1368         self.rnn_mode = rnn_mode
1369     def make_node(self, w, desc, layer, isize, typecode):
1370         w = as_gpuarray_variable(w, infer_context_name(w))
1371         assert w.ndim == 1
1372         layer = as_scalar(layer).astype('int32')
1373         isize = as_tensor_variable(isize).astype('uint64')
1374         assert isize.ndim == 1
1375         typecode = as_scalar(typecode).astype('int32')
1376         _1d = GpuArrayType(w.type.dtype, [False],
1377                            context_name=w.type.context_name)
1378         _2d = GpuArrayType(w.type.dtype, [False, False],
1379                            context_name=w.type.context_name)
1380         outputs = []
1381         if self.rnn_mode == 'rnn_relu' or self.rnn_mode == 'rnn_tanh':
1382             outputs.extend([_2d(), _1d()])  # recurrent
1383         elif self.rnn_mode == 'lstm':
1384             outputs<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.extend([_2d(), _1d()])  # input input
1385             outputs.extend([_2d(), _1d()])  # input forget
1386             outputs.extend([_2d(), _1d()])  # input newmem
1387             outputs.extend([_2d(), _1d()])  # input output
1388             outputs.extend([_2d(), _1d()])  # recur input
1389             outputs.extend([_2d(), _1d()])  # recur forget
1390             outputs.extend([_2d(), _1d()])  # recur output
1391         elif self.</b></font>rnn_mode == 'gru':
1392             outputs.extend<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>([_2d(), _1d()])  # input reset
1393             outputs.extend([_2d(), _1d()])  # input update
1394             outputs.extend([_2d(), _1d()])  # input newmem
1395             outputs.extend([_2d(), _1d()])  # recur reset
1396             outputs.extend([_2d(), _1d(</b></font>)])  # recur update
1397             outputs.extend([_2d(), _1d()])  # recur newmem
1398         return Apply(self, [w, layer, rnndesc_type.make_constant(desc),
1399                             isize, typecode], outputs)
1400     def c_code(self, node, name, inputs, outputs, sub):
1401         kw = dict(fail=sub['fail'], w=inputs[0], layer=inputs[1],
1402                   desc=inputs[2], isize=inputs[3], typecode=inputs[4],
1403                   handle=sub['params'])
1404         code = """
1405   cudnnTensorDescriptor_t xdesc;
1406   cudnnFilterDescriptor_t wdesc;
1407   cudnnFilterDescriptor_t odesc;
1408   size_t nshp[2];
1409   void *w;
1410   void *o;
1411   ptrdiff_t off;
1412   size_t bshp;
1413   cudnnStatus_t err;
1414   cudnnDataType_t dt;
1415   cudnnTensorFormat_t tf;
1416   int nd;
1417   int dims[3];
1418   int strs[3];
1419   if (PyArray_DIM(%(isize)s, 0) != 2) {
1420     PyErr_SetString(PyExc_ValueError, "input_size should be of length two");
1421     %(fail)s;
1422   }
1423   switch (%(typecode)s) {
1424   case GA_FLOAT:
1425     dt = CUDNN_DATA_FLOAT;
1426     break;
1427   case GA_DOUBLE:
1428     dt = CUDNN_DATA_DOUBLE;
1429     break;
1430   case GA_HALF:
1431     dt = CUDNN_DATA_HALF;
1432     break;
1433   default:
1434     PyErr_SetString(PyExc_ValueError, "Unsupported data type");
1435     %(fail)s;
1436   }
1437   err = cudnnCreateTensorDescriptor(&amp;xdesc);
1438   if (err != CUDNN_STATUS_SUCCESS) {
1439     PyErr_SetString(PyExc_RuntimeError, "Could not create xdesc");
1440     %(fail)s;
1441   }
1442   dims[0] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 0);
1443   dims[1] = *(npy_uint64 *)PyArray_GETPTR1(%(isize)s, 1);
1444   dims[2] = 1;
1445   strs[0] = dims[2] * dims[1];
1446   strs[1] = dims[2];
1447   strs[2] = 1;
1448   err = cudnnSetTensorNdDescriptor(xdesc, dt, 3, dims, strs);
1449   if (err != CUDNN_STATUS_SUCCESS) {
1450     cudnnDestroyTensorDescriptor(xdesc);
1451     PyErr_Format(PyExc_RuntimeError, "Could not set xdesc: %%s",
1452                  cudnnGetErrorString(err));
1453     %(fail)s;
1454   }
1455   if (c_make_filter(%(w)s, &amp;wdesc)) {
1456     cudnnDestroyTensorDescriptor(xdesc);
1457     %(fail)s
1458   }
1459   err = cudnnCreateFilterDescriptor(&amp;odesc);
1460   if (err != CUDNN_STATUS_SUCCESS) {
1461     PyErr_SetString(PyExc_RuntimeError, "could not create odesc");
1462     cudnnDestroyTensorDescriptor(xdesc);
1463     cudnnDestroyFilterDescriptor(wdesc);
1464     %(fail)s
1465   }
1466   w = PyGpuArray_DEV_DATA(%(w)s);
1467   nshp[0] = PyGpuArray_DIM(%(w)s, 0);
1468   nshp[1] = 1;
1469         for i in range(len(outputs) // 2):
1470             code += get_params(i, outputs[2 * i], outputs[(2 * i) + 1])
1471         code += """
1472   cudnnDestroyTensorDescriptor(xdesc);
1473   cudnnDestroyFilterDescriptor(wdesc);
1474   cudnnDestroyFilterDescriptor(odesc);
1475     An object that allow us to use CuDNN RNN implementation.
1476     TODO: make an example how to use. You can check Theano tests
1477     test_dnn_rnn_gru() and test_dnn_rnn_lstm() in the file
1478     theano/gpuarray/tests/test_dnn.py for now.
1479     Parameters
1480     ----------
1481     dtype : data type of computation
1482     hidden_size : int
1483         hidden layer dimension.
1484     num_layers : int
1485         number of the recurrent layer you want to set.
1486     rnn_mode : {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'}
1487         rnn_relu: A single-gate recurrent neural network with a ReLU activation function.
1488         .. math::
1489         h_t=ReLU(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})
1490         rnn_tanh: A single-gate recurrent neural network with a tanh activation function.
1491         .. math::
1492         h_t=tanh(W_ix_t+U_ih_{t-1}+b_{wi}+b_{Ri})
1493         lstm: A four-gate Long Short-Term Memory network with no peephole connections.
1494         gru: A three-gate network consisting of Gated Recurrent Units.
1495     input_mode : {'linear', 'skip'}
1496         linear: input will be multiplied by a biased matrix
1497         skip: No operation is performed on the input.  The size must match the hidden size.
1498     direction_mode : {'unidirectional', 'bidirectional'}
1499         unidirectional: The network operates recurrently from the first input to the last.
1500         bidirectional: The network operates from first to last then from last to first and concatenates the results at each layer.
1501         Get the size of the shared variable for the parameters of the RNN.
1502         This will return a size (in items) necessary to store all the
1503         parameters for the RNN.  You should allocate a variable of
1504         that size to store those parameters.  The order and layout of
1505         the parameters is opaque.
1506         Parameters
1507         ----------
1508         input_size: (int, int)
1509             Size of the input blocks
1510         Split the opaque parameter block into components.
1511         Parameters
1512         ----------
1513         w: GpuArraySharedVariable
1514             opaque parameter block
1515         layer: int
1516             ID of the layer
1517         input_size: (int, int)
1518             Size of the input blocks
1519         Apply the RNN to some data
1520         Parameters
1521         ----------
1522         w:
1523             opaque parameter block
1524         x:
1525             input
1526         hx:
1527             initial hidden state
1528         cx:
1529             initial cell state (for LSTM)
1530     Performs batch normalization of the given inputs, using the mean and
1531     variance of the inputs.
1532     Parameters
1533     ----------
1534     mode : {'per-activation', 'spatial'}
1535         Whether to normalize per activation or share normalization factors
1536         across spatial dimensions (i.e., all dimensions past the second).
1537     gamma : tensor
1538         Learnable scale factors. Must match the dimensionality of `inputs`,
1539         but have sizes of `1` for all axes normalized over (i.e., in the first
1540         dimension for ``mode='per-activation'`, and additionally in all
1541         dimensions past the second for ``mode='spatial'``).
1542     beta : tensor
1543         Learnable biases. Must match the tensor layout of `gamma`.
1544     epsilon : float
1545         Epsilon value used in the batch normalization formula. Minimum allowed
1546         value is 1e-5 (imposed by cuDNN).
1547     running_average_factor : float
1548         Factor for updating the values or `running_mean` and `running_var`.
1549         If the factor is close to one, the running averages will update quickly,
1550         if the factor is close to zero it will update slowly.
1551     running_mean : tensor or None
1552         Previous value of the running mean. If this is given, the new value
1553         ``running_mean * (1 - r_a_factor) + batch mean * r_a_factor``
1554         will be returned as one of the outputs of this function.
1555         `running_mean` and `running_var` should either both be given or
1556         both be None.
1557     running_var : tensor or None
1558         Previous value of the running variance. If this is given, the new value
1559         ``running_var * (1 - r_a_factor) + (m / (m - 1)) * batch var * r_a_factor``
1560         will be returned as one of the outputs of this function,
1561         where `m` is the product of lengths of the averaged-over dimensions.
1562         `running_mean` and `running_var` should either both be given or
1563         both be None.
1564     Returns
1565     -------
1566     out : tensor
1567         Batch-normalized inputs.
1568     mean : tensor
1569         Means of `inputs` across the normalization axes.
1570     invstd : tensor
1571         Inverse standard deviations of `inputs` across the normalization axes.
1572     new_running_mean : tensor
1573         New value of the running mean (only if both `running_mean` and
1574         `running_var` were given).
1575     new_running_var : tensor
1576         New value of the running variance (only if both `running_var` and
1577         `running_mean` were given).
1578     Notes
1579     -----
1580     Requires cuDNN 5 and Theano 0.9dev2 or more recent.
1581     For 4d tensors, returned values are equivalent to:
1582     .. code-block:: python
1583         axes = 0 if mode == 'per-activation' else (0, 2, 3)
1584         mean = inputs.mean(axes, keepdims=True)
1585         var = inputs.var(axes, keepdims=True)
1586         invstd = T.inv(T.sqrt(var + epsilon))
1587         out = (inputs - mean) * gamma * invstd + beta
1588         m = T.cast(T.prod(inputs.shape) / T.prod(mean.shape), 'float32')
1589         running_mean = running_mean * (1 - running_average_factor) + \\
1590                        mean * running_average_factor
1591         running_var = running_var * (1 - running_average_factor) + \\
1592                       (m / (m - 1)) * var * running_average_factor
1593     For 5d tensors, the axes are (0, 2, 3, 4).
1594         params_shape <font color="#e77471"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= gamma.shape
1595         inputs = theano.tensor.flatten(inputs, 5)
1596         gamma = theano.tensor.flatten(gamma, 5)
1597         beta = theano.</b></font>tensor.flatten(beta, 5)
1598         if running_averages:
1599             running_mean = theano.tensor.flatten(running_mean, 5)
1600             running_var = theano.tensor.flatten(running_var, 5)
1601     batchnorm_op = GpuDnnBatchNorm(mode=mode, running_averages=running_averages)
1602     if running_averages:
1603         out, mean, invstd, new_running_mean, new_running_var = batchnorm_op(
1604             gpu_contiguous(inputs), gpu_contiguous(gamma),
1605             gpu_contiguous(beta), epsilon=epsilon,
1606             running_average_factor=running_average_factor,
1607             running_mean=gpu_contiguous(running_mean),
1608             running_var=gpu_contiguous(running_var))
1609         if new_running_mean.broadcastable != running_mean.broadcastable:
1610             new_running_mean = tensor.patternbroadcast(new_running_mean, running_mean.broadcastable)
1611         if new_running_var.broadcastable != running_var.broadcastable:
1612             new_running_var = tensor.patternbroadcast(new_running_var, running_var.broadcastable)
1613         result = (out, mean, invstd, new_running_mean, new_running_var)
1614     else:
1615         result = batchnorm_op(gpu_contiguous(inputs), gpu_contiguous(gamma),
1616                               gpu_contiguous(beta), epsilon=epsilon)
1617     if ndim &lt; 4:
1618         result = tuple(theano.tensor.flatten(r, ndim) for r in result)
1619     elif ndim &gt; 5:
1620         result = (theano.tensor.reshape(result[0], inputs_shape),) + tuple(
1621             theano.tensor.reshape(r, params_shape) for r in result[1:])
1622     return result
1623 def dnn_batch_normalization_test(inputs, gamma, beta, mean, var,
1624                                  mode='per-activation', epsilon=1e-4):
1625     ndim = inputs.ndim
1626     if gamma.ndim != ndim or beta.ndim != ndim:
1627         raise ValueError("gamma and beta must be of the same dimensionality "
1628                          "as inputs; got %d and %d instead of %d" %
1629                          (gamma.ndim, beta.ndim, ndim))
1630     if mean.ndim != ndim or var.ndim != ndim:
1631         raise ValueError("mean and var must be of the same dimensionality "
1632                          "as inputs; got %d and %d instead of %d" %
1633                          (mean.ndim, var.ndim, ndim))
1634     if epsilon &lt; 1e-5:
1635         raise ValueError("epsilon must be at least 1e-5, got %f" % epsilon)
1636 <a name="3"></a>    if ndim &lt; 4:
1637         inputs = theano.tensor.shape_padright(inputs, 4 - ndim)
1638         gamma = theano.tensor.shape_padright(gamma, 4 - ndim)
1639         beta = theano.tensor.shape_padright<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(beta, 4 - ndim)
1640         mean = theano.tensor.shape_padright(mean, 4 - ndim)
1641         var = theano.tensor.shape_padright(var, 4 - ndim)
1642     elif ndim &gt; 5:
1643         inputs_shape = inputs.shape
1644         inputs = theano.tensor.flatten(inputs, 5)
1645         gamma = theano.tensor.flatten(gamma, 5)
1646         beta = theano.</b></font>tensor.flatten(beta, 5)
1647         mean = theano.tensor.flatten(mean, 5)
1648         var = theano.tensor.flatten(var, 5)
1649     batchnorm_op = GpuDnnBatchNormInference(mode=mode)
1650     result = batchnorm_op(gpu_contiguous(inputs), gpu_contiguous(gamma),
1651                           gpu_contiguous(beta), gpu_contiguous(mean),
1652                           gpu_contiguous(var), epsilon=epsilon)
1653     if ndim &lt; 4:
1654         result = theano.tensor.flatten(result, ndim)
1655     elif ndim &gt; 5:
1656         result = theano.tensor.reshape(result, inputs_shape)
1657     return result
1658 class GpuDnnTransformerGrid(DnnBase):
1659     __props__ = ()
1660     _cop_num_inputs = 2
1661     _cop_num_outputs = 1
1662     _f16_ok = True
1663     check_input = False
1664     def __init__(self):
1665         DnnBase.__init__(self, ["c_code/dnn_sptf_grid.c"], "APPLY_SPECIFIC(dnn_sptf_grid)")
1666     def make_node(self, theta, out_dims):
1667         context_name = infer_context_name(theta)
1668         theta = gpu_contiguous(as_gpuarray_variable(theta, context_name))
1669         assert theta.dtype in ('float16', 'float32', 'float64')
1670         assert theta.ndim == 3
1671         out_dims = cpu_contiguous(as_tensor_variable(out_dims))
1672         assert out_dims.dtype in theano.tensor.basic.integer_dtypes
1673         assert out_dims.ndim == 1
1674         out_dims = theano.tensor.basic.cast(out_dims, 'int64')
1675         grid = GpuArrayType(dtype=theta.dtype,
1676                             broadcastable=(theta.type.ndim + 1) * (False,),
1677                             context_name=context_name)()
1678         inputs = [theta, out_dims]
1679         outputs = [grid]
1680         return Apply(self, inputs, outputs)
1681     def grad(self, inputs, grads):
1682         theta, out_dims = inputs
1683         dgrid = grads[0]
1684         dtheta = GpuDnnTransformerGradT()(dgrid)
1685         return [dtheta, grad_not_implemented(self, 1, out_dims)]
1686 class GpuDnnTransformerSampler(DnnBase):
1687     __props__ = ()
1688     _cop_num_inputs = 2
1689     _cop_num_outputs = 1
1690     _f16_ok = True
1691     check_input = False
1692     def __init__(self):
1693         DnnBase.__init__(self, ["c_code/dnn_sptf_sampler.c"], "APPLY_SPECIFIC(dnn_sptf_sampler)")
1694     def make_node(self, img, grid):
1695         context_name = infer_context_name(img, grid)
1696         img = gpu_contiguous(as_gpuarray_variable(img, context_name))
1697         if img.type.ndim != 4:
1698             raise TypeError('img must be a 4D tensor')
1699         elif img.dtype not in ('float16', 'float32', 'float64'):
1700             raise TypeError('img type must be floating-point')
1701         grid = gpu_contiguous(as_gpuarray_variable(grid, context_name))
1702         if grid.type.ndim != 4:
1703             raise TypeError('grid must be a 4D tensor')
1704         elif grid.dtype not in ('float16', 'float32', 'float64'):
1705             raise TypeError('grid type must be floating-point')
1706         out = GpuArrayType(dtype=img.dtype,
1707                            broadcastable=img.type.ndim * (False,),
1708                            context_name=context_name)()
1709         inputs = [img, grid]
1710         outputs = [out]
1711         return Apply(self, inputs, outputs)
1712     def grad(self, inputs, grads):
1713         img, grid = inputs
1714         dy = grads[0]
1715         dimg, dgrid = GpuDnnTransformerGradI()(img, grid, dy)
1716         return [dimg, dgrid]
1717 class GpuDnnTransformerGradI(DnnBase):
1718     __props__ = ()
1719     _cop_num_inputs = 3
1720     _cop_num_outputs = 2
1721     _f16_ok = True
1722     check_input = False
1723     def __init__(self):
1724         DnnBase.__init__(self, ["c_code/dnn_sptf_gi.c"], "APPLY_SPECIFIC(dnn_sptf_gi)")
1725     def make_node(self, img, grid, dy):
1726         context_name = infer_context_name(img, grid, dy)
1727         img = as_gpuarray_variable(gpu_contiguous(img), context_name)
1728         if img.ndim != 4:
1729             raise TypeError('img must have 4 dimensions.')
1730         grid = as_gpuarray_variable(gpu_contiguous(grid), context_name)
1731         if img.ndim != grid.ndim:
1732             raise TypeError('grid should have the same number of dimensions as img')
1733         dy = as_gpuarray_variable(dy, context_name)
1734         if dy.ndim != 4:
1735             raise TypeError('dy must have 4 dimensions.')
1736         dimg = img.type()
1737         dgrid = grid.type()
1738         inputs = [img, grid, dy]
1739         outputs = [dimg, dgrid]
1740         return Apply(self, inputs, outputs)
1741 class GpuDnnTransformerGradT(DnnBase):
1742     __props__ = ()
1743     _cop_num_inputs = 1
1744     _cop_num_outputs = 1
1745 <a name="26"></a>    _f16_ok = True
1746     check_input = False
1747     <font color="#68818b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def __init__(self):
1748         DnnBase.__init__(self, ["c_code/dnn_sptf_gt.c"], "APPLY_SPECIFIC(dnn_sptf_gt)")
1749     def make_node(self, dgrid):
1750         context_name = infer_context_name(dgrid)
1751         dgrid = as_gpuarray_variable(dgrid, context_name)
1752         assert dgrid.dtype in ('float16'</b></font>, 'float32', 'float64')
1753         assert dgrid.ndim == 4
1754         dtheta = GpuArrayType(dtype=dgrid.dtype,
1755                               broadcastable=(dgrid.type.ndim - 1) * (False,),
1756                               context_name=context_name)()
1757         inputs = [dgrid]
1758         outputs = [dtheta]
1759         return Apply(self, inputs, outputs)
1760 def dnn_spatialtf(img, theta, scale_width=1, scale_height=1):
1761     out_dims = (<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>img.shape[0], img.shape[1],
1762                 theano.tensor.ceil(img.shape[2] * scale_height),
1763                 theano.tensor.</b></font>ceil(img.shape[3] * scale_width))
1764     out_dims = tuple([as_scalar(v).astype('int64') for v in out_dims])
1765     grid = GpuDnnTransformerGrid()(theta, out_dims)
1766     sampler = GpuDnnTransformerSampler()(img, grid)
1767     return sampler
1768 def local_abstractconv_cudnn_graph(op, context_name, inputs, outputs):
1769     if (not isinstance(op, (AbstractConv2d,
1770                             AbstractConv2d_gradWeights,
1771                             AbstractConv2d_gradInputs))):
1772         return
1773     if version(raises=False) &lt; 6000 and op.filter_dilation != (1, 1):
1774         return None
1775     if op.unshared:
1776         return None
1777     if isinstance(op.border_mode, tuple) and any(isinstance(p, tuple) for p in op.border_mode):
1778         return None
1779     inp1 = inputs[0]
1780     inp2 = inputs[1]
1781     if not dnn_available(inp1.type.context_name):
1782         return
1783     if op.filter_flip:
1784         conv_mode = 'conv'
1785     else:
1786         conv_mode = 'cross'
1787     if isinstance(op, AbstractConv2d):
1788         rval = dnn_conv(inp1, inp2,
1789                         border_mode=op.border_mode,
1790                         subsample=op.subsample,
1791                         dilation=op.filter_dilation,
1792                         direction_hint='forward!',
1793 <a name="20"></a>                        conv_mode=conv_mode,
1794                         num_groups=op.num_groups)
1795     elif isinstance(op, AbstractConv2d_gradWeights):
1796         shape = (<font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[1], inp1.shape[1] // op.num_groups,
1797                  inputs[2][0], inputs[2][1])
1798         rval = dnn_gradweight(inp1, inp2, shape,
1799                               border_mode=</b></font>op.border_mode,
1800                               subsample=op.subsample,
1801                               dilation=op.filter_dilation,
1802 <a name="24"></a>                              conv_mode=conv_mode,
1803                               num_groups=op.num_groups)
1804     elif isinstance(op, AbstractConv2d_gradInputs):
1805         shape = (<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[0], inp1.shape[1] * op.num_groups,
1806                  inputs[2][0], inputs[2][1])
1807         rval = dnn_gradinput(</b></font>inp1, inp2, shape,
1808                              border_mode=op.border_mode,
1809                              subsample=op.subsample,
1810                              dilation=op.filter_dilation,
1811                              conv_mode=conv_mode,
1812                              num_groups=op.num_groups)
1813     return [rval]
1814 def local_abstractconv3d_cudnn_graph(op, context_name, inputs, outputs):
1815     if (not isinstance(op, (AbstractConv3d,
1816                             AbstractConv3d_gradWeights,
1817                             AbstractConv3d_gradInputs))):
1818         return
1819     if version(raises=False) &lt; 6000 and op.filter_dilation != (1, 1, 1):
1820         return None
1821     inp1 = inputs[0]
1822     inp2 = inputs[1]
1823     if not dnn_available(inp1.type.context_name):
1824         return
1825     if op.filter_flip:
1826         conv_mode = 'conv'
1827     else:
1828         conv_mode = 'cross'
1829     if isinstance(op, AbstractConv3d):
1830         rval = dnn_conv3d(inp1, inp2,
1831                           border_mode=op.border_mode,
1832                           subsample=op.subsample,
1833                           dilation=op.filter_dilation,
1834                           direction_hint='forward!',
1835 <a name="9"></a>                          conv_mode=conv_mode,
1836                           num_groups=op.num_groups)
1837     elif isinstance(op, AbstractConv3d_gradWeights):
1838         shape = (<font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[1], inp1.shape[1] // op.num_groups,
1839                  inputs[2][0], inputs[2][1], inputs[2][2])
1840         rval = dnn_gradweight3d(inp1, inp2, shape,
1841                                 border_mode=</b></font>op.border_mode,
1842                                 subsample=op.subsample,
1843                                 dilation=op.filter_dilation,
1844 <a name="8"></a>                                conv_mode=conv_mode,
1845                                 num_groups=op.num_groups)
1846     elif isinstance(op, AbstractConv3d_gradInputs):
1847         shape = (<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>inp2.shape[0], inp1.shape[1] * op.num_groups,
1848                  inputs[2][0], inputs[2][1], inputs[2][2])
1849         rval = dnn_gradinput3d(inp1, inp2, shape,
1850                                border_mode=</b></font>op.border_mode,
1851                                subsample=op.subsample,
1852                                dilation=op.filter_dilation,
1853                                conv_mode=conv_mode,
1854                                num_groups=op.num_groups)
1855     return [rval]
1856 @local_optimizer([AbstractConv2d, AbstractConv3d])
1857 def local_abstractconv_cudnn(node):
1858     ctx = infer_context_name(*node.inputs)
1859     if not isinstance(node.inputs[0].type, GpuArrayType):
1860         return
1861     if node.op.unshared:
1862         return None
1863     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
1864         return None
1865     if isinstance(node.op, AbstractConv2d):
1866         with inherit_stack_trace(node.outputs):
1867             return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
1868     elif isinstance(node.op, AbstractConv3d):
1869         with inherit_stack_trace(node.outputs):
1870             return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
1871 @local_optimizer([AbstractConv2d, AbstractConv2d_gradWeights, AbstractConv2d_gradInputs])
1872 def local_abstractconv_cudnn_alt(node):
1873     if(not isinstance(node.op, (AbstractConv2d, AbstractConv2d_gradWeights,
1874        AbstractConv2d_gradInputs))):
1875         return
1876     if version(raises=False) &lt; 6000 and node.op.filter_dilation != (1, 1):
1877         return None
1878     if node.op.unshared:
1879         return None
1880     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
1881         return None
1882     inp1 = node.inputs[0]
1883     inp2 = node.inputs[1]
1884 <a name="5"></a>    if not dnn_available(inp1.type.context_name):
1885         return
1886     op = node<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op
1887     border_mode = node.op.border_mode
1888     subsample = node.op.subsample
1889     filter_dilation = node.op.filter_dilation
1890     num_groups = node.op.num_groups
1891     precision, _ = get_precision(None, [inp1</b></font>, inp2])
1892     if node.op.filter_flip:
1893         conv_mode = 'conv'
1894     else:
1895         conv_mode = 'cross'
1896     if isinstance(op, AbstractConv2d):
1897         if border_mode == 'half' or subsample != (1, 1) or num_groups != 1:
1898             return None
1899         if border_mode == 'full':
1900             direction_hint = 'bprop inputs'
1901         elif border_mode == 'valid' and filter_dilation == (1, 1):
1902             direction_hint = 'bprop weights'
1903         else:
1904             return None
1905         rval = dnn_conv(inp1, inp2,
1906                         border_mode=border_mode,
1907                         subsample=subsample,
1908                         dilation=filter_dilation,
1909                         direction_hint=direction_hint,
1910                         conv_mode=conv_mode,
1911 <a name="19"></a>                        num_groups=num_groups)
1912     elif isinstance(op, AbstractConv2d_gradWeights):
1913         if(<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>border_mode == 'valid' and subsample == (1, 1) and
1914            filter_dilation == (1, 1) and num_groups == 1):
1915             img = gpu_contiguous(inp1)
1916             topgrad = gpu_contiguous(inp2)
1917             ctx_name = infer_context_name(img, topgrad)
1918             img = gpu_contiguous(img.dimshuffle(</b></font>1, 0, 2, 3))
1919             topgrad = gpu_contiguous(topgrad.dimshuffle(1, 0, 2, 3))
1920             ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
1921             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
1922             out_shp = get_conv_output_shape(ishape,
1923                                             tshape,
1924                                             border_mode=border_mode,
1925                                             subsample=subsample,
1926                                             filter_dilation=filter_dilation)
1927             out_shp = assert_conv_shape(out_shp)
1928             out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
1929             desc = GpuDnnConvDesc(border_mode=border_mode,
1930                                   subsample=subsample,
1931                                   dilation=filter_dilation,
1932                                   conv_mode='cross',
1933                                   precision=precision)(out.shape)
1934             conv = GpuDnnConv(algo=None, num_groups=num_groups)(img, topgrad, out, desc)
1935             if conv_mode == 'conv':
1936                 conv = conv[:, :, ::-1, ::-1]
1937             rval = as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3), ctx_name)
1938         else:
1939             return None
1940     elif isinstance(op, AbstractConv2d_gradInputs):
1941         if border_mode == 'valid' and subsample == (1, 1) and num_groups == 1:
1942             kerns = gpu_contiguous(inp1.dimshuffle(1, 0, 2, 3))
1943             topgrad = gpu_contiguous(inp2)
1944             ctx_name = infer_context_name(kerns, topgrad)
1945             conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
1946             desc = GpuDnnConvDesc(border_mode='full',
1947                                   subsample=subsample,
1948                                   dilation=filter_dilation,
1949                                   conv_mode=conv_mode,
1950                                   precision=precision)(kerns.shape)
1951             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
1952             kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
1953             shape = get_conv_output_shape(tshape,
1954                                           kshape,
1955                                           border_mode='full',
1956                                           subsample=subsample,
1957                                           filter_dilation=filter_dilation)
1958             shape = assert_conv_shape(shape)
1959             out = GpuAllocEmpty(dtype=topgrad.dtype, context_name=ctx_name)(*shape)
1960             rval = GpuDnnConv(algo=None, num_groups=num_groups)(topgrad, kerns, out, desc)
1961         else:
1962             return None
1963     return [rval]
1964 @local_optimizer([AbstractConv3d, AbstractConv3d_gradWeights, AbstractConv3d_gradInputs])
1965 def local_abstractconv3d_cudnn_alt(node):
1966     if(not isinstance(node.op, (AbstractConv3d,
1967                                 AbstractConv3d_gradWeights,
1968                                 AbstractConv3d_gradInputs))):
1969         return
1970     if version(raises=False) &lt; 6000 and node.op.filter_dilation != (1, 1, 1):
1971         return None
1972     inp1 = node.inputs[0]
1973     inp2 = node.inputs[1]
1974 <a name="12"></a>    if not dnn_available(inp1.type.context_name):
1975         return
1976     op = node<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op
1977     border_mode = node.op.border_mode
1978     subsample = node.op.subsample
1979     filter_dilation = node.op.filter_dilation
1980     num_groups = node.op.num_groups
1981     precision, _ =</b></font> get_precision(None, [inp1, inp2])
1982     if node.op.filter_flip:
1983         conv_mode = 'conv'
1984     else:
1985         conv_mode = 'cross'
1986     if isinstance(op, AbstractConv3d):
1987         if border_mode == 'half' or subsample != (1, 1, 1) or num_groups &gt; 1:
1988             return None
1989         if border_mode == 'full':
1990             direction_hint = 'bprop inputs'
1991         elif border_mode == 'valid' and filter_dilation == (1, 1, 1):
1992             direction_hint = 'bprop weights'
1993         else:
1994             return None
1995         rval = dnn_conv3d(inp1, inp2,
1996                           border_mode=border_mode,
1997                           subsample=subsample,
1998                           dilation=filter_dilation,
1999                           direction_hint=direction_hint,
2000 <a name="18"></a>                          conv_mode=conv_mode)
2001     elif isinstance(op, AbstractConv3d_gradWeights):
2002         if(<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>border_mode == 'valid' and subsample == (1, 1, 1) and
2003            filter_dilation == (1, 1, 1) and num_groups == 1):
2004             img = gpu_contiguous(inp1)
2005             topgrad = gpu_contiguous(inp2)
2006             ctx_name = infer_context_name(img, topgrad)
2007             img = gpu_contiguous(img.dimshuffle(</b></font>1, 0, 2, 3, 4))
2008             topgrad = gpu_contiguous(topgrad.dimshuffle(1, 0, 2, 3, 4))
2009             ishape = [shape_i_op(i)(img) for i in range(img.ndim)]
2010             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
2011             out_shp = get_conv_output_shape(ishape,
2012                                             tshape,
2013                                             border_mode=border_mode,
2014                                             subsample=subsample,
2015                                             filter_dilation=filter_dilation)
2016             out_shp = assert_conv_shape(out_shp)
2017             out = GpuAllocEmpty(dtype=img.dtype, context_name=ctx_name)(*out_shp)
2018             desc = GpuDnnConvDesc(border_mode=border_mode,
2019                                   subsample=subsample,
2020                                   dilation=filter_dilation,
2021                                   conv_mode='cross',
2022                                   num_groups=num_groups,
2023                                   precision=precision)(out.shape)
2024             conv = GpuDnnConv(algo=None, num_groups=num_groups)(
2025                 img, topgrad, out, desc)
2026             if conv_mode == 'conv':
2027                 conv = conv[:, :, ::-1, ::-1, ::-1]
2028             rval = as_gpuarray_variable(conv.dimshuffle(1, 0, 2, 3, 4), ctx_name)
2029         else:
2030             return None
2031     elif isinstance(op, AbstractConv3d_gradInputs):
2032         if border_mode == 'valid' and subsample == (1, 1, 1) and num_groups == 1:
2033             kerns = gpu_contiguous(inp1.dimshuffle(1, 0, 2, 3, 4))
2034             topgrad = gpu_contiguous(inp2)
2035             ctx_name = infer_context_name(kerns, topgrad)
2036             conv_mode = 'cross' if conv_mode == 'conv' else 'conv'
2037             desc = GpuDnnConvDesc(border_mode='full',
2038                                   subsample=subsample,
2039                                   dilation=filter_dilation,
2040                                   conv_mode=conv_mode,
2041                                   num_groups=num_groups,
2042                                   precision=precision)(kerns.shape)
2043             tshape = [shape_i_op(i)(topgrad) for i in range(topgrad.ndim)]
2044             kshape = [shape_i_op(i)(kerns) for i in range(kerns.ndim)]
2045             shape = get_conv_output_shape(tshape,
2046                                           kshape,
2047                                           border_mode='full',
2048                                           subsample=subsample,
2049                                           filter_dilation=filter_dilation)
2050             shape = assert_conv_shape(shape)
2051             out = GpuAllocEmpty(dtype=topgrad.dtype, context_name=ctx_name)(*shape)
2052             rval = GpuDnnConv(algo=None, num_groups=num_groups)(
2053                 topgrad, kerns, out, desc)
2054         else:
2055             return None
2056     return [rval]
2057 @local_optimizer([AbstractConv2d_gradWeights, AbstractConv3d_gradWeights])
2058 def local_abstractconv_gw_cudnn(node):
2059     ctx = infer_context_name(*node.inputs)
2060     if not isinstance(node.inputs[0].type, GpuArrayType):
2061         return
2062     if node.op.unshared:
2063         return None
2064     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
2065         return None
2066     if isinstance(node.op, AbstractConv2d_gradWeights):
2067         with inherit_stack_trace(node.outputs):
2068             return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2069     elif isinstance(node.op, AbstractConv3d_gradWeights):
2070         with inherit_stack_trace(node.outputs):
2071             return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2072 @local_optimizer([AbstractConv2d_gradInputs, AbstractConv3d_gradInputs])
2073 def local_abstractconv_gi_cudnn(node):
2074     ctx = infer_context_name(*node.inputs)
2075     if not isinstance(node.inputs[0].type, GpuArrayType):
2076         return
2077     if node.op.unshared:
2078         return None
2079     if isinstance(node.op.border_mode, tuple) and any(isinstance(p, tuple) for p in node.op.border_mode):
2080         return None
2081     if isinstance(node.op, AbstractConv2d_gradInputs):
2082         with inherit_stack_trace(node.outputs):
2083             return local_abstractconv_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2084     elif isinstance(node.op, AbstractConv3d_gradInputs):
2085         with inherit_stack_trace(node.outputs):
2086             return local_abstractconv3d_cudnn_graph(node.op, ctx, node.inputs, node.outputs)
2087 @inplace_allocempty(GpuDnnConv, 2)
2088 def local_dnn_conv_inplace(node, inputs):
2089     return [GpuDnnConv(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
2090 @inplace_allocempty(GpuDnnConvGradW, 2)
2091 def local_dnn_convgw_inplace(node, inputs):
2092     return [GpuDnnConvGradW(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
2093 @inplace_allocempty(GpuDnnConvGradI, 2)
2094 def local_dnn_convgi_inplace(node, inputs):
2095     return [GpuDnnConvGradI(algo=node.op.algo, inplace=True, num_groups=node.op.num_groups)(*inputs)]
2096 optdb.register('local_dnna_conv_inplace',
2097                tensor.opt.in2out(local_dnn_conv_inplace,
2098                                  local_dnn_convgw_inplace,
2099                                  local_dnn_convgi_inplace,
2100                                  name="local_dnna_conv_inplace"),
2101                70.0, 'fast_run', 'inplace', 'gpuarray', 'cudnn')
2102 @register_opt('cudnn')
2103 @alpha_merge(GpuDnnConv, alpha_in=4, beta_in=5)
2104 def local_dnn_conv_alpha_merge(node, *inputs):
2105     return [GpuDnnConv(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2106 @register_opt('cudnn')
2107 @alpha_merge(GpuDnnConvGradW, alpha_in=4, beta_in=5)
2108 def local_dnn_convw_alpha_merge(node, *inputs):
2109     return [GpuDnnConvGradW(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2110 @register_opt('cudnn')
2111 @alpha_merge(GpuDnnConvGradI, alpha_in=4, beta_in=5)
2112 def local_dnn_convi_alpha_merge(node, *inputs):
2113     return [GpuDnnConvGradI(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2114 @register_opt('cudnn')
2115 @output_merge(GpuDnnConv, alpha_in=4, beta_in=5, out_in=2)
2116 def local_dnn_conv_output_merge(node, *inputs):
2117     inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
2118     return [GpuDnnConv(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2119 @register_opt('cudnn')
2120 @output_merge(GpuDnnConvGradW, alpha_in=4, beta_in=5, out_in=2)
2121 def local_dnn_convw_output_merge(node, *inputs):
2122     inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
2123     return [GpuDnnConvGradW(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2124 @register_opt('cudnn')
2125 @output_merge(GpuDnnConvGradI, alpha_in=4, beta_in=5, out_in=2)
2126 def local_dnn_convi_output_merge(node, *inputs):
2127     inputs = inputs[0:2] + (gpu_contiguous(inputs[2]),) + inputs[3:]
2128     return [GpuDnnConvGradI(algo=node.op.algo, num_groups=node.op.num_groups)(*inputs)]
2129 def local_gpua_pool_dnn_alternative(op, ctx_name, inputs, outputs):
2130     if not dnn_available(ctx_name):
2131         return
2132     if not op.ignore_border:
2133         return
2134     img, ws, stride, pad = inputs
2135     nd = op.ndim
2136     if nd not in (2, 3):
2137         return
2138     img = gpu_contiguous(as_gpuarray_variable(img, ctx_name))
2139     mode = op.mode
2140     if img.ndim == nd + 2:
2141         return dnn_pool(img, ws, stride=stride, pad=pad, mode=mode)
2142     else:
2143         img_padded = pad_dims(img, 2, nd)
2144         ret_padded = dnn_pool(img_padded, ws, stride=stride, pad=pad, mode=mode)
2145         return unpad_dims(ret_padded, img, 2, nd)
2146 pool_db.register("local_gpua_pool_dnn_alternative",
2147                  op_lifter([Pool])(local_gpua_pool_dnn_alternative),
2148                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2149                  position=0)
2150 pool_db2.register("local_gpua_pool_dnn_alternative",
2151                   local_optimizer([Pool])(local_gpua_pool_dnn_alternative),
2152                   'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2153                   position=0)
2154 def local_gpua_pool_dnn_grad_stride(op, ctx_name, inputs, outputs):
2155     if not dnn_available(ctx_name):
2156         return
2157     if not op.ignore_border:
2158         return
2159     inp, out, out_grad, ws, stride, pad = inputs
2160     nd = op.ndim
2161     if nd not in (2, 3):
2162         return
2163     inp = gpu_contiguous(as_gpuarray_variable(inp, ctx_name))
2164     out = gpu_contiguous(as_gpuarray_variable(out, ctx_name))
2165     out_grad = gpu_contiguous(as_gpuarray_variable(out_grad, ctx_name))
2166     mode = op.mode
2167     if inp.ndim == nd + 2:
2168         return GpuDnnPoolGrad(mode=mode)(inp,
2169                                          out,
2170                                          out_grad,
2171                                          ws,
2172                                          stride,
2173                                          pad)
2174     else:
2175         inp_padded = pad_dims(inp, 2, nd)
2176         out_padded = pad_dims(out, 2, nd)
2177         out_grad_padded = pad_dims(out_grad, 2, nd)
2178         ret_padded = GpuDnnPoolGrad(mode=mode)(inp_padded,
2179                                                out_padded,
2180                                                out_grad_padded,
2181                                                ws,
2182                                                stride,
2183                                                pad)
2184         return unpad_dims(ret_padded, inp, 2, nd)
2185 pool_db.register("local_gpua_pool_dnn_grad_stride",
2186                  op_lifter([MaxPoolGrad])(local_gpua_pool_dnn_grad_stride),
2187                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2188                  position=0)
2189 pool_db2.register("local_gpua_pool_dnn_grad_stride",
2190                   local_optimizer([MaxPoolGrad])(local_gpua_pool_dnn_grad_stride),
2191                   'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2192                   position=0)
2193 def local_gpua_avg_pool_dnn_grad_stride(op, ctx_name, inputs, outputs):
2194     if not dnn_available(ctx_name):
2195         return
2196     if not op.ignore_border:
2197         return
2198     inp, out_grad, ws, stride, pad = inputs
2199     nd = op.ndim
2200     if nd not in (2, 3):
2201         return
2202     inp = gpu_contiguous(as_gpuarray_variable(inp, ctx_name))
2203     out_grad = gpu_contiguous(as_gpuarray_variable(out_grad, ctx_name))
2204     mode = op.mode
2205     if inp.ndim == nd + 2:
2206         return GpuDnnPoolGrad(mode=mode)(inp, out_grad, out_grad, ws, stride, pad)
2207     else:
2208         inp_padded = pad_dims(inp, 2, nd)
2209         out_grad_padded = pad_dims(out_grad, 2, nd)
2210         ret_padded = GpuDnnPoolGrad(mode=mode)(inp_padded,
2211                                                out_grad_padded,
2212                                                out_grad_padded,
2213                                                ws,
2214                                                stride,
2215                                                pad)
2216         return unpad_dims(ret_padded, inp, 2, nd)
2217 pool_db.register("local_gpua_avg_pool_dnn_grad_stride",
2218                  op_lifter([AveragePoolGrad])(local_gpua_avg_pool_dnn_grad_stride),
2219                  'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2220                  position=0)
2221 pool_db2.register("local_gpua_avg_pool_dnn_grad_stride",
2222                   local_optimizer([AveragePoolGrad])(local_gpua_avg_pool_dnn_grad_stride),
2223                   'gpuarray', 'fast_compile', 'fast_run', 'cudnn',
2224                   position=0)
2225 @register_opt('cudnn', 'fast_compile')
2226 @local_optimizer([GpuSoftmax])
2227 def local_softmax_dnn(node):
2228     if isinstance(node.op, GpuSoftmax):
2229         if not dnn_available(node.outputs[0].type.context_name):
2230             return
2231         ins = node.inputs[0].dimshuffle(0, 1, 'x', 'x')
2232         ins = gpu_contiguous(ins)
2233         out = GpuDnnSoftmax('accurate', 'channel')(ins)
2234         out = as_gpuarray_variable(out.dimshuffle(0, 1), out.type.context_name)
2235         return [out]
2236 @register_opt('cudnn', 'stabilize')
2237 <a name="23"></a>@local_optimizer([GpuElemwise])
2238 def local_log_softmax_dnn(node):
2239     if (isinstance(node<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.op, GpuElemwise) and
2240             isinstance(node.op.scalar_op, Log) and
2241             node.inputs[0].owner and
2242             isinstance(node.inputs[0].owner.</b></font>op, GpuDnnSoftmax) and
2243             len(node.inputs[0].clients) == 1):
2244         softmax_node = node.inputs[0].owner
2245         new_softmax = GpuDnnSoftmax('log', softmax_node.op.mode)
2246         return [new_softmax(softmax_node.inputs[0])]
2247 @register_opt('cudnn', 'fast_compile')
2248 @op_lifter([LogSoftmax])
2249 @register_opt2([LogSoftmax], 'fast_compile', 'cudnn')
2250 def local_gpua_logsoftmax_to_dnn(op, ctx_name, inputs, outputs):
2251     inp = inputs[0]
2252     if inp.ndim != 2:
2253         return
2254     if not dnn_available(ctx_name):
2255         return
2256     inp = inp.dimshuffle(0, 1, 'x', 'x')
2257     inp.tag.context_name = ctx_name
2258     out = GpuDnnSoftmax('log', 'channel')(gpu_contiguous(inp))
2259     return [out.dimshuffle(0, 1)]
2260 @register_opt('cudnn', 'fast_compile')
2261 @op_lifter([SoftmaxGrad])
2262 @register_opt2([SoftmaxGrad], 'cudnn', 'fast_compile')
2263 def local_gpua_softmax_dnn_grad(op, ctx_name, inputs, outputs):
2264     if not dnn_available(ctx_name):
2265         return
2266     ins = []
2267     for n in inputs:
2268         n = as_gpuarray_variable(n, ctx_name)
2269         if n.ndim != 2:
2270             return
2271         ins.append(n.dimshuffle(0, 'x', 1, 'x'))
2272     out = GpuDnnSoftmaxGrad('accurate', 'instance')(
2273         gpu_contiguous(ins[0]), gpu_contiguous(ins[1]))
2274     return [out.dimshuffle(0, 2)]
2275 @register_opt('cudnn')
2276 @local_optimizer([GpuCAReduceCuda])
2277 def local_dnn_reduction(node):
2278     if not isinstance(node.op, GpuCAReduceCuda):
2279         return
2280     if not dnn_available(node.inputs[0].type.context_name):
2281         return
2282     if version(raises=False) &lt; 6000:
2283         return
2284     if node.inputs[0].ndim &gt; 8:
2285         return
2286     acc_dtype = node.op._acc_dtype(node.inputs[0].dtype)
2287     if node.inputs[0].dtype != node.outputs[0].dtype:
2288         if (node.inputs[0].dtype == 'float64' or
2289                 node.outputs[0].dtype == 'float64'):
2290             return
2291         if acc_dtype != 'float32':
2292             return
2293     if node.inputs[0].dtype not in ['float16', 'float32', 'float64']:
2294         return
2295     if (node.inputs[0].dtype == 'float64' and acc_dtype != 'float64'):
2296         return
2297     if (node.inputs[0].dtype == 'float32' and acc_dtype != 'float32'):
2298         return
2299     if (node.inputs[0].dtype == 'float16' and acc_dtype == 'float64'):
2300         return
2301     def _identity(a):
2302         return a
2303     def _square(a):
2304         return GpuElemwise(theano.scalar.basic.sqr)(a)
2305     scal = node.op.scalar_op.name
2306     post = _identity
2307     if node.op.pre_scalar_op is not None:
2308         if isinstance(node.op.scalar_op, theano.scalar.basic.Add):
2309             if isinstance(node.op.pre_scalar_op, theano.scalar.basic.Sqr):
2310                 scal = 'norm2'
2311                 post = _square
2312             elif isinstance(node.op.pre_scalar_op, theano.scalar.basic.Abs):
2313 <a name="17"></a>                scal = 'norm1'
2314             else:
2315                 return
2316         elif (<font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>isinstance(node.op.scalar_op, theano.scalar.basic.Maximum) and
2317                 isinstance(node.op.pre_scalar_op, theano.scalar.basic.</b></font>Abs)):
2318             scal = 'absmax'
2319         else:
2320             return
2321     if not cudnn.cudnnReduceTensorOp_t.has_alias(scal):
2322         return
2323     with inherit_stack_trace(node.outputs):
2324         ret = GpuDnnReduction(scal,
2325                               node.op.axis,
2326                               acc_dtype,
2327                               node.op.dtype,
2328                               False)(node.inputs[0])
2329         return [post(ret)]
2330 @register_opt('cudnn')
2331 @local_optimizer([GpuMaxAndArgmax])
2332 def local_cudnn_maxandargmax(node):
2333     if not isinstance(node.op, GpuMaxAndArgmax):
2334         return
2335     if not dnn_available(node.inputs[0].type.context_name):
2336         return
2337     if version(raises=False) &lt; 6000:
2338         return
2339     if node.inputs[0].ndim &gt; 8:
2340         return
2341     if node.inputs[0].dtype != node.outputs[0].dtype:
2342         return
2343     if node.inputs[0].dtype not in ['float16', 'float32', 'float64']:
2344         return
2345     if (node.op.axis is not None and
2346 <a name="22"></a>            tuple(sorted(node.op.axis)) != node.op.axis):
2347         return
2348     max, arg = GpuDnnReduction<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('maximum', node.op.axis, node.outputs[0].dtype,
2349                                node.outputs[0].dtype, True)(node.inputs[</b></font>0])
2350     return (max, as_gpuarray_variable(arg.astype('int64'),
2351                                       node.outputs[1].type.context_name))
2352 @register_opt('cudnn', 'fast_compile')
2353 @op_lifter([Argmax])
2354 @register_opt2([Argmax], 'fast_compile', 'cudnn')
2355 def local_dnn_argmax(op, ctx_name, inputs, outputs):
2356     if not dnn_available(ctx_name):
2357         return
2358     if version(raises=False) &lt; 6000:
2359         return
2360     if inputs[0].ndim &gt; 8:
2361         return
2362     if inputs[0].dtype not in ['float16', 'float32', 'float64']:
2363         return
2364     if op.axis is not None and tuple(sorted(op.axis)) != op.axis:
2365         return
2366     max, arg = GpuDnnReduction('maximum', op.axis, inputs[0].dtype,
2367                                inputs[0].dtype, True)(*inputs)
2368     return [as_gpuarray_variable(arg.astype('int64'), ctx_name)]
2369 class NoCuDNNRaise(Optimizer):
2370     def apply(self, fgraph):
2371         for c in list_contexts():
2372             if not dnn_available(c):
2373                 raise AssertionError(
2374                     "cuDNN optimization was enabled, but Theano was not able "
2375                     "to use it for context " + str(c) + ". We got this error: \n" +
2376                     dnn_available.msg)
2377 gpu_seqopt.register("NoCuDNNRaise", NoCuDNNRaise(), 0, 'cudnn')
2378 def local_abstract_batch_norm_train_cudnn(op, ctx_name, inputs, outputs):
2379     x, scale, bias, epsilon, running_average_factor = inputs[:5]
2380     running_mean = inputs[5] if len(inputs) &gt; 5 else None
2381     running_var = inputs[6] if len(inputs) &gt; 6 else None
2382     axes = tuple(op.axes)
2383     if axes == (0,):
2384         mode = 'per-activation'
2385     elif axes == (0,) + tuple(range(2, x.ndim)):
2386         mode = 'spatial'
2387     else:
2388         return None
2389     try:
2390         eps = theano.tensor.get_scalar_constant_value(epsilon)
2391     except theano.tensor.NotScalarConstantError:
2392         return None
2393     if eps &lt; 1e-5:
2394         return None
2395     try:
2396         running_average_factor = theano.tensor.get_scalar_constant_value(running_average_factor)
2397     except theano.tensor.NotScalarConstantError:
2398         return None
2399     ctx = infer_context_name(*inputs)
2400     if not dnn_available(ctx):
2401         return
2402     x = as_gpuarray_variable(x, context_name=ctx)
2403     scale = as_gpuarray_variable(scale, context_name=ctx)
2404     bias = as_gpuarray_variable(bias, context_name=ctx)
2405     inputs = [x, scale, bias, mode, eps, running_average_factor]
2406     if running_mean is not None and running_var is not None:
2407         inputs.append(running_mean)
2408         inputs.append(running_var)
2409     results = list(dnn_batch_normalization_train(*inputs))
2410     return results
2411 @register_inplace()
2412 @local_optimizer([GpuDnnBatchNorm], inplace=True)
2413 def local_batch_norm_inplace_output(node):
2414     if isinstance(node.op, GpuDnnBatchNorm) and not node.op.inplace_output:
2415         return GpuDnnBatchNorm(mode=node.op.mode,
2416                                running_averages=node.op.running_averages,
2417                                inplace_running_mean=node.op.inplace_running_mean,
2418                                inplace_running_var=node.op.inplace_running_var,
2419                                inplace_output=True)(*node.inputs)
2420 @register_inplace()
2421 @local_optimizer([GpuDnnBatchNorm], inplace=True)
2422 def local_batch_norm_inplace_running_mean(node):
2423     if isinstance(node.op, GpuDnnBatchNorm) and node.op.running_averages and not node.op.inplace_running_mean:
2424         return GpuDnnBatchNorm(mode=node.op.mode,
2425                                running_averages=node.op.running_averages,
2426                                inplace_running_mean=True,
2427                                inplace_running_var=node.op.inplace_running_var,
2428                                inplace_output=node.op.inplace_output)(*node.inputs)
2429 @register_inplace()
2430 @local_optimizer([GpuDnnBatchNorm], inplace=True)
2431 def local_batch_norm_inplace_running_var(node):
2432     if isinstance(node.op, GpuDnnBatchNorm) and node.op.running_averages and not node.op.inplace_running_var:
2433         return GpuDnnBatchNorm(mode=node.op.mode,
2434                                running_averages=node.op.running_averages,
2435                                inplace_running_mean=node.op.inplace_running_mean,
2436                                inplace_running_var=True,
2437                                inplace_output=node.op.inplace_output)(*node.inputs)
2438 @register_inplace()
2439 @local_optimizer([GpuDnnBatchNormInference], inplace=True)
2440 def local_batch_norm_inference_inplace(node):
2441     if isinstance(node.op, GpuDnnBatchNormInference) and not node.op.inplace:
2442         return [GpuDnnBatchNormInference(mode=node.op.mode, inplace=True)(*node.inputs)]
2443 def local_abstract_batch_norm_train_grad_cudnn(op, ctx_name, inputs, outputs):
2444     x, dy, scale, x_mean, x_invstd, epsilon = inputs
2445     x_on_gpu = (isinstance(x.type, GpuArrayType) or
2446                 (x.owner and isinstance(x.owner.op, HostFromGpu)))
2447     dy_on_gpu = (isinstance(dy.type, GpuArrayType) or
2448                  (dy.owner and isinstance(dy.owner.op, HostFromGpu)))
2449     if not (x_on_gpu or dy_on_gpu):
2450         return None
2451     axes = tuple(op.axes)
2452     if axes == (0,):
2453         mode = 'per-activation'
2454     elif axes == (0,) + tuple(range(2, x.ndim)):
2455         mode = 'spatial'
2456     else:
2457         return None
2458 <a name="4"></a>
2459     ndim = x.ndim
2460     if ndim &lt; 4:
2461         x <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.shape_padright(x, 4 - ndim)
2462         dy = theano.tensor.shape_padright(dy, 4 - ndim)
2463         scale = theano.tensor.shape_padright(scale, 4 - ndim)
2464         x_mean = theano.tensor.shape_padright(x_mean, 4 - ndim)
2465         x_invstd =</b></font> theano.tensor.shape_padright(x_invstd, 4 - ndim)
2466 <a name="16"></a>    elif ndim &gt; 5:
2467         x_shape = x.shape
2468         params_shape = scale.shape
2469         x <font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.flatten(x, 5)
2470         dy = theano.tensor.flatten(dy, 5)
2471         scale = theano.tensor.flatten(scale, 5)
2472         x_mean =</b></font> theano.tensor.flatten(x_mean, 5)
2473         x_invstd = theano.tensor.flatten(x_invstd, 5)
2474     try:
2475         eps = theano.tensor.get_scalar_constant_value(epsilon)
2476     except theano.tensor.NotScalarConstantError:
2477         return None
2478     if eps &lt; 1e-5:
2479         return None
2480 <a name="21"></a>    ctx = infer_context_name(*inputs)
2481     if not dnn_available(ctx):
2482         return
2483     x <font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= as_gpuarray_variable(x, context_name=ctx)
2484     dy = as_gpuarray_variable(dy, context_name=ctx)
2485     scale = as_gpuarray_variable(scale, context_name=ctx)
2486     x_mean = as_gpuarray_variable(x_mean, context_name=</b></font>ctx)
2487     x_invstd = as_gpuarray_variable(x_invstd, context_name=ctx)
2488     g_wrt_inputs, g_wrt_scale, g_wrt_bias = \
2489 <a name="15"></a>        GpuDnnBatchNormGrad(mode)(x, dy, scale, x_mean, x_invstd, eps)
2490     if ndim &lt; 4:
2491         g_wrt_inputs = theano.tensor<font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.flatten(g_wrt_inputs, ndim)
2492         g_wrt_scale = theano.tensor.flatten(g_wrt_scale, ndim)
2493         g_wrt_bias = theano.tensor.flatten(g_wrt_bias, ndim)
2494     elif ndim &gt; 5:
2495         g_wrt_inputs = theano.tensor.</b></font>reshape(g_wrt_inputs, x_shape)
2496         g_wrt_scale = theano.tensor.reshape(g_wrt_scale, params_shape)
2497         g_wrt_bias = theano.tensor.reshape(g_wrt_bias, params_shape)
2498     return [g_wrt_inputs, g_wrt_scale, g_wrt_bias]
2499 def local_abstract_batch_norm_inference_cudnn(op, ctx_name, inputs, outputs):
2500     x, scale, bias, estimated_mean, estimated_variance, epsilon = inputs
2501     axes = tuple(op.axes)
2502     if axes == (0,):
2503         mode = 'per-activation'
2504     elif axes == (0,) + tuple(range(2, x.ndim)):
2505         mode = 'spatial'
2506     else:
2507         return None
2508     try:
2509         eps = theano.tensor.get_scalar_constant_value(epsilon)
2510     except theano.tensor.NotScalarConstantError:
2511         return None
2512     if eps &lt; 1e-5:
2513         return None
2514     ctx = infer_context_name(*inputs)
2515     if not dnn_available(ctx):
2516         return
2517     x = as_gpuarray_variable(x, context_name=ctx)
2518     scale = as_gpuarray_variable(scale, context_name=ctx)
2519     bias = as_gpuarray_variable(bias, context_name=ctx)
2520     estimated_mean = as_gpuarray_variable(estimated_mean, context_name=ctx)
2521     estimated_variance = as_gpuarray_variable(estimated_variance, context_name=ctx)
2522     out = dnn_batch_normalization_test(x, scale, bias, estimated_mean, estimated_variance,
2523                                        mode, eps)
2524     return [out]
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_basic_3.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <a name="1"></a><font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
2 from copy import copy, deepcopy
3 from functools import partial
4 import itertools
5 import logging
6 from nose.plugins.skip import SkipTest
7 from nose.tools import assert_raises
8 import operator
9 import os
10 import sys
11 from tempfile import mkstemp
12 import unittest
13 import warnings
14 from six import iteritems
15 from six.moves import StringIO, reduce
16 from six.moves import xrange
17 from six.moves.builtins import min as builtin_min
18 import numpy as np
19 from numpy.testing import dec, assert_array_equal, assert_allclose
20 import theano
21 from theano.compat import izip
22 from theano.compat import PY3, exc_message, operator_div
23 from theano import compile, config, function, gof, tensor, shared
24 from theano.compile import DeepCopyOp
25 from theano.compile.mode import get_default_mode
26 from theano.scalar import autocast_float_as, autocast_float
27 from theano.tensor import (
28     wvector, bvector,
29     argmin, max_and_argmax, cscalar, join,
30     horizontal_stack, vertical_stack, argmax, get_vector_length,
31     fscalar, sum, tensor3, vector, add, addbroadcast,
32     alloc, as_tensor_variable, tensor_from_scalar, ARange,
33     clip, constant, default, diag, dot, batched_dot,
34     dmatrix, dscalar, dvector, eq, eye, fill, flatten, inverse_permutation,
35     tensor4, permute_row_elements, fmatrix, fscalars, grad,
36     inplace, iscalar, matrix, minimum, matrices, maximum, mul, neq,
37     Reshape, row, scalar, scalars, second, smallest, stack, sub, Tensor,
38     tensor_copy, tensordot, TensorType, Tri, tri, tril, triu, unbroadcast,
39     var, Argmax, Join, shape, MaxAndArgmax, lscalar, zvector, exp,
40     get_scalar_constant_value, ivector, reshape, scalar_from_tensor, scal,
41     iscalars, arange, dscalars, fvector, imatrix, numeric_grad,
42     opt, lvector, true_div, max, min, Split, roll,
43     tile, patternbroadcast, Eye, Shape, Dot, PermuteRowElements,
44     ScalarFromTensor, TensorFromScalar, dtensor4, Rebroadcast, Alloc,
45     dtensor3, SpecifyShape, Mean,
46     itensor3, Tile, switch, ExtractDiag, AllocDiag,
47     nonzero, flatnonzero, nonzero_values,
48     stacklists, DimShuffle, hessian, ptp, power,
49     swapaxes, choose, Choose, NoneConst, AllocEmpty,
50     isclose, allclose, mgrid, ogrid, extract_constant,
51     )
52 from theano.tests import unittest_tools as utt
53 from theano.tests.unittest_tools import attr
54 from theano import change_flags
55 imported_scipy_special =</b></font> False
56 mode_no_scipy = get_default_mode()
57 try:
58     import scipy.special
59     import scipy.stats
60     imported_scipy_special = True
61 except ImportError:
62     if config.mode == "FAST_COMPILE":
63         mode_no_scipy = "FAST_RUN"
64 floatX = config.floatX
65 if config.mode == "FAST_COMPILE":
66     mode_opt = "FAST_RUN"
67 else:
68     mode_opt = get_default_mode()
69 utt.seed_rng()
70 test_rng = np.random.RandomState(seed=utt.fetch_seed())
71 if PY3:
72     def L(i):
73         return i
74 else:
75     def L(i):
76         return long(i)  # noqa for Python 3
77 def inplace_func(inputs, outputs, mode=None, allow_input_downcast=False,
78                  on_unused_input='raise', name=None):
79     if mode is None:
80         mode = get_default_mode()
81     return function(inputs, outputs,
82                     mode=mode,
83                     allow_input_downcast=allow_input_downcast,
84                     accept_inplace=True,
85                     on_unused_input=on_unused_input,
86                     name=name)
87 def eval_outputs(outputs, ops=(), mode=None):
88     f = inplace_func([], outputs, mode=mode)
89     variables = f()
90     if ops:
91         assert any(isinstance(node.op, ops) for node in f.maker.fgraph.apply_nodes)
92     if isinstance(variables, (tuple, list)) and len(variables) == 1:
93         return variables[0]
94     return variables
95 def get_numeric_subclasses(cls=np.number, ignore=None):
96     if ignore is None:
97         ignore = []
98     rval = []
99     dtype = np.dtype(cls)
100     dtype_num = dtype.num
101     if dtype_num not in ignore:
102         np.array(0, dtype=dtype)
103         rval.append(cls)
104         ignore.append(dtype_num)
105     for sub_ in cls.__subclasses__():
106         rval += [c for c in get_numeric_subclasses(sub_, ignore=ignore)]
107     return rval
108 def get_numeric_types(with_int=True, with_float=True, with_complex=False,
109                       only_theano_types=True):
110     if only_theano_types:
111         theano_types = [d.dtype for d in theano.scalar.all_types]
112     rval = []
113     def is_within(cls1, cls2):
114         return (cls1 is cls2 or
115                 issubclass(cls1, cls2) or
116                 isinstance(np.array([0], dtype=cls1)[0], cls2))
117     for cls in get_numeric_subclasses():
118         dtype = np.dtype(cls)
119         if ((not with_complex and is_within(cls, np.complexfloating)) or
120                 (not with_int and is_within(cls, np.integer)) or
121                 (not with_float and is_within(cls, np.floating)) or
122                 (only_theano_types and dtype not in theano_types)):
123             continue
124         rval.append([str(dtype), dtype, dtype.num])
125     return [x[1] for x in sorted(rval, key=str)]
126 def _numpy_checker(x, y):
127     x, y = x[0], y[0]
128     if (x.dtype != y.dtype or x.shape != y.shape or
129             np.any(np.abs(x - y) &gt; 1e-10)):
130         raise Exception("Output mismatch.", {'performlinker': x, 'clinker': y})
131 def safe_make_node(op, *inputs):
132     node = op(*inputs)
133     if isinstance(node, list):
134         return node[0].owner
135     else:
136         return node.owner
137 def upcast_float16_ufunc(fn):
138     def ret(*args, **kwargs):
139         out_dtype = np.find_common_type(
140             [a.dtype for a in args], [np.float16])
141         if out_dtype == 'float16':
142             sig = 'f' * fn.nin + '-&gt;' + 'f' * fn.nout
143             kwargs.update(sig=sig)
144         return fn(*args, **kwargs)
145     return ret
146 def upcast_int8_nfunc(fn):
147     def ret(*args, **kwargs):
148         args = list(args)
149         for i, a in enumerate(args):
150             if getattr(a, 'dtype', None) in ('int8', 'uint8'):
151                 args[i] = a.astype('float32')
152         return fn(*args, **kwargs)
153     return ret
154 def makeTester(name, op, expected, checks=None, good=None, bad_build=None,
155                bad_runtime=None, grad=None, mode=None, grad_rtol=None,
156                eps=1e-10, skip=False, test_memmap=True, check_name=True,
157                grad_eps=None):
158     if checks is None:
159         checks = {}
160     if good is None:
161         good = {}
162     if bad_build is None:
163         bad_build = {}
164     if bad_runtime is None:
165         bad_runtime = {}
166     if grad is None:
167         grad = {}
168     if grad is True:
169         grad = good
170     _op, _expected, _checks, _good = op, expected, checks, good
171     _bad_build, _bad_runtime, _grad = bad_build, bad_runtime, grad
172     _mode, _grad_rtol, _eps, skip_ = mode, grad_rtol, eps, skip
173     _test_memmap = test_memmap
174     _check_name = check_name
175     _grad_eps = grad_eps
176     class Checker(unittest.TestCase):
177         op = staticmethod(_op)
178         expected = staticmethod(_expected)
179         checks = _checks
180         check_name = _check_name
181         good = _good
182         bad_build = _bad_build
183         bad_runtime = _bad_runtime
184         grad = _grad
185         mode = _mode
186         skip = skip_
187         test_memmap = _test_memmap
188         def setUp(self):
189             if self.check_name:
190                 eval(self.__class__.__module__ + '.' + self.__class__.__name__)
191             self.tmp_files = []
192         def add_memmap_values(self, val_dict):
193             if not self.test_memmap:
194                 return val_dict
195             val_dict = val_dict.copy()
196             for k, v in sorted(val_dict.items()):
197                 new_k = '_'.join((k, 'memmap'))
198                 if new_k in val_dict:
199                     break
200                 new_v = []
201                 for inp in v:
202                     if type(inp) is np.ndarray and inp.size &gt; 0:
203                         f, fname = mkstemp()
204                         self.tmp_files.append((f, fname))
205                         new_inp = np.memmap(fname, dtype=inp.dtype,
206                                             mode='w+', shape=inp.shape)
207                         new_inp[...] = inp[...]
208                         new_v.append(new_inp)
209                     else:
210                         new_v.append(inp)
211                 val_dict[new_k] = new_v
212                 break
213             return val_dict
214         def tearDown(self):
215             import gc
216             gc.collect()
217             for f, fname in self.tmp_files:
218                 os.close(f)
219                 os.remove(fname)
220         def test_good(self):
221             if skip:
222                 raise SkipTest(skip)
223             good = self.add_memmap_values(self.good)
224             for testname, inputs in iteritems(good):
225                 inputs = [copy(input) for input in inputs]
226                 inputrs = [TensorType(
227                     dtype=input.dtype,
228                     broadcastable=[shape_elem == 1
229                                    for shape_elem in input.shape]
230                     )() for input in inputs]
231                 try:
232                     node = safe_make_node(self.op, *inputrs)
233                 except Exception as exc:
234                     err_msg = ("Test %s::%s: Error occurred while"
235                                " making a node with inputs %s") % (
236                                    self.op, testname, inputs)
237                     exc.args += (err_msg,)
238                     raise
239                 try:
240                     f = inplace_func(inputrs, node.outputs, mode=mode, name='test_good')
241                 except Exception as exc:
242                     err_msg = ("Test %s::%s: Error occurred while"
243                                " trying to make a Function") % (self.op, testname)
244                     exc.args += (err_msg,)
245                     raise
246                 if (isinstance(self.expected, dict) and
247                         testname in self.expected):
248                     expecteds = self.expected[testname]
249                     eps = 5e-9
250                 else:
251                     expecteds = self.expected(*inputs)
252                     eps = 1e-10
253                 if any([i.dtype in ('float32', 'int8', 'uint8', 'uint16')
254                         for i in inputs]):
255                     eps = 1e-6
256                 eps = np.max([eps, _eps])
257                 try:
258                     variables = f(*inputs)
259                 except Exception as exc:
260                     err_msg = ("Test %s::%s: Error occurred while calling"
261                                " the Function on the inputs %s") % (
262                                    self.op, testname, inputs)
263                     exc.args += (err_msg,)
264                     raise
265                 if not isinstance(expecteds, (list, tuple)):
266                     expecteds = (expecteds, )
267                 for i, (variable, expected) in enumerate(
268                         izip(variables, expecteds)):
269                     if (variable.dtype != expected.dtype or
270                             variable.shape != expected.shape or
271                             not np.allclose(variable, expected,
272                                             atol=eps, rtol=eps)):
273                         self.fail(("Test %s::%s: Output %s gave the wrong"
274                                    " value. With inputs %s, expected %s (dtype %s),"
275                                    " got %s (dtype %s). eps=%f"
276                                    " np.allclose returns %s %s") % (
277                             self.op,
278                             testname,
279                             i,
280                             inputs,
281                             expected,
282                             expected.dtype,
283                             variable,
284                             variable.dtype,
285                             eps,
286                             np.allclose(variable, expected,
287                                         atol=eps, rtol=eps),
288                             np.allclose(variable, expected)))
289                 for description, check in iteritems(self.checks):
290                     if not check(inputs, variables):
291                         self.fail(("Test %s::%s: Failed check: %s (inputs"
292                                    " were %s, outputs were %s)") % (
293                             self.op, testname, description,
294                             inputs, variables))
295         def test_bad_build(self):
296             if skip:
297                 raise SkipTest(skip)
298             for testname, inputs in iteritems(self.bad_build):
299                 inputs = [copy(input) for input in inputs]
300                 inputrs = [shared(input) for input in inputs]
301                 self.assertRaises(Exception,
302                                   safe_make_node, self.op, *inputrs)
303         @change_flags(compute_test_value='off')
304         def test_bad_runtime(self):
305             if skip:
306                 raise SkipTest(skip)
307             for testname, inputs in iteritems(self.bad_runtime):
308                 inputrs = [shared(input) for input in inputs]
309                 try:
310                     node = safe_make_node(self.op, *inputrs)
311                 except Exception as exc:
312                     err_msg = ("Test %s::%s: Error occurred while trying"
313                                " to make a node with inputs %s") % (
314                         self.op, testname, inputs)
315                     exc.args += (err_msg,)
316                     raise
317                 try:
318                     f = inplace_func([], node.outputs, mode=mode, name="test_bad_runtime")
319                 except Exception as exc:
320                     err_msg = ("Test %s::%s: Error occurred while trying"
321                                " to make a Function") % (self.op, testname)
322                     exc.args += (err_msg,)
323                     raise
324                 self.assertRaises(Exception, f, [])
325         def test_grad(self):
326             if skip:
327                 raise SkipTest(skip)
328             backup = config.warn.sum_div_dimshuffle_bug
329             config.warn.sum_div_dimshuffle_bug = False
330             try:
331                 for testname, inputs in iteritems(self.grad):
332                     inputs = [copy(input) for input in inputs]
333                     try:
334                         utt.verify_grad(self.op, inputs,
335                                         mode=self.mode,
336                                         rel_tol=_grad_rtol,
337                                         eps=_grad_eps)
338                     except Exception as exc:
339                         err_msg = ("Test %s::%s: Error occurred while"
340                                    " computing the gradient on the following"
341                                    " inputs: %s") % (self.op, testname, inputs)
342                         exc.args += (err_msg,)
343                         raise
344             finally:
345                 config.warn.sum_div_dimshuffle_bug = backup
346         def test_grad_none(self):
347             if skip:
348                 raise SkipTest(skip)
349             if not hasattr(self.op, 'grad'):
350                 return
351             for testname, inputs in iteritems(self.good):
352                 inputs = [copy(input) for input in inputs]
353                 inputrs = [TensorType(
354                     dtype=input.dtype,
355                     broadcastable=[shape_elem == 1
356                                    for shape_elem in input.shape]
357                     )() for input in inputs]
358                 if (isinstance(self.expected, dict) and
359                         testname in self.expected):
360                     expecteds = self.expected[testname]
361                 else:
362                     expecteds = self.expected(*inputs)
363                 if not isinstance(expecteds, (list, tuple)):
364                     expecteds = (expecteds, )
365                 out_grad_vars = []
366                 for out in expecteds:
367                     if str(out.dtype) in tensor.discrete_dtypes:
368                         dtype = floatX
369                     else:
370                         dtype = str(out.dtype)
371                     bcast = [shape_elem == 1 for shape_elem in out.shape]
372                     var = TensorType(dtype=dtype, broadcastable=bcast)()
373                     out_grad_vars.append(var)
374                 try:
375                     in_grad_vars = self.op.grad(inputrs, out_grad_vars)
376                 except (gof.utils.MethodNotDefined, NotImplementedError):
377                     pass
378                 else:
379                     assert None not in in_grad_vars
380     Checker.__name__ = name
381     if hasattr(Checker, '__qualname__'):
382         Checker.__qualname__ = name
383     return Checker
384 def rand(*shape):
385     r = test_rng.rand(*shape) * 2 - 1
386     return np.asarray(r, dtype=config.floatX)
387 def rand_nonzero(shape, eps=3e-4):
388     r = np.asarray(test_rng.rand(*shape), dtype=config.floatX)
389     r = r * (1 - eps) + eps * (r &gt;= 0.5)
390     r = r * 2 - 1
391     return r
392 def randint(*shape):
393     return test_rng.randint(-5, 6, shape)
394 def randuint32(*shape):
395     return np.array(test_rng.randint(5, size=shape), dtype=np.uint32)
396 def randuint16(*shape):
397     return np.array(test_rng.randint(5, size=shape), dtype=np.uint16)
398 def randcomplex(*shape):
399     r = np.asarray(test_rng.rand(*shape), dtype=config.floatX)
400     return np.complex128(2 * r - 1)
401 def randcomplex_nonzero(shape, eps=1e-4):
402     return np.complex128(rand_nonzero(shape, eps))
403 def randint_nonzero(*shape):
404     r = test_rng.randint(-5, 5, shape)
405     return r + (r == 0) * 5
406 def rand_ranged(min, max, shape):
407     return np.asarray(test_rng.rand(*shape) * (max - min) + min,
408                       dtype=config.floatX)
409 def randint_ranged(min, max, shape):
410     return test_rng.randint(min, max + 1, shape)
411 def randc128_ranged(min, max, shape):
412     return np.asarray(test_rng.rand(*shape) * (max - min) + min,
413                       dtype='complex128')
414 def rand_of_dtype(shape, dtype):
415     if dtype in tensor.discrete_dtypes:
416         return randint(*shape).astype(dtype)
417     elif dtype in tensor.float_dtypes:
418         return rand(*shape).astype(dtype)
419     elif dtype in tensor.complex_dtypes:
420         return randcomplex(*shape).astype(dtype)
421     else:
422         raise TypeError()
423 _eps = 1e-2
424 def makeBroadcastTester(op, expected, checks=None, name=None, **kwargs):
425     if checks is None:
426         checks = {}
427     if name is None:
428         name = str(op)
429     capitalize = False
430     if name.startswith('Elemwise{') and name.endswith(',no_inplace}'):
431         name = name[9:-12]
432         capitalize = True
433     elif name.endswith('_inplace'):
434         capitalize = True
435     if capitalize:
436         name = ''.join([x.capitalize() for x in name.split('_')])
437     if not name.endswith('Tester'):
438         name += "Tester"
439     if 'inplace' in kwargs:
440         if kwargs['inplace']:
441             _expected = expected
442             if not isinstance(_expected, dict):
443                 def expected(*inputs):
444                     return np.array(_expected(*inputs), dtype=inputs[0].dtype)
445             def inplace_check(inputs, outputs):
446                 return np.all(inputs[0] == outputs[0])
447             checks = dict(checks, inplace_check=inplace_check)
448         del kwargs['inplace']
449     return makeTester(name, op, expected, checks, **kwargs)
450 _good_broadcast_binary_normal = dict(
451     same_shapes=(rand(2, 3), rand(2, 3)),
452     not_same_dimensions=(rand(2, 2), rand(2)),
453     scalar=(rand(2, 3), rand(1, 1)),
454     row=(rand(2, 3), rand(1, 3)),
455     column=(rand(2, 3), rand(2, 1)),
456     integers=(randint(2, 3), randint(2, 3)),
457     uint32=(randuint32(2, 3), randuint32(2, 3)),
458     uint16=(randuint16(2, 3), randuint16(2, 3)),
459     dtype_mixup_1=(rand(2, 3), randint(2, 3)),
460     dtype_mixup_2=(randint(2, 3), rand(2, 3)),
461     complex1=(randcomplex(2, 3), randcomplex(2, 3)),
462     complex2=(randcomplex(2, 3), rand(2, 3)),
463     empty=(np.asarray([], dtype=config.floatX),
464            np.asarray([1], dtype=config.floatX)),
465     )
466 <a name="30"></a>_bad_build_broadcast_binary_normal = dict()
467 _bad_runtime_broadcast_binary_normal = dict(
468     bad_shapes=(rand(2, 3), rand<font color="#ae694a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, 2)),
469     bad_row=(rand(2, 3), rand(1, 2)))
470 _grad_broadcast_binary_normal = dict(
471     same_shapes=(rand(2, 3), rand(2, 3)),
472     scalar=</b></font>(rand(2, 3), rand(1, 1)),
473     row=(rand(2, 3), rand(1, 3)),
474     column=(rand(2, 3), rand(2, 1)),
475     )
476 def check_floatX(inputs, rval):
477     if (isinstance(rval, np.ndarray) and
478             rval.dtype == 'float64' and
479             config.cast_policy == 'numpy+floatX' and
480             config.floatX == 'float32' and
481             all(x.dtype != 'float64' for x in inputs)):
482         return rval.astype('float32')
483     else:
484         return rval
485 AddTester = makeBroadcastTester(
486     op=add,
487     expected=lambda *inputs: check_floatX(
488         inputs, reduce(lambda x, y: x + y, inputs)),
489     good=dict(
490         three_inputs_same_shapes=(rand(2, 3),
491                                   rand(2, 3),
492                                   rand(2, 3)),
493         three_inputs_same_shapes_uint=(randuint32(2, 3),
494                                        randuint32(2, 3),
495                                        randuint32(2, 3)),
496         four_inputs_broadcast=(rand(2, 3),
497                                rand(1, 3),
498                                rand(2, 1),
499                                rand(1, 1)),
500         **_good_broadcast_binary_normal),
501     bad_build=_bad_build_broadcast_binary_normal,
502     bad_runtime=_bad_runtime_broadcast_binary_normal)
503 AddInplaceTester = makeBroadcastTester(
504     op=inplace.add_inplace,
505     expected=lambda x, y: x + y,
506     good=_good_broadcast_binary_normal,
507     bad_build=_bad_build_broadcast_binary_normal,
508     bad_runtime=_bad_runtime_broadcast_binary_normal,
509     inplace=True)
510 SubTester = makeBroadcastTester(
511     op=sub,
512     expected=lambda x, y: check_floatX((x, y), x - y),
513     good=_good_broadcast_binary_normal,
514     bad_build=_bad_build_broadcast_binary_normal,
515     bad_runtime=_bad_runtime_broadcast_binary_normal,
516     grad=_grad_broadcast_binary_normal)
517 SubInplaceTester = makeBroadcastTester(op=inplace.sub_inplace,
518                                        expected=lambda x, y: x - y,
519                                        good=_good_broadcast_binary_normal,
520                                        bad_build=_bad_build_broadcast_binary_normal,
521                                        bad_runtime=_bad_runtime_broadcast_binary_normal,
522                                        inplace=True)
523 SwitchTester = makeBroadcastTester(
524     op=switch,
525     expected=np.where,
526     good=dict(all_true=(np.asarray(1, dtype=config.floatX),
527                         rand(4, 5), rand(4, 5)),
528               false_true=(np.asarray(0, dtype=config.floatX),
529                           rand(4, 5), rand(4, 5)),
530               mixed=(randint_ranged(0, 1, (4, 5)),
531                      rand(4, 5), rand(4, 5))
532               ),
533     bad_build=dict(all_true=(np.asarray(1, dtype=config.floatX),
534                              rand(4, 5))),
535     bad_runtime=dict(all_true=(np.asarray(1, dtype=config.floatX),
536                                rand(3, 5), rand(4, 5)),
537                      false_true=(np.asarray(0, dtype=config.floatX),
538                                  rand(4, 6), rand(4, 5)),
539                      ),
540     grad=dict(all_true=(np.asarray(1, dtype=config.floatX),
541                         rand(4, 5), rand(4, 5)),
542               ),
543 )
544 MaximumTester = makeBroadcastTester(
545     op=maximum,
546     expected=lambda *inputs: check_floatX(inputs, np.maximum(*inputs)),
547     good=_good_broadcast_binary_normal,
548     bad_build=_bad_build_broadcast_binary_normal,
549     bad_runtime=_bad_runtime_broadcast_binary_normal,
550     grad=_grad_broadcast_binary_normal)
551 MaximumInplaceTester = makeBroadcastTester(
552     op=inplace.maximum_inplace,
553     expected=np.maximum,
554     good=_good_broadcast_binary_normal,
555     bad_build=_bad_build_broadcast_binary_normal,
556     bad_runtime=_bad_runtime_broadcast_binary_normal,
557     inplace=True)
558 def test_maximum_minimum_grad():
559     x, y = tensor.vectors('xy')
560     for op in [tensor.maximum, tensor.minimum]:
561         o = op(x, y)
562         g = theano.grad(o.sum(), [x, y])
563         f = theano.function([x, y], g)
564         assert np.allclose(f([1], [1]), [[1], [0]])
565 MinimumTester = makeBroadcastTester(
566     op=minimum,
567     expected=lambda *inputs: check_floatX(inputs, np.minimum(*inputs)),
568     good=_good_broadcast_binary_normal,
569     bad_build=_bad_build_broadcast_binary_normal,
570     bad_runtime=_bad_runtime_broadcast_binary_normal,
571     grad=_grad_broadcast_binary_normal)
572 MinimumInplaceTester = makeBroadcastTester(
573     op=inplace.minimum_inplace,
574     expected=np.minimum,
575     good=_good_broadcast_binary_normal,
576     bad_build=_bad_build_broadcast_binary_normal,
577     bad_runtime=_bad_runtime_broadcast_binary_normal,
578     inplace=True)
579 MulTester = makeBroadcastTester(
580     op=mul,
581     expected=lambda *inputs: check_floatX(inputs, reduce(lambda x, y: x * y, inputs)),
582     good=dict(three_inputs_same_shapes=(rand(2, 3), rand(2, 3), rand(2, 3)),
583               four_inputs_broadcast=(rand(2, 3), rand(1, 3), rand(2, 1), rand(1, 1)),
584               **_good_broadcast_binary_normal),
585     bad_build=_bad_build_broadcast_binary_normal,
586     bad_runtime=_bad_runtime_broadcast_binary_normal,
587     grad=dict(three_inputs_same_shapes=(rand(2, 3), rand(2, 3), rand(2, 3)),
588               four_inputs_broadcast=(rand(2, 3), rand(1, 3), rand(2, 1), rand(1, 1)),
589               **_grad_broadcast_binary_normal))
590 MulInplaceTester = makeBroadcastTester(
591     op=inplace.mul_inplace,
592     expected=lambda x, y: x * y,
593     good=_good_broadcast_binary_normal,
594     bad_build=_bad_build_broadcast_binary_normal,
595     bad_runtime=_bad_runtime_broadcast_binary_normal,
596     inplace=True)
597 def copymod(dct, without=None, **kwargs):
598     if without is None:
599         without = []
600     rval = copy(dct)
601     for a in without:
602         if a in rval:
603             del rval[a]
604     for kw, val in iteritems(kwargs):
605 <a name="7"></a>        rval[kw] = val
606     return rval
607 _good_broadcast_div_mod_normal_float_no_complex = dict<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(
608     same_shapes=(rand(2, 3), rand_nonzero((2, 3))),
609     scalar=(rand(2, 3), rand_nonzero((1, 1))),
610     row=(rand(2, 3), rand_nonzero((1</b></font>, 3))),
611     column=(rand(2, 3), rand_nonzero((2, 1))),
612     dtype_mixup_1=(rand(2, 3), randint_nonzero(2, 3)),
613     dtype_mixup_2=(randint_nonzero(2, 3), rand_nonzero((2, 3))),
614     integer=(randint(2, 3), randint_nonzero(2, 3)),
615     uint8=(randint(2, 3).astype("uint8"),
616            randint_nonzero(2, 3).astype("uint8")),
617     uint16=(randint(2, 3).astype("uint16"),
618             randint_nonzero(2, 3).astype("uint16")),
619     int8=[np.tile(np.arange(-127, 128, dtype='int8'), [254, 1]).T,
620           np.tile(np.array(list(range(-127, 0)) + list(range(1, 128)),
621                            dtype='int8'),
622                   [255, 1])],
623     )
624 if PY3:
625     _good_broadcast_div_mod_normal_float_inplace = copymod(
626         _good_broadcast_div_mod_normal_float_no_complex,
627         empty1=(np.asarray([]), np.asarray([1])),
628         )
629 else:
630     _good_broadcast_div_mod_normal_float_inplace = copymod(
631         _good_broadcast_div_mod_normal_float_no_complex,
632         empty1=(np.asarray([], dtype=config.floatX),
633                 np.asarray([1], dtype=config.floatX)),
634         complex1=(randcomplex(2, 3), randcomplex_nonzero((2, 3))),
635         complex2=(randcomplex(2, 3), rand_nonzero((2, 3))),
636         )
637 _good_broadcast_div_mod_normal_float = copymod(
638     _good_broadcast_div_mod_normal_float_inplace,
639     empty2=(np.asarray([0], dtype=config.floatX),
640             np.asarray([], dtype=config.floatX))
641     )
642 _grad_broadcast_div_mod_normal = dict(
643     same_shapes=(rand(2, 3), rand_nonzero((2, 3))),
644     scalar=(rand(2, 3), rand_nonzero((1, 1))),
645     row=(rand(2, 3), rand_nonzero((1, 3))),
646     column=(rand(2, 3), rand_nonzero((2, 1))),
647     )
648 div_grad_rtol = None
649 if config.floatX == 'float32':
650     div_grad_rtol = 0.025
651 def _numpy_true_div(x, y):
652     out = np.true_divide(x, y)
653     if x.dtype in tensor.discrete_dtypes and y.dtype in tensor.discrete_dtypes:
654         out = theano._asarray(out, dtype=config.floatX)
655     return out
656 TrueDivTester = makeBroadcastTester(
657     op=tensor.true_div,
658     expected=_numpy_true_div,
659     good=_good_broadcast_div_mod_normal_float_no_complex,
660     grad=_grad_broadcast_div_mod_normal,
661     grad_rtol=div_grad_rtol,
662     )
663 TrueDivInplaceTester = makeBroadcastTester(
664     op=inplace.true_div_inplace,
665     expected=_numpy_true_div,
666     good=copymod(
667         _good_broadcast_div_mod_normal_float_inplace,
668         without=['integer', 'uint8', 'uint16', 'int8']),
669     grad_rtol=div_grad_rtol,
670     inplace=True)
671 _good_inv = dict(
672     normal=[5 * rand_nonzero((2, 3))],
673     integers=[randint_nonzero(2, 3)],
674     int8=[np.array(list(range(-127, 0)) + list(range(1, 127)), dtype='int8')],
675     uint8=[np.array(list(range(0, 255)), dtype='uint8')],
676     uint16=[np.array(list(range(0, 65535)), dtype='uint16')],
677     complex=[randcomplex_nonzero((2, 3))],
678     empty=[np.asarray([], dtype=config.floatX)])
679 _good_inv_inplace = copymod(_good_inv, without=['integers', 'int8', 'uint8', 'uint16', 'complex'])
680 _grad_inv = copymod(_good_inv,
681                     without=['integers', 'int8', 'uint8', 'uint16', 'complex', 'empty'])
682 _bad_runtime_inv = dict(
683     float=[np.zeros((2, 3))],
684     integers=[np.zeros((2, 3), dtype='int64')],
685     int8=[np.zeros((2, 3), dtype='int8')],
686     complex=[np.zeros((2, 3), dtype='complex128')])
687 InvTester = makeBroadcastTester(
688     op=tensor.inv,
689     expected=lambda x: upcast_int8_nfunc(np.true_divide)(np.int8(1), x),
690     good=_good_inv,
691     bad_runtime=_bad_runtime_inv,
692     grad=_grad_inv,
693     grad_rtol=div_grad_rtol)
694 InvInplaceTester = makeBroadcastTester(
695     op=inplace.inv_inplace,
696     expected=lambda x: _numpy_true_div(np.int8(1), x),
697     good=_good_inv_inplace,
698     bad_runtime=_bad_runtime_inv,
699     grad_rtol=div_grad_rtol,
700     inplace=True)
701 CeilIntDivTester = makeBroadcastTester(
702     op=tensor.ceil_intdiv,
703     expected=lambda x, y: check_floatX((x, y), (x // y) + ((x % y) != 0)),
704     good=_good_broadcast_div_mod_normal_float_no_complex,
705     name='CeilIntDiv',
706     )
707 ModTester = makeBroadcastTester(
708     op=tensor.mod,
709     expected=lambda x, y: np.asarray(
710         x % y, dtype=theano.scalar.basic.upcast(x.dtype, y.dtype)),
711     good=copymod(_good_broadcast_div_mod_normal_float,
712                  ['complex1', 'complex2']),
713     grad=_grad_broadcast_div_mod_normal,
714     grad_eps=1e-5,
715     )
716 ModInplaceTester = makeBroadcastTester(
717     op=inplace.mod_inplace,
718     expected=lambda x, y: np.asarray(
719         x % y, dtype=theano.scalar.basic.upcast(x.dtype, y.dtype)),
720     good=copymod(_good_broadcast_div_mod_normal_float_inplace,
721                  ["complex1", "complex2"]),
722     grad_eps=1e-5,
723     inplace=True)
724 _good_broadcast_pow_normal_float = dict(
725     same_shapes=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 3))),
726     scalar=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 1))),
727     row=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 3))),
728 <a name="3"></a>    column=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 1))),
729     dtype_mixup=(rand_ranged(-3, 3, (2, 3)), randint_ranged(-3, 3, (2, 3))),
730     complex1=(randcomplex(2, 3), randcomplex(2, 3)),
731     complex2=(randcomplex(2, 3), rand<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(2, 3)),
732     empty1=(np.asarray([], dtype=config.floatX),
733             np.asarray([1], dtype=config.floatX)),
734     empty2=(np.asarray([0], dtype=config.floatX),
735             np.asarray([], dtype=config.</b></font>floatX)),
736     empty3=(np.asarray([], dtype=config.floatX),
737             np.asarray([], dtype=config.floatX)),
738     )
739 _grad_broadcast_pow_normal = dict(
740     same_shapes=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 3))),
741     scalar=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 1))),
742     row=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (1, 3))),
743     column=(rand_ranged(1, 5, (2, 3)), rand_ranged(-3, 3, (2, 1))),
744     x_eq_zero=(
745         np.asarray([0.], dtype=config.floatX),
746         np.asarray([2.], dtype=config.floatX)
747         ),  # Test for issue 1780
748     )
749 _good_broadcast_pow_normal_float_pow = copy(_good_broadcast_pow_normal_float)
750 del _good_broadcast_pow_normal_float_pow["empty2"]
751 m = copy(theano.compile.get_default_mode())
752 m.check_isfinite = False
753 PowTester = makeBroadcastTester(
754     op=pow,
755     expected=lambda x, y: check_floatX((x, y), x ** y),
756     good=_good_broadcast_pow_normal_float,
757     grad=_grad_broadcast_pow_normal,
758     name='Pow',
759     mode=m
760 )
761 PowInplaceTester = makeBroadcastTester(
762     op=inplace.pow_inplace,
763     expected=lambda x, y: x ** y,
764     good=_good_broadcast_pow_normal_float_pow,
765     inplace=True,
766     mode=m
767 )
768 corner_case = np.asarray(
769     [-2.5, -2., -1.5, -1., -0.5, -.51, -.49, 0,
770      0.49, 0.5, 0.9, 1, 1.5, 2, 2.5],
771     dtype=floatX)
772 corner_case_grad = np.asarray(
773     [-2.5, -2., -1.5, -1., -0.5, -.51, -.49,
774      0.49, 0.5, 0.9, 1, 1.5, 2, 2.5],
775     dtype=floatX)
776 _good_broadcast_unary_normal_float = dict(
777     normal=[rand_ranged(-5, 5, (2, 3))],
778     corner_case=[corner_case],
779 <a name="21"></a>    complex=[randcomplex(2, 3)],
780     empty=[np.asarray([], dtype=config.floatX)])
781 _good_broadcast_unary_normal_float_no_empty <font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= copymod(
782     _good_broadcast_unary_normal_float,
783     without=['empty'])
784 _good_broadcast_unary_normal_float_no_empty_no_complex = copymod(
785     _good_broadcast_unary_normal_float_no_empty,
786     without=['complex'])
787 _good_broadcast_unary_normal_float_no_complex = copymod(
788     _good_broadcast_unary_normal_float,
789     without=['complex'])
790 _good_broadcast_unary_normal_float_no_complex_small_neg_range = dict(
791     normal=</b></font>[rand_ranged(-2, 5, (2, 3))],
792     corner_case=[corner_case],
793     empty=[np.asarray([], dtype=config.floatX)])
794 _good_broadcast_unary_normal = dict(
795     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)),
796                        dtype=config.floatX)],
797     integers=[randint_ranged(-5, 5, (2, 3))],
798     int8=[np.arange(-127, 128, dtype='int8')],
799     uint8=[np.arange(0, 255, dtype='uint8')],
800     uint16=[np.arange(0, 65535, dtype='uint16')],
801     corner_case=[corner_case],
802     complex=[randcomplex(2, 3)],
803     empty=[np.asarray([], dtype=config.floatX)],
804     )
805 _good_broadcast_unary_normal_no_complex = dict(
806     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
807     integers=[randint_ranged(-5, 5, (2, 3))],
808     int8=[np.arange(-127, 128, dtype='int8')],
809     uint8=[np.arange(0, 89, dtype='uint8')],
810     uint16=[np.arange(0, 89, dtype='uint16')],
811     corner_case=[corner_case],
812     empty=[np.asarray([], dtype=config.floatX)],
813     )
814 _grad_broadcast_unary_normal_no_complex = dict(
815     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
816     corner_case=[corner_case_grad])
817 _grad_broadcast_unary_normal = dict(
818     normal=[np.asarray(rand_ranged(-5, 5, (2, 3)), dtype=floatX)],
819     corner_case=[corner_case_grad],
820     )
821 _grad_broadcast_unary_normal_noint = dict(
822     normal=[(rand_ranged(_eps, 1 - _eps, (2, 3)) + randint(2, 3))
823             .astype(floatX)])
824 _grad_broadcast_unary_normal_small_neg_range = dict(
825     normal=[np.asarray(rand_ranged(-2, 5, (2, 3)), dtype=floatX)],
826     corner_case=[corner_case_grad])
827 _grad_broadcast_unary_normal_no_complex_no_corner_case = copymod(
828     _grad_broadcast_unary_normal_no_complex,
829     without=['corner_case'])
830 _grad_broadcast_unary_abs1_no_complex = dict(
831     normal=[np.asarray(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)), dtype=floatX)],
832     )
833 _grad_broadcast_unary_0_2_no_complex = dict(
834     normal=[np.asarray(rand_ranged(_eps, 1 - _eps, (2, 3)), dtype=floatX)],
835     )
836 AbsTester = makeBroadcastTester(
837     op=tensor.abs_,
838     expected=lambda x: abs(x),
839     good=_good_broadcast_unary_normal,
840     grad=_grad_broadcast_unary_normal)
841 _good_broadcast_unary_normal_abs = copy(_good_broadcast_unary_normal)
842 del _good_broadcast_unary_normal_abs['complex']
843 AbsInplaceTester = makeBroadcastTester(
844     op=inplace.abs__inplace,
845     expected=lambda x: np.abs(x),
846     good=_good_broadcast_unary_normal_abs,
847     inplace=True)
848 NegTester = makeBroadcastTester(
849     op=tensor.neg,
850     expected=lambda x: -x,
851     good=_good_broadcast_unary_normal,
852     grad=_grad_broadcast_unary_normal)
853 NegInplaceTester = makeBroadcastTester(
854     op=inplace.neg_inplace,
855     expected=lambda x: -x,
856     good=_good_broadcast_unary_normal,
857     inplace=True)
858 SgnTester = makeBroadcastTester(
859     op=tensor.sgn,
860     expected=np.sign,
861     good=_good_broadcast_unary_normal_no_complex,
862     grad=_grad_broadcast_unary_normal,)
863 SgnInplaceTester = makeBroadcastTester(
864     op=inplace.sgn_inplace,
865     expected=np.sign,
866     good=_good_broadcast_unary_normal_no_complex,
867     inplace=True)
868 IntDivTester = makeBroadcastTester(
869     op=tensor.int_div,
870     expected=lambda x, y: check_floatX((x, y), x // y),
871     good=_good_broadcast_div_mod_normal_float,
872     )
873 IntDivInplaceTester = makeBroadcastTester(
874     op=inplace.int_div_inplace,
875     expected=lambda x, y: check_floatX((x, y), x // y),
876     good=_good_broadcast_div_mod_normal_float_inplace,
877     inplace=True
878     )
879 CeilTester = makeBroadcastTester(
880     op=tensor.ceil,
881     expected=upcast_float16_ufunc(np.ceil),
882     good=_good_broadcast_unary_normal_no_complex,
883     grad=copymod(_grad_broadcast_unary_normal_noint,
884                  extra=[np.asarray([-2.5, -1.5, -1.51, 0.49, .98, 1.02],
885                                    dtype=floatX)]))
886 CeilInplaceTester = makeBroadcastTester(
887     op=inplace.ceil_inplace,
888     expected=upcast_float16_ufunc(np.ceil),
889     good=copymod(_good_broadcast_unary_normal_no_complex,
890                  without=['integers', 'int8', 'uint8', 'uint16']),
891     inplace=True)
892 FloorTester = makeBroadcastTester(
893     op=tensor.floor,
894     expected=upcast_float16_ufunc(np.floor),
895     good=_good_broadcast_unary_normal_no_complex,
896     grad=_grad_broadcast_unary_normal_noint)
897 FloorInplaceTester = makeBroadcastTester(
898     op=inplace.floor_inplace,
899     expected=upcast_float16_ufunc(np.floor),
900     good=copymod(_good_broadcast_unary_normal_no_complex,
901                  without=["integers", "int8", "uint8", "uint16"]),
902     inplace=True)
903 TruncInplaceTester = makeBroadcastTester(
904     op=inplace.trunc_inplace,
905     expected=upcast_float16_ufunc(np.trunc),
906     good=_good_broadcast_unary_normal_no_complex,
907     inplace=True)
908 TruncTester = makeBroadcastTester(
909     op=tensor.trunc,
910     expected=upcast_float16_ufunc(np.trunc),
911     good=_good_broadcast_unary_normal_no_complex)
912 RoundHalfToEvenTester = makeBroadcastTester(
913     op=tensor.round_half_to_even,
914     expected=np.round,
915     good=_good_broadcast_unary_normal_float_no_complex,
916     grad=_grad_broadcast_unary_normal_no_complex_no_corner_case)
917 RoundHalfToEvenInplaceTester = makeBroadcastTester(
918     op=inplace.round_half_to_even_inplace,
919     expected=np.round,
920     good=_good_broadcast_unary_normal_float_no_complex,
921     inplace=True)
922 RoundHalfAwayFromZeroTester = makeBroadcastTester(
923     op=tensor.round_half_away_from_zero,
924     expected=lambda a: theano.scalar.basic.round_half_away_from_zero_vec(a),
925     good=_good_broadcast_unary_normal_float_no_empty_no_complex,
926     grad=_grad_broadcast_unary_normal_no_complex_no_corner_case)
927 RoundHalfAwayFromZeroInplaceTester = makeBroadcastTester(
928     op=inplace.round_half_away_from_zero_inplace,
929     expected=lambda a: theano.scalar.basic.round_half_away_from_zero_vec(a),
930     good=_good_broadcast_unary_normal_float_no_empty_no_complex,
931     inplace=True)
932 SqrTester = makeBroadcastTester(
933     op=tensor.sqr,
934     expected=np.square,
935     good=_good_broadcast_unary_normal,
936     grad=_grad_broadcast_unary_normal)
937 SqrInplaceTester = makeBroadcastTester(
938     op=inplace.sqr_inplace,
939     expected=np.square,
940     good=_good_broadcast_unary_normal,
941     inplace=True)
942 ExpTester = makeBroadcastTester(
943     op=tensor.exp,
944     expected=upcast_float16_ufunc(np.exp),
945     good=dict(_good_broadcast_unary_normal,
946               int8=[np.arange(-127, 89, dtype='int8')],
947               uint8=[np.arange(0, 89, dtype='uint8')],
948               uint16=[np.arange(0, 89, dtype='uint16')]),
949     grad=_grad_broadcast_unary_normal)
950 ExpInplaceTester = makeBroadcastTester(
951     op=inplace.exp_inplace,
952     expected=np.exp,
953     good=_good_broadcast_unary_normal_float,
954     inplace=True)
955 Exp2Tester = makeBroadcastTester(
956     op=tensor.exp2,
957     expected=upcast_float16_ufunc(np.exp2),
958     good=_good_broadcast_unary_normal,
959     grad=_grad_broadcast_unary_normal)
960 Exp2InplaceTester = makeBroadcastTester(
961     op=inplace.exp2_inplace,
962     expected=np.exp2,
963     good=_good_broadcast_unary_normal_float,
964     inplace=True)
965 Expm1Tester = makeBroadcastTester(
966     op=tensor.expm1,
967     expected=upcast_float16_ufunc(np.expm1),
968     good=dict(_good_broadcast_unary_normal,
969               int8=[np.arange(-127, 89, dtype='int8')],
970               uint8=[np.arange(0, 89, dtype='uint8')],
971               uint16=[np.arange(0, 89, dtype='uint16')]),
972     grad=_grad_broadcast_unary_normal)
973 Expm1InplaceTester = makeBroadcastTester(
974     op=inplace.expm1_inplace,
975     expected=np.expm1,
976     good=_good_broadcast_unary_normal_float,
977     inplace=True)
978 _good_broadcast_unary_positive = dict(
979     normal=(rand_ranged(0.001, 5, (2, 3)),),
980     integers=(randint_ranged(1, 5, (2, 3)),),
981     uint8=[np.arange(1, 256, dtype='uint8')],
982     complex=(randc128_ranged(1, 5, (2, 3)),),
983     empty=(np.asarray([], dtype=config.floatX),),
984     )
985 _good_broadcast_unary_positive_float = copymod(
986     _good_broadcast_unary_positive,
987     without=['integers', 'uint8'])
988 _grad_broadcast_unary_positive = dict(normal=(rand_ranged(_eps, 5, (2, 3)),),)
989 LogTester = makeBroadcastTester(
990     op=tensor.log,
991     expected=upcast_float16_ufunc(np.log),
992     good=_good_broadcast_unary_positive,
993     grad=_grad_broadcast_unary_positive)
994 LogInplaceTester = makeBroadcastTester(
995     op=inplace.log_inplace,
996     expected=np.log,
997     good=_good_broadcast_unary_positive_float,
998     inplace=True)
999 Log2Tester = makeBroadcastTester(
1000     op=tensor.log2,
1001     expected=upcast_float16_ufunc(np.log2),
1002     good=_good_broadcast_unary_positive,
1003     grad=_grad_broadcast_unary_positive)
1004 Log2InplaceTester = makeBroadcastTester(
1005     op=inplace.log2_inplace,
1006     expected=np.log2,
1007     good=_good_broadcast_unary_positive_float,
1008     inplace=True)
1009 Log10Tester = makeBroadcastTester(
1010     op=tensor.log10,
1011     expected=upcast_float16_ufunc(np.log10),
1012     good=_good_broadcast_unary_positive,
1013     grad=_grad_broadcast_unary_positive)
1014 Log10InplaceTester = makeBroadcastTester(
1015     op=inplace.log10_inplace,
1016     expected=np.log10,
1017     good=_good_broadcast_unary_positive_float,
1018     inplace=True)
1019 Log1pTester = makeBroadcastTester(
1020     op=tensor.log1p,
1021     expected=upcast_float16_ufunc(np.log1p),
1022     good=_good_broadcast_unary_positive,
1023     grad=_grad_broadcast_unary_positive)
1024 Log1pInplaceTester = makeBroadcastTester(
1025     op=inplace.log1p_inplace,
1026     expected=np.log1p,
1027     good=_good_broadcast_unary_positive_float,
1028     inplace=True)
1029 SqrtTester = makeBroadcastTester(
1030     op=tensor.sqrt,
1031     expected=upcast_float16_ufunc(np.sqrt),
1032     good=_good_broadcast_unary_positive,
1033     grad=_grad_broadcast_unary_positive)
1034 SqrtInplaceTester = makeBroadcastTester(
1035     op=inplace.sqrt_inplace,
1036     expected=np.sqrt,
1037     good=_good_broadcast_unary_positive_float,
1038     inplace=True)
1039 _good_broadcast_unary_wide = dict(
1040     normal=(rand_ranged(-1000, 1000, (2, 3)),),
1041     integers=(randint_ranged(-1000, 1000, (2, 3)),),
1042     int8=[np.arange(-127, 128, dtype='int8')],
1043     uint8=[np.arange(0, 255, dtype='uint8')],
1044     uint16=[np.arange(0, 65535, dtype='uint16')],
1045     complex=(randc128_ranged(-1000, 1000, (2, 3)),),
1046     empty=(np.asarray([], dtype=config.floatX),),)
1047 _good_broadcast_unary_wide_float = copymod(
1048     _good_broadcast_unary_wide,
1049     without=['integers', 'int8', 'uint8', 'uint16'])
1050 _grad_broadcast_unary_wide = dict(normal=(rand_ranged(-1000, 1000, (2, 3)),),)
1051 if theano.config.floatX == 'float32':
1052     angle_eps = 1e-4
1053 else:
1054     angle_eps = 1e-10
1055 Deg2radTester = makeBroadcastTester(
1056     op=tensor.deg2rad,
1057     expected=upcast_float16_ufunc(np.deg2rad),
1058     good=_good_broadcast_unary_normal_no_complex,
1059     grad=_grad_broadcast_unary_normal_no_complex,
1060     eps=angle_eps)
1061 Deg2radInplaceTester = makeBroadcastTester(
1062     op=inplace.deg2rad_inplace,
1063     expected=np.deg2rad,
1064     good=_good_broadcast_unary_normal_float_no_complex,
1065     inplace=True,
1066     eps=angle_eps)
1067 Rad2degTester = makeBroadcastTester(
1068     op=tensor.rad2deg,
1069     expected=upcast_float16_ufunc(np.rad2deg),
1070     good=_good_broadcast_unary_normal_no_complex,
1071     grad=_grad_broadcast_unary_normal_no_complex,
1072     eps=angle_eps)
1073 Rad2degInplaceTester = makeBroadcastTester(
1074     op=inplace.rad2deg_inplace,
1075     expected=np.rad2deg,
1076     good=_good_broadcast_unary_normal_float_no_complex,
1077     inplace=True,
1078     eps=angle_eps)
1079 SinTester = makeBroadcastTester(
1080     op=tensor.sin,
1081     expected=upcast_float16_ufunc(np.sin),
1082     good=_good_broadcast_unary_wide,
1083     grad=_grad_broadcast_unary_wide)
1084 SinInplaceTester = makeBroadcastTester(
1085     op=inplace.sin_inplace,
1086     expected=np.sin,
1087     good=_good_broadcast_unary_wide_float,
1088     inplace=True)
1089 _good_broadcast_unary_arcsin = dict(
1090     normal=(rand_ranged(-1, 1, (2, 3)),),
1091     integers=(randint_ranged(-1, 1, (2, 3)),),
1092     int8=[np.arange(-1, 2, dtype='int8')],
1093     uint8=[np.arange(0, 2, dtype='uint8')],
1094     uint16=[np.arange(0, 2, dtype='uint16')],
1095     complex=(randc128_ranged(-1, 1, (2, 3)),),
1096     empty=(np.asarray([], dtype=config.floatX),),)
1097 _good_broadcast_unary_arcsin_float = copymod(
1098     _good_broadcast_unary_arcsin,
1099     without=['integers', 'int8', 'uint8', 'uint16'])
1100 _grad_broadcast_unary_arcsin = dict(normal=(rand_ranged(-0.9, 0.9, (2, 3)),),)
1101 ArcsinTester = makeBroadcastTester(
1102     op=tensor.arcsin,
1103     expected=upcast_float16_ufunc(np.arcsin),
1104     good=_good_broadcast_unary_arcsin,
1105     grad=_grad_broadcast_unary_arcsin)
1106 ArcsinInplaceTester = makeBroadcastTester(
1107     op=inplace.arcsin_inplace,
1108     expected=np.arcsin,
1109     good=_good_broadcast_unary_arcsin_float,
1110     inplace=True)
1111 CosTester = makeBroadcastTester(
1112     op=tensor.cos,
1113     expected=upcast_float16_ufunc(np.cos),
1114     good=_good_broadcast_unary_wide,
1115     grad=_grad_broadcast_unary_wide)
1116 CosInplaceTester = makeBroadcastTester(
1117     op=inplace.cos_inplace,
1118     expected=np.cos,
1119     good=_good_broadcast_unary_wide_float,
1120     inplace=True)
1121 def test_py_c_match():
1122     a = tensor.TensorType(dtype='int8', broadcastable=(False,))()
1123     f = theano.function([a], tensor.arccos(a), mode='DebugMode')
1124     f(np.asarray([1, 0, -1], dtype='int8'))
1125 ArccosTester = makeBroadcastTester(
1126     op=tensor.arccos,
1127     expected=upcast_float16_ufunc(np.arccos),
1128     good=_good_broadcast_unary_arcsin,
1129     grad=_grad_broadcast_unary_arcsin)
1130 ArccosInplaceTester = makeBroadcastTester(
1131     op=inplace.arccos_inplace,
1132     expected=np.arccos,
1133     good=_good_broadcast_unary_arcsin_float,
1134     inplace=True)
1135 _good_broadcast_unary_tan = dict(
1136     normal=(rand_ranged(-3.14, 3.14, (2, 3)),),
1137     shifted=(rand_ranged(3.15, 6.28, (2, 3)),),
1138     integers=(randint_ranged(-3, 3, (2, 3)),),
1139     int8=[np.arange(-3, 4, dtype='int8')],
1140     uint8=[np.arange(0, 4, dtype='uint8')],
1141     uint16=[np.arange(0, 4, dtype='uint16')],
1142     complex=(randc128_ranged(-3.14, 3.14, (2, 3)),),
1143     empty=(np.asarray([], dtype=config.floatX),),)
1144 _grad_broadcast_unary_tan = dict(normal=(rand_ranged(-1.5, 1.5, (2, 3)),),
1145                                  shifted=(rand_ranged(1.6, 4.6, (2, 3)),))
1146 TanTester = makeBroadcastTester(
1147     op=tensor.tan,
1148     expected=upcast_float16_ufunc(np.tan),
1149     good=_good_broadcast_unary_tan,
1150     grad=_grad_broadcast_unary_tan)
1151 TanInplaceTester = makeBroadcastTester(
1152     op=inplace.tan_inplace,
1153     expected=np.tan,
1154     good=copymod(_good_broadcast_unary_tan, without=['integers', 'int8', 'uint8', 'uint16']),
1155     inplace=True)
1156 ArctanTester = makeBroadcastTester(
1157     op=tensor.arctan,
1158     expected=upcast_float16_ufunc(np.arctan),
1159     good=_good_broadcast_unary_wide,
1160     grad=_grad_broadcast_unary_wide)
1161 ArctanInplaceTester = makeBroadcastTester(
1162     op=inplace.arctan_inplace,
1163     expected=np.arctan,
1164     good=_good_broadcast_unary_wide_float,
1165     inplace=True)
1166 _good_broadcast_binary_arctan2 = dict(
1167     same_shapes=(rand(2, 3), rand(2, 3)),
1168     not_same_dimensions=(rand(2, 2), rand(2)),
1169     scalar=(rand(2, 3), rand(1, 1)),
1170     row=(rand(2, 3), rand(1, 3)),
1171     column=(rand(2, 3), rand(2, 1)),
1172     integers=(randint(2, 3), randint(2, 3)),
1173     int8=[np.arange(-127, 128, dtype='int8'),
1174           np.arange(-127, 128, dtype='int8')[:, np.newaxis]],
1175     uint8=[np.arange(0, 128, dtype='uint8'),
1176            np.arange(0, 128, dtype='uint8')[:, np.newaxis]],
1177     uint16=[np.arange(0, 128, dtype='uint16'),
1178             np.arange(0, 128, dtype='uint16')[:, np.newaxis]],
1179     dtype_mixup_1=(rand(2, 3), randint(2, 3)),
1180     dtype_mixup_2=(randint(2, 3), rand(2, 3)),
1181     empty=(np.asarray([], dtype=config.floatX),
1182            np.asarray([1], dtype=config.floatX)),
1183     )
1184 _grad_broadcast_binary_arctan2 = dict(
1185     same_shapes=(rand(2, 3), rand(2, 3)),
1186     scalar=(rand(2, 3), rand(1, 1)),
1187     row=(rand(2, 3), rand(1, 3)),
1188     column=(rand(2, 3), rand(2, 1)),
1189     )
1190 Arctan2Tester = makeBroadcastTester(
1191     op=tensor.arctan2,
1192     expected=upcast_float16_ufunc(np.arctan2),
1193     good=_good_broadcast_binary_arctan2,
1194     grad=_grad_broadcast_binary_arctan2)
1195 Arctan2InplaceTester = makeBroadcastTester(
1196     op=inplace.arctan2_inplace,
1197     expected=np.arctan2,
1198     good=copymod(_good_broadcast_binary_arctan2,
1199                  without=['integers', 'int8', 'uint8',
1200                           'uint16', 'dtype_mixup_2']),
1201     inplace=True)
1202 CoshTester = makeBroadcastTester(
1203     op=tensor.cosh,
1204     expected=upcast_float16_ufunc(np.cosh),
1205     good=dict(_good_broadcast_unary_normal,
1206               int8=[np.arange(-89, 90, dtype='int8')],
1207               uint8=[np.arange(0, 90, dtype='uint8')],
1208               uint16=[np.arange(0, 90, dtype='uint16')]),
1209     grad=_grad_broadcast_unary_normal)
1210 CoshInplaceTester = makeBroadcastTester(
1211     op=inplace.cosh_inplace,
1212     expected=np.cosh,
1213     good=_good_broadcast_unary_normal_float,
1214     inplace=True)
1215 _good_broadcast_unary_arccosh = dict(
1216     normal=(rand_ranged(1, 1000, (2, 3)),),
1217     integers=(randint_ranged(1, 1000, (2, 3)),),
1218     uint8=[np.arange(1, 256, dtype='uint8')],
1219     complex=(randc128_ranged(1, 1000, (2, 3)),),
1220     empty=(np.asarray([], dtype=config.floatX),),)
1221 _grad_broadcast_unary_arccosh = dict(normal=(rand_ranged(1 + _eps, 1000, (2, 3)),),)
1222 ArccoshTester = makeBroadcastTester(
1223     op=tensor.arccosh,
1224     expected=upcast_float16_ufunc(np.arccosh),
1225     good=_good_broadcast_unary_arccosh,
1226     grad=_grad_broadcast_unary_arccosh)
1227 ArccoshInplaceTester = makeBroadcastTester(
1228     op=inplace.arccosh_inplace,
1229     expected=np.arccosh,
1230     good=copymod(_good_broadcast_unary_arccosh, without=['integers', 'uint8']),
1231     inplace=True)
1232 SinhTester = makeBroadcastTester(
1233     op=tensor.sinh,
1234     expected=upcast_float16_ufunc(np.sinh),
1235     good=dict(_good_broadcast_unary_normal,
1236               int8=[np.arange(-89, 90, dtype='int8')],
1237               uint8=[np.arange(0, 90, dtype='uint8')],
1238               uint16=[np.arange(0, 90, dtype='uint16')]),
1239     grad=_grad_broadcast_unary_normal)
1240 SinhInplaceTester = makeBroadcastTester(
1241     op=inplace.sinh_inplace,
1242     expected=np.sinh,
1243     good=_good_broadcast_unary_normal_float,
1244     inplace=True)
1245 ArcsinhTester = makeBroadcastTester(
1246     op=tensor.arcsinh,
1247     expected=upcast_float16_ufunc(np.arcsinh),
1248     good=_good_broadcast_unary_normal,
1249     grad=_grad_broadcast_unary_normal)
1250 ArcsinhInplaceTester = makeBroadcastTester(
1251     op=inplace.arcsinh_inplace,
1252     expected=np.arcsinh,
1253     good=_good_broadcast_unary_normal_float,
1254     inplace=True)
1255 TanhTester = makeBroadcastTester(
1256     op=tensor.tanh,
1257     expected=upcast_float16_ufunc(np.tanh),
1258     good=_good_broadcast_unary_normal,
1259     grad=_grad_broadcast_unary_normal)
1260 TanhInplaceTester = makeBroadcastTester(
1261     op=inplace.tanh_inplace,
1262     expected=np.tanh,
1263     good=_good_broadcast_unary_normal_float,
1264     inplace=True)
1265 _good_broadcast_unary_arctanh = dict(
1266     normal=(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
1267     integers=(randint_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
1268     int8=[np.arange(0, 1, dtype='int8')],
1269     uint8=[np.arange(0, 1, dtype='uint8')],
1270     uint16=[np.arange(0, 1, dtype='uint16')],
1271     complex=(randc128_ranged(-1 + _eps, 1 - _eps, (2, 3)),),
1272     empty=(np.asarray([], dtype=config.floatX),),)
1273 _grad_broadcast_unary_arctanh = dict(
1274     normal=(rand_ranged(-1 + _eps, 1 - _eps, (2, 3)),),)
1275 ArctanhTester = makeBroadcastTester(
1276     op=tensor.arctanh,
1277     expected=upcast_float16_ufunc(np.arctanh),
1278     good=_good_broadcast_unary_arctanh,
1279     grad=_grad_broadcast_unary_arctanh)
1280 ArctanhInplaceTester = makeBroadcastTester(
1281     op=inplace.arctanh_inplace,
1282     expected=np.arctanh,
1283     good=copymod(_good_broadcast_unary_arctanh, without=['integers', 'int8', 'uint8', 'uint16']),
1284     inplace=True)
1285 <a name="5"></a>if imported_scipy_special:
1286     expected_erf = scipy.special.erf
1287     expected_erfc = scipy.special.erfc
1288     expected_erfinv = scipy.special<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.erfinv
1289     expected_erfcinv = scipy.special.erfcinv
1290     expected_gamma = scipy.special.gamma
1291 <a name="12"></a>    expected_gammaln = scipy.special.gammaln
1292     expected_psi = scipy.special.psi
1293     expected_tri_gamma = partial(scipy.</b></font>special.polygamma, 1)
1294     expected_chi2sf = scipy.stats.chi2<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.sf
1295     expected_j0 = scipy.special.j0
1296     expected_j1 = scipy.special.j1
1297     expected_jv = scipy.special.jv
1298     expected_i0 = scipy.special.i0
1299     expected_i1 =</b></font> scipy.special.i1
1300     expected_iv = scipy.special.iv
1301     skip_scipy = False
1302     expected_erfcx = scipy.special.erfcx
1303 else:
1304     expected_erf = []
1305     expected_erfc = []
1306     expected_erfcx = []
1307     expected_erfinv = []
1308     expected_erfcinv = []
1309     expected_gamma = []
1310     expected_gammaln = []
1311     expected_psi = []
1312     expected_tri_gamma = []
1313     expected_chi2sf = []
1314     expected_j0 = []
1315     expected_j1 = []
1316     expected_jv = []
1317     expected_i0 = []
1318     expected_i1 = []
1319     expected_iv = []
1320     skip_scipy = "scipy is not present"
1321 ErfTester = makeBroadcastTester(
1322     op=tensor.erf,
1323     expected=expected_erf,
1324     good=_good_broadcast_unary_normal,
1325     grad=_grad_broadcast_unary_normal,
1326     eps=2e-10,
1327     mode=mode_no_scipy,
1328     skip=skip_scipy)
1329 ErfInplaceTester = makeBroadcastTester(
1330     op=inplace.erf_inplace,
1331     expected=expected_erf,
1332     good=_good_broadcast_unary_normal_float,
1333     mode=mode_no_scipy,
1334     eps=2e-10,
1335     inplace=True,
1336     skip=skip_scipy)
1337 ErfcTester = makeBroadcastTester(
1338     op=tensor.erfc,
1339     expected=expected_erfc,
1340     good=_good_broadcast_unary_normal_float_no_complex,
1341     grad=_grad_broadcast_unary_normal,
1342     eps=2e-10,
1343     mode=mode_no_scipy,
1344     skip=skip_scipy)
1345 ErfcInplaceTester = makeBroadcastTester(
1346     op=inplace.erfc_inplace,
1347     expected=expected_erfc,
1348     good=_good_broadcast_unary_normal_float_no_complex,
1349     eps=2e-10,
1350     mode=mode_no_scipy,
1351     inplace=True,
1352     skip=skip_scipy)
1353 ErfcxTester = makeBroadcastTester(
1354     op=tensor.erfcx,
1355     expected=expected_erfcx,
1356     good=_good_broadcast_unary_normal_float_no_complex_small_neg_range,
1357     grad=_grad_broadcast_unary_normal_small_neg_range,
1358     eps=2e-10,
1359     mode=mode_no_scipy,
1360     skip=skip_scipy)
1361 ErfcxInplaceTester = makeBroadcastTester(
1362     op=inplace.erfcx_inplace,
1363     expected=expected_erfcx,
1364     good=_good_broadcast_unary_normal_float_no_complex_small_neg_range,
1365     eps=2e-10,
1366     mode=mode_no_scipy,
1367     inplace=True,
1368     skip=skip_scipy)
1369 ErfinvTester = makeBroadcastTester(
1370     op=tensor.erfinv,
1371     expected=expected_erfinv,
1372     good={'normal': [rand_ranged(-.9, .9, (2, 3))],
1373           'empty': [np.asarray([], dtype=config.floatX)]},
1374     grad=_grad_broadcast_unary_abs1_no_complex,
1375     eps=2e-10,
1376     mode=mode_no_scipy,
1377     skip=skip_scipy)
1378 ErfcinvTester = makeBroadcastTester(
1379     op=tensor.erfcinv,
1380     expected=expected_erfcinv,
1381     good={'normal': [rand_ranged(0.001, 1.9, (2, 3))],
1382           'empty': [np.asarray([], dtype=config.floatX)]},
1383     grad=_grad_broadcast_unary_0_2_no_complex,
1384     eps=2e-10,
1385     mode=mode_no_scipy,
1386     skip=skip_scipy)
1387 _good_broadcast_unary_gammaln = dict(
1388     normal=(rand_ranged(-1 + 1e-2, 10, (2, 3)),),
1389     empty=(np.asarray([], dtype=config.floatX),),
1390     int=(randint_ranged(1, 10, (2, 3)),),
1391     uint8=(randint_ranged(1, 6, (2, 3)).astype('uint8'),),
1392     uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),),
1393     uint64=(randint_ranged(1, 10, (2, 3)).astype('uint64'),))
1394 _grad_broadcast_unary_gammaln = dict(
1395     normal=(rand_ranged(1e-1, 8, (2, 3)),),)
1396 GammaTester = makeBroadcastTester(
1397     op=tensor.gamma,
1398     expected=expected_gamma,
1399     good=_good_broadcast_unary_gammaln,
1400     grad=_grad_broadcast_unary_gammaln,
1401     mode=mode_no_scipy,
1402     eps=1e-5,
1403     skip=skip_scipy)
1404 GammaInplaceTester = makeBroadcastTester(
1405     op=inplace.gamma_inplace,
1406     expected=expected_gamma,
1407     good=_good_broadcast_unary_gammaln,
1408     mode=mode_no_scipy,
1409     eps=1e-5,
1410     inplace=True,
1411     skip=skip_scipy)
1412 GammalnTester = makeBroadcastTester(
1413     op=tensor.gammaln,
1414     expected=expected_gammaln,
1415     good=_good_broadcast_unary_gammaln,
1416     grad=_grad_broadcast_unary_gammaln,
1417     eps=2e-10,
1418     mode=mode_no_scipy,
1419     skip=skip_scipy)
1420 GammalnInplaceTester = makeBroadcastTester(
1421     op=inplace.gammaln_inplace,
1422     expected=expected_gammaln,
1423     good=_good_broadcast_unary_gammaln,
1424     eps=2e-10,
1425     mode=mode_no_scipy,
1426     inplace=True,
1427     skip=skip_scipy)
1428 _good_broadcast_unary_psi = dict(
1429     normal=(rand_ranged(1, 10, (2, 3)),),
1430     empty=(np.asarray([], dtype=config.floatX),),
1431     int=(randint_ranged(1, 10, (2, 3)),),
1432     uint8=(randint_ranged(1, 10, (2, 3)).astype('uint8'),),
1433     uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),))
1434 PsiTester = makeBroadcastTester(
1435     op=tensor.psi,
1436     expected=expected_psi,
1437     good=_good_broadcast_unary_psi,
1438     eps=2e-10,
1439     mode=mode_no_scipy,
1440     skip=skip_scipy)
1441 PsiInplaceTester = makeBroadcastTester(
1442     op=inplace.psi_inplace,
1443     expected=expected_psi,
1444     good=_good_broadcast_unary_psi,
1445     eps=2e-10,
1446     mode=mode_no_scipy,
1447     inplace=True,
1448     skip=skip_scipy)
1449 _good_broadcast_unary_tri_gamma = _good_broadcast_unary_psi
1450 TriGammaTester = makeBroadcastTester(
1451     op=tensor.tri_gamma,
1452     expected=expected_tri_gamma,
1453     good=_good_broadcast_unary_psi,
1454     eps=2e-8,
1455     mode=mode_no_scipy,
1456     skip=skip_scipy)
1457 TriGammaInplaceTester = makeBroadcastTester(
1458     op=inplace.tri_gamma_inplace,
1459     expected=expected_tri_gamma,
1460     good=_good_broadcast_unary_tri_gamma,
1461     eps=2e-8,
1462     mode=mode_no_scipy,
1463     inplace=True,
1464     skip=skip_scipy)
1465 <a name="27"></a>
1466 _good_broadcast_unary_chi2sf = dict(
1467     normal=(rand_ranged(1, 10, (2, 3)),
1468             np.asarray(1, dtype<font color="#e77471"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=config.floatX)),
1469     empty=(np.asarray([], dtype=config.floatX),
1470            np.asarray(1, dtype=config.</b></font>floatX)),
1471     integers=(randint_ranged(1, 10, (2, 3)),
1472               np.asarray(1, dtype=config.floatX)),
1473     uint8=(randint_ranged(1, 10, (2, 3)).astype('uint8'),
1474            np.asarray(1, dtype=config.floatX)),
1475     uint16=(randint_ranged(1, 10, (2, 3)).astype('uint16'),
1476             np.asarray(1, dtype=config.floatX)))
1477 Chi2SFTester = makeBroadcastTester(
1478     op=tensor.chi2sf,
1479     expected=expected_chi2sf,
1480     good=_good_broadcast_unary_chi2sf,
1481     eps=2e-10,
1482     mode=mode_no_scipy,
1483     skip=skip_scipy,
1484     name='Chi2SF')
1485 Chi2SFInplaceTester = makeBroadcastTester(
1486     op=inplace.chi2sf_inplace,
1487     expected=expected_chi2sf,
1488     good=_good_broadcast_unary_chi2sf,
1489     eps=2e-10,
1490     mode=mode_no_scipy,
1491     inplace=True,
1492     skip=skip_scipy,
1493     name='Chi2SF')
1494 _good_broadcast_unary_bessel = dict(
1495     normal=(rand_ranged(-10, 10, (2, 3)),),
1496     empty=(np.asarray([], dtype=config.floatX),),
1497     int=(randint_ranged(-10, 10, (2, 3)),),
1498     uint8=(randint_ranged(0, 10, (2, 3)).astype('uint8'),),
1499     uint16=(randint_ranged(0, 10, (2, 3)).astype('uint16'),))
1500 _grad_broadcast_unary_bessel = dict(
1501     normal=(rand_ranged(-10., 10., (2, 3)),),)
1502 _good_broadcast_binary_bessel = dict(
1503     normal=(rand_ranged(-5, 5, (2, 3)),
1504             rand_ranged(0, 10, (2, 3))),
1505     empty=(np.asarray([], dtype=config.floatX),
1506            np.asarray([], dtype=config.floatX)),
1507     integers=(randint_ranged(-5, 5, (2, 3)),
1508               randint_ranged(-10, 10, (2, 3))),
1509     uint8=(randint_ranged(0, 5, (2, 3)).astype('uint8'),
1510            randint_ranged(0, 10, (2, 3)).astype('uint8')),
1511     uint16=(randint_ranged(0, 5, (2, 3)).astype('uint16'),
1512             randint_ranged(0, 10, (2, 3)).astype('uint16')))
1513 _grad_broadcast_binary_bessel = dict(
1514     normal=(rand_ranged(1, 5, (2, 3)),
1515             rand_ranged(0, 10, (2, 3))))
1516 J0Tester = makeBroadcastTester(
1517     op=tensor.j0,
1518     expected=expected_j0,
1519     good=_good_broadcast_unary_bessel,
1520     grad=_grad_broadcast_unary_bessel,
1521     eps=2e-10,
1522     mode=mode_no_scipy,
1523     skip=skip_scipy)
1524 J0InplaceTester = makeBroadcastTester(
1525     op=inplace.j0_inplace,
1526     expected=expected_j0,
1527     good=_good_broadcast_unary_bessel,
1528     eps=2e-10,
1529     mode=mode_no_scipy,
1530     inplace=True,
1531     skip=skip_scipy)
1532 J1Tester = makeBroadcastTester(
1533     op=tensor.j1,
1534     expected=expected_j1,
1535     good=_good_broadcast_unary_bessel,
1536     grad=_grad_broadcast_unary_bessel,
1537     eps=2e-10,
1538     mode=mode_no_scipy,
1539     skip=skip_scipy)
1540 J1InplaceTester = makeBroadcastTester(
1541     op=inplace.j1_inplace,
1542     expected=expected_j1,
1543     good=_good_broadcast_unary_bessel,
1544     eps=2e-10,
1545     mode=mode_no_scipy,
1546     inplace=True,
1547     skip=skip_scipy)
1548 JvTester = makeBroadcastTester(
1549     op=tensor.jv,
1550     expected=expected_jv,
1551 <a name="31"></a>    good=_good_broadcast_binary_bessel,
1552     eps=2e-10,
1553     mode=mode_no_scipy,
1554     skip<font color="#3ea99f"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=skip_scipy)
1555 JvInplaceTester = makeBroadcastTester(
1556     op=inplace.jv_inplace,
1557     expected=expected_jv,
1558     good=_good_broadcast_binary_bessel,
1559     eps=2e-10,
1560     mode=mode_no_scipy,
1561     inplace=True,
1562     skip=skip_scipy)
1563 def</b></font> test_verify_jv_grad():
1564     if skip_scipy:
1565         raise SkipTest("SciPy needed")
1566     v_val, x_val = _grad_broadcast_binary_bessel['normal']
1567     def fixed_first_input_jv(x):
1568         return tensor.jv(v_val, x)
1569     utt.verify_grad(fixed_first_input_jv, [x_val])
1570 I0Tester = makeBroadcastTester(
1571     op=tensor.i0,
1572     expected=expected_i0,
1573     good=_good_broadcast_unary_bessel,
1574     grad=_grad_broadcast_unary_bessel,
1575     eps=2e-10,
1576     mode=mode_no_scipy,
1577     skip=skip_scipy)
1578 I0InplaceTester = makeBroadcastTester(
1579     op=inplace.i0_inplace,
1580     expected=expected_i0,
1581     good=_good_broadcast_unary_bessel,
1582     eps=2e-10,
1583     mode=mode_no_scipy,
1584     inplace=True,
1585     skip=skip_scipy)
1586 I1Tester = makeBroadcastTester(
1587     op=tensor.i1,
1588     expected=expected_i1,
1589     good=_good_broadcast_unary_bessel,
1590     grad=_grad_broadcast_unary_bessel,
1591     eps=2e-10,
1592     mode=mode_no_scipy,
1593     skip=skip_scipy)
1594 I1InplaceTester = makeBroadcastTester(
1595     op=inplace.i1_inplace,
1596     expected=expected_i1,
1597     good=_good_broadcast_unary_bessel,
1598     eps=2e-10,
1599     mode=mode_no_scipy,
1600     inplace=True,
1601     skip=skip_scipy)
1602 IvTester = makeBroadcastTester(
1603     op=tensor.iv,
1604     expected=expected_iv,
1605 <a name="29"></a>    good=_good_broadcast_binary_bessel,
1606     eps=2e-10,
1607     mode=mode_no_scipy,
1608     skip<font color="#af7a82"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=skip_scipy)
1609 IvInplaceTester = makeBroadcastTester(
1610     op=inplace.iv_inplace,
1611     expected=expected_iv,
1612     good=_good_broadcast_binary_bessel,
1613     eps=2e-10,
1614     mode=mode_no_scipy,
1615     inplace=True,
1616     skip=skip_scipy)
1617 def</b></font> test_verify_iv_grad():
1618     if skip_scipy:
1619         raise SkipTest("SciPy needed")
1620     v_val, x_val = _grad_broadcast_binary_bessel['normal']
1621     def fixed_first_input_iv(x):
1622         return tensor.iv(v_val, x)
1623     utt.verify_grad(fixed_first_input_iv, [x_val])
1624 ZerosLikeTester = makeBroadcastTester(
1625     op=tensor.zeros_like,
1626     expected=np.zeros_like,
1627     good=_good_broadcast_unary_normal,
1628     grad=_grad_broadcast_unary_normal,
1629     name='ZerosLike')
1630 OnesLikeTester = makeBroadcastTester(
1631     op=tensor.ones_like,
1632     expected=np.ones_like,
1633     good=_good_broadcast_unary_normal,
1634     grad=_grad_broadcast_unary_normal,
1635     name='OnesLike')
1636 _good_complex_from_polar = dict(
1637     same_shapes=(abs(rand(2, 3)), rand(2, 3)),
1638     not_same_dimensions=(abs(rand(2, 2)), rand(2)),
1639     scalar=(abs(rand(2, 3)), rand(1, 1)),
1640     row=(abs(rand(2, 3)), rand(1, 3)),
1641     column=(abs(rand(2, 3)), rand(2, 1)),
1642     integers=(abs(randint(2, 3)), randint(2, 3)),
1643     empty=(np.asarray([], dtype=config.floatX),
1644            np.asarray([1], dtype=config.floatX)),)
1645 _grad_complex_from_polar = dict(
1646     same_shapes=(abs(rand(2, 3)), rand(2, 3)),
1647     scalar=(abs(rand(2, 3)), rand(1, 1)),
1648     row=(abs(rand(2, 3)), rand(1, 3)),
1649     column=(abs(rand(2, 3)), rand(2, 1)))
1650 ComplexFromPolarTester = makeBroadcastTester(
1651     op=tensor.complex_from_polar,
1652     expected=lambda r, theta: r * np.cos(theta) + 1j * r * np.sin(theta),
1653     good=_good_complex_from_polar)
1654 ConjTester = makeBroadcastTester(
1655     op=tensor.conj,
1656     expected=np.conj,
1657     good=_good_broadcast_unary_normal)
1658 ConjInplaceTester = makeBroadcastTester(
1659     op=inplace.conj_inplace,
1660     expected=np.conj,
1661     good=_good_broadcast_unary_normal,
1662     inplace=True)
1663 DotTester = makeTester(
1664     name='DotTester',
1665     op=dot,
1666     expected=lambda x, y: np.dot(x, y),
1667     checks={},
1668     good=dict(correct1=(rand(5, 7), rand(7, 5)),
1669               correct2=(rand(5, 7), rand(7, 9)),
1670               correct3=(rand(5, 7), rand(7)),
1671               correct4=(rand(5), rand(5, 7)),
1672               mixed1=(rand(5).astype('float32'), rand(5, 7)),
1673               mixed2=(rand(5).astype('float64'), rand(5, 7)),
1674               complex1=(randcomplex(5, 7), randcomplex(7)),
1675               complex2=(rand(5, 7), randcomplex(7)),
1676               complex3=(randcomplex(5, 7), rand(7)),
1677               empty1=(np.asarray([], dtype=config.floatX),
1678                       np.asarray([], dtype=config.floatX)),
1679               empty2=(rand(5, 0), rand(0, 2)),
1680               empty3=(rand(0, 5), rand(5, 0)),
1681               ),
1682     bad_build=dict(),
1683     bad_runtime=dict(bad1=(rand(5, 7), rand(5, 7)),
1684                      bad2=(rand(5, 7), rand(8, 3))))
1685 BatchedDotTester = makeTester(
1686     name='BatchedDotTester',
1687     op=batched_dot,
1688     expected=(lambda xs, ys:
1689               np.asarray(
1690                   list(x * y if x.ndim == 0 or y.ndim == 0 else np.dot(x, y)
1691                        for x, y in zip(xs, ys)),
1692                   dtype=theano.scalar.upcast(xs.dtype, ys.dtype))),
1693     checks={},
1694     grad=dict(correct1=(rand(3, 5, 7), rand(3, 7, 5)),
1695               correct2=(rand(3, 5, 7), rand(3, 7, 9)),
1696               correct3=(rand(3, 5, 7), rand(3, 7)),
1697               correct4=(rand(3, 5), rand(3, 5, 7)),
1698               correct5=(rand(3), rand(3, 5, 7)),
1699               correct6=(rand(3, 5), rand(3)),
1700               correct7=(rand(3, 5), rand(3, 5)),
1701               correct8=(rand(3), rand(3)),
1702               correct9=(rand(3, 5, 7, 11), rand(3)),
1703               correct10=(rand(3, 2, 6, 5), rand(3, 5)),
1704               correct11=(rand(3, 2, 6, 5), rand(3, 5, 7)),
1705               correct12=(rand(3, 2, 6, 5), rand(3, 7, 5, 8)),
1706               mixed1=(rand(3, 5).astype('float32'),
1707                       rand(3, 5, 7)),
1708               mixed2=(rand(3, 5).astype('float64'),
1709                       rand(3, 5, 7))),
1710     good=dict(correct1=(rand(3, 5, 7), rand(3, 7, 5)),
1711               correct2=(rand(3, 5, 7), rand(3, 7, 9)),
1712               correct3=(rand(3, 5, 7), rand(3, 7)),
1713               correct4=(rand(3, 5), rand(3, 5, 7)),
1714               correct5=(rand(3), rand(3, 5, 7)),
1715               correct6=(rand(3, 5), rand(3)),
1716               correct7=(rand(3, 5), rand(3, 5)),
1717               correct8=(rand(3), rand(3)),
1718               correct9=(rand(3, 5, 7, 11), rand(3)),
1719               correct10=(rand(3, 7, 11, 5), rand(3, 5)),
1720               correct11=(rand(3, 7, 11, 5), rand(3, 5, 13)),
1721               correct12=(rand(3, 7, 11, 5), rand(3, 13, 5, 17)),
1722               mixed1=(rand(3, 5).astype('float32'),
1723 <a name="28"></a>                      rand(3, 5, 7)),
1724               mixed2=(rand(3, 5).astype('float64'),
1725                       rand(3, 5, 7))),
1726     bad_build=dict(no_batch_axis2=(rand(), rand<font color="#717d7d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, 5)),
1727                    no_batch_axis3=(rand(3, 5), rand())),
1728     bad_runtime=dict(batch_dim_mismatch1=(rand(2, 5, 7), rand(3, 7, 9)),
1729                      batch_dim_mismatch2=</b></font>(rand(3, 5, 7), rand(2, 7, 9)),
1730                      batch_dim_mismatch3=(rand(3), rand(5)),
1731                      bad_dim1=(rand(3, 5, 7), rand(3, 5, 7)),
1732                      bad_dim2=(rand(3, 5, 7), rand(3, 8, 3)),
1733                      bad_dim3=(rand(3, 5), rand(3, 7)),
1734                      bad_dim4=(rand(3, 5, 7, 11), rand(3, 5)),
1735                      bad_dim5=(rand(3, 5, 7, 11), rand(3, 5, 13)),
1736                      bad_dim6=(rand(3, 5, 7, 11), rand(3, 13, 5, 17))))
1737 def _numpy_second(x, y):
1738     return np.broadcast_arrays(x, y)[1]
1739 ALL_DTYPES = ('int8', 'int16', 'int32', 'int64', 'float32', 'float64',
1740               'uint8', 'uint16',
1741               'complex64', 'complex128')
1742 REAL_DTYPES = ALL_DTYPES[:6]
1743 COMPLEX_DTYPES = ALL_DTYPES[-2:]
1744 def multi_dtype_checks(shape1, shape2, dtypes=ALL_DTYPES, nameprefix=''):
1745     for dtype1, dtype2 in itertools.combinations(dtypes, 2):
1746         name1 = '%s_%s_%s' % (nameprefix, dtype1, dtype2)
1747         name2 = '%s_%s_%s' % (nameprefix, dtype2, dtype1)
1748         obj1 = rand_of_dtype(shape1, dtype1)
1749         obj2 = rand_of_dtype(shape2, dtype2)
1750         yield (name1, (obj1, obj2))
1751         yield (name2, (obj2, obj1))
1752 def multi_dtype_cast_checks(shape, dtypes=ALL_DTYPES, nameprefix=''):
1753     for dtype1, dtype2 in itertools.combinations(dtypes, 2):
1754         name1 = '%s_%s_%s' % (nameprefix, dtype1, dtype2)
1755         name2 = '%s_%s_%s' % (nameprefix, dtype2, dtype1)
1756         obj1 = rand_of_dtype(shape, dtype1)
1757         obj2 = rand_of_dtype(shape, dtype2)
1758         yield (name1, (obj1, dtype2))
1759         yield (name2, (obj2, dtype1))
1760 SecondBroadcastTester = makeTester(
1761     name='SecondBroadcastTester',
1762     op=second,
1763     expected=_numpy_second,
1764     good=dict(itertools.chain(
1765         multi_dtype_checks((4, 5), (5,)),
1766         multi_dtype_checks((2, 3, 2), (3, 2)),
1767         multi_dtype_checks((2, 3, 2), (2,)),
1768         )),
1769     bad_runtime=dict(
1770         fail1=(rand(5, 4), rand(5)),
1771         fail2=(rand(3, 2, 3), rand(6, 9)),
1772         fail3=(randint(6, 2, 9), rand(3, 2)),
1773         )
1774     )
1775 SecondSameRankTester = makeTester(
1776     name='SecondSameRankTester',
1777     op=second,
1778     expected=_numpy_second,
1779     good=dict(itertools.chain(
1780         multi_dtype_checks((4, 5), (4, 5)),
1781         multi_dtype_checks((1, 2), (3, 2)),
1782         multi_dtype_checks((3, 2), (1, 2)),
1783         )),
1784     bad_runtime=dict(itertools.chain(
1785         multi_dtype_checks((4, 5), (5, 4)),
1786         multi_dtype_checks((1, 5), (5, 4)),
1787         )),
1788     mode=get_default_mode().excluding(
1789         'local_fill_to_alloc',
1790         'local_useless_fill')
1791     )
1792 AllocTester = makeBroadcastTester(
1793     name='AllocTester',
1794     op=alloc,
1795     expected=(lambda x, *shp: np.zeros(shp, dtype=x.dtype) + x),
1796     good=dict(
1797         correct01=(rand(), np.int32(7)),
1798         correct01_bcast=(rand(1), np.int32(7)),
1799         correct02=(rand(), np.int32(4), np.int32(7)),
1800         correct12=(rand(7), np.int32(4), np.int32(7)),
1801         correct13=(rand(7), np.int32(2), np.int32(4), np.int32(7)),
1802         correct23=(rand(4, 7), np.int32(2), np.int32(4), np.int32(7)),
1803         correctb1=(rand(1, 7), np.int32(4), np.int32(7)),
1804         correctb2=(rand(1, 7), np.int32(2), np.int32(4), np.int32(7)),
1805         correctb3=(rand(7, 1), np.int32(7), np.int32(4)),
1806         correctb4=(rand(7, 1), np.int32(2), np.int32(7), np.int32(4)),
1807         ),
1808     bad_runtime=dict(
1809         bad_shape12=(rand(7), np.int32(7), np.int32(5)),
1810         ),
1811     bad_build=dict(
1812         vec=(rand(1), [np.int32(2)]),
1813         too_big32=(rand(6, 2, 4), np.int32(6), np.int32(2)),
1814         too_big32b=(rand(6, 2, 4), np.int32(6), np.int32(4)),
1815         too_big32c=(rand(6, 2, 4), np.int32(2), np.int32(4)),
1816         too_big32d=(rand(6, 2, 4), np.int32(2), np.int32(6)),
1817         too_big32e=(rand(6, 2, 4), np.int32(4), np.int32(6)),
1818         too_big32f=(rand(6, 2, 4), np.int32(4), np.int32(2)),
1819         ),
1820     )
1821 s1, s2, s3 = randint_ranged(1, 13, (3,))
1822 Alloc01GradTester = makeBroadcastTester(
1823     name='Alloc01GradTester',
1824     op=(lambda x: alloc(x, s1)),
1825     expected=(lambda x: np.zeros((s1,), dtype=x.dtype) + x),
1826     grad=dict(
1827         x1=(rand(),),
1828         x2=(rand(),),
1829         x3=(rand(),),
1830         ),
1831     )
1832 Alloc13GradTester = makeBroadcastTester(
1833     name='Alloc13GradTester',
1834     op=(lambda x: alloc(x, s1, s2, s3)),
1835     expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
1836     grad=dict(
1837         x1=(rand(s3),),
1838         x2=(rand(s3),),
1839         x3=(rand(s3),),
1840         ),
1841     )
1842 Allocb1GradTester = makeBroadcastTester(
1843     name='Allocb1GradTester',
1844     op=lambda x: alloc(x, s1, s2),
1845     expected=(lambda x: np.zeros((s1, s2), dtype=x.dtype) + x),
1846     grad=dict(
1847         x1=(rand(1, s2),),
1848         x2=(rand(1, s2),),
1849         x3=(rand(1, s2),),
1850     ),
1851 )
1852 Allocb2GradTester = makeBroadcastTester(
1853     name='Allocb2GradTester',
1854     op=lambda x: alloc(x, s1, s2, s3),
1855     expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
1856     grad=dict(
1857         x1=(rand(1, s3),),
1858         x2=(rand(1, s3),),
1859         x3=(rand(1, s3),),
1860     ),
1861 )
1862 Allocb3GradTester = makeBroadcastTester(
1863     name='Allocb3GradTester',
1864     op=lambda x: alloc(x, s1, s2),
1865     expected=(lambda x: np.zeros((s1, s2), dtype=x.dtype) + x),
1866     grad=dict(
1867         x1=(rand(s1, 1),),
1868         x2=(rand(s1, 1),),
1869         x3=(rand(s1, 1),),
1870     ),
1871 )
1872 Allocb4GradTester = makeBroadcastTester(
1873     name='Allocb4GradTester',
1874     op=lambda x: alloc(x, s1, s2, s3),
1875     expected=(lambda x: np.zeros((s1, s2, s3), dtype=x.dtype) + x),
1876     grad=dict(
1877         x1=(rand(s2, 1),),
1878         x2=(rand(s2, 1),),
1879         x3=(rand(s2, 1),),
1880     ),
1881 )
1882 AllocDimshuffleGradTester = makeBroadcastTester(
1883     name='Allocb4GradTester',
1884     op=lambda x: alloc(x.dimshuffle('x', 'x', 0), 1, s2, s3),
1885     expected=(lambda x: np.zeros((1, s2, s3), dtype=x.dtype) + x),
1886     grad=dict(
1887         x1=(rand(s3),),
1888         x2=(rand(s3),),
1889         x3=(rand(s3),),
1890     ),
1891 )
1892 AllocDimshuffleGradTester2 = makeBroadcastTester(
1893     name='Allocb4GradTester',
1894     op=lambda x: alloc(x.dimshuffle('x', 0), 1, s2, s3),
1895     expected=(lambda x: np.zeros((1, s2, s3), dtype=x.dtype) + x),
1896     grad=dict(
1897         x1=(rand(s3),),
1898         x2=(rand(s3),),
1899         x3=(rand(s3),),
1900     ),
1901 )
1902 class ApplyDefaultTestOp(theano.Op):
1903     def __init__(self, id):
1904         self.default_output = id
1905     def make_node(self, x):
1906         x = theano.tensor.as_tensor_variable(x)
1907         return theano.Apply(self, [x], [x.type()])
1908 class TestAsTensorVariable(unittest.TestCase):
1909     def setUp(self):
1910         self.x = tensor.scalar('x')
1911     def test_one_output(self):
1912         good_apply_var = ApplyDefaultTestOp(0).make_node(self.x)
1913         as_tensor_variable(good_apply_var)
1914     def test_below_zero_output(self):
1915         bad_apply_var = ApplyDefaultTestOp(-1).make_node(self.x)
1916         self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
1917     def test_above_output_len(self):
1918         bad_apply_var = ApplyDefaultTestOp(2).make_node(self.x)
1919         self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
1920     def test_list(self):
1921         bad_apply_var = ApplyDefaultTestOp([0, 1]).make_node(self.x)
1922         self.assertRaises(AttributeError, as_tensor_variable, bad_apply_var)
1923     def test_strip_leading_broadcastable(self):
1924         x = tensor.TensorType(config.floatX, (True, False))('x')
1925         x = as_tensor_variable(x, ndim=1)
1926         assert(x.ndim == 1)
1927         x = tensor.matrix('x', dtype=config.floatX)
1928         self.assertRaises(ValueError, as_tensor_variable, x, ndim=1)
1929 class TestAlloc(unittest.TestCase):
1930     dtype = config.floatX
1931     mode = mode_opt
1932     shared = staticmethod(theano.shared)
1933     allocs = [tensor.Alloc()] * 3
1934     def setUp(self):
1935         self.rng = np.random.RandomState(seed=utt.fetch_seed())
1936     def test_alloc_constant_folding(self):
1937         test_params = np.asarray(self.rng.randn(50 * 60),
1938                                  self.dtype)
1939         some_vector = vector('some_vector', dtype=self.dtype)
1940         some_matrix = some_vector.reshape((60, 50))
1941         variables = self.shared(np.ones((50,), dtype=self.dtype))
1942         idx = tensor.constant(np.arange(50))
1943         for alloc_, (subtensor, n_alloc) in zip(self.allocs, [
1944                 (some_matrix[:60], 2),
1945                 (some_matrix[arange(60)], 2),
1946                 (some_matrix[idx, idx], 1)
1947         ]):
1948             derp = sum(dot(subtensor, variables))
1949             fobj = theano.function([some_vector], derp, mode=self.mode)
1950             grad_derp = theano.grad(derp, some_vector)
1951             fgrad = theano.function([some_vector], grad_derp,
1952                                     mode=self.mode)
1953             topo_obj = fobj.maker.fgraph.toposort()
1954             assert np.sum([isinstance(node.op, type(alloc_))
1955                            for node in topo_obj]) == 0
1956             topo_grad = fgrad.maker.fgraph.toposort()
1957             assert np.sum([isinstance(node.op, type(alloc_))
1958                            for node in topo_grad]) == n_alloc, (
1959                                alloc_, subtensor, n_alloc, topo_grad)
1960             fobj(test_params)
1961             fgrad(test_params)
1962     def test_alloc_output(self):
1963         val = tensor.constant(self.rng.randn(1, 1), dtype=self.dtype)
1964         for alloc_ in self.allocs:
1965             out = alloc_(val, 50, 60)
1966             f = theano.function([], out, mode=self.mode)
1967             topo = f.maker.fgraph.toposort()
1968             assert np.sum([isinstance(node.op, type(alloc_))
1969                            for node in topo]) == 1
1970             assert not isinstance(topo[0].op, DeepCopyOp)
1971     def test_ones(self):
1972         for shp in [[], 1, [1], [1, 2], [1, 2, 3]]:
1973             ones = theano.function([], [tensor.ones(shp)], mode=self.mode)
1974             assert np.allclose(ones(), np.ones(shp))
1975         x = scalar()
1976         shp = []
1977         ones_scalar = theano.function([], [tensor.ones(x.shape)],
1978                                       mode=self.mode)
1979         assert np.allclose(ones_scalar(), np.ones(shp))
1980         for (typ, shp) in [(vector, [3]), (matrix, [3, 4])]:
1981             x = typ()
1982             ones_tensor = theano.function([x], [tensor.ones(x.shape)],
1983                                           mode=self.mode)
1984             inp = np.zeros(shp, dtype=config.floatX)
1985             assert np.allclose(ones_tensor(inp),
1986                                np.ones(shp))
1987     def test_zeros(self):
1988         for shp in [[], 1, [1], [1, 2], [1, 2, 3]]:
1989             zeros = theano.function([], [tensor.zeros(shp)],
1990                                     mode=self.mode)
1991             assert np.allclose(zeros(), np.zeros(shp))
1992         x = scalar()
1993         shp = []
1994         zeros_scalar = theano.function([], [tensor.zeros(x.shape)],
1995                                        mode=self.mode)
1996         assert np.allclose(zeros_scalar(), np.zeros(shp))
1997         for (typ, shp) in [(vector, [3]), (matrix, [3, 4])]:
1998             x = typ()
1999             zeros_tensor = theano.function([x], [tensor.zeros(x.shape)],
2000                                            mode=self.mode)
2001             inp = np.zeros(shp, dtype=config.floatX)
2002             assert np.allclose(zeros_tensor(inp),
2003                                np.zeros(shp))
2004 def test_eye():
2005     def check(dtype, N, M_=None, k=0):
2006         M = M_
2007         if M is None and theano.config.mode in ['DebugMode', 'DEBUG_MODE']:
2008             M = N
2009         N_symb = tensor.iscalar()
2010         M_symb = tensor.iscalar()
2011         k_symb = tensor.iscalar()
2012         f = function([N_symb, M_symb, k_symb],
2013                      eye(N_symb, M_symb, k_symb, dtype=dtype))
2014         result = f(N, M, k)
2015         assert np.allclose(result, np.eye(N, M_, k, dtype=dtype))
2016         assert result.dtype == np.dtype(dtype)
2017     for dtype in ALL_DTYPES:
2018         yield check, dtype, 3
2019         yield check, dtype, 3, 5
2020         yield check, dtype, 5, 3
2021         yield check, dtype, 3, 3, 1
2022         yield check, dtype, 3, 3, -1
2023         yield check, dtype, 3, 5, 1
2024         yield check, dtype, 3, 5, -1
2025         yield check, dtype, 5, 3, 1
2026         yield check, dtype, 5, 3, -1
2027 class test_triangle(unittest.TestCase):
2028     def test_tri(self):
2029         def check(dtype, N, M_=None, k=0):
2030             M = M_
2031             if M is None and theano.config.mode in ['DebugMode', 'DEBUG_MODE']:
2032                 M = N
2033             N_symb = tensor.iscalar()
2034             M_symb = tensor.iscalar()
2035             k_symb = tensor.iscalar()
2036             f = function([N_symb, M_symb, k_symb],
2037                          tri(N_symb, M_symb, k_symb, dtype=dtype))
2038             result = f(N, M, k)
2039             self.assertTrue(
2040                 np.allclose(result, np.tri(N, M_, k, dtype=dtype)))
2041             self.assertTrue(result.dtype == np.dtype(dtype))
2042         for dtype in ALL_DTYPES:
2043             yield check, dtype, 3
2044             yield check, dtype, 3, 5
2045             yield check, dtype, 5, 3
2046             yield check, dtype, 3, 3, 1
2047             yield check, dtype, 3, 3, -1
2048             yield check, dtype, 3, 5, 1
2049             yield check, dtype, 3, 5, -1
2050             yield check, dtype, 5, 3, 1
2051             yield check, dtype, 5, 3, -1
2052     def test_tril_triu(self):
2053         def check_l(m, k=0):
2054             m_symb = matrix(dtype=m.dtype)
2055             k_symb = iscalar()
2056             f = function([m_symb, k_symb], tril(m_symb, k_symb))
2057             result = f(m, k)
2058             self.assertTrue(np.allclose(result, np.tril(m, k)))
2059             self.assertTrue(result.dtype == np.dtype(dtype))
2060         def check_u(m, k=0):
2061             m_symb = matrix(dtype=m.dtype)
2062             k_symb = iscalar()
2063             f = function([m_symb, k_symb], triu(m_symb, k_symb))
2064             result = f(m, k)
2065             self.assertTrue(np.allclose(result, np.triu(m, k)))
2066             self.assertTrue(result.dtype == np.dtype(dtype))
2067         for dtype in ALL_DTYPES:
2068             m = rand_of_dtype((10, 10), dtype)
2069             yield check_l, m, 0
2070             yield check_l, m, 1
2071             yield check_l, m, -1
2072             yield check_u, m, 0
2073             yield check_u, m, 1
2074             yield check_u, m, -1
2075             m = rand_of_dtype((10, 5), dtype)
2076             yield check_l, m, 0
2077             yield check_l, m, 1
2078             yield check_l, m, -1
2079             yield check_u, m, 0
2080             yield check_u, m, 1
2081             yield check_u, m, -1
2082 class test_nonzero(unittest.TestCase):
2083     def test_nonzero(self):
2084         def check(m):
2085             m_symb = theano.tensor.tensor(dtype=m.dtype,
2086                                           broadcastable=(False,) * m.ndim)
2087             f_tuple = function([m_symb], nonzero(m_symb, return_matrix=False))
2088             f_matrix = function([m_symb], nonzero(m_symb, return_matrix=True))
2089             self.assertTrue(np.allclose(f_matrix(m),
2090                                         np.vstack(np.nonzero(m))))
2091             for i, j in zip(f_tuple(m), np.nonzero(m)):
2092                 self.assertTrue(np.allclose(i, j))
2093         rand0d = np.array(rand())
2094         self.assertRaises(ValueError, check, rand0d)
2095         rand1d = rand(8)
2096         rand1d[:4] = 0
2097         check(rand1d)
2098         rand2d = rand(8, 9)
2099         rand2d[:4] = 0
2100         check(rand2d)
2101         rand3d = rand(8, 9, 10)
2102         rand3d[:4] = 0
2103         check(rand3d)
2104         rand4d = rand(8, 9, 10, 11)
2105         rand4d[:4] = 0
2106         check(rand4d)
2107     def test_flatnonzero(self):
2108         def check(m):
2109             m_symb = theano.tensor.tensor(dtype=m.dtype,
2110                                           broadcastable=(False,) * m.ndim)
2111             f = function([m_symb], flatnonzero(m_symb))
2112             result = f(m)
2113             assert np.allclose(result, np.flatnonzero(m))
2114         rand0d = np.array(rand())
2115         self.assertRaises(ValueError, check, rand0d)
2116         rand1d = rand(8)
2117         rand1d[:4] = 0
2118         check(rand1d)
2119         rand2d = rand(8, 9)
2120         rand2d[:4] = 0
2121         check(rand2d)
2122         rand3d = rand(8, 9, 10)
2123         rand3d[:4] = 0
2124         check(rand3d)
2125         rand4d = rand(8, 9, 10, 11)
2126         rand4d[:4] = 0
2127         check(rand4d)
2128     def test_nonzero_values(self):
2129         def check(m):
2130             m_symb = theano.tensor.tensor(dtype=m.dtype,
2131                                           broadcastable=(False,) * m.ndim)
2132             f = function([m_symb], nonzero_values(m_symb))
2133             result = f(m)
2134             assert np.allclose(result, m[np.nonzero(m)])
2135         rand0d = rand()
2136         self.assertRaises(ValueError, check, rand0d)
2137         rand1d = rand(8)
2138         rand1d[:4] = 0
2139         check(rand1d)
2140         rand2d = rand(8, 9)
2141         rand2d[:4] = 0
2142         check(rand2d)
2143         rand3d = rand(8, 9, 10)
2144         rand3d[:4] = 0
2145         check(rand3d)
2146         rand4d = rand(8, 9, 10, 11)
2147         rand4d[:4] = 0
2148         check(rand4d)
2149 def test_identity():
2150     def check(dtype):
2151         obj = rand_of_dtype((2,), dtype)
2152         sym = tensor.vector(dtype=dtype)
2153         f = function([sym], tensor_copy(sym))
2154         assert np.all(obj == f(obj))
2155         assert obj.dtype == f(obj).dtype
2156         topo = f.maker.fgraph.toposort()
2157         assert len(topo) == 1
2158         if theano.config.mode != 'FAST_COMPILE':
2159             assert isinstance(topo[0].op, DeepCopyOp)
2160     for dtype in ALL_DTYPES:
2161         yield check, dtype
2162 class CastTester(unittest.TestCase):
2163     def test_good_between_real_types(self):
2164         good = itertools.chain(
2165             multi_dtype_cast_checks((2,), dtypes=REAL_DTYPES),
2166             [('%s_%s' % (rand_of_dtype((2,), dtype), dtype),
2167               (rand_of_dtype((2,), dtype), dtype))
2168              for dtype in ALL_DTYPES])
2169         for testname, (obj, dtype) in good:
2170             inp = tensor.vector(dtype=obj.dtype)
2171             out = tensor.cast(inp, dtype=dtype)
2172             f = function([inp], out)
2173             assert f(obj).dtype == np.dtype(dtype)
2174             out2 = inp.astype(dtype=dtype)
2175             assert out2.type == out.type
2176     def test_cast_from_real_to_complex(self):
2177         for real_dtype in REAL_DTYPES:
2178             for complex_dtype in COMPLEX_DTYPES:
2179                 inp = tensor.vector(dtype=real_dtype)
2180                 out = tensor.cast(inp, dtype=complex_dtype)
2181                 f = function([inp], out)
2182                 obj = rand_of_dtype((2, ), real_dtype)
2183                 assert f(obj).dtype == np.dtype(complex_dtype)
2184     def test_cast_from_complex_to_real_raises_error(self):
2185         for real_dtype in REAL_DTYPES:
2186             for complex_dtype in COMPLEX_DTYPES:
2187                 inp = tensor.vector(dtype=real_dtype)
2188                 self.assertRaises(TypeError, tensor.cast(
2189                     inp, dtype=complex_dtype))
2190 ClipTester = makeTester(
2191     name='ClipTester',
2192     op=clip,
2193     expected=lambda x, y, z: np.clip(x, y, z),
2194     good=dict(correct1=((5 * rand(5, 5)).astype('float32'),
2195                         np.array(-1, dtype='float32'),
2196                         np.array(1, dtype='float32')),
2197               correct2=((5 * rand(5, 5)).astype('float64'),
2198                         np.array(-1, dtype='float64'),
2199                         np.array(1, dtype='float64')),
2200               correct3=(randint(5, 5).astype('int8'),
2201                         np.array(-1, dtype='int8'),
2202                         np.array(1, dtype='int8')),
2203               correct4=(randint(5, 5).astype('int16'),
2204                         np.array(-1, dtype='int16'),
2205                         np.array(1, dtype='int16')),
2206               correct5=(randint(5, 5).astype('int32'),
2207                         np.array(-1, dtype='int32'),
2208                         np.array(1, dtype='int32')),
2209               correct6=(randint(5, 5).astype('int64'),
2210                         np.array(-1, dtype='int64'),
2211                         np.array(1, dtype='int64')),
2212               correct8=(randint(0, 5).astype('uint8'),
2213                         np.array(2, dtype='uint8'),
2214                         np.array(4, dtype='uint8')),
2215               correct9=(randint(0, 5).astype('uint16'),
2216                         np.array(2, dtype='uint16'),
2217                         np.array(4, dtype='uint16')),)
2218     )
2219 BackwardsClipTester = makeTester(
2220     name='BackwardsClipTester',
2221     op=clip,
2222     expected=lambda x, y, z: np.where(x &lt; y, y, np.minimum(x, z)),
2223     good=dict(correct7=((5 * rand(5, 5)).astype('float64'),
2224                         np.array(1, dtype='float64'),
2225                         np.array(-1, dtype='float64')),)
2226     )
2227 class T_Clip(unittest.TestCase):
2228     def test_complex_value(self):
2229         for dtype in ['complex64', 'complex128']:
2230             a = tensor.vector(dtype=dtype)
2231             b = tensor.scalar()
2232             c = tensor.scalar()
2233             self.assertRaises(TypeError, clip, a, b, c)
2234     def test_clip_repeat_grad(self):
2235         x, y = tensor.vectors('xy')
2236         a = clip(x, y, x)
2237         g = theano.gradient.grad(a.sum(), x)
2238         fn = theano.function([x, y], [g])
2239         a2 = clip(x, x, y)
2240         g2 = theano.gradient.grad(a2.sum(), x)
2241         fn2 = theano.function([x, y], [g2])
2242         a3 = theano.tensor.clip(x, x, x)
2243         g3 = theano.gradient.grad(a3.sum(), x)
2244         fn3 = theano.function([x], [g3])
2245         rng = np.random.RandomState(utt.fetch_seed())
2246         nvals = 50
2247         xval = rng.rand(nvals).astype(config.floatX)
2248         yval_mn = rng.rand(nvals).astype(config.floatX) - 1.0
2249         yval_mx = rng.rand(nvals).astype(config.floatX) + 1.0
2250         aval, = fn(xval, yval_mn)
2251         aval2, = fn2(xval, yval_mx)
2252         aval3, = fn3(xval)
2253         self.assertTrue(np.all(aval == 1.))
2254         self.assertTrue(np.all(aval2 == 1.))
2255         self.assertTrue(np.all(aval3 == 1.))
2256     def test_clip_repeat_verify_grad(self):
2257         utt.verify_grad(
2258             op=lambda x: clip(x, 0, x),
2259             pt=[rand_nonzero((3, 7))])
2260         utt.verify_grad(
2261             op=lambda x: clip(x, x, 0),
2262             pt=[rand_nonzero((3, 7))])
2263         utt.verify_grad(
2264             op=lambda x: clip(0, x, x),
2265             pt=[rand_nonzero((3, 7))])
2266         utt.verify_grad(
2267             op=lambda x: clip(x, x, x),
2268             pt=[rand_nonzero((3, 7))])
2269 def test_batched_dot():
2270     first = theano.tensor.tensor3("first")
2271     second = theano.tensor.tensor3("second")
2272     output = theano.tensor.basic.batched_dot(first, second)
2273     first_val = np.random.rand(10, 10, 20).astype(config.floatX)
2274     second_val = np.random.rand(10, 20, 5).astype(config.floatX)
2275     result_fn = theano.function([first, second], output)
2276     result = result_fn(first_val, second_val)
2277     assert result.shape[0] == first_val.shape[0]
2278     assert result.shape[1] == first_val.shape[1]
2279     assert result.shape[2] == second_val.shape[2]
2280     first_mat = theano.tensor.dmatrix("first")
2281     second_mat = theano.tensor.dmatrix("second")
2282     output = theano.tensor.basic.batched_dot(first_mat, second_mat)
2283     first_mat_val = np.random.rand(10, 10).astype(config.floatX)
2284     second_mat_val = np.random.rand(10, 10).astype(config.floatX)
2285     result_fn = theano.function([first_mat, second_mat], output)
2286     result = result_fn(first_mat_val, second_mat_val)
2287     assert result.shape[0] == first_mat_val.shape[0]
2288 def test_batched_dot_not_contiguous():
2289     def np_genarray(*_shape):
2290         size = 1
2291         for dimsize in _shape:
2292             size *= dimsize
2293         return np.arange(size, dtype=floatX).reshape(_shape)
2294     X = tensor3()
2295     W = tensor3()
2296     Z = batched_dot(X, W)
2297     f = function([X, W], Z)
2298     w = np_genarray(30, 10, 5)
2299     reversed_x_container = np_genarray(20, 40, 30)
2300     x_container = reversed_x_container.T
2301     def check_first_dim(inverted):
2302         direction = -1 if inverted else 1
2303         x = x_container[::direction, ::2, ::2]
2304         assert x.shape == (30, 20, 10)
2305         assert x.strides[0] == direction * np.dtype(floatX).itemsize
2306         assert not (x.flags['C_CONTIGUOUS'] or x.flags['F_CONTIGUOUS'])
2307         result = f(x, w)
2308         ref_result = np.asarray(list(np.dot(u, v) for u, v in zip(x, w)))
2309         utt.assert_allclose(ref_result, result)
2310     for inverted in (0, 1):
2311         yield (check_first_dim, inverted)
2312 def test_batched_tensordot():
2313     first = theano.tensor.tensor4("first")
2314     second = theano.tensor.tensor4("second")
2315     axes = [[1, 2], [3, 1]]
2316     output = theano.tensor.basic.batched_tensordot(first, second, axes)
2317     first_val = np.random.rand(8, 10, 20, 3).astype(config.floatX)
2318     second_val = np.random.rand(8, 20, 5, 10).astype(config.floatX)
2319     result_fn = theano.function([first, second], output)
2320     result = result_fn(first_val, second_val)
2321     assert result.shape[0] == first_val.shape[0]
2322     assert result.shape[1] == first_val.shape[3]
2323     assert result.shape[2] == second_val.shape[2]
2324     first_mat = theano.tensor.dmatrix("first")
2325     second_mat = theano.tensor.dmatrix("second")
2326     axes = 1
2327     output = theano.tensor.basic.batched_tensordot(first_mat, second_mat, axes)
2328     first_mat_val = np.random.rand(10, 4).astype(config.floatX)
2329     second_mat_val = np.random.rand(10, 4).astype(config.floatX)
2330     result_fn = theano.function([first_mat, second_mat], output)
2331     result = result_fn(first_mat_val, second_mat_val)
2332     assert result.shape[0] == first_mat_val.shape[0]
2333     assert len(result.shape) == 1
2334 def test_tensor_values_eq_approx():
2335     a = np.asarray([-np.inf, -1, 0, 1, np.inf, np.nan])
2336     assert TensorType.values_eq_approx(a, a)
2337     b = np.asarray([np.inf, -1, 0, 1, np.inf, np.nan])
2338     assert not TensorType.values_eq_approx(a, b)
2339     b = np.asarray([-np.inf, -1, 0, 1, -np.inf, np.nan])
2340     assert not TensorType.values_eq_approx(a, b)
2341     b = np.asarray([np.inf, -1, 0, 1, 5, np.nan])
2342     assert TensorType.values_eq_approx(a, b, allow_remove_inf=True)
2343     b = np.asarray([np.inf, -1, 0, 1, 5, 6])
2344     assert not TensorType.values_eq_approx(a, b, allow_remove_inf=True)
2345     b = np.asarray([np.inf, -1, 0, 1, 5, np.nan])
2346     assert not TensorType.values_eq_approx(a, b, allow_remove_nan=False)
2347     b = np.asarray([-np.inf, -1, 0, 1, np.inf, 6])
2348     assert not TensorType.values_eq_approx(a, b, allow_remove_nan=False)
2349 <a name="6"></a>def test_nan_inf_constant_signature():
2350     test_constants <font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [
2351         [np.nan, np.inf, 0, 1],
2352         [np.nan, np.inf, -np.inf, 1],
2353         [0, np.inf, -np.inf, 1],
2354         [0, 3, -np.inf, 1],
2355         [0, 3, np.</b></font>inf, 1],
2356         [np.nan, 3, 4, 1],
2357         [0, 3, 4, 1],
2358         np.nan,
2359         np.inf,
2360         -np.inf,
2361         0,
2362         1,
2363         ]
2364     n = len(test_constants)
2365     for i in xrange(n):
2366         for j in xrange(n):
2367             x = constant(test_constants[i])
2368             y = constant(test_constants[j])
2369             assert (x.signature() == y.signature()) == (i == j)
2370     x = tensor.scalar()
2371     mode = get_default_mode()
2372     if isinstance(mode, theano.compile.debugmode.DebugMode):
2373         mode = copy(mode)
2374         mode.check_isfinite = False
2375     f = theano.function([x], eq(x, np.nan), mode=mode)
2376     assert f(0) == 0
2377     assert f(np.nan) == 0
2378 def test_isnan():
2379     for x in [tensor.matrix(), tensor.imatrix(), tensor.matrix(dtype='bool')]:
2380         y = tensor.isnan(x)
2381         assert isinstance(y.owner.op, tensor.Elemwise) == (
2382             x.dtype not in tensor.discrete_dtypes)
2383         assert y.dtype == 'bool'
2384         y = tensor.isnan_(x)
2385         assert isinstance(y.owner.op, tensor.Elemwise)
2386         assert y.dtype == 'bool'
2387         f = theano.function([x], y, allow_input_downcast=True)
2388         f([[0, 1, 2]])
2389 class T_Shape(unittest.TestCase):
2390     def test_basic0(self):
2391         s = shape(np.ones((5, 3)))
2392         self.assertTrue((eval_outputs([s]) == [5, 3]).all())
2393     def test_basic1(self):
2394         s = shape(np.ones((2)))
2395         self.assertTrue((eval_outputs([s]) == [2]).all())
2396     def test_basic2(self):
2397         s = shape(np.ones((5, 3, 10)))
2398         self.assertTrue((eval_outputs([s]) == [5, 3, 10]).all())
2399 class T_max_and_argmax(unittest.TestCase):
2400     def setUp(self):
2401         utt.seed_rng()
2402         MaxAndArgmax.debug = 0
2403     def test0(self):
2404         n = as_tensor_variable(5.0)
2405         v, i = eval_outputs(max_and_argmax(n))
2406         self.assertTrue(v == 5.0)
2407         self.assertTrue(i == 0)
2408         assert i.dtype == 'int64'
2409         v = eval_outputs(max_and_argmax(n)[0].shape)
2410         assert len(v) == 0
2411         v = eval_outputs(max_and_argmax(n)[1].shape)
2412         assert len(v) == 0
2413     def test1(self):
2414         n = as_tensor_variable([1, 2, 3, 2, -6])
2415         v, i = eval_outputs(max_and_argmax(n))
2416         self.assertTrue(v == 3)
2417         self.assertTrue(i == 2)
2418         assert i.dtype == 'int64'
2419         v = eval_outputs(max_and_argmax(n)[0].shape)
2420         assert len(v) == 0
2421     def test2(self):
2422         data = rand(2, 3)
2423         n = as_tensor_variable(data)
2424         for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2425                                 ([0, 1], None), ([1, 0], None),
2426                                 (NoneConst.clone(), None),
2427                                 (constant(0), 0)]:
2428             v, i = eval_outputs(max_and_argmax(n, axis))
2429             assert i.dtype == 'int64'
2430             self.assertTrue(np.all(v == np.max(data, np_axis)))
2431             self.assertTrue(np.all(i == np.argmax(data, np_axis)))
2432             v_shape = eval_outputs(max_and_argmax(n, axis)[0].shape)
2433             assert tuple(v_shape) == np.max(data, np_axis).shape
2434     def test2_float16(self):
2435         data = (rand(20, 30).astype("float16") - 0.5) * 20
2436         n = shared(data)
2437         for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2438                                 ([0, 1], None), ([1, 0], None),
2439                                 (NoneConst.clone(), None),
2440                                 (constant(0), 0)]:
2441             v, i = eval_outputs(max_and_argmax(n, axis), (MaxAndArgmax,))
2442             assert i.dtype == 'int64'
2443             self.assertTrue(np.all(v == np.max(data, np_axis)))
2444             self.assertTrue(np.all(i == np.argmax(data, np_axis)))
2445             v_shape = eval_outputs(max_and_argmax(n, axis)[0].shape)
2446             assert tuple(v_shape) == np.max(data, np_axis).shape
2447     def test2_invalid(self):
2448         n = as_tensor_variable(rand(2, 3))
2449         _logger = logging.getLogger('theano.gof.opt')
2450         oldlevel = _logger.level
2451         _logger.setLevel(logging.CRITICAL)
2452         try:
2453             try:
2454                 eval_outputs(max_and_argmax(n, 3))
2455                 assert False
2456             except ValueError:
2457                 pass
2458         finally:
2459             _logger.setLevel(oldlevel)
2460     def test2_invalid_neg(self):
2461         n = as_tensor_variable(rand(2, 3))
2462         old_stderr = sys.stderr
2463         sys.stderr = StringIO()
2464         try:
2465             try:
2466                 eval_outputs(max_and_argmax(n, -3))
2467                 assert False
2468             except ValueError:
2469                 pass
2470         finally:
2471             sys.stderr = old_stderr
2472     def test2_valid_neg(self):
2473         n = as_tensor_variable(rand(2, 3))
2474         v, i = eval_outputs(max_and_argmax(n, -1))
2475         assert i.dtype == 'int64'
2476         self.assertTrue(v.shape == (2,))
2477         self.assertTrue(i.shape == (2,))
2478         self.assertTrue(np.all(v == np.max(n.value, -1)))
2479         self.assertTrue(np.all(i == np.argmax(n.value, -1)))
2480         v, i = eval_outputs(max_and_argmax(n, -2))
2481         assert i.dtype == 'int64'
2482         self.assertTrue(v.shape == (3,))
2483         self.assertTrue(i.shape == (3,))
2484         self.assertTrue(np.all(v == np.max(n.value, -2)))
2485         self.assertTrue(np.all(i == np.argmax(n.value, -2)))
2486         v = eval_outputs(max_and_argmax(n, -1)[0].shape)
2487         assert v == (2)
2488         v = eval_outputs(max_and_argmax(n, -2)[0].shape)
2489         assert v == (3)
2490 <a name="24"></a>    def test3(self):
2491         data = rand(2, 3, 4)
2492         n = as_tensor_variable(data)
2493         for (<font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2494                                 ([0, 1, 2], None), ([1, 2, 0], None)]:
2495             v, i = eval_outputs(</b></font>max_and_argmax(n, axis))
2496             assert i.dtype == 'int64'
2497             self.assertTrue(np.all(v == np.max(data, np_axis)))
2498             self.assertTrue(np.all(i == np.argmax(data, np_axis)))
2499             v = eval_outputs(max_and_argmax(n, axis)[0].shape)
2500             assert tuple(v) == np.max(data, np_axis).shape
2501     def test_arg_grad(self):
2502         x = matrix()
2503         cost = argmax(x, axis=0).sum()
2504         gx = grad(cost, x)
2505         val = tensor.get_scalar_constant_value(gx)
2506         assert val == 0.0
2507     def test_grad(self):
2508         data = rand(2, 3)
2509         n = as_tensor_variable(data)
2510         def safe_verify_grad(func, data):
2511             data_tensor, = data
2512             data_vector = data_tensor.flatten()
2513             diff = np.abs(data_vector.reshape((-1, 1)) - data_vector)
2514             for i in xrange(len(diff)):
2515                 diff[i, i] = 1
2516             eps = builtin_min(numeric_grad.type_eps[config.floatX],
2517                               diff.min() / 2)
2518             utt.verify_grad(func, data, eps=eps)
2519         def check_grad_max(data, max_grad_data, axis=None):
2520             assert axis in [0, None]
2521             z = np.zeros_like(data)
2522             z = z.flatten()
2523             argmax = np.argmax(data, axis=axis)
2524             if argmax.ndim == 0:
2525                 z[argmax] += 1
2526             else:
2527                 for id, v in enumerate(argmax):
2528                     z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
2529                       id] += 1
2530             z = z.reshape(data.shape)
2531             assert np.all(max_grad_data == z)
2532         for axis in (-1, 0, 1, None):
2533             for j in xrange(2):
2534                 safe_verify_grad(lambda v: max_and_argmax(v, axis=axis)[j],
2535                                  [data])
2536                 if axis != 1:
2537                     safe_verify_grad(lambda v: max_and_argmax(v.flatten(),
2538                                                               axis=axis)[j],
2539                                      [data])
2540             if axis in (0, None):
2541                 check_grad_max(data, eval_outputs(grad(
2542                     max_and_argmax(n, axis=axis)[0].sum(), n)), axis=axis)
2543             check_grad_max(data, eval_outputs(grad(
2544                 max_and_argmax(n.flatten())[0], n)))
2545         data = rand(3, 4, 5)
2546         for i in [0, 1, 2]:
2547             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[0], [data])
2548             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[1], [data])
2549         data = rand(2, 3, 4, 5)
2550         for i in [0, 1, 2, 3]:
2551             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[0], [data])
2552             safe_verify_grad(lambda v: max_and_argmax(v, axis=[i])[1], [data])
2553         for i in [[0, 1], [0, 0]]:
2554             safe_verify_grad(lambda v: max_and_argmax(v, axis=i)[0], [data])
2555             safe_verify_grad(lambda v: max_and_argmax(v, axis=i)[1], [data])
2556     def test_preserve_broadcastable(self):
2557         x = tensor.matrix().dimshuffle('x', 0, 'x', 1, 'x')
2558         y = x.max(axis=1)
2559         assert y.type.broadcastable == (True, True, False, True)
2560     def test_multiple_axes(self):
2561         data = np.arange(24).reshape(3, 2, 4)
2562         x = as_tensor_variable(data)
2563         v, i = eval_outputs(max_and_argmax(x, [1, -1]))
2564         assert np.all(v == np.array([7, 15, 23]))
2565         assert np.all(i == np.array([7, 7, 7]))
2566         v = eval_outputs(max_and_argmax(x, [1, -1])[0].shape)
2567         assert tuple(v) == np.max(data, (1, -1)).shape
2568     def test_zero_shape(self):
2569         x = tensor.matrix()
2570         m, i = max_and_argmax(x, axis=1)
2571         f = theano.function([x], [m, i])
2572         xv = np.zeros((0, 4), dtype=floatX)
2573         mv, iv = f(xv)
2574         assert mv.shape == (0,)
2575         assert iv.shape == (0,)
2576     def test_numpy_input(self):
2577         ar = np.array([1, 2, 3])
2578         max, argmax = max_and_argmax(ar, axis=None)
2579         self.assertEqual(max.eval(), 3)
2580         self.assertEqual(argmax.eval(), 2)
2581 class T_argmin_argmax(unittest.TestCase):
2582     def setUp(self):
2583         utt.seed_rng()
2584         MaxAndArgmax.debug = 0
2585     def test_scalar(self):
2586         for fct in [argmin, argmax]:
2587             n = as_tensor_variable(5.0)
2588             i = eval_outputs(fct(n))
2589             self.assertTrue(i == 0)
2590             v = eval_outputs(fct(n).shape)
2591             assert len(v) == 0
2592     def test_list(self):
2593         n = as_tensor_variable([1, 2, 3, 2, -6])
2594         i = eval_outputs(argmin(n))
2595         self.assertTrue(i == 4)
2596         v = eval_outputs(argmin(n).shape)
2597         assert len(v) == 0
2598         n = as_tensor_variable([1, 2, 3, 2, -6])
2599         i = eval_outputs(argmax(n))
2600         self.assertTrue(i == 2)
2601         v = eval_outputs(argmax(n).shape)
2602         assert len(v) == 0
2603     def test2(self):
2604         data = rand(2, 3)
2605         n = as_tensor_variable(data)
2606         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2607             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2608                                     ([0, 1], None), ([1, 0], None)]:
2609                 v = eval_outputs(fct(n, axis))
2610                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2611                 v_shape = eval_outputs(fct(n, axis).shape)
2612                 assert tuple(v_shape) == nfct(data, np_axis).shape
2613     def test2_float16(self):
2614         data = (rand(20, 30).astype("float16") - 0.5) * 20
2615         n = shared(data)
2616         mode = get_default_mode().including("local_max_and_argmax", "uncanonicalize")
2617         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2618             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2619                                     ([0, 1], None), ([1, 0], None)]:
2620                 v = eval_outputs(fct(n, axis), (Argmax,), mode=mode)
2621                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2622                 v_shape = eval_outputs(fct(n, axis).shape, mode=mode)
2623                 assert tuple(v_shape) == nfct(data, np_axis).shape
2624     def test2_invalid(self):
2625         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2626             n = as_tensor_variable(rand(2, 3))
2627             _logger = logging.getLogger('theano.gof.opt')
2628             oldlevel = _logger.level
2629             _logger.setLevel(logging.CRITICAL)
2630             try:
2631                 try:
2632                     eval_outputs(fct(n, 3))
2633                     assert False
2634                 except ValueError:
2635                     pass
2636             finally:
2637                 _logger.setLevel(oldlevel)
2638     def test2_invalid_neg(self):
2639         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2640             n = as_tensor_variable(rand(2, 3))
2641             old_stderr = sys.stderr
2642             sys.stderr = StringIO()
2643             try:
2644                 try:
2645                     eval_outputs(fct(n, -3))
2646                     assert False
2647                 except ValueError:
2648                     pass
2649             finally:
2650                 sys.stderr = old_stderr
2651     def test2_valid_neg(self):
2652         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2653             n = as_tensor_variable(rand(2, 3))
2654             i = eval_outputs(fct(n, -1))
2655             self.assertTrue(i.shape == (2,))
2656             self.assertTrue(np.all(i == nfct(n.value, -1)))
2657             i = eval_outputs(fct(n, -2))
2658             self.assertTrue(i.shape == (3,))
2659             self.assertTrue(np.all(i == nfct(n.value, -2)))
2660             v = eval_outputs(fct(n, -1).shape)
2661             assert v == (2)
2662             v = eval_outputs(fct(n, -2).shape)
2663             assert v == (3)
2664     def test3(self):
2665         data = rand(2, 3, 4)
2666         n = as_tensor_variable(data)
2667         for fct, nfct in [(argmax, np.argmax), (argmin, np.argmin)]:
2668             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (2, 2),
2669                                     (None, None), ([0, 1, 2], None),
2670                                     ([1, 0, 2], None)]:
2671                 v = eval_outputs(fct(n, axis))
2672                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2673                 v_shape = eval_outputs(fct(n, axis).shape)
2674                 assert tuple(v_shape) == nfct(data, np_axis).shape
2675     def test_grad_argmin(self):
2676         data = rand(2, 3)
2677         n = as_tensor_variable(data)
2678         n.name = 'n'
2679         utt.verify_grad(lambda v: argmin(v, axis=-1), [data])
2680         utt.verify_grad(lambda v: argmin(v, axis=[0]), [data])
2681         utt.verify_grad(lambda v: argmin(v, axis=[1]), [data])
2682         utt.verify_grad(lambda v: argmin(v.flatten()), [data])
2683         try:
2684             cost = argmin(n, axis=-1)
2685             cost.name = None
2686             grad(cost, n)
2687             raise Exception('Expected an error')
2688         except TypeError:
2689             pass
2690     def test_grad_argmax(self):
2691         data = rand(2, 3)
2692         n = as_tensor_variable(data)
2693         utt.verify_grad(lambda v: argmax(v, axis=-1), [data])
2694         utt.verify_grad(lambda v: argmax(v, axis=[0]), [data])
2695         utt.verify_grad(lambda v: argmax(v, axis=[1]), [data])
2696         utt.verify_grad(lambda v: argmax(v.flatten()), [data])
2697         try:
2698             grad(argmax(n, axis=-1), n)
2699             raise Exception('Expected an error')
2700         except TypeError:
2701             pass
2702     def test_uint(self):
2703         for dtype in ('uint8', 'uint16', 'uint32', 'uint64'):
2704             itype = np.iinfo(dtype)
2705             data = np.array([itype.min + 3, itype.min, itype.max - 5, itype.max], dtype)
2706             n = as_tensor_variable(data)
2707             i = eval_outputs(argmin(n))
2708             self.assertEqual(i, 1)
2709             i = eval_outputs(argmax(n))
2710             self.assertEqual(i, 3)
2711     def test_bool(self):
2712         data = np.array([True, False], 'bool')
2713         n = as_tensor_variable(data)
2714         i = eval_outputs(argmin(n))
2715         self.assertEqual(i, 1)
2716         i = eval_outputs(argmax(n))
2717         self.assertEqual(i, 0)
2718 class T_min_max(unittest.TestCase):
2719     def setUp(self):
2720         utt.seed_rng()
2721         MaxAndArgmax.debug = 0
2722     def test_scalar(self):
2723         for fct in [max, min]:
2724             n = as_tensor_variable(5.0)
2725             v = eval_outputs(fct(n))
2726             self.assertTrue(v == 5.0)
2727             v = eval_outputs(fct(n).shape)
2728             assert len(v) == 0
2729     def test_list(self):
2730         for fct, nfct in [(max, np.max), (min, np.min)]:
2731             n = as_tensor_variable([1, 2, 3, 2, -6])
2732             v = eval_outputs([fct(n)])
2733             self.assertTrue(v == nfct(n.value))
2734             v = eval_outputs(fct(n).shape)
2735             assert len(v) == 0
2736     def test2(self):
2737         data = rand(2, 3)
2738         n = as_tensor_variable(data)
2739         for fct, nfct in [(max, np.max), (min, np.min)]:
2740             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (None, None),
2741                                     ([0, 1], None), ([1, 0], None)]:
2742                 v = eval_outputs(fct(n, axis))
2743                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2744                 v_shape = eval_outputs(fct(n, axis).shape)
2745                 assert tuple(v_shape) == nfct(data, np_axis).shape
2746     def test2_invalid(self):
2747         for fct in [max, min]:
2748             n = as_tensor_variable(rand(2, 3))
2749             _logger = logging.getLogger('theano.gof.opt')
2750             oldlevel = _logger.level
2751             _logger.setLevel(logging.CRITICAL)
2752             try:
2753                 try:
2754                     eval_outputs(fct(n, 3))
2755                     assert False
2756                 except ValueError:
2757                     pass
2758             finally:
2759                 _logger.setLevel(oldlevel)
2760     def test2_invalid_neg(self):
2761         for fct in [max, min]:
2762             n = as_tensor_variable(rand(2, 3))
2763             old_stderr = sys.stderr
2764             sys.stderr = StringIO()
2765             try:
2766                 try:
2767                     eval_outputs(fct(n, -3))
2768                     assert False
2769                 except ValueError:
2770                     pass
2771             finally:
2772                 sys.stderr = old_stderr
2773     def test2_valid_neg(self):
2774         for fct, nfct in [(max, np.max), (min, np.min)]:
2775             n = as_tensor_variable(rand(2, 3))
2776             v = eval_outputs(fct(n, -1))
2777             self.assertTrue(v.shape == (2,))
2778             self.assertTrue(np.all(v == nfct(n.value, -1)))
2779             v = eval_outputs(fct(n, -2))
2780             self.assertTrue(v.shape == (3,))
2781             self.assertTrue(np.all(v == nfct(n.value, -2)))
2782             v = eval_outputs(fct(n, -1).shape)
2783             assert v == (2)
2784             v = eval_outputs(fct(n, -2).shape)
2785             assert v == (3)
2786     def test3(self):
2787         data = rand(2, 3, 4)
2788         n = as_tensor_variable(data)
2789         for fct, nfct in [(max, np.max), (min, np.min)]:
2790             for (axis, np_axis) in [(-1, -1), (0, 0), (1, 1), (2, 2),
2791                                     (None, None), ([0, 1, 2], None),
2792                                     ([1, 0, 2], None)]:
2793                 v = eval_outputs(fct(n, axis))
2794                 self.assertTrue(np.all(v == nfct(data, np_axis)))
2795                 v_shape = eval_outputs(fct(n, axis).shape)
2796                 assert tuple(v_shape) == nfct(data, np_axis).shape
2797     def test3b(self):
2798         data = rand(2, 3, 4)
2799         n = as_tensor_variable(data)
2800         for fct, nfct in [(max, np.max), (min, np.min)]:
2801             for axis in [[0, 1], [1, 2], [0, 2]]:
2802                 v = eval_outputs(fct(n, axis))
2803                 np_v = nfct(nfct(data, axis[1]), axis[0])
2804                 self.assertTrue(np.all(v == np_v))
2805                 v_shape = eval_outputs(fct(n, axis).shape)
2806                 assert tuple(v_shape) == np_v.shape
2807     def test_grad_max(self):
2808         data = rand(2, 3)
2809         n = as_tensor_variable(data)
2810         def check_grad_max(data, max_grad_data, axis=None):
2811             assert axis in [0, None]
2812             z = np.zeros_like(data)
2813             z = z.flatten()
2814             argmax = np.argmax(data, axis=axis)
2815             if argmax.ndim == 0:
2816                 z[np.argmax(data, axis=axis)] += 1
2817             else:
2818                 for id, v in enumerate(argmax):
2819                     z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
2820                       id] += 1
2821             z = z.reshape(data.shape)
2822             assert np.all(max_grad_data == z)
2823         utt.verify_grad(lambda v: max(v, axis=-1), [data])
2824         utt.verify_grad(lambda v: max(v, axis=[0]), [data])
2825         check_grad_max(data, eval_outputs(grad(max(n, axis=0).sum(), n)),
2826                        axis=0)
2827         utt.verify_grad(lambda v: max(v, axis=[1]), [data])
2828         utt.verify_grad(lambda v: max(v.flatten()), [data])
2829         check_grad_max(data, eval_outputs(grad(max(n.flatten()), n)))
2830     def test_grad_min(self):
2831         data = rand(2, 3)
2832         n = as_tensor_variable(data)
2833         def check_grad_min(data, min_grad_data, axis=None):
2834             assert axis in [0, None]
2835             z = np.zeros_like(data)
2836             z = z.flatten()
2837             argmin = np.argmin(data, axis=axis)
2838             if argmin.ndim == 0:
2839                 z[np.argmin(data, axis=axis)] += 1
2840             else:
2841                 for id, v in enumerate(argmin):
2842                     z[v * np.prod(data.shape[data.ndim - 1:axis:-1]) +
2843                       id] += 1
2844             z = z.reshape(data.shape)
2845             assert np.all(min_grad_data == z)
2846         utt.verify_grad(lambda v: min(v, axis=-1), [data])
2847         utt.verify_grad(lambda v: min(v, axis=[0]), [data])
2848         check_grad_min(data, eval_outputs(grad(min(n, axis=0).sum(), n)),
2849                        axis=0)
2850         utt.verify_grad(lambda v: min(v, axis=[1]), [data])
2851         utt.verify_grad(lambda v: min(v.flatten()), [data])
2852         check_grad_min(data, eval_outputs(grad(min(n.flatten()), n)))
2853     def _grad_list(self):
2854         data = rand(2, 3)
2855         for fct in [max_and_argmax, max, min]:
2856             utt.verify_grad(lambda v: fct(v, axis=[0, 1]), [data])
2857     def test_uint(self):
2858         for dtype in ('uint8', 'uint16', 'uint32', 'uint64'):
2859             itype = np.iinfo(dtype)
2860             data = np.array([itype.min + 3, itype.min, itype.max - 5, itype.max], dtype)
2861             n = as_tensor_variable(data)
2862             self.assertEqual(min(n).dtype, dtype)
2863             i = eval_outputs(min(n))
2864             self.assertEqual(i, itype.min)
2865             self.assertEqual(max(n).dtype, dtype)
2866             i = eval_outputs(max(n))
2867             self.assertEqual(i, itype.max)
2868     def test_bool(self):
2869         data = np.array([True, False], 'bool')
2870         n = as_tensor_variable(data)
2871         self.assertEqual(min(n).dtype, 'bool')
2872         i = eval_outputs(min(n))
2873         self.assertEqual(i, False)
2874         self.assertEqual(max(n).dtype, 'bool')
2875         i = eval_outputs(max(n))
2876         self.assertEqual(i, True)
2877 def test_basic_allclose():
2878     assert tensor.basic._allclose(-0.311023883434, -0.311022856884)
2879 class T_outer(unittest.TestCase):
2880     def test_outer(self):
2881         for m in range(4):
2882             for n in range(4):
2883                 x = tensor.tensor(dtype='floatX', broadcastable=(False,) * m)
2884                 y = tensor.tensor(dtype='floatX', broadcastable=(False,) * n)
2885                 s1 = np.random.randint(1, 10, m)
2886                 s2 = np.random.randint(1, 10, n)
2887                 v1 = np.asarray(np.random.rand(*s1)).astype(floatX)
2888                 v2 = np.asarray(np.random.rand(*s2)).astype(floatX)
2889                 o = tensor.outer(x, y).eval({x: v1, y: v2})
2890                 assert_allclose(o, np.outer(v1, v2))
2891     def test_grad(self):
2892         for shp0, shp1 in [((1,), (2,)),
2893                            ((3,), (1,)),
2894                            ((1,), (1,)),
2895                            ((3,), (2,)),
2896                            ((3, 2), (1, 1)),
2897                            ((3, 2), (1, 4)),
2898                            ((3, 2), (4, 1)),
2899                            ((3, 2), (4, 5)),
2900                            ((1, 2), (4, 5)),
2901                            ((3, 1), (4, 5)),
2902                            ((1, 1), (4, 5)),
2903                            ((1, 1), (1, 1)),
2904                            ]:
2905             data0 = np.random.rand(*shp0).astype(floatX)
2906             data1 = np.random.rand(*shp1).astype(floatX)
2907             utt.verify_grad(tensor.outer, [data0, data1])
2908 class T_GetVectorLength(unittest.TestCase):
2909     def test_get_vector_length(self):
2910         x = theano.shared(np.zeros((2, 3, 4, 5)))
2911         assert len(list(x.shape)) == 4
2912         assert len(list(x.shape[2:4])) == 2
2913         assert len(list(x.shape[2:])) == 2
2914         assert len(list(x.shape[1:4])) == 3
2915         assert len(list(x.shape[2:2])) == 0
2916         assert len(list(x.shape[1:5])) == 3
2917         assert len(list(x.shape[1:10])) == 3
2918         assert len(list(x.shape[1:10:2])) == 2
2919         assert len(list(x.shape[-1:4])) == 1
2920         assert len(list(x.shape[-6:4])) == 4
2921         assert len(list(x.shape[1:-2])) == 1
2922         assert len(list(x.shape[1:-1])) == 2
2923 class T_Join_and_Split(unittest.TestCase):
2924     def setUp(self):
2925         Join.debug = False
2926         utt.seed_rng()
2927         self.mode = theano.compile.get_default_mode().excluding(
2928             'constant_folding')
2929         self.join_op = Join()
2930         self.split_op_class = Split
2931         self.make_vector_op = opt.MakeVector()
2932         self.floatX = config.floatX
2933         self.hide_error = theano.config.mode not in [
2934             'DebugMode', 'DEBUG_MODE', 'FAST_COMPILE']
2935         self.shared = shared
2936     def eval_outputs_and_check_join(self, outputs):
2937         f = theano.function([], outputs, self.mode)
2938         topo = f.maker.fgraph.toposort()
2939         assert [True for node in topo
2940                 if isinstance(node.op, type(self.join_op))]
2941         variables = f()
2942         if isinstance(variables, (tuple, list)) and len(variables) == 1:
2943             return variables[0]
2944         return variables
2945     def eval_outputs_and_check_vector(self, outputs,
2946                                       make_vector_op=None):
2947         if make_vector_op is None:
2948             make_vector_op = self.make_vector_op
2949         f = theano.function([], outputs, self.mode)
2950         topo = f.maker.fgraph.toposort()
2951         assert [True for node in topo
2952                 if isinstance(node.op, type(make_vector_op))]
2953         variables = f()
2954         if isinstance(variables, (tuple, list)) and len(variables) == 1:
2955             return variables[0]
2956         return variables
2957     def test_join_scalar(self):
2958         a = as_tensor_variable(1)
2959         b = as_tensor_variable(2)
2960         self.assertRaises(TypeError, join, 0, a, b)
2961     def test_stack_mixed_type_constants(self):
2962         a = as_tensor_variable(1)
2963         b = as_tensor_variable(2.0)
2964         c = tensor._shared(np.asarray(3.0, dtype=self.floatX))
2965         s = stack([a, b, c])
2966         want = np.array([1, 2, 3])
2967         out = self.eval_outputs_and_check_vector([s], opt.MakeVector())
2968         self.assertTrue((out == want).all())
2969     def test_stack_scalar(self):
2970         a = self.shared(np.asarray(1., dtype=self.floatX))
2971         b = as_tensor_variable(2.)
2972         c = as_tensor_variable(3.)
2973         s = stack([a, b, c])
2974         want = np.array([1, 2, 3])
2975         out = self.eval_outputs_and_check_vector([s])
2976         self.assertTrue((out == want).all())
2977     def test_stack_scalar_make_vector(self):
2978         a = tensor.scalar('a', dtype=self.floatX)
2979         b = tensor.scalar('b', dtype=self.floatX)
2980         s = stack([a, b, a, b])
2981         f = function([a, b], s, mode=self.mode)
2982         val = f(1, 2)
2983         self.assertTrue(np.all(val == [1, 2, 1, 2]))
2984         topo = f.maker.fgraph.toposort()
2985         assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
2986         assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
2987         assert f.maker.fgraph.outputs[0].dtype == self.floatX
2988     def test_stack_scalar_make_vector_dtype(self):
2989         a = tensor.iscalar('a')
2990         b = tensor.lscalar('b')
2991         s = stack([a, b, a, b])
2992         f = function([a, b], s, mode=self.mode)
2993         val = f(1, 2)
2994         self.assertTrue(np.all(val == [1, 2, 1, 2]))
2995         topo = f.maker.fgraph.toposort()
2996         assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
2997         assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
2998         assert f.maker.fgraph.outputs[0].dtype == 'int64'
2999     def test_stack_scalar_make_vector_constant(self):
3000         a = tensor.iscalar('a')
3001         b = tensor.lscalar('b')
3002         s = stack([10, a, b, np.int8(3)])
3003         f = function([a, b], s, mode=self.mode)
3004         val = f(1, 2)
3005         self.assertTrue(np.all(val == [10, 1, 2, 3]))
3006         topo = f.maker.fgraph.toposort()
3007         assert len([n for n in topo if isinstance(n.op, opt.MakeVector)]) &gt; 0
3008         assert len([n for n in topo if isinstance(n, type(self.join_op))]) == 0
3009         assert f.maker.fgraph.outputs[0].dtype == 'int64'
3010     def test_stack_new_interface(self):
3011         warnings.simplefilter('always', DeprecationWarning)
3012         a = tensor.imatrix('a')
3013         b = tensor.imatrix('b')
3014         s1 = stack(a, b)
3015         s2 = stack([a, b])
3016         f = function([a, b], [s1, s2], mode=self.mode)
3017         v1, v2 = f([[1, 2]], [[3, 4]])
3018         self.assertTrue(v1.shape == v2.shape)
3019         self.assertTrue(np.all(v1 == v2))
3020         s3 = stack([a, b], 1)
3021         f = function([a, b], s3, mode=self.mode)
3022         v3 = f([[1, 2]], [[3, 4]])
3023         v4 = np.array([[[1, 2], [3, 4]]])
3024         self.assertTrue(v3.shape == v4.shape)
3025         self.assertTrue(np.all(v3 == v4))
3026         v1 = [[1, 2, 3], [4, 5, 6]]
3027         v2 = [[7, 8, 9], [10, 11, 12]]
3028         s = stack([a, b], axis=-1)
3029         f = function([a, b], s, mode=self.mode)
3030         v = np.zeros((2, 3, 2))
3031         v[:, :, 0] = v1
3032         v[:, :, 1] = v2
3033         out = f(v1, v2)
3034         self.assertTrue(v.shape == out.shape)
3035         self.assertTrue(np.all(v == out))
3036         s = stack([a, b], axis=-2)
3037         f = function([a, b], s, mode=self.mode)
3038         v = np.zeros((2, 2, 3))
3039         v[:, 0, :] = v1
3040         v[:, 1, :] = v2
3041         out = f(v1, v2)
3042         self.assertTrue(v.shape == out.shape)
3043         self.assertTrue(np.all(v == out))
3044         self.assertRaises(IndexError, stack, [a, b], 4)
3045         self.assertRaises(IndexError, stack, [a, b], -4)
3046         with warnings.catch_warnings(record=True) as w:
3047             s = stack(a, b)
3048             assert len(w) == 1
3049             assert issubclass(w[-1].category, DeprecationWarning)
3050         with warnings.catch_warnings(record=True) as w:
3051             s = stack([a, b])
3052             s = stack([a, b], 1)
3053             s = stack([a, b], axis=1)
3054             s = stack(tensors=[a, b])
3055             s = stack(tensors=[a, b], axis=1)
3056             assert not w
3057     def test_stack_hessian(self):
3058         a = tensor.dvector('a')
3059         b = tensor.dvector('b')
3060         A = stack([a, b])
3061         B = A.T.dot(A)
3062         Ha, Hb = hessian(B.sum(), [a, b])
3063         a_v = np.random.rand(4)
3064         b_v = np.random.rand(4)
3065         f = theano.function([a, b], [Ha, Hb])
3066         Ha_v, Hb_v = f(a_v, b_v)
3067         assert Ha_v.shape == (4, 4)
3068         assert Hb_v.shape == (4, 4)
3069         assert np.allclose(Ha_v, 2.)
3070         assert np.allclose(Hb_v, 2.)
3071     def test_stack_hessian2(self):
3072         a = tensor.dvector('a')
3073         b = tensor.dvector('b')
3074         A = stack([a, b])
3075         Ha, Hb = hessian(A.sum(), [a, b])
3076         a_v = np.random.rand(4)
3077         b_v = np.random.rand(4)
3078         f = theano.function([a, b], [Ha, Hb])
3079         Ha_v, Hb_v = f(a_v, b_v)
3080         assert Ha_v.shape == (4, 4)
3081         assert Hb_v.shape == (4, 4)
3082         assert np.allclose(Ha_v, 0.)
3083         assert np.allclose(Hb_v, 0.)
3084     def test_join_concatenate_one_element(self):
3085         m = tensor.fmatrix()
3086         c = tensor.concatenate([m])
3087         f = theano.function(inputs=[m], outputs=[c],
3088                             mode=self.mode.including('local_join_1'))
3089         topo = f.maker.fgraph.toposort()
3090         assert len(topo) == 1
3091         assert isinstance(topo[0].op, DeepCopyOp)
3092     def test_join_vector(self):
3093         a = self.shared(np.array([1, 2, 3], dtype=self.floatX))
3094         b = as_tensor_variable(np.array([7, 8, 9], dtype=self.floatX))
3095         s = join(0, a, b)
3096         want = np.array([1, 2, 3, 7, 8, 9])
3097         out = self.eval_outputs_and_check_join([s])
3098         self.assertTrue((out == want).all())
3099     def test_roll(self):
3100         for get_shift in [lambda a:a, lambda x:theano.shared(x)]:
3101             a = self.shared(np.array([1, 2, 3, 4, 5, 6], dtype=self.floatX))
3102             b = roll(a, get_shift(2))
3103             want = np.array([5, 6, 1, 2, 3, 4])
3104             out = theano.function([], b)()
3105             assert (out == want).all()
3106             b = roll(a, get_shift(-1), 0)
3107             want = np.array([2, 3, 4, 5, 6, 1])
3108             out = theano.function([], b)()
3109             assert (out == want).all()
3110             a = self.shared(np.arange(21).reshape((3, 7)).astype(self.floatX))
3111             b = roll(a, get_shift(-2), 1)
3112             want = np.roll(a.get_value(borrow=True), -2, 1)
3113             out = theano.function([], b)()
3114             assert (out == want).all()
3115             a = self.shared(np.arange(24).reshape((3, 2, 4)).astype(self.floatX))
3116             b = roll(a, get_shift(-2), -2)
3117             want = np.roll(a.get_value(borrow=True), -2, -2)
3118             out = theano.function([], b)()
3119             assert (out == want).all()
3120             want = np.roll(a.get_value(borrow=True), -2, 0)
3121             b = roll(a, get_shift(-2), 0)
3122             out = theano.function([], b)()
3123             assert (out == want).all()
3124             want = np.roll(a.get_value(borrow=True), 2)
3125             b = roll(a, get_shift(2))
3126             out = theano.function([], b)()
3127             assert (out == want).all()
3128             want = np.roll(a.get_value(borrow=True), 4, 0)
3129             b = roll(a, get_shift(4), 0)
3130             out = theano.function([], b)()
3131             assert (out == want).all()
3132             want = np.roll(a.get_value(borrow=True), -4, 0)
3133             b = roll(a, get_shift(-4), 0)
3134             out = theano.function([], b)()
3135             assert (out == want).all()
3136     def test_stack_vector(self):
3137         a = self.shared(np.array([1, 2, 3], dtype=self.floatX))
3138         b = as_tensor_variable(np.array([7, 8, 9], dtype=self.floatX))
3139         s = stack([a, b])
3140         want = np.array([[1, 2, 3], [7, 8, 9]])
3141         out = self.eval_outputs_and_check_join([s])
3142         self.assertTrue((out == want).all())
3143     def test_join_matrix0(self):
3144         a = self.shared(np.array([[1, 2, 3], [4, 5, 6]],
3145                                  dtype=self.floatX))
3146         b = as_tensor_variable(np.array([[7, 8, 9]], dtype=self.floatX))
3147         s = join(0, a, b)
3148         want = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
3149         out = self.eval_outputs_and_check_join([s])
3150         self.assertTrue((out == want).all())
3151     def test_join_matrix1(self):
3152         av = np.array([[.1, .2, .3], [.4, .5, .6]], dtype='float32')
3153         bv = np.array([[.7], [.8]], dtype='float32')
3154         a = self.shared(av)
3155         b = as_tensor_variable(bv)
3156         s = join(1, a, b)
3157         want = np.array([[.1, .2, .3, .7], [.4, .5, .6, .8]],
3158                         dtype='float32')
3159         out = self.eval_outputs_and_check_join([s])
3160         self.assertTrue((out == want).all())
3161         utt.verify_grad(lambda a, b: join(1, a, b), [av, bv],
3162                         mode=self.mode)
3163     def test_join_matrix_dtypes(self):
3164         if "float32" in self.shared.__name__:
3165             raise SkipTest(
3166                 "The shared variable constructor"
3167                 " need to support other dtype then float32")
3168         av = np.array([[1, 2, 3], [4, 5, 6]], dtype='int8')
3169         bv = np.array([[7], [8]], dtype='float32')
3170         a = self.shared(av)
3171         b = as_tensor_variable(bv)
3172         s = join(1, a, b)
3173         want = np.array([[1, 2, 3, 7], [4, 5, 6, 8]], dtype='float32')
3174         out = self.eval_outputs_and_check_join([s])
3175         self.assertTrue((out == want).all())
3176         grad(s.sum(), b)
3177         grad(s.sum(), a)
3178         utt.verify_grad(lambda b: join(1, a, b), [bv],
3179                         eps=1.0e-2, mode=self.mode)
3180     def test_join_matrix_ints(self):
3181         if "float32" in self.shared.__name__:
3182             raise SkipTest(
3183                 "The shared variable constructor"
3184                 " need to support other dtype then float32")
3185         av = np.array([[1, 2, 3], [4, 5, 6]], dtype='int8')
3186         bv = np.array([[7], [8]], dtype='int32')
3187         a = self.shared(av)
3188         b = as_tensor_variable(bv)
3189         s = join(1, a, b)
3190         want = np.array([[1, 2, 3, 7], [4, 5, 6, 8]], dtype='float32')
3191         out = self.eval_outputs_and_check_join([s])
3192         self.assertTrue((out == want).all())
3193         assert (np.asarray(grad(s.sum(), b).eval()) == 0).all()
3194         assert (np.asarray(grad(s.sum(), a).eval()) == 0).all()
3195     def test_join_matrix1_using_vertical_stack(self):
3196         a = self.shared(np.array([[1, 2, 3], [4, 5, 6]], dtype=self.floatX))
3197         b = as_tensor_variable(np.array([[7, 8, 9]], dtype=self.floatX))
3198         c = as_tensor_variable(np.array([[9, 8, 7]], dtype=self.floatX))
3199         s = vertical_stack(a, b, c)
3200         want = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [9, 8, 7]])
3201         out = self.eval_outputs_and_check_join([s])
3202         self.assertTrue((out == want).all())
3203     def test_join_matrix1_using_horizontal_stack(self):
3204         av = np.array([[.1, .2, .3], [.4, .5, .6]], dtype='float32')
3205         bv = np.array([[.7], [.8]], dtype='float32')
3206         cv = np.array([[.3, .2, .1], [.6, .5, .4]], dtype='float32')
3207         a = self.shared(av)
3208         b = as_tensor_variable(bv)
3209         c = as_tensor_variable(cv)
3210         s = horizontal_stack(a, b, c)
3211         want = np.array([[.1, .2, .3, .7, .3, .2, .1],
3212                          [.4, .5, .6, .8, .6, .5, .4]],
3213                         dtype='float32')
3214         out = self.eval_outputs_and_check_join([s])
3215         self.assertTrue((out == want).all())
3216         utt.verify_grad(lambda a, b: join(1, a, b), [av, bv],
3217                         mode=self.mode)
3218     def test_join_matrixV(self):
3219         v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
3220         a = self.shared(v)
3221         b = as_tensor_variable(v)
3222         ax = lscalar()
3223         s = join(ax, a, b)
3224         f = inplace_func([ax], [s], mode=self.mode)
3225         topo = f.maker.fgraph.toposort()
3226         assert [True for node in topo
3227                 if isinstance(node.op, type(self.join_op))]
3228         want = np.array([[.1, .2, .3], [.4, .5, .6],
3229                          [.1, .2, .3], [.4, .5, .6]])
3230         got = f(0)
3231         assert np.allclose(got, want)
3232         want = np.array([[.1, .2, .3, .1, .2, .3],
3233                          [.4, .5, .6, .4, .5, .6]])
3234         got = f(1)
3235         assert np.allclose(got, want)
3236         utt.verify_grad(lambda a, b: join(0, a, b), [v, 2 * v], mode=self.mode)
3237         utt.verify_grad(lambda a, b: join(1, a, b), [v, 2 * v], mode=self.mode)
3238     def test_join_matrixV_negative_axis(self):
3239         v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
3240         a = self.shared(v)
3241         b = as_tensor_variable(v)
3242         ax = lscalar()
3243         s = join(ax, a, b)
3244         f = inplace_func([ax], [s], mode=self.mode)
3245         topo = f.maker.fgraph.toposort()
3246         assert [True for node in topo
3247                 if isinstance(node.op, type(self.join_op))]
3248         want = np.array([[.1, .2, .3, .1, .2, .3],
3249                          [.4, .5, .6, .4, .5, .6]])
3250         got = f(-1)
3251         assert np.allclose(got, want)
3252         want = np.array([[.1, .2, .3], [.4, .5, .6],
3253                          [.1, .2, .3], [.4, .5, .6]])
3254         got = f(-2)
3255         assert np.allclose(got, want)
3256         self.assertRaises(IndexError, f, -3)
3257     def test_join_matrixC_negative_axis(self):
3258         v = np.array([[.1, .2, .3], [.4, .5, .6]], dtype=self.floatX)
3259         a = self.shared(v)
3260         b = as_tensor_variable(v)
3261         s = join(-1, a, b)
3262         f = theano.function([], [s], mode=self.mode)
3263         topo = f.maker.fgraph.toposort()
3264         assert [True for node in topo
3265                 if isinstance(node.op, type(self.join_op))]
3266         want = np.array([[.1, .2, .3, .1, .2, .3],
3267                          [.4, .5, .6, .4, .5, .6]])
3268         got = f()
3269         assert np.allclose(got, want)
3270         s = join(-2, a, b)
3271         f = theano.function([], [s], mode=self.mode)
3272         topo = f.maker.fgraph.toposort()
3273         assert [True for node in topo
3274                 if isinstance(node.op, type(self.join_op))]
3275         want = np.array([[.1, .2, .3], [.4, .5, .6],
3276                          [.1, .2, .3], [.4, .5, .6]])
3277         got = f()
3278         assert np.allclose(got, want)
3279         self.assertRaises(IndexError, join, -3, a, b)
3280         utt.verify_grad(lambda a, b: join(-1, a, b), [v, 2 * v],
3281                         mode=self.mode)
3282     def test_vector_len(self):
3283         x = lscalar('x')
3284         y = dscalar('y')
3285         triple = as_tensor_variable((x, y, 9.0))
3286         assert 3 == get_vector_length(triple)
3287         a, b, c = triple
3288         f = function([x, y], [b, c, a], mode=self.mode)
3289         topo = f.maker.fgraph.toposort()
3290         assert [True for node in topo if isinstance(node.op, opt.MakeVector)]
3291         assert np.allclose(f(4, 5), [5, 9, 4])
3292     def test_broadcastable_flag_assignment_mixed_otheraxes(self):
3293         rng = np.random.RandomState(seed=utt.fetch_seed())
3294         a_val = rng.rand(1, 4, 1).astype(self.floatX)
3295         b_val = rng.rand(1, 3, 1).astype(self.floatX)
3296         a = self.shared(a_val, broadcastable=(False, False, True))
3297         b = self.shared(b_val, broadcastable=(True, False, True))
3298         c = self.join_op(1, a, b)
3299         assert c.type.broadcastable[0] and c.type.broadcastable[2]
3300         assert not c.type.broadcastable[1]
3301         c = self.join_op(theano.tensor.constant(1), a, b)
3302         assert c.type.broadcastable[0] and c.type.broadcastable[2]
3303         assert not c.type.broadcastable[1]
3304         c = self.join_op(theano.tensor.cast(theano.tensor.constant(1),
3305                                             dtype="int32"),
3306                          a, b)
3307         assert c.type.broadcastable[0] and c.type.broadcastable[2]
3308         assert not c.type.broadcastable[1]
3309         f = function([], c, mode=self.mode)
3310         topo = f.maker.fgraph.toposort()
3311         assert [True for node in topo
3312                 if isinstance(node.op, type(self.join_op))]
3313         f()
3314         utt.verify_grad((lambda a, b: join(1, a, b)), [a_val, b_val], rng=rng,
3315                         mode=self.mode)
3316         a.set_value(rng.rand(2, 4, 1).astype(self.floatX))
3317         self.assertRaises(ValueError, f)
3318     def test_broadcastable_flag_assignment_mixed_thisaxes(self):
3319         rng = np.random.RandomState(seed=utt.fetch_seed())
3320         a_val = rng.rand(2, 4, 1).astype(self.floatX)
3321         b_val = rng.rand(1, 4, 1).astype(self.floatX)
3322         a = self.shared(a_val, broadcastable=(False, False, True))
3323         b = self.shared(b_val, broadcastable=(True, False, True))
3324         c = self.join_op(0, a, b)
3325         assert not c.type.broadcastable[0]
3326         f = function([], c, mode=self.mode)
3327         topo = f.maker.fgraph.toposort()
3328         assert [True for node in topo
3329                 if isinstance(node.op, type(self.join_op))]
3330         f()
3331         utt.verify_grad((lambda a, b: join(0, a, b)), [a_val, b_val], rng=rng,
3332                         mode=self.mode)
3333         self.assertRaises(TypeError, b.set_value,
3334                           rng.rand(3, 4, 1).astype(self.floatX))
3335         a = TensorType(dtype=self.floatX, broadcastable=[0, 0, 1])()
3336         b = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1])()
3337         c = self.join_op(0, a, b)
3338         f = function([a, b], c, mode=self.mode)
3339         bad_b_val = rng.rand(3, 4, 1).astype(self.floatX)
3340         self.assertRaises(TypeError, f, a_val, bad_b_val)
3341     def test_broadcastable_flags_all_broadcastable_on_joinaxis(self):
3342         rng = np.random.RandomState(seed=utt.fetch_seed())
3343         a_val = rng.rand(1, 4, 1).astype(self.floatX)
3344         b_val = rng.rand(1, 4, 1).astype(self.floatX)
3345         a = self.shared(a_val, broadcastable=(True, False, True))
3346         b = self.shared(b_val, broadcastable=(True, False, True))
3347         c = self.join_op(0, a, b)
3348         assert not c.type.broadcastable[0]
3349         f = function([], c, mode=self.mode)
3350         topo = f.maker.fgraph.toposort()
3351         assert [True for node in topo
3352                 if isinstance(node.op, type(self.join_op))]
3353         f()
3354         utt.verify_grad((lambda a, b: join(0, a, b)), [a_val, b_val], rng=rng,
3355                         mode=self.mode)
3356     def test_broadcastable_single_input_broadcastable_dimension(self):
3357         rng = np.random.RandomState(seed=utt.fetch_seed())
3358         a_val = rng.rand(1, 4, 1).astype(self.floatX)
3359         a = self.shared(a_val, broadcastable=(True, False, True))
3360         b = self.join_op(0, a)
3361         assert b.type.broadcastable[0]
3362         assert b.type.broadcastable[2]
3363         assert not b.type.broadcastable[1]
3364         f = function([], b, mode=self.mode)
3365         topo = f.maker.fgraph.toposort()
3366         if theano.config.mode != 'FAST_COMPILE':
3367             assert not [True for node in topo
3368                         if isinstance(node.op, type(self.join_op))]
3369         f()
3370         utt.verify_grad((lambda a: join(0, a)), [a_val], rng=rng,
3371                         mode=self.mode)
3372         self.assertRaises(TypeError, a.set_value,
3373                           rng.rand(2, 4, 1).astype(self.floatX))
3374     def test_broadcastable_flags_many_dims_and_inputs(self):
3375         a = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 0, 0, 0])()
3376         b = TensorType(dtype=self.floatX, broadcastable=[1, 1, 1, 0, 0, 0])()
3377         c = TensorType(dtype=self.floatX, broadcastable=[1, 0, 0, 0, 0, 0])()
3378         d = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 1, 0, 1])()
3379         e = TensorType(dtype=self.floatX, broadcastable=[1, 0, 1, 0, 0, 1])()
3380         f = self.join_op(0, a, b, c, d, e)
3381         fb = f.type.broadcastable
3382         assert not fb[0] and fb[1] and fb[2] and fb[3] and not fb[4] and fb[5]
3383         g = self.join_op(1, a, b, c, d, e)
3384         gb = g.type.broadcastable
3385         assert gb[0] and not gb[1] and gb[2] and gb[3] and not gb[4] and gb[5]
3386         h = self.join_op(4, a, b, c, d, e)
3387         hb = h.type.broadcastable
3388         assert hb[0] and hb[1] and hb[2] and hb[3] and not hb[4] and hb[5]
3389         f = function([a, b, c, d, e], f, mode=self.mode)
3390         topo = f.maker.fgraph.toposort()
3391         assert [True for node in topo
3392                 if isinstance(node.op, type(self.join_op))]
3393         rng = np.random.RandomState(seed=utt.fetch_seed())
3394         a_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3395         b_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3396         c_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3397         d_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3398         e_val = rng.rand(1, 1, 1, 1, 2, 1).astype(self.floatX)
3399         f(a_val, b_val, c_val, d_val, e_val)
3400         utt.verify_grad((lambda a, b, c, d, e: join(0, a, b, c, d, e)),
3401                         [a_val, b_val, c_val, d_val, e_val], rng=rng,
3402                         mode=self.mode)
3403         bad_val = rng.rand(2, 1, 1, 1, 2, 1).astype(self.floatX)
3404         self.assertRaises(TypeError, f, bad_val, b_val, c_val, d_val, e_val)
3405         self.assertRaises(TypeError, f, a_val, bad_val, c_val, d_val, e_val)
3406         self.assertRaises(TypeError, f, a_val, b_val, bad_val, d_val, e_val)
3407         self.assertRaises(TypeError, f, a_val, b_val, c_val, bad_val, e_val)
3408         self.assertRaises(TypeError, f, a_val, b_val, c_val, d_val, bad_val)
3409         bad_a_val = rng.rand(1, 2, 1, 1, 2, 1).astype(self.floatX)
3410         bad_b_val = rng.rand(1, 1, 1, 1, 2, 2).astype(self.floatX)
3411         bad_c_val = rng.rand(1, 1, 2, 1, 2, 1).astype(self.floatX)
3412         bad_d_val = rng.rand(1, 2, 1, 1, 2, 1).astype(self.floatX)
3413         bad_e_val = rng.rand(1, 1, 1, 2, 2, 1).astype(self.floatX)
3414         self.assertRaises(ValueError, f, bad_a_val, b_val, c_val, d_val, e_val)
3415         self.assertRaises(ValueError, f, a_val, bad_b_val, c_val, d_val, e_val)
3416         self.assertRaises(ValueError, f, a_val, b_val, bad_c_val, d_val, e_val)
3417         self.assertRaises(ValueError, f, a_val, b_val, c_val, bad_d_val, e_val)
3418         self.assertRaises(ValueError, f, a_val, b_val, c_val, d_val, bad_e_val)
3419     def test_infer_shape_join(self):
3420         def get_mat(s1, s2):
3421             return np.asarray(np.random.uniform(size=(s1, s2)),
3422                               dtype=self.floatX)
3423         x1 = self.shared(get_mat(3, 4))
3424         x2 = self.shared(get_mat(2, 4))
3425         x3 = self.shared(get_mat(1, 4))
3426         z = self.join_op(0, x1, x2, x3)
3427         f = theano.function([], z.shape, mode=self.mode)
3428         topo = f.maker.fgraph.toposort()
3429         out = f()
3430         assert (out == [6, 4]).all()
3431         if theano.config.mode != 'FAST_COMPILE':
3432             for node in f.maker.fgraph.toposort():
3433                 assert not isinstance(node.op, type(self.join_op))
3434         x1.set_value(get_mat(3, 4))
3435         x2.set_value(get_mat(3, 4))
3436         x3.set_value(get_mat(3, 5))
3437         z = self.join_op(1, x1, x2, x3)
3438         f = theano.function([], z.shape, mode=self.mode)
3439         topo = f.maker.fgraph.toposort()
3440         out = f()
3441         assert (out == [3, 13]).all()
3442         if theano.config.mode != 'FAST_COMPILE':
3443             for node in topo:
3444                 assert not isinstance(node.op, type(self.join_op))
3445         with change_flags(compute_test_value='off'):
3446             x1.set_value(get_mat(3, 4))
3447             x2.set_value(get_mat(3, 4))
3448             x3.set_value(get_mat(2, 5))
3449             if not self.hide_error:
3450                 self.assertRaises(ValueError, f)
3451             else:
3452                 f()
3453     def test_rebroadcast(self):
3454         x = tensor.TensorType(self.floatX, [False, False, True])()
3455         u = tensor.TensorType(self.floatX, [False, False, True])()
3456         tensor.concatenate([x, -u], axis=2)
3457     def test_concatenate_same(self):
3458         rng = np.random.RandomState(seed=utt.fetch_seed())
3459         T_shared = self.shared(rng.rand(3, 4).astype(self.floatX))
3460         Tout = tensor.concatenate([T_shared, T_shared])
3461         f = function([], Tout, mode=self.mode)
3462         out = f()
3463         if theano.config.mode != 'FAST_COMPILE':
3464             assert [True for node in f.maker.fgraph.toposort()
3465                     if isinstance(node.op, type(self.join_op))]
3466         assert np.allclose(out,
3467                            np.concatenate([T_shared.get_value(),
3468                                            T_shared.get_value()]))
3469     def test_mixed_ndim_error(self):
3470         rng = np.random.RandomState(seed=utt.fetch_seed())
3471         v = self.shared(rng.rand(4).astype(self.floatX))
3472         m = self.shared(rng.rand(4, 4).astype(self.floatX))
3473         self.assertRaises(TypeError, self.join_op, 0, v, m)
3474     def test_split_0elem(self):
3475         rng = np.random.RandomState(seed=utt.fetch_seed())
3476         m = self.shared(rng.rand(4, 6).astype(self.floatX))
3477         o = self.split_op_class(2)(m, 0, [4, 0])
3478         f = function([], o, mode=self.mode)
3479         assert any([isinstance(node.op, self.split_op_class)
3480                     for node in f.maker.fgraph.toposort()])
3481         o1, o2 = f()
3482         assert np.allclose(o1, m.get_value(borrow=True))
3483         assert np.allclose(o2, m.get_value(borrow=True)[4:])
3484     @change_flags(compute_test_value='off')
3485     def test_split_neg(self):
3486         rng = np.random.RandomState(seed=utt.fetch_seed())
3487         m = self.shared(rng.rand(4, 6).astype(self.floatX))
3488         o = self.split_op_class(2)(m, 0, [5, -1])
3489         f = function([], o, mode=self.mode)
3490         assert any([isinstance(node.op, self.split_op_class)
3491                     for node in f.maker.fgraph.toposort()])
3492         self.assertRaises(ValueError, f)
3493 def test_join_inplace():
3494     s = tensor.lscalar()
3495     x = tensor.vector('x')
3496     z = tensor.zeros((s,))
3497     join = Join(view=0)
3498     c = join(0, x, z, z)
3499     f = theano.function([theano.In(x, borrow=True), s], theano.Out(c, borrow=True))
3500     data = np.array([3, 4, 5], dtype=theano.config.floatX)
3501     print(f(data, 0))
3502     if theano.config.mode not in ["DebugMode", "DEBUG_MODE"]:
3503         assert f(data, 0) is data
3504     assert np.allclose(f(data, 0), [3, 4, 5])
3505 def test_join_oneInput():
3506     x_0 <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.fmatrix()
3507     x_1 = theano.tensor.fmatrix()
3508     x_2 = theano.tensor.fvector()
3509     join_0 = theano.tensor.concatenate([x_0], axis=</b></font>1)
3510     join_1 = theano.tensor.concatenate([x_0, x_1, theano.tensor.shape_padright(x_2)],
3511                                        axis=1)
3512     assert join_0 is x_0
3513     assert join_1 is not x_0
3514 class test_comparison(unittest.TestCase):
3515     def setUp(self):
3516         utt.seed_rng()
3517         self.mode = None
3518         self.shared = shared
3519         self.dtypes = ['float64', 'float32', 'complex64', 'complex128']
3520     def inplace_func(self, inputs, outputs, check_isfinite=None):
3521         mode = self.mode
3522         if check_isfinite is False:
3523             if mode is None:
3524                 mode = get_default_mode()
3525             mode.check_isfinite = False
3526         f = inplace_func(inputs, outputs, mode=mode)
3527         return f
3528     def test_gt(self):
3529         for dtype in self.dtypes:
3530             l = np.asarray([0., -1., 1.], dtype=dtype)
3531             r = np.asarray([0., 1., -1.], dtype=dtype)
3532             for x, y, err in [
3533                 (self.shared(l.astype(dtype)),
3534                  self.shared(r.astype(dtype)), False),
3535                 (l, self.shared(r.astype(dtype)), True),
3536                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3537                 (self.shared(l.astype(dtype)), r, False),
3538                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3539             ]:
3540                 try:
3541                     fn = self.inplace_func([], x &gt; y)
3542                     v = fn()
3543                     self.assertTrue(np.all(v == (l &gt; r)), (v, (l &gt; r)))
3544                 except TypeError:
3545                     assert err
3546     def test_lt(self):
3547         for dtype in self.dtypes:
3548             l = np.asarray([0., -1., 1.], dtype=dtype)
3549             r = np.asarray([0., 1., -1.], dtype=dtype)
3550             for x, y, err in [
3551                 (self.shared(l.astype(dtype)), self.shared(r.astype(dtype)), False),
3552                 (l, self.shared(r.astype(dtype)), True),
3553                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3554                 (self.shared(l.astype(dtype)), r, False),
3555                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3556             ]:
3557                 try:
3558                     fn = self.inplace_func([], x &lt; y)
3559                     v = fn()
3560                     self.assertTrue(np.all(v == (l &lt; r)), (v, (l &lt; r)))
3561                 except TypeError:
3562                     assert err
3563     def test_le(self):
3564         for dtype in self.dtypes:
3565             l = np.asarray([0., -1., 1.], dtype=dtype)
3566             r = np.asarray([0., 1., -1.], dtype=dtype)
3567             for x, y, err in [
3568                 (self.shared(l.astype(dtype)),
3569                  self.shared(r.astype(dtype)), False),
3570                 (l, self.shared(r.astype(dtype)), True),
3571                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3572                 (self.shared(l.astype(dtype)), r, False),
3573                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3574             ]:
3575                 try:
3576                     fn = self.inplace_func([], x &lt;= y)
3577                     v = fn()
3578                     self.assertTrue(np.all(v == (l &lt;= r)), (v, (l &lt;= r)))
3579                 except TypeError:
3580                     assert err
3581     def test_ge(self):
3582         for dtype in self.dtypes:
3583             l = np.asarray([0., -1., 1.], dtype=dtype)
3584             r = np.asarray([0., 1., -1.], dtype=dtype)
3585             for x, y, err in [
3586                 (self.shared(l.astype(dtype)),
3587                  self.shared(r.astype(dtype)), False),
3588                 (l, self.shared(r.astype(dtype)), True),
3589                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3590                 (self.shared(l.astype(dtype)), r, False),
3591                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3592             ]:
3593                 try:
3594                     fn = self.inplace_func([], x &gt;= y)
3595                     v = fn()
3596                     self.assertTrue(np.all(v == (l &gt;= r)), (v, (l &gt;= r)))
3597                 except TypeError:
3598                     assert err
3599     def test_eq(self):
3600         for dtype in self.dtypes:
3601             l = np.asarray([0., -1., 1.], dtype=dtype)
3602             r = np.asarray([0., 1., -1.], dtype=dtype)
3603             for x, y, err in [
3604                 (self.shared(l.astype(dtype)),
3605                  self.shared(r.astype(dtype)), False),
3606                 (l, self.shared(r.astype(dtype)), True),
3607                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3608                 (self.shared(l.astype(dtype)), r, False),
3609                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3610             ]:
3611                 try:
3612                     fn = self.inplace_func([], eq(x, y))
3613                     v = fn()
3614                     self.assertTrue(np.all(v == (l == r)), (v, (l == r)))
3615                 except TypeError:
3616                     assert err
3617     def test_neq(self):
3618         for dtype in self.dtypes:
3619             l = np.asarray([0., -1., 1.], dtype=dtype)
3620             r = np.asarray([0., 1., -1.], dtype=dtype)
3621             for x, y, err in [
3622                 (self.shared(l.astype(dtype)),
3623                  self.shared(r.astype(dtype)), False),
3624                 (l, self.shared(r.astype(dtype)), True),
3625                 (tensor.constant(l), self.shared(r.astype(dtype)), False),
3626                 (self.shared(l.astype(dtype)), r, False),
3627                 (self.shared(l.astype(dtype)), tensor.constant(r), False),
3628             ]:
3629                 try:
3630                     fn = self.inplace_func([], neq(x, y))
3631                     v = fn()
3632                     self.assertTrue(np.all(v == (l != r)), (v, (l != r)))
3633                 except TypeError:
3634                     assert err
3635     def test_isclose(self):
3636         for dtype in self.dtypes:
3637             l = np.asarray(
3638                 [0., 1., -1., 0.,
3639                  np.nan, np.inf, -np.inf, np.inf],
3640                 dtype=dtype)
3641             r = np.asarray(
3642                 [0., 1.0001, -1.000000000001, np.nan,
3643                  np.nan, np.inf, np.inf, 0.],
3644                 dtype=dtype)
3645             for x, y, err in [
3646                 (self.shared(l.astype(dtype)),
3647                  self.shared(r.astype(dtype)), False),
3648                 (l, self.shared(r.astype(dtype)), True),
3649                 (constant(l), self.shared(r.astype(dtype)), False),
3650                 (self.shared(l.astype(dtype)), r, False),
3651                 (self.shared(l.astype(dtype)), constant(r), False),
3652             ]:
3653                 try:
3654                     o1 = isclose(x, y, equal_nan=False)
3655                     fn1 = self.inplace_func([], o1, check_isfinite=False)
3656                     o2 = isclose(x, y, equal_nan=True)
3657                     fn2 = self.inplace_func([], o2, check_isfinite=False)
3658                     v1 = fn1()
3659                     v2 = fn2()
3660                     self.assertTrue(
3661                         np.all(
3662                             v1 == np.asarray(
3663                                 [True, False, True, False,
3664                                  False, True, False, False],
3665                                 dtype="bool"
3666                             )
3667                         ),
3668                         np.all(
3669                             v2 == np.asarray(
3670                                 [True, False, True, False,
3671                                  True, True, False, False],
3672                                 dtype="bool"
3673                             )
3674                         )
3675                     )
3676                 except TypeError:
3677                     if not dtype.startswith('complex'):
3678                         raise
3679                         assert err
3680     def test_allclose(self):
3681         for dtype in self.dtypes:
3682             l = np.asarray(
3683                 [0., 1., -1., 0.,
3684                  np.nan, np.inf, -np.inf, np.inf],
3685                 dtype=dtype)
3686             r = np.asarray(
3687                 [0., 1.0001, -1.000000000001, np.nan,
3688                  np.nan, np.inf, np.inf, 0.],
3689                 dtype=dtype)
3690             for x, y, err in [
3691                 (self.shared(l.astype(dtype)),
3692                  self.shared(r.astype(dtype)), False),
3693                 (l, self.shared(r.astype(dtype)), True),
3694                 (constant(l), self.shared(r.astype(dtype)), False),
3695                 (self.shared(l.astype(dtype)), r, False),
3696                 (self.shared(l.astype(dtype)), constant(r), False),
3697             ]:
3698                 try:
3699                     fn = self.inplace_func([], allclose(x, y, equal_nan=False),
3700                                            check_isfinite=False)
3701                     v = fn()
3702                     self.assertTrue(np.all(v == np.allclose(l, r)))
3703                 except TypeError:
3704                     if not dtype.startswith('complex'):
3705                         assert err
3706 class test_bitwise(unittest.TestCase):
3707     dtype = ['int8', 'int16', 'int32', 'int64', ]
3708     def test_or(self):
3709         for dtype in self.dtype:
3710             x, y = vector(dtype=dtype), vector(dtype=dtype)
3711             fn = inplace_func([x, y], x | y)
3712             l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3713             r = theano._asarray([0, 1, 0, 1], dtype=dtype)
3714             v = fn(l, r)
3715             self.assertTrue(np.all(v == (operator.or_(l, r))), (l, r, v))
3716     def test_xor(self):
3717         for dtype in self.dtype:
3718             x, y = vector(dtype=dtype), vector(dtype=dtype)
3719             fn = inplace_func([x, y], x ^ y)
3720             ix = x
3721             ix = inplace.xor_inplace(ix, y)
3722             gn = inplace_func([x, y], ix)
3723             l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3724             r = theano._asarray([0, 1, 0, 1], dtype=dtype)
3725             v = fn(l, r)
3726             self.assertTrue(np.all(v == (operator.xor(l, r))), (l, r, v))
3727             v = gn(l, r)
3728             self.assertTrue(np.all(l == np.asarray([0, 1, 1, 0])), l)
3729     def test_and(self):
3730         for dtype in self.dtype:
3731             x, y = vector(dtype=dtype), vector(dtype=dtype)
3732             fn = inplace_func([x, y], x &amp; y)
3733             l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3734             r = theano._asarray([0, 1, 0, 1], dtype=dtype)
3735             v = fn(l, r)
3736             self.assertTrue(np.all(v == (operator.and_(l, r))), (l, r, v))
3737     def test_inv(self):
3738         for dtype in self.dtype:
3739             x = vector(dtype=dtype)
3740             fn = inplace_func([x], ~x)
3741             for l in [[0, 0, 1, 1], [0, 1, 0, 1],
3742                       [0, 0, 1, 1], [0, 1, 0, 1],
3743                       [-1, 2 ** 16, 2 ** 16 - 1]
3744                       ]:
3745                 l = theano._asarray([0, 0, 1, 1], dtype=dtype)
3746                 v = fn(l)
3747                 self.assertTrue(np.all(v == (~l)), (l, v))
3748     def test_eye(self):
3749         n = iscalar()
3750         m = iscalar()
3751         k = iscalar()
3752         fn = theano.function([m, n, k], eye(m, n, k))
3753         self.assertTrue(np.all(fn(5, 6, 1) == np.eye(5, 6, 1)))
3754 class T_add(unittest.TestCase):
3755     def setUp(self):
3756         utt.seed_rng()
3757     def test_complex_all_ops(self):
3758         for nbits in (64, 128):
3759             a = shared(np.ones(3, dtype='complex%i' % nbits) + 0.5j)
3760             b = shared(np.ones(3, dtype='complex%i' % nbits) + 1.5j)
3761             tests = (("+", lambda x, y: x + y),
3762                      ("-", lambda x, y: x - y),
3763                      ("*", lambda x, y: x * y),
3764                      ("/", lambda x, y: x / y))
3765             for s, fn in tests:
3766                 f = inplace_func([], fn(a, b))
3767                 self.assertTrue(a.type.values_eq_approx(fn(
3768                     a.get_value(), b.get_value()), f()))
3769     def test_grad_scalar_l(self):
3770         utt.verify_grad(add, [np.asarray([3.0]), rand(3)])
3771     def test_grad_scalar_r(self):
3772         utt.verify_grad(add, [rand(3), np.asarray([3.0])])
3773     def test_grad_row(self):
3774         utt.verify_grad(add, [rand(3, 5), rand(1, 5)])
3775     def test_grad_col(self):
3776         utt.verify_grad(add, [rand(3, 5), rand(3, 1)])
3777 class T_ceil(unittest.TestCase):
3778     def test_complex(self):
3779         self.assertRaises(TypeError, tensor.ceil, tensor.zvector())
3780 class T_exp(unittest.TestCase):
3781     def test_grad_0(self):
3782         utt.verify_grad(exp, [
3783             np.asarray([[1.5089518, 1.48439076, -4.7820262],
3784                         [2.04832468, 0.50791564, -1.58892269]])])
3785     def test_grad_1(self):
3786         utt.verify_grad(inplace.exp_inplace, [
3787             np.asarray([[1.5089518, 1.48439076, -4.7820262],
3788                         [2.04832468, 0.50791564, -1.58892269]])])
3789     if theano.config.cycle_detection == 'fast' and theano.config.mode != 'FAST_COMPILE':
3790         test_grad_1 = unittest.expectedFailure(test_grad_1)
3791     def test_int(self):
3792         x = ivector()
3793         f = function([x], exp(x))
3794         exp_3 = f([3])
3795         assert exp_3.dtype == 'float64'
3796     def test_complex(self):
3797         x = zvector()
3798         assert exp(x).dtype == 'complex128'
3799         f = function([x], exp(x))
3800         exp_3 = f([3 + 2j])
3801         assert np.allclose(exp_3, np.exp(3 + 2j))
3802 class T_divimpl(unittest.TestCase):
3803     def test_impls(self):
3804         i = iscalar()
3805         ii = lscalar()
3806         d = dscalar()
3807         f = fscalar()
3808         c = cscalar()
3809         assert np.allclose(function([i, d], i / d)(5, 7.0), (5.0 / 7.0))
3810         assert np.allclose(function([i, d], d / i)(5, 7.0), (7.0 / 5.0))
3811         assert np.allclose(function([i, f], i / f)(5, 11.0), (5.0 / 11.0))
3812         assert np.allclose(function([i, f], f / i)(5, 11.0), (11.0 / 5.0))
3813         assert np.allclose(function([i, ii], i // ii)(5, 3), (5 // 3))
3814         assert np.allclose(function([i, ii], ii // i)(5, 3), (3 // 5))
3815         assert np.allclose(function([i, ii], true_div(i, ii))(5, 3),
3816                            (5. / 3.))
3817         assert np.allclose(function([i, ii], true_div(ii, i))(5, 3),
3818                            (3. / 5.))
3819         assert np.allclose(function([i, c], i / c)(5, np.complex(5, 3)),
3820                            (5. / (5 + 3j)))
3821         assert np.allclose(function([i, c], c / i)(5, np.complex(5, 3)),
3822                            ((5 + 3j) / 5.))
3823 class T_mean(unittest.TestCase):
3824     def test_regression_mean_of_ndarray_failure(self):
3825         try:
3826             tensor.mean(np.zeros(1))
3827         except AttributeError:
3828             self.fail()
3829     def test_mean_f16(self):
3830         x = tensor.vector(dtype='float16')
3831         y = x.mean()
3832         f = theano.function([x], y)
3833         utt.assert_allclose(f(np.ones((100000,), dtype='float16')), 1.0)
3834     def test0(self):
3835         x = tensor.vector()
3836         f = theano.function([x], tensor.mean(x))
3837         data = rand(50)
3838         assert np.allclose(f(data), np.mean(data))
3839     def test_list(self):
3840         ll = [theano.shared(0.), theano.shared(2.)]
3841         tensor.mean(ll).eval() == 1
3842 class test_matinv(unittest.TestCase):
3843     def mat_reciprocal(self, dim):
3844         rng = np.random.RandomState(seed=utt.fetch_seed())
3845         a, b = matrices('ab')
3846         ab = a * b
3847         diff = ab - as_tensor_variable(np.ones((dim, dim),
3848                                                dtype=config.floatX))
3849         ssdiff = sum((diff ** 2.0))
3850         g_b = grad(ssdiff, b)
3851         fn = inplace_func([a, b], [ssdiff, g_b])
3852         x = rng.rand(dim, dim) + 0.1      # Initialized s.t. x is not too tiny
3853         w = rng.rand(dim, dim)
3854         x = np.asarray(x, dtype=config.floatX)
3855         w = np.asarray(w, dtype=config.floatX)
3856         for i in xrange(100):
3857             ssd, gw = fn(x, w)
3858             if i == 0:
3859                 ssd0 = ssd
3860             w -= 0.4 * gw
3861         return ssd0, ssd
3862     def test_reciprocal(self):
3863         ssd0, ssd = self.mat_reciprocal(3)
3864         rng = np.random.RandomState(seed=utt.fetch_seed())
3865         x = rng.rand(3, 3) + 0.1
3866         w = rng.rand(3, 3)
3867         x = np.asarray(x, dtype=config.floatX)
3868         w = np.asarray(w, dtype=config.floatX)
3869         ones = np.ones((3, 3), dtype=config.floatX)
3870         myssd0 = np.sum((x * w - ones) ** 2.0)
3871         for i in xrange(100):
3872             gw = 2 * (x * w - ones) * x  # derivative of dMSE/dw
3873             myssd = np.sum((x * w - ones) ** 2)
3874             w -= 0.4 * gw
3875         self.assertAlmostEqual(ssd0, myssd0)
3876         self.assertAlmostEqual(ssd, myssd)
3877 class t_dot(unittest.TestCase):
3878     def setUp(self):
3879         utt.seed_rng()
3880     def cmp_dot(self, x, y):
3881         def spec(x):
3882             x = np.asarray(x)
3883             return type(x), x.dtype, x.shape
3884 <a name="25"></a>        nz = np.dot(x, y)
3885         tz = eval_outputs([dot(as_tensor_variable(x), as_tensor_variable(y))])
3886         self.assertTrue(tz.dtype == nz.dtype,
3887                         (tz<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.dtype, tz.dtype.num, nz.dtype, nz.dtype.num))
3888         self.assertTrue(tz.shape == nz.shape, (tz.</b></font>shape, nz.shape))
3889         utt.assert_allclose(nz, tz, rtol=1e-4, atol=1e-4)
3890 <a name="19"></a>
3891     def test_Op_dims(self):
3892         _dot = theano<font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.tensor.basic._dot
3893         d0 = scalar()
3894         d1 = vector()
3895         d2 = matrix()
3896         d3 = tensor3()
3897         self.assertRaises(</b></font>TypeError, _dot, d0, d0)
3898         self.assertRaises(TypeError, _dot, d0, d1)
3899         self.assertRaises(TypeError, _dot, d0, d2)
3900         self.assertRaises(TypeError, _dot, d0, d3)
3901         self.assertRaises(TypeError, _dot, d1, d0)
3902 <a name="14"></a>        _dot(d1, d1)
3903         _dot(d1, d2)
3904         self.assertRaises(TypeError, _dot, d1, d3)
3905         self<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertRaises(TypeError, _dot, d2, d0)
3906         _dot(d2, d1)
3907         _dot(d2, d2)
3908         self.assertRaises(TypeError, _dot, d2, d3)
3909         self.assertRaises(TypeError, _dot, d3, d0)
3910         self.assertRaises(TypeError, _dot, d3, d1)
3911         self.assertRaises(TypeError, _dot, d3, d2)
3912         self.assertRaises(</b></font>TypeError, _dot, d3, d3)
3913     def test_dot_0d_0d(self):
3914         self.cmp_dot(rand(), rand())
3915     def test_dot_0d_1d(self):
3916         self.cmp_dot(rand(), rand(5))
3917     def test_dot_0d_2d(self):
3918         self.cmp_dot(rand(), rand(6, 7))
3919     def test_dot_0d_3d(self):
3920         self.cmp_dot(rand(), rand(8, 6, 7))
3921     def test_dot_1d_0d(self):
3922         self.cmp_dot(rand(5), rand())
3923     def test_dot_1d_1d(self):
3924         self.cmp_dot(rand(5), rand(5))
3925     def test_dot_1d0_1d0(self):
3926         self.cmp_dot(rand(0), rand(0))
3927     def test_dot_1d_1d0(self):
3928         self.assertRaises(ValueError, self.cmp_dot, rand(5), rand(0))
3929     def test_dot_1d0_1d(self):
3930         self.assertRaises(ValueError, self.cmp_dot, rand(0), rand(5))
3931     def test_dot_1d_2d(self):
3932         self.cmp_dot(rand(6), rand(6, 7))
3933     def test_dot_1d0_2d(self):
3934         self.cmp_dot(rand(0), rand(0, 7))
3935     def test_dot_1d_2d0(self):
3936         self.cmp_dot(rand(6), rand(6, 0))
3937     def test_dot_1d0_2d0(self):
3938         self.cmp_dot(rand(0), rand(0, 0))
3939     def test_dot_1d_3d(self):
3940         self.cmp_dot(rand(6), rand(8, 6, 7))
3941     def test_dot_2d_0d(self):
3942         self.cmp_dot(rand(5, 6), rand())
3943     def test_dot_2d_1d(self):
3944         self.cmp_dot(rand(5, 6), rand(6))
3945     def test_dot_2d0_1d(self):
3946         self.cmp_dot(rand(0, 6), rand(6))
3947     def test_dot_2d_1d0(self):
3948         self.cmp_dot(rand(5, 0), rand(0))
3949     def test_dot_2d0_1d0(self):
3950         self.cmp_dot(rand(0, 0), rand(0))
3951     def test_dot_2d_2d(self):
3952         self.cmp_dot(rand(5, 6), rand(6, 7))
3953     def test_dot_2d0_2d(self):
3954         self.cmp_dot(rand(0, 6), rand(6, 7))
3955     def test_dot_2d_2d0(self):
3956         self.cmp_dot(rand(5, 6), rand(6, 0))
3957     def test_dot_2d0_2d0(self):
3958         self.cmp_dot(rand(0, 6), rand(6, 0))
3959     def test_dot_2d_0_2d(self):
3960         self.cmp_dot(rand(5, 0), rand(0, 7))
3961     def test_dot_2d0_0_2d0(self):
3962         self.cmp_dot(rand(0, 6), rand(6, 0))
3963     def test_dot_2d_3d(self):
3964         self.cmp_dot(rand(5, 6), rand(8, 6, 7))
3965     def test_dot_3d_0d(self):
3966         self.cmp_dot(rand(4, 5, 6), rand())
3967     def test_dot_3d_1d(self):
3968         self.cmp_dot(rand(4, 5, 6), rand(6))
3969     def test_dot_3d_2d(self):
3970         self.cmp_dot(rand(4, 5, 6), rand(6, 7))
3971     def test_dot_3d_3d(self):
3972         self.cmp_dot(rand(4, 5, 6), rand(8, 6, 7))
3973     def not_aligned(self, x, y):
3974         ctv_backup = config.compute_test_value
3975         config.compute_test_value = 'off'
3976         try:
3977             z = dot(x, y)
3978         finally:
3979             config.compute_test_value = ctv_backup
3980         _logger = logging.getLogger('theano.gof.opt')
3981         oldlevel = _logger.level
3982         _logger.setLevel(logging.CRITICAL)
3983         try:
3984             try:
3985                 eval_outputs([z])
3986                 assert False    # should have raised exception
3987             except ValueError as e:
3988                 e0 = exc_message(e)
3989                 self.assertTrue(
3990                     e0.split()[1:4] == ['are', 'not', 'aligned'] or
3991                     e0.split()[0:2] == ['Shape', 'mismatch:'] or
3992                     (e0.split()[0:4] ==
3993                         ['Incompatible', 'shapes', 'for', 'gemv']) or
3994                     e)
3995         finally:
3996             _logger.setLevel(oldlevel)
3997     def test_align_1_1(self):
3998         self.not_aligned(rand(5), rand(6))
3999     def test_align_1_2(self):
4000         self.not_aligned(rand(5), rand(6, 4))
4001     def test_align_1_3(self):
4002         self.not_aligned(rand(5), rand(6, 4, 7))
4003     def test_align_2_1(self):
4004         self.not_aligned(rand(5, 4), rand(6))
4005     def test_align_2_2(self):
4006         self.not_aligned(rand(5, 4), rand(6, 7))
4007     def test_align_2_3(self):
4008         self.not_aligned(rand(5, 4), rand(6, 7, 8))
4009     def test_align_3_1(self):
4010         self.not_aligned(rand(5, 4, 3), rand(6))
4011     def test_align_3_2(self):
4012         self.not_aligned(rand(5, 4, 3), rand(6, 7))
4013     def test_align_3_3(self):
4014 <a name="0"></a>        self.not_aligned(rand(5, 4, 3), rand(6, 7, 8))
4015     def test_grad(self):
4016         utt<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.verify_grad(dot, [rand(2, 3), rand(3, 2)])
4017         utt.verify_grad(dot, [rand(2), rand(2, 3)])
4018         utt.verify_grad(dot, [rand(3, 2), rand(2)])
4019         utt.verify_grad(dot, [rand(2), rand(2)])
4020         utt.verify_grad(dot, [rand(), rand(2)])
4021 <a name="2"></a>        utt.verify_grad(dot, [rand(), rand(2, 5)])
4022         utt.verify_grad(dot, [rand(2), rand()])
4023         utt.verify_grad(dot, [rand(2, 5), rand()])
4024         utt.</b></font>verify_grad<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(dot, [rand(2, 3, 4), rand(4)])
4025         utt.verify_grad(dot, [rand(3), rand(2, 3, 4)])
4026         utt.verify_grad(dot, [rand(4, 3), rand(2, 3, 4)])
4027         utt.verify_grad(dot, [rand(2, 3, 4), rand(4, 5)])
4028         utt.verify_grad(dot, [rand(2, 3, 4), rand(</b></font>3, 4, 5)])
4029     @attr('slow')
4030     def test_broadcastable_patterns(self):
4031         def val_for(r):
4032             if r.dtype.startswith('complex'):
4033                 if r.ndim == 0:
4034                     return np.asarray(np.complex(1.1, 2.1),
4035                                       dtype=r.dtype)
4036                 if r.ndim == 1:
4037                     if r.dtype == 'complex64':
4038                         return np.complex64([np.complex(1.2, 2.2)])
4039                     elif r.dtype == 'complex128':
4040                         return np.complex128([np.complex(1.2, 2.2)])
4041                 elif r.ndim == 2:
4042                     if r.dtype == 'complex64':
4043                         return np.complex64([[np.complex(1.3, 2.3)]])
4044                     elif r.dtype == 'complex128':
4045                         return np.complex128([[np.complex(1.3, 2.3)]])
4046             if r.ndim == 0:
4047                 return np.asarray(1.1, dtype=r.dtype)
4048             if r.ndim == 1:
4049                 return np.asarray([1.2], dtype=r.dtype)
4050             elif r.ndim == 2:
4051                 return np.asarray([[1.3]], dtype=r.dtype)
4052             raise ValueError()
4053         for dtype0 in ('float32', 'float64', 'complex64'):
4054             for dtype1 in ('float32', 'complex64', 'complex128'):
4055                 for bc0 in ((True,), (False,), (True, True),
4056                             (True, False), (False, True),
4057                             (False, False)):
4058                     x = TensorType(dtype=dtype0, broadcastable=bc0)()
4059                     for bc1 in ((True,), (False,), (True, True),
4060                                 (True, False), (False, True),
4061                                 (False, False)):
4062                         y = TensorType(dtype=dtype1, broadcastable=bc1)()
4063                         z = dot(x, y)
4064                         t = TensorType(dtype=dtype0,
4065                                        broadcastable=z.broadcastable)()
4066                         rval = z * 3 + 2 * t
4067                         f = function([x, y, t], rval)
4068                         xval = val_for(x)
4069                         yval = val_for(y)
4070                         tval = val_for(t)
4071                         f(xval, yval, tval)  # debugmode checks result
4072                         if (dtype0.startswith('float') and
4073                                 dtype1.startswith('float')):
4074                             g = grad(z.sum(), x)
4075                             assert g.broadcastable == x.broadcastable
4076                             g = grad(z.sum(), y)
4077                             assert g.broadcastable == y.broadcastable
4078 class T_tensorfromscalar(unittest.TestCase):
4079     def test0(self):
4080 <a name="17"></a>        s = scal.constant(56)
4081         t = tensor_from_scalar(s)
4082         self.assertTrue(t.owner.op is tensor_from_scalar)
4083         self<font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
4084         self.assertTrue(t.type.ndim == 0, t.type.ndim)
4085         self.</b></font>assertTrue(t.type.dtype == s.type.dtype)
4086         v = eval_outputs([t])
4087         self.assertTrue(v == 56, v)
4088         self.assertTrue(isinstance(v, np.ndarray))
4089         self.assertTrue(v.shape == (), v.shape)
4090     def test1(self):
4091 <a name="23"></a>        s = scal.constant(56)
4092         t = as_tensor_variable(s)
4093         self.assertTrue(t.owner.op is tensor_from_scalar)
4094         self<font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
4095         self.assertTrue(t.type.ndim == 0, t.type.</b></font>ndim)
4096         self.assertTrue(t.type.dtype == s.type.dtype)
4097         v = eval_outputs([t])
4098         self.assertTrue(v == 56, v)
4099         self.assertTrue(isinstance(v, np.ndarray))
4100         self.assertTrue(v.shape == (), v.shape)
4101         g = grad(t, s)
4102         self.assertTrue(eval_outputs([g]) == 0.)
4103     def test2(self):
4104         s = scal.constant(56.)
4105         t = as_tensor_variable(s)
4106         self.assertTrue(t.owner.op is tensor_from_scalar)
4107         self.assertTrue(t.type.broadcastable == (), t.type.broadcastable)
4108         self.assertTrue(t.type.ndim == 0, t.type.ndim)
4109         self.assertTrue(t.type.dtype == s.type.dtype)
4110         v = eval_outputs([t])
4111         self.assertTrue(v == 56., v)
4112         self.assertTrue(isinstance(v, np.ndarray))
4113         self.assertTrue(v.shape == (), v.shape)
4114         g = grad(t, s)
4115         self.assertTrue(eval_outputs([g]) == 1.)
4116 class T_scalarfromtensor(unittest.TestCase):
4117     def test0(self):
4118         tt = constant(56)  # scal.constant(56)
4119         ss = scalar_from_tensor(tt)
4120         self.assertTrue(ss.owner.op is scalar_from_tensor)
4121         self.assertTrue(ss.type.dtype == tt.type.dtype)
4122         v = eval_outputs([ss])
4123         self.assertTrue(v == 56, v)
4124         if config.cast_policy == 'custom':
4125             self.assertTrue(isinstance(v, np.int8))
4126         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4127             self.assertTrue(isinstance(
4128                 v, getattr(np, str(np.asarray(56).dtype))))
4129         else:
4130             raise NotImplementedError(config.cast_policy)
4131         self.assertTrue(v.shape == (), v.shape)
4132         tt = lscalar()
4133         ss = scalar_from_tensor(tt)
4134         ss.owner.op.grad([tt], [ss])
4135         fff = function([tt], ss)
4136         v = fff(np.asarray(5))
4137         self.assertTrue(v == 5, v)
4138         self.assertTrue(isinstance(v, np.int64))
4139         self.assertTrue(v.shape == (), v.shape)
4140 class test_grad(unittest.TestCase):
4141     class Obj1(gof.op.Op):
4142         def __init__(self):
4143             self.gval0 = scalar('e')
4144             self.gval1 = scalar('f')
4145         def make_node(self):
4146             inputs = [scalar('a'), scalar('c')]
4147             outputs = [scalar('b'), scalar('d')]
4148             return gof.Apply(self, inputs, outputs)
4149         def grad(self, inp, grads):
4150             x0, x1 = inp
4151             gz0, gz1 = grads
4152             return self.gval0, self.gval1
4153     def test_1param(self):
4154         o = test_grad.Obj1()
4155         a1 = o.make_node()
4156         self.assertTrue(o.gval0 is tensor.grad(a1.outputs[0], a1.inputs[0]))
4157     def test_Nparam(self):
4158         o = test_grad.Obj1()
4159         a1 = o.make_node()
4160         g0, g1 = grad(a1.outputs[0], a1.inputs)
4161         g0.name = None
4162         self.assertTrue(o.gval0 is g0)
4163         self.assertTrue(o.gval1 is g1)
4164     def test_grad_keep_type(self):
4165         X = tensor.matrix()
4166         y = X.sum()
4167         G = tensor.grad(y, [X])
4168         assert isinstance(G, list)
4169         G = tensor.grad(y, X)
4170         assert not isinstance(G, list)
4171     def test_1None_rval(self):
4172         o = test_grad.Obj1()
4173         a1 = o.make_node()
4174         g = grad(a1.outputs[0], a1.outputs[1],
4175                  disconnected_inputs='ignore')
4176         self.assertTrue(g.owner.op == fill)
4177         self.assertTrue(g.owner.inputs[1].data == 0)
4178         self.assertRaises(TypeError, grad, a1.outputs[0], 'wtf')
4179     def test_NNone_rval(self):
4180         o = test_grad.Obj1()
4181         a1 = o.make_node()
4182         g0, g1, g2 = grad(a1.outputs[0], a1.inputs + [scalar('z')],
4183                           disconnected_inputs='ignore')
4184         self.assertTrue(o.gval0 is g0)
4185         self.assertTrue(o.gval1 is g1)
4186         self.assertTrue(g2.owner.op == fill)
4187         self.assertTrue(g2.owner.inputs[1].data == 0)
4188     def test_zero_gradient_shape(self):
4189         x = dmatrix()
4190         f = theano.function([x], grad(dscalar(), x,
4191                                       disconnected_inputs='ignore'))
4192         a = np.ones((3, 7))
4193         self.assertTrue((f(a) == 0).all())  # Zero gradient.
4194         self.assertTrue(a.shape == f(a).shape)  # With proper shape.
4195     def test_cost_is_scalar(self):
4196         v = vector()
4197         m = matrix()
4198         self.assertRaises(TypeError, grad, v, v)
4199         self.assertRaises(TypeError, grad, m, m)
4200 class T_op_cache(unittest.TestCase):
4201     def setUp(self):
4202         utt.seed_rng()
4203     def test0(self):
4204         v = matrix()
4205         v.name = 'v'
4206         gv = fill(v / v, 1.0) / v - (fill(v / v, 1.0) * v) / (v * v)
4207         fn_py = inplace_func([v], gv)
4208         fn_c_or_py = inplace_func([v], gv)
4209         a = rand(5, 2).astype(config.floatX)
4210         self.assertTrue(np.all(fn_py(a) == fn_c_or_py(a)))
4211 class T_reshape(utt.InferShapeTester, utt.TestOptimizationMixin):
4212     def __init__(self, name, shared=tensor._shared, op=Reshape, mode=None,
4213                  ignore_topo=(DeepCopyOp, opt.MakeVector,
4214                               opt.Shape_i, DimShuffle, theano.tensor.Elemwise)):
4215         self.shared = shared
4216         self.op = op
4217         self.mode = mode
4218         self.ignore_topo = ignore_topo
4219         super(T_reshape, self).__init__(name)
4220     def function(self, inputs, outputs, ignore_empty=False):
4221         f = function(inputs, outputs, mode=self.mode)
4222         if self.mode is not None or theano.config.mode != "FAST_COMPILE":
4223             topo = f.maker.fgraph.toposort()
4224             topo_ = [node for node in topo if not isinstance(node.op,
4225                                                              self.ignore_topo)]
4226             if ignore_empty:
4227                 assert len(topo_) &lt;= 1, topo_
4228             else:
4229                 assert len(topo_) == 1, topo_
4230             if len(topo_) &gt; 0:
4231                 assert type(topo_[0].op) is self.op
4232         return f
4233     def test_reshape(self):
4234         a = dvector()
4235         b = dmatrix()
4236         d = dmatrix()
4237         c = reshape(b, as_tensor_variable(6), ndim=1)
4238         f = self.function([b], c)
4239         b_val1 = np.asarray([[0, 1, 2], [3, 4, 5]])
4240         c_val1 = np.asarray([0, 1, 2, 3, 4, 5])
4241         b_val2 = b_val1.T
4242         c_val2 = np.asarray([0, 3, 1, 4, 2, 5])
4243         f_out1 = f(b_val1)
4244         f_out2 = f(b_val2)
4245         assert np.all(f_out1 == c_val1), (f_out1, c_val1)
4246         assert np.all(f_out2 == c_val2), (f_out2, c_val2)
4247         c = reshape(b, (as_tensor_variable(6),), ndim=1)
4248         f = self.function([b], c)
4249         assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
4250                       np.asarray([0, 1, 2, 3, 4, 5]))
4251         c = reshape(b, d.shape)
4252         f = self.function([b, d], c)
4253         assert np.all(f(np.asarray<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>([[0, 1, 2], [3, 4, 5]]),
4254                         [[0, 1], [2, 3], [4, 5]]) ==
4255                       np.asarray([[0</b></font>, 1], [2, 3], [4, 5]]))
4256         c = reshape(a, [2, 3])
4257         f = self.function([a], c)
4258         assert np.all(f(np.asarray([0, 1, 2, 3, 4, 5])) ==
4259                       np.asarray([[0, 1, 2], [3, 4, 5]]))
4260         a_val = np.asarray([0, 1, 2, 3, 4, 5])
4261         a_val_copy = np.asarray([0, 1, 2, 3, 4, 5])
4262         b_val = np.asarray([[0, 1, 2], [3, 4, 5]])
4263         f_sub = self.function([a, b], c - b)
4264         assert np.all(f_sub(a_val, b_val) == 0.0)
4265         assert np.all(a_val == a_val_copy)
4266         a_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4267         a_val_copy = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4268         b_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4269         f_sub = self.function([a, b], c - b)
4270         assert np.all(f_sub(a_val, b_val) == 0.0)
4271         assert np.all(a_val == a_val_copy)
4272         def just_vals(v):
4273             return Reshape(2)(v, theano._asarray([2, 3], dtype='int32'))
4274         utt.verify_grad(just_vals, [a_val], mode=self.mode)
4275         self._compile_and_check([a], [c], (a_val,), self.op)
4276         c = reshape(b, (b.shape[0], b.shape[1], 1))
4277         f = self.function([b], c, ignore_empty=True)
4278         assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
4279                       np.asarray([[[0], [1], [2]], [[3], [4], [5]]]))
4280         assert (f.maker.fgraph.toposort()[-1].outputs[0].type.broadcastable ==
4281                 (False, False, True))
4282         c = reshape(b, (b.shape[1], b.shape[0], 1))
4283         f = self.function([b], c, ignore_empty=True)
4284         assert np.all(f(np.asarray([[0, 1, 2], [3, 4, 5]])) ==
4285                       np.asarray([[[0], [1]], [[2], [3]], [[4], [5]]]))
4286         assert (f.maker.fgraph.toposort()[-1].outputs[0].type.broadcastable ==
4287                 (False, False, True))
4288     def test_m1(self):
4289         t = tensor3()
4290         rng = np.random.RandomState(seed=utt.fetch_seed())
4291         val = rng.uniform(size=(3, 4, 5)).astype(config.floatX)
4292         for out in [t.reshape([-1]), t.reshape([-1, 5]),
4293                     t.reshape([5, -1]), t.reshape([5, -1, 3])]:
4294             self._compile_and_check([t], [out], [val], self.op)
4295     def test_reshape_long_in_shape(self):
4296         v = dvector('v')
4297         r = v.reshape((v.shape[0], L(1)))
4298         print(r.eval({v: np.arange(5.)}))
4299         assert np.allclose(r.eval({v: np.arange(5.)}).T,
4300                            np.arange(5.))
4301     def test_bad_shape(self):
4302         a = matrix('a')
4303         shapes = ivector('shapes')
4304         rng = np.random.RandomState(seed=utt.fetch_seed())
4305         a_val = rng.uniform(size=(3, 4)).astype(config.floatX)
4306         r = a.reshape(shapes, ndim=1)
4307         f = self.function([a, shapes], r)
4308         self.assertRaises(ValueError, f, a_val, [13])
4309         r = a.reshape(shapes, ndim=2)
4310         f = self.function([a, shapes], r)
4311         self.assertRaises(ValueError, f, a_val, [-1, 5])
4312         self.assertRaises(ValueError, f, a_val, [7, -1])
4313         self.assertRaises(ValueError, f, a_val, [7, 5])
4314         self.assertRaises(ValueError, f, a_val, [-1, -1])
4315     def test_0(self):
4316         x = fvector('x')
4317         f = self.function([x], x.reshape((0, 100)))
4318         assert f(np.ndarray((0,), dtype='float32')).shape == (0, 100)
4319     def test_empty_shp(self):
4320         const = theano.tensor.constant([1]).reshape(())
4321         f = function([], const)
4322         assert f().shape == ()
4323 def test_make_column_matrix_broadcastable():
4324     a = tensor.dmatrix()
4325     b = a.reshape((a.shape[0], )).dimshuffle(0, 'x')
4326     f = function([a], b)
4327     assert (f(np.zeros((3, 1))) + np.ones(2) == np.ones((3, 2))).all()
4328 def test_flatten_outdimNone():
4329     a = dmatrix()
4330     c = flatten(a)
4331     f = inplace_func([a], c)
4332     a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4333     c_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4334     assert np.all(f(a_val) == c_val)
4335     f = inplace_func([a], c)
4336     assert np.all(f(a_val) == c_val)
4337     utt.verify_grad(flatten, [a_val])
4338 def test_flatten_scalar():
4339     a = dscalar()
4340     c = flatten(a)
4341     f = inplace_func([a], c)
4342     a_val = theano._asarray(3.0, dtype='float64')
4343     c_val = theano._asarray([3.0], dtype='float64')
4344     assert np.all(f(a_val) == c_val)
4345     f = inplace_func([a], c)
4346     assert np.all(f(a_val) == c_val)
4347 def test_flatten_ndim1():
4348     a = dmatrix()
4349     c = flatten(a, 1)
4350     f = inplace_func([a], c)
4351     a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4352     c_val = theano._asarray([0, 1, 2, 3, 4, 5], dtype='float64')
4353     assert np.all(f(a_val) == c_val)
4354     f = inplace_func([a], c)
4355     assert np.all(f(a_val) == c_val)
4356     utt.verify_grad(flatten, [a_val])
4357 def test_flatten_ndim2():
4358     a = dmatrix()
4359     c = flatten(a, 2)
4360     f = inplace_func([a], c)
4361     a_val = theano._asarray([[0, 1, 2], [3, 4, 5]], dtype='float64')
4362     assert np.all(f(a_val) == a_val)
4363     f = inplace_func([a], c)
4364     assert np.all(f(a_val) == a_val)
4365     flatten_2 = partial(flatten, ndim=2)
4366     utt.verify_grad(flatten_2, [a_val])
4367 def test_flatten_ndim2_of_3():
4368     a = TensorType('float64', (False, False, False))()
4369     c = flatten(a, 2)
4370     f = inplace_func([a], c)
4371     a_val = theano._asarray([[[0, 1], [2, 3]], [[4, 5], [6, 7]]],
4372                             dtype='float64')
4373     c_val = theano._asarray([[0, 1, 2, 3], [4, 5, 6, 7]], dtype='float64')
4374     assert np.all(f(a_val) == c_val)
4375     f = inplace_func([a], c)
4376     assert np.all(f(a_val) == c_val)
4377     flatten_2 = partial(flatten, ndim=2)
4378     utt.verify_grad(flatten_2, [a_val])
4379     flatten_2 = partial(flatten, outdim=2)
4380     utt.verify_grad(flatten_2, [a_val])
4381 def test_flatten_broadcastable():
4382     inp = TensorType('float64', (False, False, False, False))()
4383     out = flatten(inp, ndim=2)
4384     assert out.broadcastable == (False, False)
4385     inp = TensorType('float64', (False, False, False, True))()
4386     out = flatten(inp, ndim=2)
4387     assert out.broadcastable == (False, False)
4388     inp = TensorType('float64', (False, True, False, True))()
4389     out = flatten(inp, ndim=2)
4390     assert out.broadcastable == (False, False)
4391     inp = TensorType('float64', (False, True, True, True))()
4392     out = flatten(inp, ndim=2)
4393     assert out.broadcastable == (False, True)
4394     inp = TensorType('float64', (True, False, True, True))()
4395     out = flatten(inp, ndim=3)
4396     assert out.broadcastable == (True, False, True)
4397 def test_flatten_ndim_invalid():
4398     a = dmatrix()
4399     assert_raises(ValueError, flatten, a, 3)
4400     assert_raises(ValueError, flatten, a, 0)
4401 def test_is_flat():
4402     assert tensor.is_flat(tensor.as_tensor_variable(np.zeros((10))))
4403     assert tensor.is_flat(tensor.as_tensor_variable(np.zeros((10, 10, 10))),
4404                           ndim=3)
4405     assert not tensor.is_flat(
4406         tensor.as_tensor_variable(np.zeros((10, 10, 10))))
4407     assert tensor.is_flat(tensor.vector())
4408     assert tensor.is_flat(tensor.tensor3(), ndim=3)
4409     assert not tensor.is_flat(tensor.tensor3())
4410     X = tensor.tensor4()
4411     assert tensor.is_flat(X.reshape((-1, )))
4412     assert tensor.is_flat(X.reshape((10, 10, -1)), ndim=3)
4413     assert not tensor.is_flat(X.reshape((10, 10, -1)))
4414     X = tensor.tensor4()
4415     assert tensor.is_flat(X.reshape((tensor.iscalar(), )))
4416     assert tensor.is_flat(X.reshape((tensor.iscalar(), ) * 3), ndim=3)
4417     assert not tensor.is_flat(X.reshape((tensor.iscalar(), ) * 3))
4418 def test_tile():
4419     def run_tile(x, x_, reps, use_symbolic_reps):
4420         if use_symbolic_reps:
4421             rep_symbols = [iscalar() for _ in range(len(reps))]
4422             f = function([x] + rep_symbols, tile(x, rep_symbols))
4423             return f(*([x_] + list(reps)))
4424         else:
4425             f = function([x], tile(x, reps))
4426             return f(x_)
4427     rng = np.random.RandomState(utt.fetch_seed())
4428     for use_symbolic_reps in [False, True]:
4429         x = vector()
4430         x_ = rng.randn(5).astype(config.floatX)
4431         assert np.all(run_tile(x, x_, (2,), use_symbolic_reps) ==
4432                       np.tile(x_, (2,)))
4433         x = matrix()
4434         x_ = rng.randn(2, 4).astype(config.floatX)
4435         assert np.all(run_tile(x, x_, (2, 3), use_symbolic_reps) ==
4436                       np.tile(x_, (2, 3)))
4437         x = tensor3()
4438         x_ = rng.randn(2, 4, 3).astype(config.floatX)
4439         assert np.all(run_tile(x, x_, (2, 3, 4), use_symbolic_reps) ==
4440                       np.tile(x_, (2, 3, 4)))
4441         x = tensor4()
4442         x_ = rng.randn(2, 4, 3, 5).astype(config.floatX)
4443         assert np.all(run_tile(x, x_, (2, 3, 4, 6), use_symbolic_reps) ==
4444                       np.tile(x_, (2, 3, 4, 6)))
4445     test_shape = [2, 4, 3, 5]
4446     k = 0
4447     for xtype in [vector(), matrix(), tensor3(), tensor4()]:
4448         x = xtype
4449         k = k + 1
4450         x_ = rng.randn(*test_shape[0:k]).astype(config.floatX)
4451         reps_ = 2
4452         f = function([x], tile(x, reps_))
4453         assert np.all(f(x_) == np.tile(x_, reps_))
4454         reps = iscalar()
4455         reps_ = 2
4456         f = function([x, reps], tile(x, reps))
4457         assert np.all(f(x_, reps_) == np.tile(x_, reps_))
4458         reps = ivector()
4459         reps_ = [2] if k == 1 or k == 2 else [2, 3]
4460         ndim_ = k
4461         f = function([x, reps], tile(x, reps, ndim_))
4462         assert np.all(f(x_, reps_) == np.tile(x_, reps_))
4463         reps_ = [2, 3, 4]
4464         f = function([x], tile(x, reps_))
4465         assert np.all(f(x_) == np.tile(x_, reps_))
4466         d = iscalar()
4467         reps = [2, d, 4]
4468         f = function([x, d], tile(x, reps))
4469         reps_ = [2, 3, 4]
4470         assert np.all(f(x_, 3) == np.tile(x_, reps_))
4471         r = [2, 3, 4, 5, 6]
4472         reps_ = r[:k + 1]  # len(reps_) = x.ndim+1
4473         f = function([x], tile(x, reps_))
4474         assert np.all(f(x_) == np.tile(x_, reps_))
4475         ndim_ = len(reps_)
4476         f = function([x], tile(x, reps_, ndim_))
4477         assert np.all(f(x_) == np.tile(x_, reps_))
4478         ndim_ = len(reps_) + 1
4479         f = function([x], tile(x, reps_, ndim_))
4480         assert np.all(f(x_) == np.tile(x_, [1] + reps_))
4481         r = [2, 3, 4, 5]
4482         if k &gt; 1:
4483             ndim_ = k + 1
4484             reps_ = r[:k - 1]
4485             f = function([x], tile(x, reps_, ndim_))
4486             assert np.all(f(x_) == np.tile(x_, [1, 1] + reps_))
4487         reps = ivector()
4488         np.testing.assert_raises(ValueError, tile, x, reps)
4489         for reps in [2.5, fscalar(), fvector()]:
4490             np.testing.assert_raises(ValueError, tile, x, reps)
4491         reps = imatrix()
4492         np.testing.assert_raises(ValueError, tile, x, reps)
4493         for reps in [[2, 3, 4], iscalar(), ivector()]:
4494             if k &gt; 1:
4495                 ndim = k - 1
4496                 np.testing.assert_raises(ValueError, tile, x, reps, ndim)
4497         r = [2, 3, 4, 5, 6]
4498         reps = r[:k + 1]
4499         ndim = k
4500         np.testing.assert_raises(ValueError, tile, x, reps, ndim)
4501         reps = ivector()
4502         r = [2, 3, 4, 5, 6, 7]
4503         reps_ = r[:k + 2]
4504         ndim_ = k + 1
4505         f = function([x, reps], tile(x, reps, ndim_))
4506         np.testing.assert_raises(AssertionError, f, x_, reps_)
4507 def test_tile_grad():
4508     def grad_tile(x, reps, np_x):
4509         y = tile(x, reps)
4510         z = y.sum()
4511         g = theano.function([x], grad(z, x))
4512         grad_res = g(np_x)
4513         assert np.all(grad_res == np.prod(reps))
4514     rng = np.random.RandomState(utt.fetch_seed())
4515     grad_tile(vector('x'), [3], rng.randn(5).astype(config.floatX))
4516     grad_tile(matrix('x'), [3, 4], rng.randn(2, 3).astype(config.floatX))
4517     grad_tile(tensor3('x'), [3, 4, 5],
4518               rng.randn(2, 4, 3).astype(config.floatX))
4519     grad_tile(tensor4('x'), [3, 4, 5, 6],
4520               rng.randn(2, 4, 3, 5).astype(config.floatX))
4521 class TestARange(unittest.TestCase):
4522     def setUp(self):
4523         utt.seed_rng()
4524     def test_Op_integers(self):
4525         start, stop, step = iscalars('start', 'stop', 'step')
4526         out = ARange(start.type.dtype)(start, stop, step)
4527         f = function([start, stop, step], out)
4528         assert np.all(f(0, 5, 1) == np.arange(0, 5, 1))
4529         assert np.all(f(2, 11, 4) == np.arange(2, 11, 4))
4530         assert np.all(f(-5, 1, 1) == np.arange(-5, 1, 1))
4531         assert np.all(f(10, 2, -2) == np.arange(10, 2, -2))
4532         assert np.all(f(10, 2, 2) == np.arange(10, 2, 2))
4533         assert np.all(f(0, 0, 1) == np.arange(0, 0, 1))
4534     def test_grads(self):
4535         def f(start, stop, step):
4536             return ARange(start.type.dtype)(start, stop, step)
4537         rng = np.random.RandomState(utt.fetch_seed())
4538         for start, stop, step in [(0, 4.9, 1),
4539                                   (5.1, 0, -0.5),
4540                                   (1, 5.1, 0.5)]:
4541             utt.verify_grad(f, [np.asarray(start).astype(config.floatX),
4542                                 np.asarray(stop).astype(config.floatX),
4543                                 np.asarray(step).astype(config.floatX)],
4544                             rng=rng)
4545     def test_integers(self):
4546         start, stop, step = iscalars('start', 'stop', 'step')
4547         out = arange(start, stop, step)
4548         f = function([start, stop, step], out)
4549         if config.cast_policy == 'custom':
4550             assert out.dtype == 'int64'
4551         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4552             numpy_dtype = np.arange(np.array(1, dtype='int32')).dtype
4553             assert out.dtype == numpy_dtype
4554         else:
4555             raise NotImplementedError(config.cast_policy)
4556         assert np.all(f(0, 5, 1) == np.arange(0, 5, 1))
4557         assert np.all(f(2, 11, 4) == np.arange(2, 11, 4))
4558         assert np.all(f(-5, 1, 1) == np.arange(-5, 1, 1))
4559         assert np.all(f(10, 2, -2) == np.arange(10, 2, -2))
4560         assert np.all(f(10, 2, 2) == np.arange(10, 2, 2))
4561         assert np.all(f(0, 0, 1) == np.arange(0, 0, 1))
4562     def test_float32(self):
4563         start, stop, step = fscalars('start', 'stop', 'step')
4564         out = arange(start, stop, step)
4565         f = function([start, stop, step], out)
4566 <a name="15"></a>        if config.cast_policy == 'custom':
4567             assert out.dtype == start.type.dtype
4568         elif config.cast_policy == 'numpy':
4569             numpy_dtype = np.arange(np<font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.array(0, dtype=start.dtype),
4570                                     np.array(1, dtype=stop.dtype),
4571                                     np.array(1, dtype=step.dtype)).</b></font>dtype
4572             assert out.dtype == numpy_dtype
4573         elif config.cast_policy == 'numpy+floatX':
4574             assert out.dtype == config.floatX
4575         else:
4576             raise NotImplementedError(config.cast_policy)
4577         arg_vals = [(0, 5, 1), (2, 11, 4), (-5, 1.1, 1.2), (1.3, 2, -2.1),
4578                     (10, 2, 2)]
4579         for arg_v in arg_vals:
4580             start_v, stop_v, step_v = arg_v
4581             start_v_, stop_v_, step_v_ = np.asarray(arg_v,
4582                                                     dtype=start.type.dtype)
4583             f_val = f(start_v_, stop_v_, step_v_)
4584             if config.cast_policy == 'custom':
4585                 expected_val = np.arange(start_v, stop_v, step_v,
4586                                          dtype=start.type.dtype)
4587             elif config.cast_policy in ('numpy', 'numpy+floatX'):
4588                 expected_val = np.arange(start_v_, stop_v_, step_v_,
4589                                          dtype=out.dtype)
4590             else:
4591                 raise NotImplementedError(config.cast_policy)
4592             assert np.all(f_val == expected_val)
4593     def test_float64(self):
4594         start, stop, step = dscalars('start', 'stop', 'step')
4595         out = arange(start, stop, step)
4596         f = function([start, stop, step], out)
4597         assert out.dtype == start.type.dtype
4598         arg_vals = [(0, 5, 1), (2, 11, 4), (-5, 1.1, 1.2),
4599                     (1.3, 2, -2.1), (10, 2, 2)]
4600         for arg_v in arg_vals:
4601             start_v, stop_v, step_v = arg_v
4602             start_v_, stop_v_, step_v_ = np.asarray(arg_v,
4603                                                     dtype=start.type.dtype)
4604             f_val = f(start_v_, stop_v_, step_v_)
4605             if config.cast_policy == 'custom':
4606                 expected_val = np.arange(start_v, stop_v, step_v,
4607                                          dtype=start.type.dtype)
4608             elif config.cast_policy in ('numpy', 'numpy+floatX'):
4609                 expected_val = np.arange(start_v_, stop_v_, step_v_)
4610             else:
4611                 raise NotImplementedError(config.cast_policy)
4612             assert np.all(f_val == expected_val)
4613     def test_default_step(self):
4614         start, stop = iscalars('start', 'stop')
4615         out = arange(start, stop)
4616         f = function([start, stop], out)
4617         if config.cast_policy == 'custom':
4618             assert out.dtype == 'int64'
4619         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4620             assert out.dtype == np.arange(np.int32(0),
4621                                           np.int32(1)).dtype
4622         else:
4623             raise NotImplementedError(config.cast_policy)
4624         assert np.all(f(0, 5) == np.arange(0, 5))
4625         assert np.all(f(-5, 1) == np.arange(-5, 1))
4626         assert np.all(f(0, 0) == np.arange(0, 0))
4627         dstart, dstop = dscalars('start', 'stop')
4628         dout = arange(dstart, dstop)
4629         df = function([dstart, dstop], dout)
4630         assert dout.dtype == dstart.type.dtype
4631         assert np.all(df(0.2, 5.3) == np.arange(0.2, 5.3))
4632         assert np.all(df(0.8, 5.3) == np.arange(0.8, 5.3))
4633         assert np.all(df(-0.7, 5.3) == np.arange(-0.7, 5.3))
4634     def test_default_start(self):
4635         stop = iscalar('stop')
4636         out = arange(stop)
4637         f = function([stop], out)
4638         if config.cast_policy == 'custom':
4639             assert out.dtype == 'int64'
4640         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4641             assert out.dtype == np.arange(np.int32(1)).dtype
4642         else:
4643             raise NotImplementedError(config.cast_policy)
4644         assert np.all(f(8) == np.arange(8))
4645         assert np.all(f(-2) == np.arange(-2))
4646         fstop = fscalar('stop')
4647         fout = arange(fstop)
4648         ff = function([fstop], fout)
4649         if config.cast_policy == 'custom':
4650             assert fout.dtype == fstop.type.dtype
4651         elif config.cast_policy == 'numpy':
4652             assert fout.dtype == np.arange(np.float32(1)).dtype
4653         elif config.cast_policy == 'numpy+floatX':
4654             if config.floatX == 'float32':
4655                 assert fout.dtype == 'float32'
4656             else:
4657                 assert fout.dtype == np.arange(np.float32(1)).dtype
4658         else:
4659             raise NotImplementedError(config.cast_policy)
4660         fstop_values = [0.2, -0.7, 8.5]
4661         for fstop_v in fstop_values:
4662             fstop_v32 = np.float32(fstop_v)
4663             assert np.all(ff(fstop_v32) == np.arange(fstop_v))
4664     def test_upcast(self):
4665         if config.cast_policy == 'custom':
4666             assert arange(iscalar()).dtype == 'int64'
4667             assert arange(fscalar()).dtype == fscalar().dtype
4668             assert arange(dscalar()).dtype == dscalar().dtype
4669             assert arange(iscalar(), fscalar()).dtype == dscalar().dtype
4670             assert arange(iscalar(), dscalar()).dtype == dscalar().dtype
4671             assert arange(fscalar(), dscalar()).dtype == dscalar().dtype
4672             assert arange(iscalar(), fscalar(), dscalar()).dtype == \
4673                 dscalar().dtype
4674         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4675             for dtype in get_numeric_types():
4676                 arange_dtype = arange(scalar(dtype=str(dtype))).dtype
4677                 numpy_dtype = np.arange(np.array(1, dtype=dtype)).dtype
4678                 if (dtype != 'float64' and
4679                         numpy_dtype == 'float64' and
4680                         config.cast_policy == 'numpy+floatX' and
4681                         config.floatX == 'float32'):
4682                     assert arange_dtype == 'float32'
4683                 else:
4684                     assert arange_dtype == numpy_dtype
4685                 for stop_dtype in get_numeric_types():
4686                     arange_dtype = arange(
4687                         start=scalar(dtype=str(dtype)),
4688                         stop=scalar(dtype=str(stop_dtype))).dtype
4689                     numpy_dtype = np.arange(
4690                         start=np.array(0, dtype=dtype),
4691                         stop=np.array(1, dtype=stop_dtype)).dtype
4692                     if (dtype != 'float64' and
4693                             stop_dtype != 'float64' and
4694                             numpy_dtype == 'float64' and
4695                             config.cast_policy == 'numpy+floatX' and
4696                             config.floatX == 'float32'):
4697                         assert arange_dtype == 'float32'
4698                     else:
4699                         assert arange_dtype == numpy_dtype
4700 <a name="11"></a>
4701                     for step_dtype in get_numeric_types():
4702                         arange_dtype <font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= arange(
4703                             start=scalar(dtype=str(dtype)),
4704                             stop=scalar(dtype=str(stop_dtype)),
4705                             step=scalar(dtype=str(step_dtype))).</b></font>dtype
4706                         numpy_dtype = np.arange(
4707                             start=np.array(0, dtype=dtype),
4708                             stop=np.array(1, dtype=stop_dtype),
4709                             step=np.array(1, dtype=step_dtype)).dtype
4710                         if (dtype != 'float64' and
4711                                 stop_dtype != 'float64' and
4712                                 step_dtype != 'float64' and
4713                                 numpy_dtype == 'float64' and
4714                                 config.cast_policy == 'numpy+floatX' and
4715                                 config.floatX == 'float32'):
4716                             assert arange_dtype == 'float32'
4717                         else:
4718                             assert arange_dtype == numpy_dtype
4719         else:
4720             raise NotImplementedError(config.cast_policy)
4721     def test_dtype_cache(self):
4722         start, stop, step = iscalars('start', 'stop', 'step')
4723         out1 = arange(start, stop, step)
4724         out2 = arange(start, stop, step, dtype=out1.dtype)
4725         out3 = arange(start, stop, 2., dtype=out1.dtype)
4726         out4 = arange(start, stop, 2.)
4727         assert out1.owner.op is out2.owner.op
4728         assert out2.owner.op is out3.owner.op
4729         assert out3.owner.op is not out4.owner.op
4730     def test_infer_shape(self):
4731         start, stop, step = iscalars('start', 'stop', 'step')
4732         out = arange(start, stop, step)
4733         mode = theano.config.mode
4734         if mode == 'FAST_COMPILE':
4735             mode = 'FAST_RUN'
4736         mode = compile.mode.get_mode(mode).excluding('fusion')
4737         f = function([start, stop, step], out.shape, mode=mode)
4738         assert len(f.maker.fgraph.toposort()) == 9
4739         if config.cast_policy == 'custom':
4740             assert out.dtype == 'int64'
4741         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4742             numpy_dtype = np.arange(np.array(0, dtype=start.dtype),
4743                                     np.array(1, dtype=stop.dtype),
4744                                     np.array(1, dtype=step.dtype)).dtype
4745             assert out.dtype == numpy_dtype
4746         else:
4747             raise NotImplementedError(config.cast_policy)
4748         assert np.all(f(0, 5, 1) == len(np.arange(0, 5, 1)))
4749         assert np.all(f(2, 11, 4) == len(np.arange(2, 11, 4)))
4750         assert np.all(f(-5, 1, 1) == len(np.arange(-5, 1, 1)))
4751         assert np.all(f(10, 2, -2) == len(np.arange(10, 2, -2)))
4752         assert np.all(f(10, 2, 2) == len(np.arange(10, 2, 2)))
4753         assert np.all(f(0, 0, 1) == len(np.arange(0, 0, 1)))
4754         out = arange(start, stop, 1)
4755         f = function([start, stop], out.shape, mode=mode)
4756         assert len(f.maker.fgraph.toposort()) == 5
4757         if config.cast_policy == 'custom':
4758             assert out.dtype == 'int64'
4759         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4760             assert out.dtype == np.arange(
4761                 np.int32(0), np.int32(1), np.int32(1)).dtype
4762         else:
4763             raise NotImplementedError(config.cast_policy)
4764         assert np.all(f(0, 5) == len(np.arange(0, 5)))
4765         assert np.all(f(2, 11) == len(np.arange(2, 11)))
4766         assert np.all(f(-5, 1) == len(np.arange(-5, 1)))
4767         assert np.all(f(10, 2) == len(np.arange(10, 2)))
4768         assert np.all(f(10, 2) == len(np.arange(10, 2)))
4769         assert np.all(f(0, 0) == len(np.arange(0, 0)))
4770         assert np.all(f(-64, 64) == len(np.arange(-64, 64)))
4771         assert arange(-64, 64).shape.eval() == [128]
4772         assert arange(-64, 64, 2).shape.eval() == [64]
4773         out = arange(0, stop, 1)
4774         f = function([stop], out.shape, mode=mode)
4775         assert len(f.maker.fgraph.toposort()) == 2
4776         if config.cast_policy == 'custom':
4777             assert out.dtype == 'int64'
4778         elif config.cast_policy in ('numpy', 'numpy+floatX'):
4779             numpy_dtype = np.arange(0,
4780                                     np.array(1, dtype=stop.dtype),
4781                                     1).dtype
4782             assert out.dtype == numpy_dtype
4783         else:
4784             raise NotImplementedError(config.cast_policy)
4785         assert np.all(f(5) == len(np.arange(0, 5)))
4786         assert np.all(f(11) == len(np.arange(0, 11)))
4787         assert np.all(f(1) == len(np.arange(0, 1)))
4788         assert np.all(f(2) == len(np.arange(0, 2)))
4789         assert np.all(f(2) == len(np.arange(0, 2)))
4790         assert np.all(f(0) == len(np.arange(0, 0)))
4791 class TestNdGrid(unittest.TestCase):
4792     def setUp(self):
4793         pass
4794     def test_mgrid_numpy_equiv(self):
4795         nmgrid = (np.mgrid[0:1:.1, 1:10:1., 10:100:10.],
4796                   np.mgrid[0:2:1, 1:10:1, 10:100:10])
4797         tmgrid = (mgrid[0:1:.1, 1:10:1., 10:100:10.],
4798                   mgrid[0:2:1, 1:10:1, 10:100:10])
4799         for n, t in zip(nmgrid, tmgrid):
4800             for ng, tg in zip(n, t):
4801                 utt.assert_allclose(ng, tg.eval())
4802     def test_ogrid_numpy_equiv(self):
4803         nogrid = (np.ogrid[0:1:.1, 1:10:1., 10:100:10.],
4804                   np.ogrid[0:2:1, 1:10:1, 10:100:10])
4805         togrid = (ogrid[0:1:.1, 1:10:1., 10:100:10.],
4806                   ogrid[0:2:1, 1:10:1, 10:100:10])
4807         for n, t in zip(nogrid, togrid):
4808             for ng, tg in zip(n, t):
4809                 utt.assert_allclose(ng, tg.eval())
4810     def test_mgrid_theano_variable_numpy_equiv(self):
4811         nfmgrid = np.mgrid[0:1:.1, 1:10:1., 10:100:10.]
4812         nimgrid = np.mgrid[0:2:1, 1:10:1, 10:100:10]
4813         i, j, k = dscalars('i', 'j', 'k')
4814         l, m, n = iscalars('l', 'm', 'n')
4815         tfmgrid = mgrid[i:1:.1, 1:j:1., 10:100:k]
4816         timgrid = mgrid[l:2:1, 1:m:1, 10:100:n]
4817         ff = theano.function([i, j, k], tfmgrid)
4818         fi = theano.function([l, m, n], timgrid)
4819         for n, t in zip((nfmgrid, nimgrid), (ff(0, 10, 10.), fi(0, 10, 10))):
4820             for ng, tg in zip(n, t):
4821                 utt.assert_allclose(ng, tg)
4822     def test_ogrid_theano_variable_numpy_equiv(self):
4823         nfogrid = np.ogrid[0:1:.1, 1:10:1., 10:100:10.]
4824         niogrid = np.ogrid[0:2:1, 1:10:1, 10:100:10]
4825         i, j, k = dscalars('i', 'j', 'k')
4826         l, m, n = iscalars('l', 'm', 'n')
4827         tfogrid = ogrid[i:1:.1, 1:j:1., 10:100:k]
4828         tiogrid = ogrid[l:2:1, 1:m:1, 10:100:n]
4829         ff = theano.function([i, j, k], tfogrid)
4830         fi = theano.function([l, m, n], tiogrid)
4831         for n, t in zip((nfogrid, niogrid), (ff(0, 10, 10.), fi(0, 10, 10))):
4832             for ng, tg in zip(n, t):
4833                 utt.assert_allclose(ng, tg)
4834 <a name="26"></a>
4835 class TestInversePermutation(unittest.TestCase):
4836     <font color="#68818b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def setUp(self):
4837         utt.seed_rng()
4838     def test_dim1(self):
4839         p = ivector()
4840         inv = inverse_permutation(p)
4841         assert inv.dtype == p.</b></font>dtype
4842         f_inverse = function([p], inv)
4843         rng = np.random.RandomState(utt.fetch_seed())
4844         p_val = rng.permutation(10).astype('int32')
4845         inv_val = f_inverse(p_val)
4846         assert np.all(f_inverse(inv_val) == p_val)
4847         assert np.all(p_val[inv_val] == np.arange(10))
4848         assert np.all(inv_val[p_val] == np.arange(10))
4849     def test_dim2(self):
4850         p = imatrix()
4851         inv = inverse_permutation(p)
4852         f_inverse = function([p], inv)
4853         rng = np.random.RandomState(utt.fetch_seed())
4854         p_val = np.asarray([rng.permutation(10) for i in range(7)],
4855                            dtype='int32')
4856         inv_val = f_inverse(p_val)
4857         assert np.all(f_inverse(inv_val) == p_val)
4858         for p_row, i_row in zip(p_val, inv_val):
4859             assert np.all(p_row[i_row] == np.arange(10))
4860             assert np.all(i_row[p_row] == np.arange(10))
4861 class TestPermuteRowElements(unittest.TestCase):
4862     def setUp(self):
4863         utt.seed_rng()
4864     def test_1_1(self):
4865         input = dvector()
4866         p = ivector()
4867         out = permute_row_elements(input, p)
4868         permute = function([input, p], out)
4869         rng = np.random.RandomState(utt.fetch_seed())
4870         input_val = rng.uniform(size=(5,))
4871         p_val = rng.permutation(5).astype('int32')
4872         out_val = permute(input_val, p_val)
4873         out_bis = input_val[p_val]
4874         assert np.all(out_val == out_bis)
4875         def permute_fixed(s_input):
4876             return permute_row_elements(s_input, p_val)
4877         utt.verify_grad(permute_fixed, [input_val])
4878     def test_2_1(self):
4879         input = matrix()
4880         p = ivector()
4881         out = permute_row_elements(input, p)
4882         permute = function([input, p], out)
4883         rng = np.random.RandomState(utt.fetch_seed())
4884         input_val = rng.uniform(size=(3, 5)).astype(config.floatX)
4885         p_val = rng.permutation(5).astype('int32')
4886         out_val = permute(input_val, p_val)
4887         out_bis = np.asarray([r[p_val] for r in input_val])
4888         assert np.all(out_val == out_bis)
4889         def permute_fixed(s_input):
4890             return permute_row_elements(s_input, p_val)
4891         utt.verify_grad(permute_fixed, [input_val])
4892     def test_2_2(self):
4893         input = matrix()
4894         p = imatrix()
4895         out = permute_row_elements(input, p)
4896         permute = function([input, p], out)
4897         rng = np.random.RandomState(utt.fetch_seed())
4898         input_val = rng.uniform(size=(3, 5)).astype(config.floatX)
4899         p_val = np.asarray([rng.permutation(5) for i in range(3)],
4900                            dtype='int32')
4901         out_val = permute(input_val, p_val)
4902         out_bis = np.asarray([i_row[p_row]
4903                               for i_row, p_row in zip(input_val, p_val)])
4904         assert np.all(out_val == out_bis)
4905         def permute_fixed(s_input):
4906             return permute_row_elements(s_input, p_val)
4907         utt.verify_grad(permute_fixed, [input_val])
4908     def test_1_2(self):
4909         input = vector()
4910         p = imatrix()
4911         out = permute_row_elements(input, p)
4912         permute = function([input, p], out)
4913         rng = np.random.RandomState(utt.fetch_seed())
4914         input_val = rng.uniform(size=(5,)).astype(config.floatX)
4915         p_val = np.asarray([rng.permutation(5)
4916                             for i in range(3)], dtype='int32')
4917         out_val = permute(input_val, p_val)
4918         out_bis = np.asarray([input_val[p_row] for p_row in p_val])
4919         assert np.all(out_val == out_bis)
4920         def permute_fixed(s_input):
4921             return permute_row_elements(s_input, p_val)
4922         utt.verify_grad(permute_fixed, [input_val])
4923     def test_3b_2(self):
4924         input = TensorType('floatX', (False, True, False))()
4925         p = imatrix()
4926         out = permute_row_elements(input, p)
4927         permute = function([input, p], out)
4928         rng = np.random.RandomState(utt.fetch_seed())
4929         input_val = rng.uniform(size=(4, 1, 5)).astype(config.floatX)
4930         p_val = np.asarray([rng.permutation(5) for i in range(3)],
4931                            dtype='int32')
4932         out_val = permute(input_val, p_val)
4933         out_bis = np.asarray([[in_mat[0, p_row] for p_row in p_val]
4934                               for in_mat in input_val])
4935         assert np.all(out_val == out_bis)
4936         def permute_fixed(s_input):
4937             return permute_row_elements(s_input, p_val)
4938         utt.verify_grad(permute_fixed, [input_val])
4939 class test_tensordot(unittest.TestCase):
4940     def TensorDot(self, axes):
4941         return lambda a, b: tensordot(a, b, axes)
4942     def setUp(self):
4943         utt.seed_rng()
4944     def test0(self):
4945         avec = vector()
4946         bvec = vector()
4947         axes = ((0, ), (0, ))
4948         c = tensordot(avec, bvec, axes)
4949         f1 = inplace_func([avec, bvec], c)
4950         aval = rand(5)
4951         bval = rand(5)
4952         out0 = np.tensordot(aval, bval, axes)
4953         out1 = f1(aval, bval)
4954         utt.assert_allclose(out0, out1)
4955         utt.verify_grad(self.TensorDot(axes), [aval, bval])
4956         bmat = matrix()
4957         axes = ((0, ), (1, ))
4958         c = tensordot(avec, bmat, axes)
4959         f2 = inplace_func([avec, bmat], c)
4960         aval = rand(5)
4961         bval = rand(8, 5)
4962         utt.assert_allclose(np.tensordot(aval, bval, axes),
4963                             f2(aval, bval))
4964         utt.verify_grad(self.TensorDot(axes), [aval, bval])
4965         amat = matrix()
4966 <a name="9"></a>        for axes, shps in [[((0,), (0,)), [(4, 7), (4, 9)]],
4967                            [((0,), (1,)), [(4, 7), (9, 4)]],
4968                            [((1,), (0,)), [(4, 7), (7, 9)]],
4969                            [((<font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>1,), (1,)), [(4, 7), (9, 7)]],
4970                            [((0, 1), (0, 1)), [(4, 7), (4, 7)]],
4971                            ]:
4972             c = tensordot(amat, bmat, axes)
4973             f3 =</b></font> inplace_func([amat, bmat], c)
4974             aval = rand(*shps[0])
4975             bval = rand(*shps[1])
4976             utt.assert_allclose(np.tensordot(aval, bval, axes),
4977                                 f3(aval, bval))
4978             utt.verify_grad(self.TensorDot(axes), [aval, bval])
4979         for axes, shps in [[((2,), (1,)), [(1, 2, 3, 4), (2, 3)]],
4980                            [((0,), (1,)), [(1, 2, 3, 4), (3, 1)]],
4981                            [((<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0,), (0,)), [(1, 2, 3, 4), (1, 3)]],
4982                            [((3,), (0,)), [(1, 2, 3, 4), (4, 1)]],
4983                            ]:
4984             atens = tensor4()
4985             c =</b></font> tensordot(atens, bmat, axes)
4986             f4 = inplace_func([atens, bmat], c)
4987             aval = rand(*shps[0])
4988             bval = rand(*shps[1])
4989             utt.assert_allclose(np.tensordot(aval, bval, axes),
4990                                 f4(aval, bval))
4991             utt.verify_grad(self.TensorDot(axes), [aval, bval])
4992         atens = tensor4()
4993         btens = tensor3()
4994         axes = ((1, 3), (0, 2))
4995         c = tensordot(atens, btens, axes)
4996         f5 = inplace_func([atens, btens], c)
4997         aval = rand(4, 3, 5, 2)
4998         bval = rand(3, 4, 2)
4999         utt.assert_allclose(np.tensordot(aval, bval, axes),
5000                             f5(aval, bval))
5001         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5002         axes = (axes[1], axes[0])
5003         c = tensordot(btens, atens, axes)
5004         f6 = inplace_func([btens, atens], c)
5005         utt.assert_allclose(np.tensordot(bval, aval, axes),
5006                             f6(bval, aval))
5007         utt.verify_grad(self.TensorDot(axes), [bval, aval])
5008     def test_raise_error(self):
5009         amat = matrix()
5010         bmat = matrix()
5011         bvec = vector()
5012         self.assertRaises(ValueError, tensordot, amat, bmat, (0, 1, 2))
5013         self.assertRaises(ValueError, tensordot, amat, bmat, ((0, 1), (0)))
5014         self.assertRaises(ValueError, tensordot, amat, bmat, ((0, 1, 2), (0, 1, 2)))
5015         self.assertRaises(ValueError, tensordot, amat, bvec, (0, 1))
5016         self.assertRaises(ValueError, tensordot, amat, bvec, 2)
5017     def test_weird_valid_axes(self):
5018         amat = matrix()
5019         bmat = matrix()
5020         for axes in [0,
5021                      (1, 0),
5022                      [1, 0],
5023                      (1, (0, )),
5024                      ((1, ), 0),
5025                      ([1], [0]),
5026                      ([], [])]:
5027             c = tensordot(amat, bmat, axes)
5028             f3 = inplace_func([amat, bmat], c)
5029             aval = rand(4, 7)
5030             bval = rand(7, 9)
5031             utt.assert_allclose(np.tensordot(aval, bval, axes),
5032                                 f3(aval, bval))
5033             utt.verify_grad(self.TensorDot(axes), [aval, bval])
5034     def test_scalar_axes(self):
5035         amat = fmatrix()
5036         bmat = dmatrix()
5037         axes = 1
5038         aval = rand(4, 5).astype('float32')
5039         bval = rand(5, 3)
5040         c = tensordot(amat, bmat, axes)
5041         f3 = inplace_func([amat, bmat], c)
5042         self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
5043                                     f3(aval, bval)))
5044         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5045         amat = tensor3()
5046         bmat = tensor3()
5047         axes = 2
5048         aval = rand(3, 4, 5)
5049         bval = rand(4, 5, 3)
5050         c = tensordot(amat, bmat, axes)
5051         f3 = inplace_func([amat, bmat], c)
5052         self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
5053                                     f3(aval, bval)))
5054         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5055     def test_scalar0(self):
5056         amat = matrix()
5057         bmat = matrix()
5058         axes = 0
5059         aval = rand(4, 5)
5060         bval = rand(5, 4)
5061         c = tensordot(amat, bmat, axes)
5062         f3 = inplace_func([amat, bmat], c)
5063         self.assertTrue(np.allclose(np.tensordot(aval, bval, axes),
5064                                     f3(aval, bval)))
5065         utt.verify_grad(self.TensorDot(axes), [aval, bval])
5066     def test_broadcastable1(self):
5067         x = TensorType(dtype=floatX, broadcastable=(True, False, False))('x')
5068         y = tensor3('y')
5069         z = tensordot(x, y)
5070         assert z.broadcastable == (True, False)
5071         f = inplace_func([x, y], z)
5072         xv = rand(1, 3, 4)
5073         yv = rand(3, 4, 5)
5074         zv = f(xv, yv)
5075         self.assertTrue(np.allclose(np.tensordot(xv, yv), zv))
5076     def test_broadcastable2(self):
5077         x = TensorType(dtype=floatX, broadcastable=(True, False, False))('x')
5078         y = tensor3('y')
5079         axes = [[2, 1], [0, 1]]
5080         z = tensordot(x, y, axes=axes)
5081         assert z.broadcastable == (True, False)
5082         f = inplace_func([x, y], z)
5083         xv = rand(1, 3, 4)
5084         yv = rand(4, 3, 5)
5085         zv = f(xv, yv)
5086         self.assertTrue(np.allclose(np.tensordot(xv, yv, axes=axes), zv))
5087 def test_smallest_stack():
5088     sx, sy = dscalar(), dscalar()
5089     rval = inplace_func([sx, sy], stack([sx, sy]))(-4.0, -2.0)
5090     assert type(rval) == np.ndarray
5091     assert [-4, -2] == list(rval)
5092 def test_smallest():
5093     x = dvector()
5094     y = dvector()
5095     z = dvector()
5096     f1 = inplace_func([x], smallest(x))
5097     assert np.all([1, 2, 3] == f1([1, 2, 3]))
5098     f3 = inplace_func([x, y, z], smallest(x, y, z))
5099     assert np.all([1, 2, 3] == f3([1, 3, 9], [7, 7, 7], [8, 2, 3]))
5100     sx, sy = dscalar(), dscalar()
5101     assert -4 == inplace_func([sx, sy], smallest(sx, sy))(-4.0, -2.0)
5102 def test_reshape_member_fn():
5103     x = dmatrix()
5104     y = x.reshape((4, 5, 6))
5105     assert y.owner.op == Reshape(3)
5106 def test_var():
5107     a = Tensor(dtype='float64', broadcastable=[False, False, False])()
5108     f = function([a], var(a))
5109     a_val = np.arange(60).reshape(3, 4, 5)
5110     assert np.allclose(np.var(a_val), f(a_val))
5111     f = function([a], var(a, axis=0))
5112     assert np.allclose(np.var(a_val, axis=0), f(a_val))
5113     f = function([a], var(a, axis=1))
5114     assert np.allclose(np.var(a_val, axis=1), f(a_val))
5115     f = function([a], var(a, axis=2))
5116     assert np.allclose(np.var(a_val, axis=2), f(a_val))
5117     f = function([a], var(a, axis=0, ddof=0))
5118     assert np.allclose(np.var(a_val, axis=0, ddof=0), f(a_val))
5119     f = function([a], var(a, axis=1, ddof=1))
5120     assert np.allclose(np.var(a_val, axis=1, ddof=1), f(a_val))
5121     f = function([a], var(a, axis=2, ddof=1))
5122     assert np.allclose(np.var(a_val, axis=2, ddof=1), f(a_val))
5123     f = function([a], var(a, ddof=0, corrected=True))
5124     mean_a = np.mean(a_val)
5125     centered_a = a_val - mean_a
5126     v = np.mean(centered_a ** 2)
5127     error = (np.mean(centered_a)) ** 2
5128     v = v - error
5129     assert np.allclose(v, f(a_val))
5130     f = function([a], var(a, axis=2, ddof=1, corrected=True))
5131     mean_a = np.mean(a_val, axis=2, keepdims=True)
5132     centered_a = a_val - mean_a
5133     v = np.var(a_val, axis=2, ddof=1)
5134     shp_inp = np.shape(a_val)
5135     shp = shp_inp - np.array(1)
5136     error = (np.sum(centered_a, axis=2)) ** 2
5137     error = np.true_divide(error, shp[1] * shp_inp[1])
5138     v = v - error
5139     assert np.allclose(v, f(a_val))
5140     assert theano.tensor.vector(dtype='float16').var().dtype == 'float16'
5141 class T_sum(unittest.TestCase):
5142     def test_sum_overflow(self):
5143         a = Tensor(dtype='int8', broadcastable=[False])()
5144         f = function([a], sum(a))
5145         assert f([1] * 300) == 300
5146     def test_list(self):
5147         ll = [theano.shared(0.), theano.shared(2.)]
5148         tensor.sum(ll).eval() == 2
5149 @dec.skipif(
5150     isinstance(get_default_mode(), theano.compile.debugmode.DebugMode),
5151     ("This test fails in DEBUG_MODE, but the generated code is OK. "
5152      "It is actually a problem of DEBUG_MODE, see #626."))
5153 def test_default():
5154     x, y = scalars('xy')
5155     z = default(x, y)
5156     f = function([x, y], z)
5157     assert f(1, 2) == 1
5158     assert f(None, 2) == 2
5159     assert f(1, None) == 1
5160 @dec.skipif(
5161     isinstance(get_default_mode(), theano.compile.debugmode.DebugMode),
5162     ("This test fails in DEBUG_MODE, but the generated code is OK. "
5163      "It is actually a problem of DEBUG_MODE, see #626."))
5164 def test_default_state():
5165     x, y = scalars('xy')
5166     z = default(x, 3.8)
5167     new_x = y + z
5168     f = function([y, compile.In(x, update=new_x, value=12.0)], new_x)
5169     assert f(3) == 15
5170     f['x'] = None
5171     assert np.allclose(f(1), 4.8)
5172     assert np.allclose(f(np.asarray(2.2, dtype=config.floatX)), 7)
5173 def test_autocast():
5174     backup_config = config.cast_policy
5175     for autocast_cfg in (
5176             'custom',
5177             'numpy+floatX',
5178             ):
5179         config.cast_policy = autocast_cfg
5180         try:
5181             eval('_test_autocast_' + autocast_cfg.replace('+', '_'))()
5182         finally:
5183             config.cast_policy = backup_config
5184 def _test_autocast_custom():
5185     assert config.cast_policy == 'custom'
5186     orig_autocast = autocast_float.dtypes
5187     with autocast_float_as('float32'):
5188         assert autocast_float.dtypes == ('float32',)
5189     assert autocast_float.dtypes == orig_autocast
5190     with autocast_float_as('float64'):
5191         assert autocast_float.dtypes == ('float64',)
5192     assert autocast_float.dtypes == orig_autocast
5193     with autocast_float_as('float32'):
5194         assert autocast_float.dtypes == ('float32',)
5195         with autocast_float_as('float64'):
5196             assert autocast_float.dtypes == ('float64',)
5197         assert autocast_float.dtypes == ('float32',)
5198     assert autocast_float.dtypes == orig_autocast
5199     with autocast_float_as('float32'):
5200         assert (dvector() + 1.1).dtype == 'float64'
5201         assert (fvector() + 1.1).dtype == 'float32'
5202         assert ((fvector() + theano._asarray(1.1, dtype='float64')).dtype ==
5203                 'float64')
5204         assert ((fvector() + theano._asarray(1.1, dtype='float32')).dtype ==
5205                 'float32')
5206         assert (dvector() + 1).dtype == 'float64'
5207         assert (fvector() + 1).dtype == 'float32'
5208     with autocast_float_as('float64'):
5209         assert (dvector() + 1.1).dtype == 'float64'
5210         assert (fvector() + 1.1).dtype == 'float64'
5211         assert (fvector() + 1.0).dtype == 'float64'
5212         assert ((fvector() + theano._asarray(1.1, dtype='float64')).dtype ==
5213                 'float64')
5214         assert ((fvector() + theano._asarray(1.1, dtype='float32')).dtype ==
5215                 'float32')
5216         assert (dvector() + 1).dtype == 'float64'
5217         assert (fvector() + 1).dtype == 'float32'
5218     with autocast_float_as('float32', 'float64'):
5219         assert (dvector() + 1.1).dtype == 'float64'
5220         assert (fvector() + 1.1).dtype == theano.config.floatX
5221         assert (fvector() + 1.0).dtype == 'float32'
5222         assert (dvector() + np.float32(1.1)).dtype == 'float64'
5223         assert (dvector() + np.float64(1.1)).dtype == 'float64'
5224         assert (dvector() + np.float(1.1)).dtype == 'float64'
5225         assert (fvector() + np.float32(1.1)).dtype == 'float32'
5226         assert (fvector() + np.float64(1.1)).dtype == 'float64'
5227         assert (fvector() + np.float(1.1)).dtype == theano.config.floatX
5228         assert (lvector() + np.int64(1)).dtype == 'int64'
5229         assert (lvector() + np.int32(1)).dtype == 'int64'
5230         assert (lvector() + np.int16(1)).dtype == 'int64'
5231         assert (lvector() + np.int8(1)).dtype == 'int64'
5232         assert (ivector() + np.int8(1)).dtype == 'int32'
5233         assert (wvector() + np.int8(1)).dtype == 'int16'
5234         assert (bvector() + np.int8(1)).dtype == 'int8'
5235         with autocast_float_as('float64'):
5236             assert (fvector() + 1.0).dtype == 'float64'
5237 def _test_autocast_numpy():
5238     assert config.cast_policy == 'numpy'
5239     def ok(z):
5240         assert tensor.constant(z).dtype == np.asarray(z).dtype
5241     for x in ([2 ** i for i in xrange(63)] +
5242               [0, L(0), L(1), L(2 ** 63 - 1)] +
5243               [0., 1., 1.1, 1.5]):
5244         n_x = np.asarray(x)
5245         ok(x)
5246         ok(-x)
5247         ok(x - 1)
5248         ok(-x + 1)
5249         ok(n_x)
5250 def _test_autocast_numpy_floatX():
5251     assert config.cast_policy == 'numpy+floatX'
5252     backup_floatX = config.floatX
5253     def ok(z, floatX):
5254         if (isinstance(z, float) and
5255                 floatX == 'float32' and
5256                 not hasattr(z, 'dtype')):
5257             assert tensor.constant(z).dtype == 'float32'
5258         else:
5259             assert tensor.constant(z).dtype == np.asarray(z).dtype
5260     try:
5261         for floatX in ('float32', 'float64'):
5262             config.floatX = floatX
5263             for x in ([2 ** i - 1 for i in xrange(64)] +
5264                       [0, L(0), L(1), L(2 ** 63 - 1)] +
5265                       [0., 1., 1.1, 1.5]):
5266                 ok(x, floatX)
5267                 ok(-x, floatX)
5268                 ok(x - 1, floatX)
5269                 ok(-x + 1, floatX)
5270                 ok(np.asarray(x), floatX)
5271                 ok(np.float64(x), floatX)
5272     finally:
5273         config.floatX = backup_floatX
5274 class test_arithmetic_cast(unittest.TestCase):
5275     def test_arithmetic_cast(self):
5276         backup_config = config.cast_policy
5277         dtypes = get_numeric_types(with_complex=True)
5278         def theano_scalar(dtype):
5279             return tensor.scalar(dtype=str(dtype))
5280         def numpy_scalar(dtype):
5281             return np.array(1, dtype=dtype)
5282         def theano_array(dtype):
5283             return tensor.vector(dtype=str(dtype))
5284         def numpy_array(dtype):
5285             return np.array([1], dtype=dtype)
5286         def theano_i_scalar(dtype):
5287             return theano.scalar.Scalar(str(dtype))()
5288         def numpy_i_scalar(dtype):
5289             return numpy_scalar(dtype)
5290         if config.int_division == 'int':
5291             warnings.filterwarnings('ignore', message='Division of two integer',
5292                                     category=DeprecationWarning)
5293         try:
5294             for cfg in ('numpy+floatX', ):  # Used to test 'numpy' as well.
5295                 config.cast_policy = cfg
5296                 for op in (operator.add, operator.sub, operator.mul,
5297                            operator_div, operator.floordiv):
5298                     for a_type in dtypes:
5299                         for b_type in dtypes:
5300                             is_int_division = (
5301                                 op is operator_div and
5302                                 a_type in tensor.discrete_dtypes and
5303                                 b_type in tensor.discrete_dtypes)
5304                             for combo in (
5305                                     ('scalar', 'scalar'),
5306                                     ('array', 'array'),
5307                                     ('scalar', 'array'),
5308                                     ('array', 'scalar'),
5309                                     ('i_scalar', 'i_scalar'),
5310                                     ):
5311                                 theano_args = list(
5312                                     map(eval, ['theano_%s' % c for c in combo]))
5313                                 numpy_args = list(
5314                                     map(eval, ['numpy_%s' % c for c in combo]))
5315                                 try:
5316                                     theano_dtype = op(
5317                                         theano_args[0](a_type),
5318                                         theano_args[1](b_type)).type.dtype
5319                                     assert not (is_int_division and
5320                                                 config.int_division == 'raise')
5321                                 except theano.scalar.IntegerDivisionError:
5322                                     assert (is_int_division and
5323                                             config.int_division == 'raise')
5324                                     continue
5325                                 numpy_dtypes = [
5326                                     op(numpy_args[0](a_type),
5327                                        numpy_args[1](b_type)).dtype,
5328                                     op(numpy_args[1](b_type),
5329                                        numpy_args[0](a_type)).dtype]
5330                                 numpy_dtype = theano.scalar.upcast(
5331                                     *list(map(str, numpy_dtypes)))
5332                                 if numpy_dtype == theano_dtype:
5333                                     continue
5334                                 if (cfg == 'numpy+floatX' and
5335                                         config.floatX == 'float32' and
5336                                         a_type != 'float64' and
5337                                         b_type != 'float64' and
5338                                         numpy_dtype == 'float64'):
5339                                     assert theano_dtype == 'float32'
5340                                     continue
5341                                 if 'array' in combo and 'scalar' in combo:
5342                                     array_type, scalar_type = (
5343                                         (a_type, b_type)[list(combo).index(arg)]
5344                                         for arg in ('array', 'scalar'))
5345                                     up_type = theano.scalar.upcast(array_type,
5346                                                                    scalar_type)
5347                                     if (
5348                                             scalar_type != array_type and
5349                                             array_type != up_type and
5350                                             theano_dtype == up_type and
5351                                             array_type == numpy_dtype):
5352                                         continue
5353                                 if (is_int_division and
5354                                         config.int_division == 'floatX'):
5355                                     assert theano_dtype == config.floatX
5356                                     continue
5357                                 if (cfg == 'numpy+floatX' and
5358                                         a_type == 'complex128' and
5359                                         (b_type == 'float32' or
5360                                          b_type == 'float16') and
5361                                         combo == ('scalar', 'array') and
5362                                         theano_dtype == 'complex128' and
5363                                         numpy_dtype == 'complex64'):
5364                                     raise SkipTest("Known issue with"
5365                                                    "numpy see #761")
5366                                 assert False
5367         finally:
5368             config.cast_policy = backup_config
5369             if config.int_division == 'int':
5370                 warnings.filterwarnings(
5371                     'default',
5372                     message='Division of two integer',
5373                     category=DeprecationWarning)
5374 class T_long_tensor(unittest.TestCase):
5375     def test_fit_int64(self):
5376         bitwidth = theano.configdefaults.python_int_bitwidth()
5377         for exponent in xrange(bitwidth):
5378             val = L(2 ** exponent - 1)
5379             scalar_ct = constant(val)
5380             assert scalar_ct.dtype in tensor.int_dtypes, (exponent, val, scalar_ct.dtype)
5381             assert scalar_ct.value == val
5382             vector_ct = constant([val, val])
5383             if PY3 and bitwidth == 32:
5384                 assert vector_ct.dtype == 'int32'
5385             else:
5386                 assert vector_ct.dtype == 'int64'
5387             assert np.all(vector_ct.value == val)
5388             matrix_ct = constant([[val, val]])
5389             if PY3 and bitwidth == 32:
5390                 assert matrix_ct.dtype == 'int32'
5391             else:
5392                 assert matrix_ct.dtype == 'int64'
5393             assert np.all(matrix_ct.value == val)
5394     def test_too_big(self):
5395         val = L(2 ** 64)
5396         self.assertRaises(Exception, constant, val)
5397         self.assertRaises(Exception, constant, [val, val])
5398         self.assertRaises(Exception, constant, [[val, val]])
5399 class test_broadcast(unittest.TestCase):
5400     def test_broadcast_bigdim(self):
5401         def f():
5402             x = matrix()
5403             addbroadcast(x, 2)
5404         self.assertRaises(ValueError, f)
5405     def test_unbroadcast_addbroadcast(self):
5406         x = matrix()
5407         assert unbroadcast(x, 0) is x
5408         assert unbroadcast(x, 1) is x
5409         assert unbroadcast(x, 1, 0) is x
5410         assert unbroadcast(x, 0, 1) is x
5411         assert addbroadcast(x, 0) is not x
5412         assert addbroadcast(x, 1) is not x
5413         assert addbroadcast(x, 1, 0).owner.inputs[0] is x
5414         assert unbroadcast(addbroadcast(x, 0), 0) is x
5415         assert addbroadcast(unbroadcast(x, 0), 0) is not x
5416         x = row()
5417         assert unbroadcast(x, 0) is not x
5418         assert unbroadcast(x, 1) is x
5419         assert unbroadcast(x, 1, 0) is not x
5420         assert unbroadcast(x, 0, 1) is not x
5421         assert addbroadcast(x, 0) is x
5422         assert addbroadcast(x, 1).owner.inputs[0] is x
5423         assert addbroadcast(x, 1, 0).owner.inputs[0] is x
5424         assert addbroadcast(x, 0, 1).owner.inputs[0] is x
5425         assert unbroadcast(addbroadcast(x, 1), 1) is x
5426         assert addbroadcast(unbroadcast(x, 1), 1) is not x
5427         assert unbroadcast(unbroadcast(x, 0), 0).owner.inputs[0] is x
5428         x = TensorType(dtype='float64', broadcastable=(True, True))()
5429         assert unbroadcast(unbroadcast(x, 1), 0).owner.inputs[0] is x
5430         assert addbroadcast(unbroadcast(x, 1), 0).owner.inputs[0] is x
5431         assert addbroadcast(unbroadcast(x, 0), 0) is x
5432     def test_patternbroadcast(self):
5433         x = scalar('x')
5434         m = tensor.matrix('m')
5435         s = patternbroadcast(m, x.broadcastable)
5436         assert s is m
5437         x2 = patternbroadcast(x, x.broadcastable)
5438         assert x2 is x
5439     def test_infer_shape(self):
5440         x = matrix()
5441         y = addbroadcast(x, 0)
5442         f = theano.function([x], y.shape)
5443         assert (f(np.zeros((1, 5), dtype=config.floatX)) == [1, 5]).all()
5444         topo = f.maker.fgraph.toposort()
5445         if theano.config.mode != 'FAST_COMPILE':
5446             assert len(topo) == 2
5447             assert isinstance(topo[0].op, opt.Shape_i)
5448             assert isinstance(topo[1].op, opt.MakeVector)
5449         x = matrix()
5450         y = unbroadcast(x, 0)
5451         f = theano.function([x], y.shape)
5452         assert (f(np.zeros((2, 5), dtype=config.floatX)) == [2, 5]).all()
5453         topo = f.maker.fgraph.toposort()
5454         if theano.config.mode != 'FAST_COMPILE':
5455             assert len(topo) == 3
5456             assert isinstance(topo[0].op, opt.Shape_i)
5457             assert isinstance(topo[1].op, opt.Shape_i)
5458             assert isinstance(topo[2].op, opt.MakeVector)
5459         x = row()
5460         y = unbroadcast(x, 0)
5461         f = theano.function([x], y.shape)
5462         assert (f(np.zeros((1, 5), dtype=config.floatX)) == [1, 5]).all()
5463         topo = f.maker.fgraph.toposort()
5464         if theano.config.mode != 'FAST_COMPILE':
5465             assert len(topo) == 2
5466             assert isinstance(topo[0].op, opt.Shape_i)
5467             assert isinstance(topo[1].op, opt.MakeVector)
5468 def test_len():
5469     for shape_ in [(5,), (3, 4), (7, 4, 6)]:
5470         x = tensor.tensor(dtype='floatX', broadcastable=(False,) * len(shape_))
5471         assert_raises(TypeError, len, x)
5472 def test_mod():
5473     x, y = fscalars('xy')
5474     fn = gof.DualLinker().accept(
5475         gof.FunctionGraph([x, y], [x % y])).make_function()
5476     for a, b in ((0, 1), (1, 1), (0, -1), (1, -1), (-1, -1),
5477                  (1, 2), (-1, 2), (1, -2), (-1, -2),
5478                  (5, 3), (-5, 3), (5, -3), (-5, -3)
5479                  ):
5480         assert fn(a, b) == a % b, (a,)
5481 def test_divmod():
5482     x, y = fscalars('xy')
5483 <a name="20"></a>    d, r = divmod(x, y)
5484     fn = gof.DualLinker().accept(
5485         gof.FunctionGraph([x, y], [d, r])).make_function()
5486     for a, b in ((0, 1), (1, 1), (0, -1), (<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>1, -1), (-1, -1),
5487                  (1, 2), (-1, 2), (1, -2), (-1, -2),
5488                  (5, 3), (-5, 3), (5, -3), (-5, -3)
5489                  ):
5490         d_v, r_v = fn(a, b)
5491         d_vp, r_vp =</b></font> divmod(a, b)
5492         assert d_v == d_vp and r_v == r_vp, (a,)
5493 def test_mod_compile():
5494     x = tensor.vector()
5495     y = tensor.vector()
5496     out = tensor.switch(tensor.eq(3 % x.shape[0], 0), y, y[:-1])
5497     theano.function([x, y], out)
5498 def test_unalign():
5499     if config.floatX == 'float64':
5500         dtype = "b1,f8"
5501     else:
5502         dtype = "b1,f4"
5503     a = np.empty(10000, dtype=dtype)['f1']
5504     b = np.empty(10000, dtype=dtype)['f1']
5505     assert not a.flags.aligned
5506     assert not b.flags.aligned
5507     a[:] = rand(len(a))
5508     b[:] = rand(len(b))
5509     out_numpy = 2 * a + 3 * b
5510     av, bv = tensor.vectors('ab')
5511     f = theano.function([av, bv], 2 * av + 3 * bv)
5512     f.maker.fgraph.toposort()
5513     try:
5514         out_theano = f(a, b)
5515         assert not a.flags.aligned
5516         assert not b.flags.aligned
5517         assert np.allclose(out_numpy, out_theano)
5518         assert False
5519     except TypeError:
5520         pass
5521     a = np.empty((), dtype=dtype)['f1']
5522     b = np.empty((), dtype=dtype)['f1']
5523     assert not a.flags.aligned
5524     assert not b.flags.aligned
5525     out_numpy = 2 * a + 3 * b
5526     av, bv = tensor.scalars('ab')
5527     f = theano.function([av, bv], 2 * av + 3 * bv)
5528     f.maker.fgraph.toposort()
5529     try:
5530         out_theano = f(a, b)
5531         assert not a.flags.aligned
5532         assert not b.flags.aligned
5533         assert np.allclose(out_numpy, out_theano)
5534         assert False
5535     except TypeError:
5536         pass
5537 def test_dimshuffle_duplicate():
5538     x = tensor.vector()
5539     success = False
5540     try:
5541         tensor.DimShuffle((False, ), (0, 0))(x)
5542     except ValueError as e:
5543         assert str(e).find("may not appear twice") != -1
5544         success = True
5545     assert success
5546 class T_get_scalar_constant_value(unittest.TestCase):
5547     def test_get_scalar_constant_value(self):
5548         a = tensor.stack([1, 2, 3])
5549         assert get_scalar_constant_value(a[0]) == 1
5550         assert get_scalar_constant_value(a[1]) == 2
5551         assert get_scalar_constant_value(a[2]) == 3
5552         b = tensor.iscalar()
5553         a = tensor.stack([b, 2, 3])
5554         self.assertRaises(tensor.basic.NotScalarConstantError, get_scalar_constant_value, a[0])
5555         assert get_scalar_constant_value(a[1]) == 2
5556         assert get_scalar_constant_value(a[2]) == 3
5557         v = tensor.ivector()
5558         a = tensor.stack([v, [2], [3]])
5559         self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[0])
5560         self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[1])
5561         self.assertRaises(tensor.NotScalarConstantError, get_scalar_constant_value, a[2])
5562         v = tensor.row()
5563         assert get_scalar_constant_value(v.shape[0]) == 1
5564     def test_subtensor_of_constant(self):
5565         c = constant(rand(5))
5566         for i in range(c.value.shape[0]):
5567             assert get_scalar_constant_value(c[i]) == c.value[i]
5568         c = constant(rand(5, 5))
5569         for i in range(c.value.shape[0]):
5570             for j in range(c.value.shape[1]):
5571                 assert get_scalar_constant_value(c[i, j]) == c.value[i, j]
5572     def test_numpy_array(self):
5573         assert get_scalar_constant_value(np.array(3)) == 3
5574         self.assertRaises(
5575             tensor.NotScalarConstantError,
5576             get_scalar_constant_value,
5577             np.array([0, 1]))
5578         self.assertRaises(
5579             tensor.EmptyConstantError,
5580             get_scalar_constant_value,
5581             np.array([]))
5582     def test_make_vector(self):
5583         mv = opt.make_vector(1, 2, 3)
5584         self.assertRaises(
5585             tensor.NotScalarConstantError,
5586             get_scalar_constant_value,
5587             mv)
5588         assert get_scalar_constant_value(mv[0]) == 1
5589         assert get_scalar_constant_value(mv[1]) == 2
5590         assert get_scalar_constant_value(mv[2]) == 3
5591         assert get_scalar_constant_value(mv[np.int32(0)]) == 1
5592         assert get_scalar_constant_value(mv[np.int64(1)]) == 2
5593         assert get_scalar_constant_value(mv[np.uint(2)]) == 3
5594         t = theano.scalar.Scalar('int64')
5595         self.assertRaises(
5596             tensor.NotScalarConstantError,
5597             get_scalar_constant_value,
5598             mv[t()])
5599     def test_shape_i(self):
5600         c = theano.tensor.constant(np.random.rand(3, 4))
5601         s = opt.Shape_i(0)(c)
5602         assert get_scalar_constant_value(s) == 3
5603         s = opt.Shape_i(1)(c)
5604         assert get_scalar_constant_value(s) == 4
5605         d = theano.shared(np.random.randn(1, 1), broadcastable=(True, True))
5606         f = theano.tensor.basic.ScalarFromTensor()(opt.Shape_i(0)(d))
5607         assert get_scalar_constant_value(f) == 1
5608     def test_elemwise(self):
5609         c = theano.tensor.constant(np.random.rand())
5610         s = c + 1
5611         assert np.allclose(get_scalar_constant_value(s), c.data + 1)
5612         s = c - 1
5613         assert np.allclose(get_scalar_constant_value(s), c.data - 1)
5614         s = c * 1.2
5615         assert np.allclose(get_scalar_constant_value(s), c.data * 1.2)
5616         s = c &lt; 0.5
5617         assert np.allclose(get_scalar_constant_value(s), int(c.data &lt; 0.5))
5618         s = tensor.second(c, .4)
5619         assert np.allclose(get_scalar_constant_value(s), .4)
5620     def test_assert(self):
5621         c = theano.tensor.constant(2)
5622         x = theano.tensor.scalar()
5623         a = opt.Assert()(c, c &gt; 1)
5624         assert get_scalar_constant_value(a) == 2
5625         with change_flags(compute_test_value='off'):
5626             a = opt.Assert()(c, c &gt; 2)
5627             self.assertRaises(
5628                 tensor.NotScalarConstantError,
5629                 get_scalar_constant_value, a)
5630         a = opt.Assert()(c, c &gt; x)
5631         self.assertRaises(
5632             tensor.NotScalarConstantError,
5633             get_scalar_constant_value, a)
5634     def test_second(self):
5635         c = theano.tensor.constant(np.random.rand())
5636         shp = theano.tensor.vector()
5637         s = theano.tensor.second(shp, c)
5638         assert get_scalar_constant_value(s) == c.data
5639     def test_copy(self):
5640         c = theano.tensor.constant(3)
5641         d = extract_constant(c)
5642         d += 1
5643         e = extract_constant(c)
5644         self.assertTrue(e == 3, (c, d, e))
5645 class T_as_tensor_variable(unittest.TestCase):
5646     def test_bool(self):
5647         self.assertRaises(TypeError, as_tensor_variable, True)
5648         self.assertRaises(TypeError, as_tensor_variable, False)
5649     def test_ndarray_bool(self):
5650         ten = as_tensor_variable(np.array([True, False, False, True, True]))
5651         assert ten.type.dtype == 'bool'
5652     def test_memmap(self):
5653         inp = np.random.rand(4, 3)
5654         f, fname = mkstemp()
5655         new_inp = np.memmap(fname, dtype=inp.dtype,
5656                             mode='w+', shape=inp.shape)
5657         new_inp[...] = inp
5658         as_tensor_variable(new_inp)
5659     def test_empty_dtype(self):
5660         old = theano.config.floatX
5661         for dtype in ['float16', 'float32', 'float64']:
5662             try:
5663                 theano.config.floatX = dtype
5664                 assert theano.tensor.as_tensor_variable(()).dtype == dtype
5665                 assert theano.tensor.as_tensor_variable([]).dtype == dtype
5666             finally:
5667                 theano.config.floatX = old
5668 class test_complex_mod(unittest.TestCase):
5669     def test_fail(self):
5670         x = vector(dtype='complex64')
5671         try:
5672             x % 5
5673             assert False
5674         except theano.scalar.ComplexError:
5675             pass
5676 class test_size(unittest.TestCase):
5677     def test_matrix(self):
5678         x = tensor.matrix()
5679         y = np.zeros((5, 7), dtype=config.floatX)
5680         assert y.size == function([x], x.size)(y)
5681     def test_vector(self):
5682         x = tensor.vector()
5683         y = np.zeros(7, dtype=config.floatX)
5684         assert y.size == function([x], x.size)(y)
5685     def test_scalar(self):
5686         x = tensor.scalar()
5687         y = np.array(7, dtype=config.floatX)
5688         assert y.size == function([x], x.size)(y)
5689     def test_shared(self):
5690         y = np.zeros((1, 2, 3, 4), dtype=config.floatX)
5691         x = theano.shared(y)
5692         assert y.size == function([], x.size)()
5693 class test_diag(unittest.TestCase):
5694     def __init__(self, name, mode=None, shared=tensor._shared,
5695                  floatX=None, type=tensor.TensorType):
5696         self.mode = mode
5697         self.shared = shared
5698         if floatX is None:
5699             floatX = config.floatX
5700         self.floatX = floatX
5701         self.type = type
5702         super(test_diag, self).__init__(name)
5703     def test_diag(self):
5704         rng = np.random.RandomState(utt.fetch_seed())
5705         x = theano.tensor.vector()
5706         g = diag(x)
5707         assert isinstance(g.owner.op, AllocDiag)
5708         f = theano.function([x], g)
5709         for shp in [5, 0, 1]:
5710             m = rng.rand(shp).astype(self.floatX)
5711             v = np.diag(m)
5712             r = f(m)
5713             assert (r == v).all()
5714         xx = self.shared(rng.rand(3, 5))
5715         g = diag(xx)
5716         assert isinstance(g.owner.op, ExtractDiag)
5717         f = theano.function([], g)
5718         for shp in [(5, 3), (3, 5), (5, 1), (1, 5), (5, 0), (0, 5),
5719                     (1, 0), (0, 1)]:
5720             m = rng.rand(*shp).astype(self.floatX)
5721             xx.set_value(m)
5722             v = np.diag(m)
5723             r = f()
5724             assert (r == v).all()
5725         xx = theano.tensor.scalar()
5726         np.testing.assert_raises(ValueError, diag, xx)
5727     def test_infer_shape(self):
5728         rng = np.random.RandomState(utt.fetch_seed())
5729         x = theano.tensor.vector()
5730         g = diag(x)
5731         f = theano.function([x], g.shape)
5732         topo = f.maker.fgraph.toposort()
5733         if config.mode != 'FAST_COMPILE':
5734             assert np.sum(
5735                 [isinstance(node.op, AllocDiag) for node in topo]) == 0
5736         for shp in [5, 0, 1]:
5737             m = rng.rand(shp).astype(self.floatX)
5738             assert (f(m) == np.diag(m).shape).all()
5739         x = theano.tensor.matrix()
5740         g = diag(x)
5741         f = theano.function([x], g.shape)
5742         topo = f.maker.fgraph.toposort()
5743         if config.mode != 'FAST_COMPILE':
5744             assert np.sum(
5745                 [isinstance(node.op, ExtractDiag) for node in topo]) == 0
5746         for shp in [(5, 3), (3, 5), (5, 1), (1, 5), (5, 0), (0, 5),
5747                     (1, 0), (0, 1)]:
5748             m = rng.rand(*shp).astype(self.floatX)
5749             assert (f(m) == np.diag(m).shape).all()
5750     def test_diag_grad(self):
5751         rng = np.random.RandomState(utt.fetch_seed())
5752         x = rng.rand(5)
5753         tensor.verify_grad(diag, [x], rng=rng)
5754         x = rng.rand(5, 3)
5755         tensor.verify_grad(diag, [x], rng=rng)
5756 class TestAllocDiag(unittest.TestCase):
5757     def __init__(self, name, alloc_diag=AllocDiag, mode=None):
5758         self.alloc_diag = alloc_diag
5759         if mode is None:
5760             mode = theano.compile.mode.get_default_mode()
5761         self.mode = mode
5762         super(TestAllocDiag, self).__init__(name)
5763     def _generator(self):
5764         dims = 4
5765         shape = (5,) * dims
5766         xv = np.random.randn(*shape).astype(config.floatX)
5767         for d in xrange(1, dims + 1):
5768             x = TensorType(dtype=config.floatX, broadcastable=(False,) * d)('x')
5769             test_val = xv[((0,) * (dims - d))]
5770             yield x, test_val
5771     def test_alloc_diag_values(self):
5772         for x, test_val in self._generator():
5773             for offset, axis1, axis2 in [(0, 0, 1), (0, 1, 2), (1, 0, 1),
5774                                          (0, 1, 3), (0, 2, 3), (1, 2, 3),
5775                                          (-1, 0, 1), (-2, 0, 1), (-1, 1, 2)]:
5776                 if np.maximum(axis1, axis2) &gt; len(test_val.shape):
5777                     continue
5778                 adiag_op = self.alloc_diag(offset=offset,
5779                                            axis1=axis1,
5780                                            axis2=axis2)
5781                 f = theano.function([x], adiag_op(x))
5782                 diag_arr = f(test_val)
5783                 rediag = np.diagonal(
5784                     diag_arr,
5785                     offset=offset,
5786                     axis1=axis1,
5787                     axis2=axis2
5788                 )
5789                 assert np.all(rediag == test_val)
5790                 f_shape = theano.function([x], adiag_op(x).shape, mode='FAST_RUN')
5791                 theano.printing.debugprint(f_shape.maker.fgraph.outputs[0])
5792                 output_shape = f_shape(test_val)
5793                 assert not any(isinstance(node.op, self.alloc_diag)
5794                                for node in f_shape.maker.fgraph.toposort())
5795                 rediag_shape = np.diagonal(
5796                     np.ones(output_shape),
5797                     offset=offset,
5798                     axis1=axis1,
5799                     axis2=axis2
5800                 ).shape
5801                 assert np.all(rediag_shape == test_val.shape)
5802                 diag_x = adiag_op(x)
5803                 sum_diag_x = tensor.sum(diag_x)
5804                 grad_x = tensor.grad(sum_diag_x, x)
5805                 grad_diag_x = tensor.grad(sum_diag_x, diag_x)
5806                 f_grad_x = theano.function([x], grad_x, mode=self.mode)
5807                 f_grad_diag_x = theano.function([x], grad_diag_x, mode=self.mode)
5808                 grad_input = f_grad_x(test_val)
5809                 grad_diag_input = f_grad_diag_x(test_val)
5810                 true_grad_input = np.diagonal(
5811                     grad_diag_input,
5812                     offset=offset,
5813                     axis1=axis1,
5814                     axis2=axis2
5815                 )
5816                 assert np.all(true_grad_input == grad_input)
5817 class test_numpy_assumptions(unittest.TestCase):
5818     def test_ndarray_copy(self):
5819         assert copy(np.ndarray) is np.ndarray
5820         assert deepcopy(np.ndarray) is np.ndarray
5821     def test_dtype_equality(self):
5822         dtypes = get_numeric_types(with_complex=True)
5823         for dtype1_idx, dtype1 in enumerate(dtypes):
5824             for dtype2 in dtypes[dtype1_idx + 1:]:
5825                 assert (dtype1 == dtype2) == (str(dtype1) == str(dtype2))
5826 def test_transpose():
5827     x1 = tensor.dvector('x1')
5828     x2 = tensor.dmatrix('x2')
5829     x3 = tensor.dtensor3('x3')
5830     x1v = np.arange(24)
5831     x2v = np.arange(24).reshape(2, 12)
5832     x3v = np.arange(24).reshape(2, 3, 4)
5833     f = theano.function([x1, x2, x3], [
5834         tensor.transpose(x1),
5835         tensor.transpose(x2),
5836         tensor.transpose(x3),
5837         x1.transpose(),
5838         x2.transpose(),
5839         x3.transpose(),
5840         x2.transpose(0, 1),
5841         x3.transpose((0, 2, 1)),
5842         tensor.transpose(x2, [0, 1]),
5843         tensor.transpose(x3, [0, 2, 1]),
5844         ])
5845     t1, t2, t3, t1b, t2b, t3b, t2c, t3c, t2d, t3d = f(x1v, x2v, x3v)
5846     assert t1.shape == np.transpose(x1v).shape
5847     assert t2.shape == np.transpose(x2v).shape
5848     assert t3.shape == np.transpose(x3v).shape
5849     assert np.all(t1 == np.transpose(x1v))
5850     assert np.all(t2 == np.transpose(x2v))
5851     assert np.all(t3 == np.transpose(x3v))
5852     assert np.all(t1b == x1v.transpose())
5853     assert np.all(t2b == x2v.transpose())
5854     assert np.all(t3b == x3v.transpose())
5855     assert t2c.shape == (2, 12)
5856     assert t3c.shape == (2, 4, 3)
5857     assert np.all(t2c == x2v.transpose([0, 1]))
5858     assert np.all(t3c == x3v.transpose([0, 2, 1]))
5859     assert t2d.shape == (2, 12)
5860     assert t3d.shape == (2, 4, 3)
5861     assert np.all(t2d == np.transpose(x2v, [0, 1]))
5862     assert np.all(t3d == np.transpose(x3v, [0, 2, 1]))
5863     assert tensor.transpose(x1).name == 'x1.T'
5864     assert tensor.transpose(x2).name == 'x2.T'
5865     assert tensor.transpose(x3).name == 'x3.T'
5866     assert tensor.transpose(tensor.dmatrix()).name is None
5867 def test_stacklists():
5868     a, b, c, d = map(scalar, 'abcd')
5869     X = stacklists([[a, b],
5870                     [c, d]])
5871     f = function([a, b, c, d], X)
5872     result = f(1, 2, 3, 4)
5873     assert result.shape == (2, 2)
5874     assert np.allclose(f(1, 2, 3, 4), np.asarray([[1, 2], [3, 4]]))
5875     X = stacklists([a, b, c, d])
5876     f = function([a, b, c, d], X)
5877     result = f(1, 2, 3, 4)
5878     assert result.shape == (4,)
5879     assert np.allclose(f(1, 2, 3, 4), np.asarray([[1, 2, 3, 4]]))
5880     X = stacklists([[[a], [b]], [[c], [d]]])
5881     f = function([a, b, c, d], X)
5882     result = f(1, 2, 3, 4)
5883     assert result.shape == (2, 2, 1)
5884     a, b, c, d = [matrix(x) for x in 'abcd']
5885     X = stacklists([[a, b],
5886                     [c, d]])
5887     f = function([a, b, c, d], X)
5888     x = np.ones((4, 4), 'float32')
5889     assert f(x, x, x, x).shape == (2, 2, 4, 4)
5890 class TestSpecifyShape(unittest.TestCase):
5891     mode = None
5892     input_type = TensorType
5893     def shortDescription(self):
5894         return None
5895     def test_bad_shape(self):
5896         specify_shape = SpecifyShape()
5897         x = vector()
5898         xval = np.random.rand(2).astype(floatX)
5899         f = theano.function([x], specify_shape(x, [2]), mode=self.mode)
5900         f(xval)
5901         xval = np.random.rand(3).astype(floatX)
5902         self.assertRaises(AssertionError, f, xval)
5903         assert isinstance([n for n in f.maker.fgraph.toposort()
5904                            if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5905                           self.input_type)
5906         x = matrix()
5907         xval = np.random.rand(2, 3).astype(floatX)
5908         f = theano.function([x], specify_shape(x, [2, 3]), mode=self.mode)
5909         assert isinstance([n for n in f.maker.fgraph.toposort()
5910                            if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5911                           self.input_type)
5912         f(xval)
5913         for shape_ in [(1, 3), (2, 2), (5, 5)]:
5914             xval = np.random.rand(*shape_).astype(floatX)
5915             self.assertRaises(AssertionError, f, xval)
5916     def test_bad_number_of_shape(self):
5917         specify_shape = SpecifyShape()
5918         x = vector()
5919         shape_vec = ivector()
5920         xval = np.random.rand(2).astype(floatX)
5921         self.assertRaises(AssertionError, specify_shape, x, [])
5922         self.assertRaises(AssertionError, specify_shape, x, [2, 2])
5923         f = theano.function([x, shape_vec], specify_shape(x, shape_vec),
5924                             mode=self.mode)
5925         assert isinstance([n for n in f.maker.fgraph.toposort()
5926                            if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5927                           self.input_type)
5928         self.assertRaises(AssertionError, f, xval, [])
5929         self.assertRaises(AssertionError, f, xval, [2, 2])
5930         x = matrix()
5931         xval = np.random.rand(2, 3).astype(floatX)
5932         for shape_ in [(),
5933                        (1,),
5934                        (2, 3, 4)]:
5935             self.assertRaises(AssertionError, specify_shape, x, shape_)
5936             f = theano.function([x, shape_vec], specify_shape(x, shape_vec),
5937                                 mode=self.mode)
5938             assert isinstance([n for n in f.maker.fgraph.toposort()
5939                                if isinstance(n.op, SpecifyShape)][0].inputs[0].type,
5940                               self.input_type)
5941             self.assertRaises(AssertionError, f, xval, shape_)
5942 class TestInferShape(utt.InferShapeTester):
5943     def test_infer_shape(self):
5944         atens3 = tensor3()
5945         atens3_val = rand(4, 5, 3)
5946         for outdim in (3, 2, 1):
5947             self._compile_and_check([atens3],
5948                                     [flatten(atens3, outdim)],
5949                                     [atens3_val], Reshape,
5950                                     excluding=['local_useless_reshape'])
5951         amat = matrix()
5952         amat_val = rand(4, 5)
5953         for outdim in (2, 1):
5954             self._compile_and_check([amat],
5955                                     [flatten(amat, outdim)],
5956                                     [amat_val], Reshape,
5957                                     excluding=['local_useless_reshape'])
5958         avec = vector()
5959         avec_val = rand(4)
5960         outdim = 1
5961         self._compile_and_check([avec],
5962                                 [flatten(avec, outdim)],
5963                                 [avec_val], Reshape,
5964                                 excluding=['local_useless_reshape'])
5965         aiscal = iscalar()
5966         biscal = iscalar()
5967         ciscal = iscalar()
5968         self._compile_and_check([aiscal, biscal, ciscal],
5969                                 [Eye()(aiscal, biscal, ciscal)],
5970                                 [4, 4, 0], Eye)
5971         self._compile_and_check([aiscal, biscal, ciscal],
5972                                 [Eye()(aiscal, biscal, ciscal)],
5973                                 [4, 5, 0], Eye)
5974         self._compile_and_check([aiscal, biscal, ciscal],
5975                                 [Eye()(aiscal, biscal, ciscal)],
5976                                 [3, 5, 0], Eye)
5977         aiscal = iscalar()
5978         biscal = iscalar()
5979         ciscal = iscalar()
5980         self._compile_and_check([aiscal, biscal, ciscal],
5981                                 [Tri()(aiscal, biscal, ciscal)],
5982                                 [4, 4, 0], Tri)
5983         self._compile_and_check([aiscal, biscal, ciscal],
5984                                 [Tri()(aiscal, biscal, ciscal)],
5985                                 [4, 5, 0], Tri)
5986         self._compile_and_check([aiscal, biscal, ciscal],
5987                                 [Tri()(aiscal, biscal, ciscal)],
5988                                 [3, 5, 0], Tri)
5989         atens3 = tensor3()
5990         atens3_val = rand(4, 5, 3)
5991         atens3_diag = ExtractDiag()(atens3)
5992         self._compile_and_check([atens3], [atens3_diag],
5993                                 [atens3_val], ExtractDiag)
5994         atens3_diag = ExtractDiag(1)(atens3)
5995         self._compile_and_check([atens3], [atens3_diag],
5996                                 [atens3_val], ExtractDiag)
5997         atens3_diag = ExtractDiag(-1)(atens3)
5998         self._compile_and_check([atens3], [atens3_diag],
5999                                 [atens3_val], ExtractDiag)
6000         atens3_diag = ExtractDiag(1, 0, 2)(atens3)
6001         self._compile_and_check([atens3], [atens3_diag],
6002                                 [atens3_val], ExtractDiag)
6003         atens3_diag = ExtractDiag(1, 1, 2)(atens3)
6004         self._compile_and_check([atens3], [atens3_diag],
6005                                 [atens3_val], ExtractDiag)
6006         atens3_diag = ExtractDiag(1, 2, 0)(atens3)
6007         self._compile_and_check([atens3], [atens3_diag],
6008                                 [atens3_val], ExtractDiag)
6009         advec = dvector()
6010         advec_val = rand(4)
6011         self._compile_and_check([advec], [AllocDiag()(advec)],
6012                                 [advec_val], AllocDiag)
6013         adtens = tensor3()
6014         adtens_val = rand(4, 5, 3)
6015         self._compile_and_check([adtens],
6016                                 [Shape()(adtens)],
6017                                 [adtens_val], (opt.MakeVector, Shape))
6018         advec = dvector()
6019         bdvec = dvector()
6020         advec_val = rand(4)
6021         bdvec_val = rand(4)
6022         self._compile_and_check([advec, bdvec],
6023 <a name="18"></a>                                [Dot()(advec, bdvec)],
6024                                 [advec_val, bdvec_val],
6025                                 (Dot, tensor.blas.Dot22,
6026                                  tensor.blas<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Gemv, tensor.blas_c.CGemv))
6027         admat = dmatrix()
6028         bdmat = dmatrix()
6029         admat_val = rand(4, 5)
6030         bdmat_val = rand(5, 3)
6031         self._compile_and_check(</b></font>[admat, bdmat],
6032                                 [Dot()(admat, bdmat)],
6033                                 [admat_val, bdmat_val],
6034                                 (Dot, tensor.blas.Dot22))
6035         bdmat_val = rand(4, 5)
6036         self._compile_and_check([advec, bdmat],
6037                                 [Dot()(advec, bdmat)],
6038                                 [advec_val, bdmat_val],
6039                                 (Dot, tensor.blas.Dot22,
6040                                  tensor.blas.Gemv, tensor.blas_c.CGemv))
6041         admat_val = rand(5, 4)
6042         self._compile_and_check([admat, bdvec],
6043                                 [Dot()(admat, bdvec)],
6044                                 [admat_val, bdvec_val],
6045                                 (Dot, tensor.blas.Dot22,
6046                                  tensor.blas.Gemv, tensor.blas_c.CGemv))
6047         aivec = ivector()
6048         adtens_val = rand(4, 10, 3)
6049         aivec_val = [2, 5, 3]
6050         for aiscal_val in [1, -2]:
6051             self._compile_and_check(
6052                 [adtens, aiscal, aivec],
6053                 [Split(3)(adtens, aiscal, aivec)[0]],
6054                 [adtens_val, aiscal_val, aivec_val], (Split))
6055         cdmat = dmatrix()
6056         admat_val = rand(1, 3)
6057         bdmat_val = rand(2, 3)
6058         cdmat_val = rand(4, 3)
6059         for aiscal_val in [0, -2]:
6060             self._compile_and_check(
6061                 [aiscal, admat, bdmat, cdmat],
6062                 [Join()(aiscal, admat, bdmat, cdmat)],
6063                 [aiscal_val, admat_val, bdmat_val, cdmat_val], Join)
6064         admat_val = rand(4, 1)
6065         bdmat_val = rand(4, 3)
6066         cdmat_val = rand(4, 2)
6067         for aiscal_val in [-1, 1]:
6068             self._compile_and_check(
6069                 [aiscal, admat, bdmat, cdmat],
6070                 [Join()(aiscal, admat, bdmat, cdmat)],
6071                 [aiscal_val, admat_val, bdmat_val, cdmat_val], Join)
6072         abool = True
6073         rng = np.random.RandomState(utt.fetch_seed())
6074         advec_val = rand(5)
6075         aivec_val = rng.permutation(5).astype('int32')
6076         self._compile_and_check([advec, aivec],
6077                                 [PermuteRowElements()(advec, aivec, abool)],
6078                                 [advec_val, aivec_val], PermuteRowElements)
6079         admat_val = rand(3, 5)
6080         self._compile_and_check([admat, aivec],
6081                                 [PermuteRowElements()(admat, aivec, abool)],
6082                                 [admat_val, aivec_val], PermuteRowElements)
6083         adtens3 = dtensor3()
6084         adtens3_val = rand(3, 2, 5)
6085         self._compile_and_check([adtens3, aivec],
6086                                 [PermuteRowElements()(adtens3, aivec, abool)],
6087                                 [adtens3_val, aivec_val], PermuteRowElements)
6088         aimat = imatrix()
6089         perma = rng.permutation(5).astype('int32')
6090         permb = rng.permutation(5).astype('int32')
6091         permc = rng.permutation(5).astype('int32')
6092         aimat_val = np.vstack((perma, permb, permc))
6093         admat_val = rand(3, 5)
6094         self._compile_and_check([admat, aimat],
6095                                 [PermuteRowElements()(admat, aimat, abool)],
6096                                 [admat_val, aimat_val], PermuteRowElements)
6097         aitens3 = itensor3()
6098         perma = rng.permutation(5).astype('int32')
6099         permb = rng.permutation(5).astype('int32')
6100         permc = rng.permutation(5).astype('int32')
6101         bimat_val = np.vstack((perma, permb, permc))
6102         aitens3_val = np.empty((2, 3, 5), 'int32')
6103         aitens3_val[0, ::, ::] = aimat_val
6104         aitens3_val[1, ::, ::] = bimat_val
6105         self._compile_and_check([admat, aitens3],
6106                                 [PermuteRowElements()(admat, aitens3, abool)],
6107                                 [admat_val, aitens3_val], PermuteRowElements)
6108         aiscal = iscalar()
6109         self._compile_and_check([aiscal],
6110                                 [TensorFromScalar()(ScalarFromTensor()(aiscal))],
6111                                 [45], ScalarFromTensor,
6112                                 excluding=["local_tensor_scalar_tensor"])
6113         aiscal = scal.float64()
6114         self._compile_and_check([aiscal],
6115                                 [TensorFromScalar()(aiscal)],
6116                                 [4.], TensorFromScalar)
6117         adtens4 = dtensor4()
6118         adict = [(0, False), (1, True), (2, False), (3, True)]
6119         adtens4_val = rand(2, 1, 3, 1)
6120         self._compile_and_check([adtens4],
6121                                 [Rebroadcast(*adict)(adtens4)],
6122                                 [adtens4_val], Rebroadcast,
6123                                 warn=False)
6124         adtens4_bro = TensorType('float64', (True, True, True, False))()
6125         bdict = [(0, True), (1, False), (2, False), (3, False)]
6126         adtens4_bro_val = rand(1, 1, 1, 3)
6127         self._compile_and_check([adtens4_bro],
6128                                 [Rebroadcast(*bdict)(adtens4_bro)],
6129                                 [adtens4_bro_val], Rebroadcast)
6130 <a name="13"></a>
6131         randint = np.random.randint
6132         adscal <font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= dscalar()
6133         aiscal = lscalar()
6134         biscal = lscalar()
6135         ciscal = lscalar()
6136         discal = lscalar()
6137         adscal_val = rand()
6138         aiscal_val = randint(</b></font>3, 6, size=())
6139         biscal_val = randint(3, 6, size=())
6140         ciscal_val = randint(3, 6, size=())
6141         discal_val = randint(3, 6, size=())
6142         self._compile_and_check(
6143             [adscal, aiscal, biscal, ciscal, discal],
6144             [Alloc()(adscal, aiscal, biscal, ciscal, discal)],
6145             [adscal_val, aiscal_val, biscal_val, ciscal_val, discal_val],
6146             Alloc)
6147         adtens3_val = rand(4, 5, 3)
6148         self._compile_and_check([adtens3],
6149                                 max_and_argmax(adtens3, None),
6150                                 [adtens3_val], MaxAndArgmax)
6151         self._compile_and_check([adtens3],
6152                                 max_and_argmax(adtens3, 0),
6153                                 [adtens3_val], MaxAndArgmax)
6154         self._compile_and_check([adtens3],
6155                                 max_and_argmax(adtens3, 1),
6156                                 [adtens3_val], MaxAndArgmax)
6157         self._compile_and_check([adtens3],
6158                                 max_and_argmax(adtens3, 2),
6159                                 [adtens3_val], MaxAndArgmax)
6160         self._compile_and_check([adtens3],
6161                                 max_and_argmax(adtens3, [0, 1, 2]),
6162                                 [adtens3_val], MaxAndArgmax)
6163         self._compile_and_check([aiscal, biscal, ciscal],
6164                                 [ARange('int64')(aiscal, biscal, ciscal)],
6165                                 [0, 5, 1], ARange)
6166         self._compile_and_check([aiscal, biscal, ciscal],
6167                                 [ARange('int64')(aiscal, biscal, ciscal)],
6168                                 [2, 11, 4], ARange)
6169         self._compile_and_check([aiscal, biscal, ciscal],
6170                                 [ARange('int64')(aiscal, biscal, ciscal)],
6171                                 [-5, 1, 1], ARange)
6172         self._compile_and_check([aiscal, biscal, ciscal],
6173                                 [ARange('int64')(aiscal, biscal, ciscal)],
6174                                 [10, 2, -2], ARange)
6175         self._compile_and_check([aiscal, biscal, ciscal],
6176                                 [ARange('int64')(aiscal, biscal, ciscal)],
6177                                 [10, 2, 2], ARange)
6178         self._compile_and_check([aiscal, biscal, ciscal],
6179                                 [ARange('int64')(aiscal, biscal, ciscal)],
6180                                 [0, 0, 1], ARange)
6181         aivec_val = [3, 4, 2, 5]
6182         adtens4_val = rand(*aivec_val)
6183         self._compile_and_check([adtens4, aivec],
6184                                 [SpecifyShape()(adtens4, aivec)],
6185                                 [adtens4_val, aivec_val], SpecifyShape)
6186         adtens3_val = rand(3, 4, 5)
6187         aiscal_val = 2
6188         self._compile_and_check([adtens3],
6189                                 [Mean(None)(adtens3)],
6190                                 [adtens3_val], Mean)
6191         self._compile_and_check([adtens3],
6192                                 [Mean(aiscal_val)(adtens3)],
6193                                 [adtens3_val], Mean)
6194         admat = dmatrix()
6195         aivec = ivector()
6196         ndim = 1
6197         admat_val = rand(3, 4)
6198         self._compile_and_check([admat],
6199                                 [Reshape(ndim)(admat, [12])],
6200                                 [admat_val], Reshape)
6201         self._compile_and_check([admat],
6202                                 [Reshape(ndim)(admat, [-1])],
6203                                 [admat_val], Reshape)
6204         ndim = 2
6205         self._compile_and_check([admat],
6206                                 [Reshape(ndim)(admat, [4, 3])],
6207                                 [admat_val], Reshape)
6208         self._compile_and_check([admat],
6209                                 [Reshape(ndim)(admat, [4, -1])],
6210                                 [admat_val], Reshape)
6211         self._compile_and_check([admat],
6212                                 [Reshape(ndim)(admat, [3, -1])],
6213                                 [admat_val], Reshape)
6214         self._compile_and_check([admat],
6215                                 [Reshape(ndim)(admat, [-1, 3])],
6216                                 [admat_val], Reshape)
6217         self._compile_and_check([admat],
6218                                 [Reshape(ndim)(admat, [-1, 4])],
6219                                 [admat_val], Reshape)
6220         adtens4 = dtensor4()
6221         ndim = 4
6222         adtens4_val = rand(2, 4, 3, 5)
6223         self._compile_and_check([adtens4],
6224                                 [Reshape(ndim)(adtens4, [1, -1, 10, 4])],
6225                                 [adtens4_val], Reshape)
6226         self._compile_and_check([adtens4],
6227                                 [Reshape(ndim)(adtens4, [1, 3, 10, 4])],
6228                                 [adtens4_val], Reshape)
6229         advec = dvector()
6230         advec_val = rand(5)
6231         aivec_val = [3]
6232         ndim = 1
6233         self._compile_and_check([advec],
6234                                 [Tile(ndim)(advec, aivec_val)],
6235                                 [advec_val], Tile)
6236         admat = dmatrix()
6237         admat_val = rand(2, 4)
6238         aivec_val = [2, 3]
6239         ndim = 2
6240         self._compile_and_check([admat],
6241                                 [Tile(ndim)(admat, aivec_val)],
6242                                 [admat_val], Tile)
6243         adtens4 = dtensor4()
6244         adtens4_val = rand(2, 4, 3, 5)
6245         aivec_val = [2, 3, 1, 4]
6246         ndim = 4
6247         self._compile_and_check([adtens4],
6248                                 [Tile(ndim)(adtens4, aivec_val)],
6249                                 [adtens4_val], Tile)
6250 class TestTensorInstanceMethods(unittest.TestCase):
6251     def setUp(self):
6252         self.vars = matrices('X', 'Y')
6253         self.vals = [m.astype(floatX) for m in [rand(2, 2), rand(2, 2)]]
6254     def test_argmin(self):
6255         X, _ = self.vars
6256         x, _ = self.vals
6257         assert_array_equal(X.argmin().eval({X: x}), x.argmin())
6258     def test_argmax(self):
6259         X, _ = self.vars
6260         x, _ = self.vals
6261         assert_array_equal(X.argmax().eval({X: x}), x.argmax())
6262     def test_argsort(self):
6263         X, _ = self.vars
6264         x, _ = self.vals
6265         assert_array_equal(X.argsort().eval({X: x}), x.argsort())
6266         assert_array_equal(X.argsort(1).eval({X: x}), x.argsort(1))
6267     def test_clip(self):
6268         X, Y = self.vars
6269         x, y = self.vals
6270         Z = X.clip(Y - 0.5, Y + 0.5)
6271         z = x.clip(y - 0.5, y + 0.5)
6272         assert_array_equal(Z.eval({X: x, Y: y}), z)
6273     def test_dot(self):
6274         X, Y = self.vars
6275         x, y = self.vals
6276         assert_allclose(x.dot(y), X.dot(Y).eval({X: x, Y: y}))
6277         Z = X.dot(Y)
6278         z = x.dot(y)
6279         assert_allclose(x.dot(z), X.dot(Z).eval({X: x, Z: z}))
6280     def test_real_imag(self):
6281         X, Y = self.vars
6282         x, y = self.vals
6283         Z = X + Y * 1j
6284         z = x + y * 1j
6285         assert_array_equal(Z.real.eval({Z: z}), x)
6286         assert_array_equal(Z.imag.eval({Z: z}), y)
6287     def test_conj(self):
6288         X, Y = self.vars
6289         x, y = self.vals
6290         Z = X + Y * 1j
6291         z = x + y * 1j
6292         assert_array_equal(Z.conj().eval({Z: z}), z.conj())
6293         assert_array_equal(Z.conjugate().eval({Z: z}), z.conj())
6294     def test_round(self):
6295         X, _ = self.vars
6296         x, _ = self.vals
6297         assert_array_equal(X.round().eval({X: x}), x.round())
6298     def test_std(self):
6299         X, _ = self.vars
6300         x, _ = self.vals
6301         assert_allclose(X.std().eval({X: x}), x.std())
6302     def test_repeat(self):
6303         X, _ = self.vars
6304         x, _ = self.vals
6305         assert_array_equal(X.repeat(2).eval({X: x}), x.repeat(2))
6306     def test_trace(self):
6307         X, _ = self.vars
6308         x, _ = self.vals
6309         assert_array_equal(X.trace().eval({X: x}), x.trace())
6310     def test_ravel(self):
6311         X, _ = self.vars
6312         x, _ = self.vals
6313         assert_array_equal(X.ravel().eval({X: x}), x.ravel())
6314     def test_diagonal(self):
6315         X, _ = self.vars
6316         x, _ = self.vals
6317         assert_array_equal(X.diagonal().eval({X: x}), x.diagonal())
6318         assert_array_equal(X.diagonal(1).eval({X: x}), x.diagonal(1))
6319         assert_array_equal(X.diagonal(-1).eval({X: x}), x.diagonal(-1))
6320         for offset, axis1, axis2 in [(1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0)]:
6321             assert_array_equal(X.diagonal(offset, axis1, axis2).eval({X: x}),
6322                                x.diagonal(offset, axis1, axis2))
6323     def test_take(self):
6324         X, _ = self.vars
6325         x, _ = self.vals
6326         indices = [1, 0, 3]
6327         assert_array_equal(X.take(indices).eval({X: x}), x.take(indices))
6328         indices = [1, 0, 1]
6329         assert_array_equal(X.take(indices, 1).eval({X: x}), x.take(indices, 1))
6330         indices = np.array([-10, 5, 12], dtype='int32')
6331         assert_array_equal(X.take(indices, 1, mode='wrap').eval({X: x}),
6332                            x.take(indices, 1, mode='wrap'))
6333         assert_array_equal(X.take(indices, -1, mode='wrap').eval({X: x}),
6334                            x.take(indices, -1, mode='wrap'))
6335         assert_array_equal(X.take(indices, 1, mode='clip').eval({X: x}),
6336                            x.take(indices, 1, mode='clip'))
6337         assert_array_equal(X.take(indices, -1, mode='clip').eval({X: x}),
6338                            x.take(indices, -1, mode='clip'))
6339         self.assertRaises(IndexError, X.take(indices).eval, {X: x})
6340         self.assertRaises(IndexError, (2 * X.take(indices)).eval, {X: x})
6341         self.assertRaises(TypeError, X.take, [0.0])
6342         indices = [[1, 0, 1], [0, 1, 1]]
6343         assert_array_equal(X.take(indices, 1).eval({X: x}), x.take(indices, 1))
6344         assert_array_equal(X[:, indices].eval({X: x}), x[:, indices])
6345     def test_cumsum(self):
6346         X, _ = self.vars
6347         x, _ = self.vals
6348         assert_array_equal(X.cumsum().eval({X: x}), x.cumsum())
6349     def test_cumprod(self):
6350         X, _ = self.vars
6351         x, _ = self.vals
6352         assert_array_equal(X.cumprod().eval({X: x}), x.cumprod())
6353 def test_norm():
6354     x = theano.tensor.vector('x')
6355     n = x.norm(2)
6356     f = theano.function([x], n)
6357     assert np.allclose(f([1, 1]), np.sqrt(2))
6358 class test_cov(unittest.TestCase):
6359     def test_core(self):
6360         x = theano.tensor.matrix('x')
6361         c = theano.tensor.cov(x)
6362         f = theano.function([x], c)
6363         data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6364         assert np.allclose(f(data), np.cov(data))
6365         data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6366         assert np.allclose(f(data), np.cov(data))
6367         data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6368         assert np.allclose(f(data), np.cov(data))
6369         data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6370         assert np.allclose(f(data), np.cov(data))
6371         data = np.asarray(np.random.rand(1, 2), dtype=config.floatX)
6372         assert np.allclose(f(data), np.cov(data))
6373     def test_rowvar(self):
6374         for rowvar in [True, False]:
6375             x = theano.tensor.matrix('x')
6376             c = theano.tensor.cov(x, rowvar=rowvar)
6377             f = theano.function([x], c)
6378             data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6379             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6380             data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6381             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6382             data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6383             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6384             data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6385             assert np.allclose(f(data), np.cov(data, rowvar=rowvar))
6386         x = theano.tensor.matrix('x')
6387         c = theano.tensor.cov(x, rowvar=False)
6388         f = theano.function([x], c)
6389         data = np.asarray(np.random.rand(2, 1), dtype=config.floatX)
6390         assert np.allclose(f(data), np.cov(data, rowvar=False))
6391 <a name="16"></a>
6392     def test_y(self):
6393         x <font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.matrix('x')
6394         y = theano.tensor.matrix('y')
6395         c = theano.tensor.cov(x, y=</b></font>y)
6396         f = theano.function([x, y], c)
6397         data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6398         y = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6399         assert np.allclose(f(data, y), np.cov(data, y=y))
6400         data = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6401         y = np.asarray(np.random.rand(5, 3), dtype=config.floatX)
6402         assert np.allclose(f(data, y), np.cov(data, y=y))
6403         data = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6404         y = np.asarray(np.random.rand(10, 10), dtype=config.floatX)
6405         assert np.allclose(f(data, y), np.cov(data, y=y))
6406         data = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6407         y = np.asarray(np.random.rand(2, 2), dtype=config.floatX)
6408         assert np.allclose(f(data, y), np.cov(data, y=y))
6409     def test_ddof(self):
6410         for ddof in range(0, 5):
6411             x = theano.tensor.matrix('x')
6412             c = theano.tensor.cov(x, ddof=ddof)
6413             f = theano.function([x], c)
6414             data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6415             assert np.allclose(f(data), np.cov(data, ddof=ddof))
6416     def test_bias(self):
6417         for bias in [True, False]:
6418             x = theano.tensor.matrix('x')
6419             c = theano.tensor.cov(x, bias=bias)
6420             f = theano.function([x], c)
6421             data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6422             assert np.allclose(f(data), np.cov(data, bias=bias))
6423         for ddof in range(0, 5):
6424             for bias in [True, False]:
6425                 x = theano.tensor.matrix('x')
6426                 c = theano.tensor.cov(x, ddof=ddof, bias=bias)
6427                 f = theano.function([x], c)
6428                 data = np.asarray(np.random.rand(3, 5), dtype=config.floatX)
6429                 assert np.allclose(f(data), np.cov(data, ddof=ddof, bias=bias))
6430 class test_ptp(unittest.TestCase):
6431     def test_scalar(self):
6432         x = scalar('x')
6433         p = ptp(x)
6434         f = theano.function([x], p)
6435         y = np.asarray(rand() * 2000 - 1000, dtype=config.floatX)
6436         result = f(y)
6437         numpyResult = np.ptp(y)
6438         self.assertTrue(np.array_equal(result, numpyResult))
6439     def test_vector(self):
6440         x = vector('x')
6441         p = ptp(x, 0)
6442         f = theano.function([x], p)
6443         y = rand_ranged(-1000, 1000, [100])
6444         result = f(y)
6445         numpyResult = np.ptp(y, 0)
6446         self.assertTrue(np.array_equal(result, numpyResult))
6447     def test_matrix_first_axis(self):
6448         x = matrix('x')
6449         p = ptp(x, 1)
6450         f = theano.function([x], p)
6451         y = rand_ranged(-1000, 1000, [100, 100])
6452         result = f(y)
6453         numpyResult = np.ptp(y, 1)
6454         self.assertTrue(np.array_equal(result, numpyResult))
6455     def test_matrix_second_axis(self):
6456         x = matrix('x')
6457         p = ptp(x, 0)
6458         f = theano.function([x], p)
6459         y = rand_ranged(-1000, 1000, [100, 100])
6460         result = f(y)
6461         numpyResult = np.ptp(y, 0)
6462         self.assertTrue(np.array_equal(result, numpyResult))
6463     def test_matrix_neg_axis(self):
6464         x = matrix('x')
6465         p = ptp(x, -1)
6466         f = theano.function([x], p)
6467         y = rand_ranged(-1000, 1000, [100, 100])
6468         result = f(y)
6469         numpyResult = np.ptp(y, -1)
6470         self.assertTrue(np.array_equal(result, numpyResult))
6471     def test_matrix_no_axis(self):
6472         x = matrix('x')
6473         p = ptp(x)
6474         f = theano.function([x], p)
6475         y = rand_ranged(-1000, 1000, [100, 100])
6476         result = f(y)
6477         numpyResult = np.ptp(y)
6478         self.assertTrue(np.array_equal(result, numpyResult))
6479     def test_interface(self):
6480         x = matrix('x')
6481         p = x.ptp(1)
6482         f = theano.function([x], p)
6483         y = rand_ranged(-1000, 1000, [100, 100])
6484         result = f(y)
6485         numpyResult = np.ptp(y, 1)
6486         self.assertTrue(np.array_equal(result, numpyResult))
6487 if __name__ == '__main__':
6488     t = TestInferShape('setUp')
6489     t.setUp()
6490     t.test_infer_shape()
6491 class T_swapaxes(unittest.TestCase):
6492     def test_no_dimensional_input(self):
6493         self.assertRaises(IndexError, swapaxes, 2, 0, 1)
6494     def test_unidimensional_input(self):
6495         self.assertRaises(IndexError, swapaxes, [2, 1], 0, 1)
6496     def test_not_enough_dimension(self):
6497         self.assertRaises(IndexError, swapaxes, [[2, 1], [3, 4]], 3, 4)
6498     def test_doubleswap(self):
6499         y = matrix()
6500         n = swapaxes(y, 0, 1)
6501         f = function([y], n)
6502         testMatrix = [[2, 1], [3, 4]]
6503         self.assertTrue(np.array_equal(testMatrix, f(f(testMatrix))))
6504     def test_interface(self):
6505         x = theano.tensor.matrix()
6506         x.swapaxes(0, 1)
6507     def test_numpy_compare(self):
6508         rng = np.random.RandomState(utt.fetch_seed())
6509         A = tensor.matrix("A", dtype=theano.config.floatX)
6510         Q = swapaxes(A, 0, 1)
6511         fn = function([A], [Q])
6512         a = rng.rand(4, 4).astype(theano.config.floatX)
6513         n_s = np.swapaxes(a, 0, 1)
6514         t_s = fn(a)
6515         assert np.allclose(n_s, t_s)
6516 class T_Power(unittest.TestCase):
6517     def test_numpy_compare(self):
6518         rng = np.random.RandomState(utt.fetch_seed())
6519         A = tensor.matrix("A", dtype=theano.config.floatX)
6520         Q = power(A, 3)
6521         fn = function([A], [Q])
6522         a = rng.rand(4, 4).astype(theano.config.floatX)
6523         n_p = np.power(a, 3)
6524         t_p = fn(a)
6525         assert np.allclose(n_p, t_p)
6526     def test_multiple_power(self):
6527         x = tensor.vector()
6528         y = [1, 2, 3]
6529         z = power(x, y)
6530         f = function([x], z)
6531         assert np.allclose(f([1, 2, 3]), [1, 4, 27])
6532     def test_wrong_shape(self):
6533         x = tensor.vector()
6534         y = [1, 2, 3]
6535         z = power(x, y)
6536         f = function([x], z)
6537         self.assertRaises(ValueError, f, [1, 2, 3, 4])
6538 class T_Choose(utt.InferShapeTester):
6539     op = staticmethod(choose)
6540     op_class = Choose
6541     modes = ['raise', 'wrap', 'clip']
6542     def test_numpy_compare(self):
6543         a = tensor.vector(dtype='int32')
6544         b = tensor.matrix(dtype='float32')
6545         A = np.random.randint(0, 4, 4).astype('int32')
6546         B = np.asarray(np.random.rand(4, 4), dtype='float32')
6547         for m in self.modes:
6548             f = function([a, b], choose(a, b, mode=m))
6549             t_c = f(A, B)
6550             n_c = np.choose(A, B, mode=m)
6551             assert np.allclose(t_c, n_c)
6552     def test_method(self):
6553         a = tensor.vector(dtype='int32')
6554         b = tensor.matrix(dtype='float32')
6555         A = np.random.randint(0, 4, 4).astype('int32')
6556         B = np.asarray(np.random.rand(4, 4), dtype='float32')
6557         for m in self.modes:
6558             f = function([a, b], a.choose(b, mode=m))
6559             t_c = f(A, B)
6560             n_c = A.choose(B, mode=m)
6561             assert np.allclose(t_c, n_c)
6562     def test_broadcasted(self):
6563         a = tensor.scalar(dtype='int32')
6564         b = tensor.matrix(dtype='float32')
6565         A = 3
6566         B = np.asarray(np.random.rand(4, 4), dtype='float32')
6567         for m in self.modes:
6568             f = function([a, b], choose(a, b, mode=m))
6569             t_c = f(A, B)
6570             n_c = np.choose(A, B, mode=m)
6571             assert np.allclose(t_c, n_c)
6572         b = theano.tensor.col(dtype='float32')
6573         B = np.asarray(np.random.rand(4, 1), dtype='float32')
6574         for m in self.modes:
6575             f = function([a, b], choose(a, b, mode=m))
6576             assert choose(a, b, mode=m).broadcastable[0]
6577             t_c = f(A, B)
6578             n_c = np.choose(A, B, mode=m)
6579             assert np.allclose(t_c, n_c)
6580     def test_dtype_error(self):
6581         a = tensor.scalar(dtype='float32')
6582         b = tensor.matrix(dtype='float32')
6583         self.assertRaises(TypeError, choose, a, b)
6584     def test_numpy_compare_tuple(self):
6585         a = tensor.tensor3(dtype='int32')
6586         b = tensor.tensor3(dtype='float32')
6587         c = tensor.tensor3(dtype='float32')
6588         A = np.random.randint(0, 2, (2, 1, 1)).astype('int32')
6589         B = np.asarray(np.random.rand(1, 6, 1), dtype='float32')
6590         C = np.asarray(np.random.rand(1, 1, 5), dtype='float32')
6591         for m in self.modes:
6592             f = function([a, b, c], choose(a, (b, c), mode=m))
6593             t_c = f(A, B, C)
6594             n_c = np.choose(A, (B, C), mode=m)
6595             assert np.allclose(t_c, n_c)
6596     def test_infer_shape(self):
6597         for shp1, shp2 in [
6598             ((5, 4), (7, 4)),
6599             ((1, 4), (7, 4)),
6600             ((5, 1), (7, 4)),
6601             ((5, 4), (1, 4)),
6602             ((5, 4), (7, 1)),
6603             ((5, 4), (4,)),
6604             ((1, 4), (4,)),
6605             ((5, 1), (4,)),
6606             ((5, 4), (1,)),
6607             ((4,), (5, 4)),
6608             ((1,), (5, 4)),
6609             ((4,), (1, 4)),
6610             ((4,), (3, 1)),
6611             ((4,), (4,)),
6612             ((1,), (4,)),
6613             ((4,), (1,)),
6614             ((1,), (1,)),
6615         ]:
6616             a = tensor.tensor(dtype='int32',
6617                               broadcastable=[n == 1 for n in shp1])
6618             c = tensor.tensor(dtype='float32',
6619                               broadcastable=[n == 1 for n in shp2])
6620             A = np.asarray(np.random.rand(*shp1) * shp2[0], dtype='int32')
6621             C = np.asarray(np.random.rand(*shp2) * shp2[0], dtype='float32')
6622             self._compile_and_check([a, c],  # theano.function inputs
6623                                     [self.op(a, c)],  # theano.function outputs
6624                                     [A, C],
6625                                     self.op_class)
6626     def ___test_infer_shape_tuple(self):
6627         a <font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= tensor.tensor3(dtype='int32')
6628         b = tensor.tensor3(dtype='int32')
6629         c = tensor.tensor3(dtype='int32')
6630         A = np.asarray(</b></font>[1, 0], dtype='int32').reshape((2, 1, 1))
6631         B = np.asarray(np.random.rand(1, 4, 1), dtype='int32')
6632         C = np.asarray(np.random.rand(1, 1, 7), dtype='int32')
6633         f = function([a, b, c], choose(a, (b, c)))
6634         shape = (2, 4, 7)
6635         assert np.allclose(f(A, B, C).shape, shape)
6636         self._compile_and_check([a, b, c],  # theano.function inputs
6637                                 [self.op(a, (b, c))],  # theano.function outputs
6638                                 [A, B, C],
6639                                 self.op_class)
6640 def test_allocempty():
6641     f = theano.function([], AllocEmpty("float32")(2, 3))
6642     assert len(f.maker.fgraph.apply_nodes) == 1
6643     out = f()
6644     assert out.shape == (2, 3)
6645     assert out.dtype == 'float32'
6646 def test_symbolic_slice():
6647     x = theano.tensor.tensor4('x')
6648     a, b = x.shape[:2]
6649     output = a.eval({x: np.zeros((5, 4, 3, 2), dtype=theano.config.floatX)})
6650     assert output == np.array(5)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
