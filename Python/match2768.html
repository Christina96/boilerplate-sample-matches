<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for opt.py &amp; test_blas_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for opt.py &amp; test_blas_1.py
      </h3>
<h1 align="center">
        3.2%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>opt.py (8.013029%)<th>test_blas_1.py (2.0157325%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(662-671)<td><a href="#" name="0">(2001-2011)</a><td align="center"><font color="#ff0000">23</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1738-1744)<td><a href="#" name="1">(730-740)</a><td align="center"><font color="#d20000">19</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(1763-1772)<td><a href="#" name="2">(1808-1817)</a><td align="center"><font color="#bc0000">17</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(703-712)<td><a href="#" name="3">(1817-1826)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(1555-1559)<td><a href="#" name="4">(932-934)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(290-291)<td><a href="#" name="5">(608-612)</a><td align="center"><font color="#850000">12</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(264-267)<td><a href="#" name="6">(2037-2041)</a><td align="center"><font color="#850000">12</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(254-256)<td><a href="#" name="7">(453-455)</a><td align="center"><font color="#850000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>opt.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import numpy as np
3 import scipy
4 import theano
5 from theano import gof, scalar, tensor
6 from theano.compat import izip
7 from theano.tensor import blas
8 from theano.tensor.opt import register_specialize, register_canonicalize
9 from theano.sparse import (CSC, CSR, csm_properties,
10                            csm_grad, usmm, csm_indices, csm_indptr,
11                            csm_data)
12 from theano.sparse import basic as sparse
13 _is_sparse_variable = sparse._is_sparse_variable
14 _is_dense = sparse._is_dense
15 @gof.local_optimizer([csm_properties])
16 def local_csm_properties_csm(node):
17     if node.op == csm_properties:
18         csm, = node.inputs
19         if csm.owner and (csm.owner.op == CSC or csm.owner.op == CSR):
20             ret_var = [theano.tensor.patternbroadcast(i, o.broadcastable)
21                        for i, o in izip(csm.owner.inputs, node.outputs)]
22             return ret_var
23     return False
24 register_specialize(local_csm_properties_csm)
25 @gof.local_optimizer([sparse.Remove0])
26 def local_inplace_remove0(node):
27     if isinstance(node.op, sparse.Remove0) and not node.op.inplace:
28         new_op = node.op.__class__(inplace=True)
29         new_node = new_op(*node.inputs)
30         return [new_node]
31     return False
32 theano.compile.optdb.register(
33     'local_inplace_remove0',
34     gof.TopoOptimizer(local_inplace_remove0,
35                       failure_callback=gof.TopoOptimizer.warn_inplace),
36     60, 'fast_run', 'inplace')
37 class AddSD_ccode(gof.op.Op):
38     __props__ = ("format", "inplace")
39     def __init__(self, format, inplace=False, *args, **kwargs):
40         gof.Op.__init__(self, *args, **kwargs)
41         self.inplace = inplace
42         self.format = format
43         if self.inplace:
44             self.destroy_map = {0: [3]}
45     def __str__(self):
46         inp = ''
47         if self.inplace:
48             inp = ',inplace'
49         return "%s{%s%s}" % (self.__class__.__name__,
50                              self.format, inp)
51     def make_node(self, x, y):
52         x, y = sparse.as_sparse_variable(x), tensor.as_tensor_variable(y)
53         out_dtype = scalar.upcast(x.type.dtype, y.type.dtype)
54         if self.inplace:
55             assert out_dtype == y.dtype
56         indices, indptr, data = csm_indices(x), csm_indptr(x), csm_data(x)
57         assert self.format == x.type.format
58         assert y.type.ndim == 2
59         out = tensor.TensorType(dtype=out_dtype,
60                                 broadcastable=y.type.broadcastable)()
61         return gof.Apply(self,
62                          [data, indices, indptr, y],
63                          [out])
64     def c_code(self, node, name, inputs, outputs, sub):
65         (_data, _indices, _indptr, y) = inputs
66         (z,) = outputs
67         inplace = int(self.inplace)
68         format = {'csc': 0, 'csr': 1}[self.format]
69         out_typenum = node.outputs[0].type.dtype_specs()[2]
70         code = """
71                 Py_XDECREF(%(z)s);
72                 if (!%(inplace)s){
73                     if(PyArray_TYPE(%(y)s) != %(out_typenum)s){
74                         %(z)s = (PyArrayObject *) PyArray_FromArray(%(y)s,  PyArray_DescrFromType(%(out_typenum)s), 0);
75                     }else{
76                         %(z)s = (PyArrayObject *) PyArray_NewCopy(%(y)s, NPY_CORDER);
77                     }
78                 }else{
79                   %(z)s = %(y)s;
80                   Py_XINCREF(%(z)s);
81                 }
82                 npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
83                 const dtype_%(_indptr)s* __restrict__ indptr = (dtype_%(_indptr)s*)PyArray_DATA(%(_indptr)s);
84                 const dtype_%(_indices)s* __restrict__ indices = (dtype_%(_indices)s*)PyArray_DATA(%(_indices)s);
85                 const dtype_%(_data)s* __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
86                 dtype_%(y)s* ydata = (dtype_%(y)s*)PyArray_DATA(%(y)s);
87                 dtype_%(z)s* zdata = (dtype_%(z)s*)PyArray_DATA(%(z)s);
88                 npy_intp Yi = PyArray_STRIDES(%(y)s)[0]/PyArray_DESCR(%(y)s)-&gt;elsize;
89                 npy_intp Yj = PyArray_STRIDES(%(y)s)[1]/PyArray_DESCR(%(y)s)-&gt;elsize;
90                 npy_intp pos;
91                 if (%(format)s == 0){
92                 for (npy_intp col = 0; col &lt; N; ++col){
93                   for (dtype_%(_indptr)s ind = indptr[col]; ind &lt; indptr[col+1]; ++ind){
94                     npy_intp row = indices[ind];
95                     pos = row * Yi + col * Yj;
96                     zdata[pos] = ydata[pos] + data[ind];
97                   }
98                 }
99                 }else{
100                 for (npy_intp row = 0; row &lt; N; ++row){
101                   for (dtype_%(_indptr)s ind = indptr[row]; ind &lt; indptr[row+1]; ++ind){
102                     npy_intp col = indices[ind];
103                     pos = row * Yi + col * Yj;
104                     zdata[pos] = ydata[pos] + data[ind];
105                   }
106                  }
107                 }
108     Optimization to insert inplace versions of AddSD.
109     Convert AddSD to faster AddSD_ccode.
110     Structured Dot CSC is like dot, except that only the gradient wrt non-zero
111     elements of the sparse matrix `a` are calculated and propagated.
112     The output is presumed to be a dense matrix, and is represented by a
113     TensorType instance.
114     Parameters
115     ----------
116     a
117         A sparse matrix in csc format.
118     b
119         A sparse or dense matrix.
120     Returns
121     -------
122     The dot product of `a` and `b`.
123     Notes
124     -----
125     The grad implemented is structured.
126     This op is used as an optimization for StructuredDot.
127     """
128         dtype_out = scalar.upcast<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(a_val.type.dtype, b.type.dtype)
129         r = gof.Apply(self, [a_val, a_ind, a_ptr, a_nrows, b],
130                       [tensor.tensor(</b></font>dtype_out,
131                                      (False, b.type.broadcastable[1]))])
132         return r
133     def perform(self, node, inputs, outputs):
134         (out,) = outputs
135         a = scipy.sparse.csc_matrix((a_val, a_ind, a_ptr),
136                                     (a_nrows, b<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.shape[0]),
137                                     copy=False)
138         out[0] = theano._asarray(a * b, dtype=node.outputs[0].type.</b></font>dtype)
139         assert _is_dense(out[0])  # scipy 0.7 automatically converts to dense
140     def c_code(self, node, name, inputs, outputs, sub):
141         (a_val, a_ind, a_ptr, a_nrows, b) = inputs
142         (z,) = outputs
143         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
144             raise NotImplementedError('Complex types are not supported for a_val')
145             raise NotImplementedError('Complex types are not supported for b')
146         typenum_z = node.outputs<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[0].type.dtype_specs()[2]  # retrieve dtype number
147         typenum_a_val = node.inputs[0].type.dtype_specs()[</b></font>2]  # retrieve dtype number
148         typenum_b = node.inputs[4].type.dtype_specs()[2]  # retrieve dtype number
149         rval = """
150         if (PyArray_NDIM(%(a_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_val) != 1"); %(fail)s;}
151         if (PyArray_NDIM(%(a_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ind) != 1"); %(fail)s;}
152         if (PyArray_NDIM(%(a_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ptr) != 1"); %(fail)s;}
153         if (PyArray_NDIM(%(a_nrows)s) != 0) {PyErr_SetString(PyExc_NotImplementedError, "rank(nrows) != 0"); %(fail)s;}
154         if (PyArray_NDIM(%(b)s) != 2) {PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 2"); %(fail)s;}
155         if (PyArray_TYPE(%(a_val)s) != %(typenum_a_val)s) {
156         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for a_val"); %(fail)s;}
157         if (PyArray_TYPE(%(b)s) != %(typenum_b)s) {
158         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for b"); %(fail)s;}
159         if (PyArray_TYPE(%(a_ind)s) != NPY_INT32) {
160         PyErr_SetString(PyExc_NotImplementedError, "a_ind dtype not INT32"); %(fail)s;}
161         if (PyArray_TYPE(%(a_ptr)s) != NPY_INT32)
162         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr dtype not INT32"); %(fail)s;}
163         if (PyArray_TYPE(%(a_nrows)s) != NPY_INT32)
164         {PyErr_SetString(PyExc_NotImplementedError, "a_nrows dtype not INT32"); %(fail)s;}
165         if (PyArray_DIMS(%(a_val)s)[0] != PyArray_DIMS(%(a_ind)s)[0])
166         {PyErr_SetString(PyExc_NotImplementedError, "a_val and a_ind have different lengths"); %(fail)s;}
167         if (PyArray_DIMS(%(a_ptr)s)[0] != PyArray_DIMS(%(b)s)[0]+1)
168         {PyErr_SetString(PyExc_NotImplementedError, "a's number of columns doesn't match b's rows"); %(fail)s;}
169         if ((!%(z)s)
170             || (PyArray_DIMS(%(z)s)[0] != ((npy_int32 *)PyArray_DATA(%(a_nrows)s))[0])
171             || (PyArray_DIMS(%(z)s)[1] != PyArray_DIMS(%(b)s)[1])
172             )
173         {
174             {Py_XDECREF(%(z)s);}
175             npy_intp dims[] = {0, 0};
176             dims[0] = ((npy_int32 *)PyArray_DATA(%(a_nrows)s))[0];
177             dims[1] = PyArray_DIMS(%(b)s)[1];
178             %(z)s = (PyArrayObject*) PyArray_SimpleNew(2, dims, %(typenum_z)s);
179         }
180         {
181             // sparse array has size MxK, dense KxN, output MxN
182             npy_intp M = PyArray_DIMS(%(z)s)[0];
183             npy_intp N = PyArray_DIMS(%(z)s)[1];
184             npy_intp K = PyArray_DIMS(%(b)s)[0];
185             if (N &gt; 0x7fffffffL)
186             {PyErr_SetString(PyExc_NotImplementedError, "array too big (overflows int32 index)"); %(fail)s;}
187             // strides tell you how many bytes to skip to go to next column/row entry
188             npy_intp Szm = PyArray_STRIDES(%(z)s)[0] / PyArray_DESCR(%(z)s)-&gt;elsize;
189             npy_intp Szn = PyArray_STRIDES(%(z)s)[1] / PyArray_DESCR(%(z)s)-&gt;elsize;
190             //npy_intp Sbm = PyArray_STRIDES(%(b)s)[0] / PyArray_DESCR(%(b)s)-&gt;elsize;
191             npy_intp Sbn = PyArray_STRIDES(%(b)s)[1] / PyArray_DESCR(%(b)s)-&gt;elsize;
192             npy_intp Sval = PyArray_STRIDES(%(a_val)s)[0] / PyArray_DESCR(%(a_val)s)-&gt;elsize;
193             npy_intp Sind = PyArray_STRIDES(%(a_ind)s)[0] / PyArray_DESCR(%(a_ind)s)-&gt;elsize;
194             npy_intp Sptr = PyArray_STRIDES(%(a_ptr)s)[0] / PyArray_DESCR(%(a_ptr)s)-&gt;elsize;
195             // pointers to access actual data in the arrays passed as params.
196             dtype_%(z)s*     __restrict__ Dz   = (dtype_%(z)s*)PyArray_DATA(%(z)s);
197             const dtype_%(a_val)s* __restrict__ Dval = (dtype_%(a_val)s*)PyArray_DATA(%(a_val)s);
198             const npy_int32 * __restrict__ Dind = (npy_int32*)PyArray_DATA(%(a_ind)s);
199             const npy_int32 * __restrict__ Dptr = (npy_int32*)PyArray_DATA(%(a_ptr)s);
200             //npy_intp nnz = PyArray_DIMS(%(a_ind)s)[0];
201             //clear the output array
202             memset(Dz, 0, M*N*sizeof(dtype_%(z)s));
203             //iterate over the sparse array, making the most of an entry wherever we find it.
204             //
205             // Normal matrix matrix multiply: A MxK, B KxN =&gt;  Z = AB
206             // for m
207             //   for n
208             //     for k
209             //        z[m, n] += a[m, k] * b[k, n]
210             // Here instead: Z =
211             // for k
212             //   for m (sparse)
213             //     for n
214             //        z[m, n] += a[m, k] * b[k, n]
215             // loop over inner dimension
216             for (npy_int32 k = 0; k &lt; K; ++k)
217             {
218                 // get pointer to k-th row of dense matrix
219                 const dtype_%(b)s* __restrict__ bk = (dtype_%(b)s*)(PyArray_BYTES(%(b)s) + PyArray_STRIDES(%(b)s)[0] * k);
220                 // loop over sparse column indices through index pointer array
221                 // (amounts to looping over rows M of sparse matrix)
222                 for (npy_int32 m_idx = Dptr[k * Sptr]; m_idx &lt; Dptr[(k+1) * Sptr]; ++m_idx)
223                 {
224                     npy_int32 m = Dind[m_idx * Sind]; // row index of non-null value for column K
225                     const dtype_%(a_val)s Amk = Dval[m_idx * Sval]; // actual value at that location
226                     // pointer to m-th row of the output matrix Z
227                     dtype_%(z)s* __restrict__ zm = (dtype_%(z)s*)(PyArray_BYTES(%(z)s) + PyArray_STRIDES(%(z)s)[0] * m);
228                     //RESOLVE: a.shape[0] equals z.shape[0], why is this not an equality constraint?
229                     if (m &gt;= PyArray_DIMS(%(z)s)[0])
230                     {PyErr_SetString(PyExc_NotImplementedError, "illegal row index in a"); %(fail)s;}
231                     // loop over final dimension (cols of dense matrix) and perform dot product
232                     if ((Szn == 1) &amp;&amp; (Sbn == 1)) {
233                         for(npy_int32 n = 0; n &lt; N; ++n)
234                         {
235                             zm[n] += Amk * bk[n];
236                         }
237                     }
238                     else
239                     {
240                         for(npy_int32 n = 0; n &lt; N; ++n)
241                         {
242                             zm[n*Szn] += Amk * bk[n*Sbn];
243                         }
244                     }
245                 }
246             }
247         }
248         """ % dict(locals(), **sub)
249         return rval
250     def c_code_cache_version(self):
251         return (3,)
252 sd_csc = StructuredDotCSC()
253 class StructuredDotCSR(gof.Op):
254     """
255     Structured Dot CSR is like dot, except that only the
256     gradient wrt non-zero elements of the sparse matrix
257     `a` are calculated and propagated.
258     The output is presumed to be a dense matrix, and is represented by a
259     TensorType instance.
260     Parameters
261     ----------
262     a
263         A sparse matrix in csr format.
264     b
265         A sparse or dense matrix.
266     Returns
267     -------
268     matrix
269         The dot product of `a` and `b`.
270     Notes
271     -----
272     The grad implemented is structured.
273     This op is used as an optimization for StructuredDot.
274     """
275     __props__ = ()
276     def make_node(self, a_val, a_ind, a_ptr, b):
277         self.dtype_out = scalar.upcast(a_val.type.dtype, b.type.dtype)
278         r = gof.Apply(self, [a_val, a_ind, a_ptr, b],
279                       [tensor.tensor(self.dtype_out,
280                                      (False, b.type.broadcastable[1]))])
281         return r
282     def perform(self, node, inputs, outputs):
283         (a_val, a_ind, a_ptr, b) = inputs
284         (out,) = outputs
285         a = scipy.sparse.csr_matrix(
286             (a_val, a_ind, a_ptr),
287             (len(a_ptr) - 1, b.shape[0]),
288             copy=True)  # use view_map before setting this to False
289         out[0] = a * b
290         assert _is_dense(out[0])
291     def c_code(self, node, name, inputs, outputs, sub):
292         """
293         C-implementation of the dot product of the sparse matrix A and matrix B.
294         Parameters
295         ----------
296         a_val
297             Non-zero values of the sparse matrix.
298         a_ind
299             Column indices of the non-null values (.indices of a
300             scipy.csc_matrix).
301         a_ptr
302             Indicates col indices for col. i are in the range
303             a_ptr[i]:a_ptr[i+1].
304         n_cols
305             Number of columns of sparse matrix.
306         b
307             Dense matrix to perform dot product with, as in dot(a, b).
308         z
309             Return value.
310         sub
311             TODO, not too sure, something to do with weave probably.
312         """
313         (a_val, a_ind, a_ptr, b) = inputs
314         (z,) = outputs
315         typenum_z = tensor.TensorType(self.dtype_out, []).dtype_specs()[2]
316         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
317             raise NotImplementedError('Complex types are not supported for a_val')
318         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
319             raise NotImplementedError('Complex types are not supported for b')
320         return """
321         if (PyArray_NDIM(%(a_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_val) != 1"); %(fail)s;}
322         if (PyArray_NDIM(%(a_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ind) != 1"); %(fail)s;}
323         if (PyArray_NDIM(%(a_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ptr) != 1"); %(fail)s;}
324         if (PyArray_NDIM(%(b)s) != 2) {PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 2"); %(fail)s;}
325         if (PyArray_TYPE(%(a_ind)s) != NPY_INT32) {
326         PyErr_SetString(PyExc_NotImplementedError, "a_ind dtype not INT32"); %(fail)s;}
327         if (PyArray_TYPE(%(a_ptr)s) != NPY_INT32)
328         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr dtype not INT32"); %(fail)s;}
329         if (PyArray_DIMS(%(a_val)s)[0] != PyArray_DIMS(%(a_ind)s)[0])
330         {PyErr_SetString(PyExc_NotImplementedError, "a_val and a_ind have different lengths"); %(fail)s;}
331         if ((!%(z)s)
332             || (PyArray_DIMS(%(z)s)[0] != PyArray_DIMS(%(a_ptr)s)[0]-1) //a's rows
333             || (PyArray_DIMS(%(z)s)[1] != PyArray_DIMS(%(b)s)[1])       //b's columns
334             )
335         {
336             {Py_XDECREF(%(z)s);}
337             npy_intp dims[] = {0, 0};
338             dims[0] = PyArray_DIMS(%(a_ptr)s)[0]-1;
339             dims[1] = PyArray_DIMS(%(b)s)[1];
340             %(z)s = (PyArrayObject*) PyArray_SimpleNew(2, dims, %(typenum_z)s);
341         }
342         {
343             // sparse array has size MxK, dense KxN, output MxN
344             npy_intp M = PyArray_DIMS(%(z)s)[0];
345             npy_intp N = PyArray_DIMS(%(z)s)[1];
346             npy_intp K = PyArray_DIMS(%(b)s)[0];
347             if (N &gt; 0x7fffffffL)
348             {PyErr_SetString(PyExc_NotImplementedError, "array too big (overflows int32 index)"); %(fail)s;}
349             // strides tell you how many bytes to skip to go to next column/row entry
350             npy_intp Szm = PyArray_STRIDES(%(z)s)[0] / PyArray_DESCR(%(z)s)-&gt;elsize;
351             npy_intp Szn = PyArray_STRIDES(%(z)s)[1] / PyArray_DESCR(%(z)s)-&gt;elsize;
352             npy_intp Sbm = PyArray_STRIDES(%(b)s)[0] / PyArray_DESCR(%(b)s)-&gt;elsize;
353             npy_intp Sbn = PyArray_STRIDES(%(b)s)[1] / PyArray_DESCR(%(b)s)-&gt;elsize;
354             npy_intp Sval = PyArray_STRIDES(%(a_val)s)[0] / PyArray_DESCR(%(a_val)s)-&gt;elsize;
355             npy_intp Sind = PyArray_STRIDES(%(a_ind)s)[0] / PyArray_DESCR(%(a_ind)s)-&gt;elsize;
356             npy_intp Sptr = PyArray_STRIDES(%(a_ptr)s)[0] / PyArray_DESCR(%(a_ptr)s)-&gt;elsize;
357             // pointers to access actual data in the arrays passed as params.
358             dtype_%(z)s* __restrict__ Dz = (dtype_%(z)s*)PyArray_DATA(%(z)s);
359             const dtype_%(a_val)s* __restrict__ Dval = (dtype_%(a_val)s*)PyArray_DATA(%(a_val)s);
360             const npy_int32 * __restrict__ Dind = (npy_int32*)PyArray_DATA(%(a_ind)s);
361             const npy_int32 * __restrict__ Dptr = (npy_int32*)PyArray_DATA(%(a_ptr)s);
362             //npy_intp nnz = PyArray_DIMS(%(a_ind)s)[0];
363             //clear the output array
364             memset(Dz, 0, M*N*sizeof(dtype_%(z)s));
365             //iterate over the sparse array, making the most of an entry wherever we find it.
366             // Normal matrix matrix multiply:
367             // for m
368             //   for n
369             //     for k
370             //        z[m, n] += a[m, k] * b[k, n]
371             // Here instead:
372             // for m
373             //   for k (sparse)
374             //     for n
375             //        z[m, n] += a[m, k] * b[k, n]
376             // loop over inner dimension
377             for (npy_int64 m = 0; m &lt; M; ++m)
378             {
379                 // pointer to m-th row of the output matrix Z
380                 dtype_%(z)s* __restrict__ zm = (dtype_%(z)s*)(PyArray_BYTES(%(z)s) + PyArray_STRIDES(%(z)s)[0] * m);
381                 // loop over sparse rows indices through index pointer array
382                 // (amounts to looping over cols k of sparse matrix)
383                 for (npy_int32 k_idx = Dptr[m * Sptr]; k_idx &lt; Dptr[(m+1) * Sptr]; ++k_idx)
384                 {
385                     npy_int32 k = Dind[k_idx * Sind]; // col index of non-null value for row m
386                     const dtype_%(a_val)s Amk = Dval[k_idx * Sval]; // actual value at that location
387                     // get pointer to k-th row of dense matrix
388                     const dtype_%(b)s* __restrict__ bk = (dtype_%(b)s*)(PyArray_BYTES(%(b)s) + PyArray_STRIDES(%(b)s)[0] * k);
389                     // loop over final dimension (cols of dense matrix) and perform dot product
390                     for(npy_int32 n = 0; n &lt; N; ++n)
391                     {
392                         zm[n*Szn] += Amk * bk[n*Sbn];
393                     }
394                 }
395             }
396         }
397         """ % dict(locals(), **sub)
398     def c_code_cache_version(self):
399         return (2,)
400 sd_csr = StructuredDotCSR()
401 @gof.local_optimizer([sparse._structured_dot])
402 def local_structured_dot(node):
403     if node.op == sparse._structured_dot:
404         a, b = node.inputs
405         if a.type.format == 'csc':
406             a_val, a_ind, a_ptr, a_shape = csm_properties(a)
407             a_nsparse = a_shape[0]
408             return [sd_csc(a_val, a_ind, a_ptr, a_nsparse, b)]
409         if a.type.format == 'csr':
410             a_val, a_ind, a_ptr, a_shape = csm_properties(a)
411             return [sd_csr(a_val, a_ind, a_ptr, b)]
412     return False
413 class UsmmCscDense(gof.Op):
414     """
415     Performs the expression is `alpha` * `x` `y` + `z`.
416     Parameters
417     ----------
418     x
419         Matrix variable.
420     y
421         Matrix variable.
422     z
423         Dense matrix.
424     alpha
425         A tensor scalar.
426     Returns
427     -------
428     The dense matrix resulting from `alpha` * `x` `y` + `z`.
429     Notes
430     -----
431     The grad is not implemented for this op.
432     Optimized version os Usmm when `x` is in csc format and `y` is dense.
433     """
434     __props__ = ("inplace",)
435     def __init__(self, inplace):
436         self.inplace = inplace
437         if inplace:
438             self.destroy_map = {0: [6]}
439     def __str__(self):
440             return 'UsmmCscDense{inplace}'
441         else:
442             re<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>turn 'UsmmCscDense{no_inplace}'
443     def make_node(self, alpha, x_val, x_ind, x_ptr, x_nrows, y, z):
444         alpha = tensor.as_tensor_variable(alpha)
445         x_val = tensor.as_tensor_variable(x_val)
446         x_ind = tensor.as_tensor_variable(x_ind)
447         x_ptr = tensor.as_tensor_variable(x_ptr)
448         x_nrows = tensor.as_tensor_variable(x_nrows)
449         y = tensor.as_tensor_variable(y)
450         z = tensor.as_tensor_variable(</b></font>z)
451         assert x_ind.dtype == 'int32'
452         assert x_ptr.dtype == 'int32'
453         assert x_nrows.dtype == 'int32'
454         assert alpha.ndim == 2 and alpha.type.broadcastable == (True, True)
455         assert x_val.ndim == 1
456         assert y.ndim == 2
457         assert z.ndim == 2
458         dtype_out = scalar.upcast(alpha.type.dtype, x_val.type.dtype,
459                                   y.type.dtype, z.type.dtype)
460         if dtype_out not in ('float32', 'float64'):
461             raise NotImplementedError('only float types are supported in '
462                                       'operands')
463         if self.inplace:
464             assert z.type.dtype == dtype_out
465         if dtype_out != alpha.type.dtype:
466             alpha = tensor.cast(alpha, dtype_out)
467         if dtype_out != x_val.type.dtype:
468             x_val = tensor.cast(x_val, dtype_out)
469         if dtype_out != y.type.dtype:
470             y = tensor.cast(y, dtype_out)
471         if dtype_out != z.type.dtype:
472             z = tensor.cast(z, dtype_out)
473             self, [alpha, x_val, x_ind, x_ptr, x_nrows, y, z],
474             [tensor.tensor(dtype_out, (False, y.type.broadcastable[1]))])
475         r<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>eturn r
476     def c_support_code(self):
477         return blas.blas_header_text()
478     def c_libraries(self):
479         return blas.ldflags()
480     def c_compile_args(self):
481         return blas.ldflags(</b></font>libs=False, flags=True)
482     def c_lib_dirs(self):
483         return blas.ldflags(libs=False, libs_dir=True)
484     def c_header_dirs(self):
485         return blas.ldflags(libs=False, include_dir=True)
486     def c_code(self, node, name, inputs, outputs, sub):
487         alpha, x_val, x_ind, x_ptr, x_nrows, y, z = inputs
488         zn = outputs[0]
489         if node.inputs[1].type.dtype in ('complex64', 'complex128'):
490             raise NotImplementedError('Complex types are not supported for '
491                                       'x_val')
492         if node.inputs[5].type.dtype in ('complex64', 'complex128'):
493             raise NotImplementedError('Complex types are not supported for y')
494         if node.inputs[6].type.dtype != node.outputs[0].type.dtype:
495             raise NotImplementedError('z and output must have same type')
496         if node.inputs[1].type.dtype == "float32":
497             conv_type = "float"
498             axpy = "saxpy_"
499         else:
500             conv_type = "double"
501             axpy = "daxpy_"
502         typenum_alpha = node.inputs[0].type.dtype_specs()[2]
503         typenum_x_val = node.inputs[1].type.dtype_specs()[2]
504         typenum_y = node.inputs[5].type.dtype_specs()[2]
505         typenum_z = node.inputs[6].type.dtype_specs()[2]
506         typenum_zn = node.outputs[0].type.dtype_specs()[2]
507         inplace = int(self.inplace)
508         rval = """
509         if (PyArray_NDIM(%(x_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(x_val) != 1"); %(fail)s;}
510         if (PyArray_NDIM(%(x_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(x_ind) != 1"); %(fail)s;}
511         if (PyArray_NDIM(%(x_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(x_ptr) != 1"); %(fail)s;}
512         if (PyArray_NDIM(%(x_nrows)s) != 0) {PyErr_SetString(PyExc_NotImplementedError, "rank(nrows) != 0"); %(fail)s;}
513         if (PyArray_NDIM(%(y)s) != 2) {PyErr_SetString(PyExc_NotImplementedError, "rank(y) != 2"); %(fail)s;}
514         if (PyArray_TYPE(%(x_val)s) != %(typenum_x_val)s) {
515         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for x_val"); %(fail)s;}
516         if (PyArray_TYPE(%(y)s) != %(typenum_y)s) {
517         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for y"); %(fail)s;}
518         if (PyArray_TYPE(%(z)s) != %(typenum_z)s) {
519         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for z"); %(fail)s;}
520         if (PyArray_TYPE(%(alpha)s) != %(typenum_alpha)s) {
521         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for alpha"); %(fail)s;}
522         if (PyArray_TYPE(%(x_ind)s) != NPY_INT32) {
523         PyErr_SetString(PyExc_NotImplementedError, "x_ind dtype not INT32"); %(fail)s;}
524         if (PyArray_TYPE(%(x_ptr)s) != NPY_INT32)
525         {PyErr_SetString(PyExc_NotImplementedError, "x_ptr dtype not INT32"); %(fail)s;}
526         if (PyArray_TYPE(%(x_nrows)s) != NPY_INT32)
527         {PyErr_SetString(PyExc_NotImplementedError, "x_nrows dtype not INT32"); %(fail)s;}
528         if (PyArray_DIMS(%(x_val)s)[0] != PyArray_DIMS(%(x_ind)s)[0])
529         {PyErr_SetString(PyExc_NotImplementedError, "x_val and x_ind have different lengths"); %(fail)s;}
530         if (PyArray_DIMS(%(x_ptr)s)[0] != PyArray_DIMS(%(y)s)[0]+1)
531         {PyErr_SetString(PyExc_NotImplementedError, "x's number of columns doesn't match y's rows"); %(fail)s;}
532         if (PyArray_DIMS(%(z)s)[0] != ((npy_int32 *)PyArray_DATA(%(x_nrows)s))[0] || PyArray_DIMS(%(z)s)[1] != PyArray_DIMS(%(y)s)[1])
533         {PyErr_SetString(PyExc_NotImplementedError, "The dimension of the allocated output doesn't match the correct output size."); %(fail)s;}
534         if (PyArray_SIZE(%(alpha)s) != 1)
535         {PyErr_SetString(PyExc_NotImplementedError, "The number of element in alpha must be 1"); %(fail)s;}
536         if (PyArray_NDIM(%(alpha)s) != 2)
537         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of alpha must be 2"); %(fail)s;}
538         if (PyArray_NDIM(%(x_val)s) != 1)
539         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of x_val must be 1"); %(fail)s;}
540         if (PyArray_NDIM(%(y)s) != 2)
541         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of y must be 2"); %(fail)s;}
542         if (PyArray_NDIM(%(z)s) != 2)
543         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of z must be 2"); %(fail)s;}
544         if (%(inplace)s)
545         {
546             if (%(typenum_zn)s != %(typenum_z)s) {
547             PyErr_SetString(PyExc_NotImplementedError, "When inplace the output dtype must be the same as the input"); %(fail)s;}
548             Py_XDECREF(%(zn)s);
549             %(zn)s = %(z)s;
550             Py_INCREF(%(zn)s);
551         }
552         else if (!%(zn)s
553             || (PyArray_DIMS(%(zn)s)[0] != ((npy_int32 *)PyArray_DATA(%(x_nrows)s))[0])
554             || (PyArray_DIMS(%(zn)s)[1] != PyArray_DIMS(%(y)s)[1])
555             )
556         {
557             {Py_XDECREF(%(zn)s);}
558             npy_intp dims[] = {0, 0};
559             dims[0] = ((npy_int32 *)PyArray_DATA(%(x_nrows)s))[0];
560             dims[1] = PyArray_DIMS(%(y)s)[1];
561             %(zn)s = (PyArrayObject*) PyArray_SimpleNew(2, dims, %(typenum_zn)s);
562         }
563         {
564             // sparse array has size MxK, dense KxN, output MxN
565             npy_intp M = PyArray_DIMS(%(zn)s)[0];
566             npy_intp N = PyArray_DIMS(%(zn)s)[1];
567             npy_intp K = PyArray_DIMS(%(y)s)[0];
568             // pointers to access actual data in the arrays passed as params.
569             const dtype_%(x_val)s* __restrict__ Dval = (dtype_%(x_val)s*)PyArray_DATA(%(x_val)s);
570             const npy_int32 * __restrict__ Dind = (npy_int32*)PyArray_DATA(%(x_ind)s);
571             const npy_int32 * __restrict__ Dptr = (npy_int32*)PyArray_DATA(%(x_ptr)s);
572             const dtype_%(alpha)s alpha = ((dtype_%(alpha)s*)PyArray_DATA(%(alpha)s))[0];
573             npy_intp Sz = PyArray_STRIDES(%(z)s)[1] / PyArray_DESCR(%(z)s)-&gt;elsize;
574             npy_intp Szn = PyArray_STRIDES(%(zn)s)[1] / PyArray_DESCR(%(zn)s)-&gt;elsize;
575             npy_intp Sval = PyArray_STRIDES(%(x_val)s)[0] / PyArray_DESCR(%(x_val)s)-&gt;elsize;
576             npy_intp Sind = PyArray_STRIDES(%(x_ind)s)[0] / PyArray_DESCR(%(x_ind)s)-&gt;elsize;
577             npy_intp Sptr = PyArray_STRIDES(%(x_ptr)s)[0] / PyArray_DESCR(%(x_ptr)s)-&gt;elsize;
578             npy_intp Sy = PyArray_STRIDES(%(y)s)[1] / PyArray_DESCR(%(y)s)-&gt;elsize;
579             // blas expects ints; convert here (rather than just making N etc ints) to avoid potential overflow in the negative-stride correction
580             if ((N &gt; 0x7fffffffL)||(Sy &gt; 0x7fffffffL)||(Szn &gt; 0x7fffffffL)||(Sy &lt; -0x7fffffffL)||(Szn &lt; -0x7fffffffL))
581             {PyErr_SetString(PyExc_NotImplementedError, "array too big for BLAS (overflows int32 index)"); %(fail)s;}
582             int N32 = N;
583             int Sy32 = Sy;
584             int Szn32 = Szn;
585             if (!(%(inplace)s))
586             {
587                 if (PyArray_CopyInto(%(zn)s, %(z)s))
588                 {
589                     Py_XDECREF(%(zn)s);
590                     %(fail)s;
591                 }
592             }
593             for (npy_intp k = 0; k &lt; K; ++k)
594             {
595                 for (npy_int32 m_idx = Dptr[k * Sptr]; m_idx &lt; Dptr[(k+1)*Sptr]; ++m_idx)
596                 {
597                     const npy_int32 m = Dind[m_idx * Sind]; // row index of non-null value for column K
598                     const dtype_%(x_val)s Amk = alpha * Dval[m_idx * Sval]; // actual value at that location
599                     dtype_%(y)s* y_row = (dtype_%(y)s*)(PyArray_BYTES(%(y)s) + PyArray_STRIDES(%(y)s)[0] * k);
600                     // axpy expects pointer to the beginning of memory arrays,
601                     // so when the stride is negative, we need to get the
602                     // last element
603                     if (Sy &lt; 0)
604                         y_row += (K - 1) * Sy;
605                     dtype_%(zn)s* z_row = (dtype_%(zn)s*)(PyArray_BYTES(%(zn)s) + PyArray_STRIDES(%(zn)s)[0] * m);
606                     if (Szn &lt; 0)
607                         z_row += (N - 1) * Szn;
608                     %(axpy)s(&amp;N32, (%(conv_type)s*)&amp;Amk, (%(conv_type)s*)y_row, &amp;Sy32, (%(conv_type)s*)z_row, &amp;Szn32);
609                 }
610             }
611         }
612         """ % dict(locals(), **sub)
613         return rval
614     def c_code_cache_version(self):
615         return (3, blas.blas_header_version())
616 usmm_csc_dense = UsmmCscDense(inplace=False)
617 usmm_csc_dense_inplace = UsmmCscDense(inplace=True)
618 local_usmm = gof.opt.PatternSub(
619     (theano.tensor.sub, 'z',
620      (theano.tensor.mul,
621       {'pattern': 'alpha',
622        'constraint': lambda expr: (np.all(expr.type.broadcastable) and
623                                    theano.config.blas.ldflags)},
624       (sparse._dot, 'x', 'y'))),
625     (usmm, (theano.tensor.neg, 'alpha'), 'x', 'y', 'z'))
626 register_specialize(local_usmm, name="local_usmm")
627 @gof.local_optimizer([usmm_csc_dense])
628 def local_usmm_csc_dense_inplace(node):
629     if node.op == usmm_csc_dense:
630         return [usmm_csc_dense_inplace(*node.inputs)]
631 register_specialize(local_usmm_csc_dense_inplace, 'cxx_only', 'inplace')
632 @gof.local_optimizer([usmm])
633 def local_usmm_csx(node):
634     """
635     usmm -&gt; usmm_csc_dense
636     """
637     if node.op == usmm:
638         alpha, x, y, z = node.inputs
639         x_is_sparse_variable = _is_sparse_variable(x)
640         y_is_sparse_variable = _is_sparse_variable(y)
641         if x_is_sparse_variable and not y_is_sparse_variable:
642             if x.type.format == 'csc':
643                 x_val, x_ind, x_ptr, x_shape = csm_properties(x)
644                 x_nsparse = x_shape[0]
645                 dtype_out = scalar.upcast(alpha.type.dtype, x.type.dtype,
646                                           y.type.dtype, z.type.dtype)
647                 if dtype_out not in ('float32', 'float64'):
648                     return False
649                 if y.type.dtype != dtype_out:
650                     return False
651                 return [usmm_csc_dense(alpha, x_val, x_ind, x_ptr,
652                                        x_nsparse, y, z)]
653     return False
654 register_specialize(local_usmm_csx, 'cxx_only')
655 class CSMGradC(gof.Op):
656     __props__ = ()
657     def make_node(self, a_val, a_ind, a_ptr, a_dim,
658                   b_val, b_ind, b_ptr, b_dim):
659         return gof.Apply(self, [a_val, a_ind, a_ptr, a_dim,
660                          b_val, b_ind, b_ptr, b_dim], [b_val.type()])
661     def c_code(self, node, name, inputs, outputs, sub):
662         (a_val, a_ind, a_ptr, a_dim,
663          b_val, b_ind, b_ptr, b_dim) = inputs
664         (z,) = outputs
665         typenum_z = node.outputs[0].type.dtype_specs()[2]
666         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
667             raise NotImplementedError('Complex types are not supported for a_val')
668         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
669             raise NotImplementedError('Complex types are not supported for b_val')
670         return """
671         if (PyArray_NDIM(%(a_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_val) != 1"); %(fail)s;}
672         if (PyArray_NDIM(%(a_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ind) != 1"); %(fail)s;}
673         if (PyArray_NDIM(%(a_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ptr) != 1"); %(fail)s;}
674         if (PyArray_NDIM(%(b_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(b_val) != 1"); %(fail)s;}
675         if (PyArray_NDIM(%(b_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(b_ind) != 1"); %(fail)s;}
676         if (PyArray_NDIM(%(b_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(b_ptr) != 1"); %(fail)s;}
677         if (PyArray_TYPE(%(a_ind)s) != NPY_INT32) {
678         PyErr_SetString(PyExc_NotImplementedError, "a_ind dtype not INT32"); %(fail)s;}
679         if (PyArray_TYPE(%(a_ptr)s) != NPY_INT32)
680         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr dtype not INT32"); %(fail)s;}
681         if (PyArray_TYPE(%(b_ind)s) != NPY_INT32) {
682         PyErr_SetString(PyExc_NotImplementedError, "b_ind dtype not INT32"); %(fail)s;}
683         if (PyArray_TYPE(%(b_ptr)s) != NPY_INT32)
684         {PyErr_SetString(PyExc_NotImplementedError, "b_ptr dtype not INT32"); %(fail)s;}
685         if (PyArray_DIMS(%(a_val)s)[0] != PyArray_DIMS(%(a_ind)s)[0])
686         {PyErr_SetString(PyExc_NotImplementedError, "a_val and a_ind have different lengths"); %(fail)s;}
687         if (PyArray_DIMS(%(b_val)s)[0] != PyArray_DIMS(%(b_ind)s)[0])
688         {PyErr_SetString(PyExc_NotImplementedError, "b_val and b_ind have different lengths"); %(fail)s;}
689         if (PyArray_DIMS(%(a_ptr)s)[0] != PyArray_DIMS(%(b_ptr)s)[0])
690         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr and b_ptr have different lengths"); %(fail)s;}
691         if ((!%(z)s) || (PyArray_DIMS(%(z)s)[0] != PyArray_DIMS(%(a_val)s)[0]))
692         {
693             {Py_XDECREF(%(z)s);}
694             npy_intp dims[] = {0};
695             dims[0] = PyArray_DIMS(%(a_val)s)[0];
696             %(z)s = (PyArrayObject*) PyArray_SimpleNew(1, dims, %(typenum_z)s);
697         }
698         {
699             // sparse array has size MxK, dense KxN, output MxN
700             npy_intp M = PyArray_DIMS(%(a_ptr)s)[0] - 1;
701             npy_intp a_dim_0 = ((npy_int32 *)PyArray_DATA(%(a_dim)s))[0];
702             npy_intp a_dim_1 = ((npy_int32 *)PyArray_DATA(%(a_dim)s))[1];
703             npy_intp sp_dim = (M == a_dim_0)?a_dim_1:a_dim_0;
704             // strides tell you how many bytes to skip to go to next column/row entry
705             npy_intp Sz = PyArray_STRIDES(%(z)s)[0] / PyArray_DESCR(%(z)s)-&gt;elsize;
706             npy_intp Sa_val = PyArray_STRIDES(%(a_val)s)[0] / PyArray_DESCR(%(a_val)s)-&gt;elsize;
707             npy_intp Sa_ind = PyArray_STRIDES(%(a_ind)s)[0] / PyArray_DESCR(%(a_ind)s)-&gt;elsize;
708             npy_intp Sa_ptr = PyArray_STRIDES(%(a_ptr)s)[0] / PyArray_DESCR(%(a_ptr)s)-&gt;elsize;
709             npy_intp Sb_val = PyArray_STRIDES(%(b_val)s)[0] / PyArray_DESCR(%(b_val)s)-&gt;elsize;
710             npy_intp Sb_ind = PyArray_STRIDES(%(b_ind)s)[0] / PyArray_DESCR(%(b_ind)s)-&gt;elsize;
711             npy_intp Sb_ptr = PyArray_STRIDES(%(b_ptr)s)[0] / PyArray_DESCR(%(b_ptr)s)-&gt;elsize;
712             // pointers to access actual data in the arrays passed as params.
713             dtype_%(z)s* __restrict__ Dz = (dtype_%(z)s*)PyArray_DATA(%(z)s);
714             const dtype_%(a_val)s* __restrict__ Da_val = (dtype_%(a_val)s*)PyArray_DATA(%(a_val)s);
715             const npy_int32 * __restrict__ Da_ind = (npy_int32*)PyArray_DATA(%(a_ind)s);
716             const npy_int32 * __restrict__ Da_ptr = (npy_int32*)PyArray_DATA(%(a_ptr)s);
717             const dtype_%(b_val)s* __restrict__ Db_val = (dtype_%(b_val)s*)PyArray_DATA(%(b_val)s);
718             const npy_int32 * __restrict__ Db_ind = (npy_int32*)PyArray_DATA(%(b_ind)s);
719             const npy_int32 * __restrict__ Db_ptr = (npy_int32*)PyArray_DATA(%(b_ptr)s);
720             npy_intp nnz = PyArray_DIMS(%(a_ind)s)[0];
721             dtype_%(b_val)s b_row[sp_dim];
722             //clear the output array
723             for (npy_int64 i = 0; i &lt; nnz; ++i)
724             {
725                 Dz[i*Sz] = 0;
726             }
727             memset(b_row, 0, sp_dim*sizeof(dtype_%(b_val)s));
728             // loop over inner dimension
729             for (npy_int64 m = 0; m &lt; M; ++m)
730             {
731                 for (npy_int32 j_ptr = Db_ptr[m * Sb_ptr];
732                     j_ptr &lt; Db_ptr[(m + 1) * Sb_ptr]; j_ptr++) {
733                     b_row[Db_ind[j_ptr * Sb_ind]] += Db_val[j_ptr*Sb_val];
734                 }
735                 for (npy_int32 j_ptr = Da_ptr[m * Sa_ptr];
736                     j_ptr &lt; Da_ptr[(m + 1) * Sa_ptr]; j_ptr++) {
737                     Dz[j_ptr*Sz] = b_row[Da_ind[j_ptr * Sa_ind]];
738                 }
739                 for (npy_int32 j_ptr = Db_ptr[m * Sb_ptr];
740                     j_ptr &lt; Db_ptr[(m + 1) * Sb_ptr]; j_ptr++) {
741                     b_row[Db_ind[j_ptr * Sb_ind]] = 0;
742                 }
743             }
744         }
745         """ % dict(locals(), **sub)
746     def c_code_cache_version(self):
747         return (3,)
748 csm_grad_c = CSMGradC()
749 @gof.local_optimizer([csm_grad(None)])
750 def local_csm_grad_c(node):
751     """
752     csm_grad(None) -&gt; csm_grad_c
753     """
754     if node.op == csm_grad(None):
755         return [csm_grad_c(*node.inputs)]
756     return False
757 class MulSDCSC(gof.Op):
758     """
759     Multiplication of sparse matrix by a broadcasted dense vector
760     element wise.
761     Parameters
762     ----------
763     a_data
764         Sparse matrix data.
765     a_indices
766         Sparse matrix indices.
767     a_indptr
768         Sparse matrix indptr.
769     b
770         Tensor type matrix.
771     Returns
772     -------
773     The multiplication of the two matrices element-wise.
774     Notes
775     -----
776     `a_data`, `a_indices` and `a_indptr` must be the properties of a sparse
777     matrix in csc format.
778     The dtype of `a_data`, i.e. the dtype of the sparse matrix, cannot be a
779     complex type.
780     This op is used as an optimization of mul_s_d.
781     """
782     __props__ = ()
783     def make_node(self, a_data, a_indices, a_indptr, b):
784         assert b.type.ndim == 2
785         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
786                                [tensor.tensor(b.dtype, (False,))])
787     def c_code_cache_version(self):
788         return (3,)
789     def c_code(self, node, name, inputs, outputs, sub):
790         (_data, _indices, _indptr, _b,) = inputs
791         (_zout,) = outputs
792         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
793             raise NotImplementedError('Complex types are not supported for a')
794         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
795             raise NotImplementedError('Complex types are not supported for b')
796         return """
797         if (PyArray_NDIM(%(_b)s) != 2) {
798             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 2");
799             %(fail)s;}
800         if (PyArray_NDIM(%(_data)s) != 1) {
801             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
802             %(fail)s;}
803         if (PyArray_NDIM(%(_indices)s) != 1) {
804             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
805             %(fail)s;}
806         if (PyArray_NDIM(%(_indptr)s) != 1) {
807             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
808             %(fail)s;}
809         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
810         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
811         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
812         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
813         if (!%(_zout)s ||
814             (PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0]) ||
815             !(PyArray_ISCONTIGUOUS(%(_zout)s)))
816         {
817             Py_XDECREF(%(_zout)s);
818             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
819                   PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
820             if (!%(_zout)s)
821             {
822                 PyErr_SetString(PyExc_MemoryError,
823                     "Could not allocate output memory.");
824                 %(fail)s;
825             }
826         }
827         { //makes it compile even though labels jump over variable definitions.
828             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
829             //TODO: error checking with this
830             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
831             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
832             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
833             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
834             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
835             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0];
836             // loop over columns
837             for (npy_intp j = 0; j &lt; N; ++j)
838             {
839                 // for each non-null value in the sparse column
840                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
841                 {
842                     // extract row index of non-null value
843                     npy_int32 i = indices[i_idx];
844                     // extract i-th row of dense matrix
845                     const dtype_%(_b)s* __restrict__ b_row = (dtype_%(_b)s*)(PyArray_BYTES(%(_b)s) + Sb * i);
846                     // write resulting gradient to sparse output
847                     zout[i_idx] = data[i_idx] * b_row[j];
848                 }
849             }
850         }
851         """ % dict(locals(), **sub)
852     def __str__(self):
853         return self.__class__.__name__
854 mul_s_d_csc = MulSDCSC()
855 class MulSDCSR(gof.Op):
856     """
857     Multiplication of sparse matrix by a broadcasted dense vector
858     element wise.
859     Parameters
860     ----------
861     a_data
862         Sparse matrix data.
863     a_indices
864         Sparse matrix indices.
865     a_indptr
866         Sparse matrix indptr.
867     b
868         Tensor type matrix.
869     Returns
870     -------
871     The multiplication of the two matrix element wise.
872     Notes
873     -----
874     `a_data`, `a_indices` and `a_indptr` must be the properties
875     of a sparse matrix in csr format.
876     The dtype of `a_data`, i.e. the dtype of the sparse matrix,
877     cannot be a complex type.
878     This op is used as an optimization of mul_s_d.
879     """
880     __props__ = ()
881     def make_node(self, a_data, a_indices, a_indptr, b):
882         assert b.type.ndim == 2
883         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
884                                [tensor.tensor(b.dtype, (False,))])
885     def c_code_cache_version(self):
886         return (3,)
887     def c_code(self, node, name, inputs, outputs, sub):
888         (_data, _indices, _indptr, _b,) = inputs
889         (_zout,) = outputs
890         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
891             raise NotImplementedError('Complex types are not supported for a')
892         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
893             raise NotImplementedError('Complex types are not supported for b')
894         return """
895         if (PyArray_NDIM(%(_b)s) != 2) {
896             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 2");
897             %(fail)s;}
898         if (PyArray_NDIM(%(_data)s) != 1) {
899             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
900             %(fail)s;}
901         if (PyArray_NDIM(%(_indices)s) != 1) {
902             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
903             %(fail)s;}
904         if (PyArray_NDIM(%(_indptr)s) != 1) {
905             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
906             %(fail)s;}
907         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
908         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
909         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
910         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
911         if (!%(_zout)s ||
912             (PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0]) ||
913             !(PyArray_ISCONTIGUOUS(%(_zout)s)))
914         {
915             Py_XDECREF(%(_zout)s);
916             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
917                     PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
918             if (!%(_zout)s)
919             {
920                 PyErr_SetString(PyExc_MemoryError,
921                     "Could not allocate output memory.");
922                 %(fail)s;
923             }
924         }
925         { //makes it compile even though labels jump over variable definitions.
926             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
927             //TODO: error checking with this
928             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
929             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
930             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
931             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
932             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
933             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0];
934             // loop over columns
935             for (npy_intp j = 0; j &lt; N; ++j)
936             {
937                 // extract i-th row of dense matrix
938                 const dtype_%(_b)s* __restrict__ b_row = (dtype_%(_b)s*)(PyArray_BYTES(%(_b)s) + Sb * j);
939                 // for each non-null value in the sparse column
940                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
941                 {
942                     // extract row index of non-null value
943                     npy_int32 i = indices[i_idx];
944                     // write resulting gradient to sparse output
945                     zout[i_idx] = data[i_idx] * b_row[i];
946                 }
947             }
948         }
949         """ % dict(locals(), **sub)
950     def __str__(self):
951         return self.__class__.__name__
952 mul_s_d_csr = MulSDCSR()
953 @gof.local_optimizer([sparse.mul_s_d])
954 def local_mul_s_d(node):
955     if node.op == sparse.mul_s_d:
956         x, y = node.inputs
957         x_is_sparse_variable = _is_sparse_variable(x)
958         if x_is_sparse_variable:
959             svar = x
960             dvar = y
961         else:
962             svar = y
963             dvar = x
964         if dvar.type.ndim != 2:
965             return False
966         if svar.type.format == 'csc':
967             CSx = sparse.CSC
968             mul_s_d_csx = mul_s_d_csc
969         elif svar.type.format == 'csr':
970             CSx = sparse.CSR
971             mul_s_d_csx = mul_s_d_csr
972         else:
973             raise NotImplementedError
974         if x.dtype != y.dtype:
975             return
976         c_data = mul_s_d_csx(sparse.csm_data(svar),
977                              sparse.csm_indices(svar),
978                              sparse.csm_indptr(svar), dvar)
979         return [CSx(c_data,
980                     sparse.csm_indices(svar),
981                     sparse.csm_indptr(svar),
982                     sparse.csm_shape(svar))]
983     return False
984 register_specialize(local_mul_s_d, 'cxx_only')
985 class MulSVCSR(gof.Op):
986     """
987     Multiplication of sparse matrix by a broadcasted dense vector
988     element wise.
989     Parameters
990     ----------
991     a_data
992         Sparse matrix data.
993     a_indices
994         Sparse matrix indices.
995     a_indptr
996         Sparse matrix indptr.
997     b
998         Tensor type matrix.
999     Returns
1000     -------
1001     The multiplication of the two matrix element wise.
1002     Notes
1003     -----
1004     `a_data`, `a_indices` and `a_indptr` must be the properties
1005     of a sparse matrix in csr format.
1006     The dtype of `a_data`, i.e. the dtype of the sparse matrix,
1007     cannot be a complex type.
1008     This op is used as an optimization of MulSV.
1009     """
1010     __props__ = ()
1011     def make_node(self, a_data, a_indices, a_indptr, b):
1012         assert b.type.ndim == 1
1013         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
1014                                [tensor.tensor(b.dtype, (False,))])
1015     def c_code_cache_version(self):
1016         return (2,)
1017     def c_code(self, node, name, inputs, outputs, sub):
1018         _data, _indices, _indptr, _b, = inputs
1019         _zout, = outputs
1020         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
1021             raise NotImplementedError('Complex types are not supported for a')
1022         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
1023             raise NotImplementedError('Complex types are not supported for b')
1024         return """
1025         if (PyArray_NDIM(%(_b)s) != 1) {
1026             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 1");
1027             %(fail)s;
1028         }
1029         if (PyArray_NDIM(%(_data)s) != 1) {
1030             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
1031             %(fail)s;
1032         }
1033         if (PyArray_NDIM(%(_indices)s) != 1) {
1034             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
1035             %(fail)s;
1036         }
1037         if (PyArray_NDIM(%(_indptr)s) != 1) {
1038             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
1039             %(fail)s;
1040         }
1041         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
1042         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
1043         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
1044         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
1045         if (!%(_zout)s
1046             || PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0]
1047             || !PyArray_ISCONTIGUOUS(%(_zout)s))
1048         {
1049             Py_XDECREF(%(_zout)s);
1050             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
1051                     PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
1052         }
1053         { //makes it compile even though labels jump over variable definitions.
1054             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
1055             //TODO: error checking with this
1056             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
1057             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
1058             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
1059             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
1060             const dtype_%(_b)s* __restrict__ Db = (dtype_%(_b)s*)PyArray_DATA(%(_b)s);
1061             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
1062             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0] / PyArray_DESCR(%(_b)s)-&gt;elsize;
1063             // loop over rows
1064             for (npy_intp j = 0; j &lt; N; ++j)
1065             {
1066                 // for each non-null value in the sparse column
1067                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
1068                 {
1069                     // extract row index of non-null value
1070                     npy_int32 i = indices[i_idx];
1071                     zout[i_idx] = data[i_idx] * Db[i * Sb];
1072                 }
1073             }
1074         }
1075         """ % dict(locals(), **sub)
1076     def __str__(self):
1077         return self.__class__.__name__
1078 mul_s_v_csr = MulSVCSR()
1079 @gof.local_optimizer([sparse.mul_s_v])
1080 def local_mul_s_v(node):
1081     if node.op == sparse.mul_s_v:
1082         x, y = node.inputs
1083         x_is_sparse_variable = _is_sparse_variable(x)
1084         if x_is_sparse_variable:
1085             svar = x
1086             dvar = y
1087         else:
1088             svar = y
1089             dvar = x
1090         if dvar.type.ndim != 1:
1091             return False
1092         elif svar.type.format == 'csr':
1093             CSx = sparse.CSR
1094             mul_s_v_csx = mul_s_v_csr
1095         else:
1096             return False
1097         s_val, s_ind, s_ptr, s_shape = sparse.csm_properties(svar)
1098         c_data = mul_s_v_csx(s_val, s_ind, s_ptr, dvar)
1099         return [CSx(c_data, s_ind, s_ptr, s_shape)]
1100     return False
1101 register_specialize(local_mul_s_v, 'cxx_only')
1102 class StructuredAddSVCSR(gof.Op):
1103     """
1104     Structured addition of a sparse matrix and a dense vector.
1105     The elements of the vector are are only added to the corresponding
1106     non-zero elements. Therefore, this operation outputs another sparse
1107     matrix.
1108     Parameters
1109     ----------
1110     a_data
1111         Sparse matrix data.
1112     a_indices
1113         Sparse matrix indices.
1114     a_indptr
1115         Sparse matrix indptr.
1116     b
1117         Tensor type vector.
1118     Returns
1119     -------
1120     A sparse matrix containing the addition of the vector to the data of the
1121     sparse matrix.
1122     Notes
1123     -----
1124     The a_* are the properties of a sparse matrix in csr format.
1125     This op is used as an optimization for StructuredAddSV.
1126     """
1127     __props__ = ()
1128     <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, a_data, a_indices, a_indptr, b):
1129         b = tensor.as_tensor_variable(b)
1130         a_data = tensor.as_tensor_variable(a_data)
1131         a_indices = tensor.as_tensor_variable(a_indices)
1132         a_indptr = tensor.as_tensor_variable(</b></font>a_indptr)
1133         assert a_data.type.ndim == 1
1134         assert a_indices.type.ndim == 1
1135         assert a_indptr.type.ndim == 1
1136         assert b.type.ndim == 1
1137         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
1138                                [tensor.tensor(b.dtype, (False,))])
1139     def c_code_cache_version(self):
1140         return (3,)
1141     def c_code(self, node, name, inputs, outputs, sub):
1142         _data, _indices, _indptr, _b, = inputs
1143         _zout, = outputs
1144         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
1145             raise NotImplementedError('Complex types are not supported for a')
1146         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
1147             raise NotImplementedError('Complex types are not supported for b')
1148         return """
1149         if (PyArray_NDIM(%(_b)s) != 1) {
1150             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 1");
1151             %(fail)s;
1152         }
1153         if (PyArray_NDIM(%(_data)s) != 1) {
1154             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
1155             %(fail)s;
1156         }
1157         if (PyArray_NDIM(%(_indices)s) != 1) {
1158             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
1159             %(fail)s;
1160         }
1161         if (PyArray_NDIM(%(_indptr)s) != 1) {
1162             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
1163             %(fail)s;
1164         }
1165         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
1166         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
1167         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
1168         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
1169         if (!%(_zout)s
1170             || (PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0])
1171             || !(PyArray_ISCONTIGUOUS(%(_zout)s)))
1172         {
1173             Py_XDECREF(%(_zout)s);
1174             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
1175                     PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
1176             if (!%(_zout)s)
1177             {
1178                 PyErr_SetString(PyExc_MemoryError,
1179                     "Could not allocate output memory.");
1180                 %(fail)s;
1181             }
1182         }
1183         { //makes it compile even though labels jump over variable definitions.
1184             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
1185             //TODO: error checking with this
1186             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
1187             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
1188             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
1189             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
1190             const dtype_%(_b)s* __restrict__ Db = (dtype_%(_b)s*)PyArray_DATA(%(_b)s);
1191             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
1192             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0] / PyArray_DESCR(%(_b)s)-&gt;elsize;
1193             // loop over columns
1194             for (npy_intp j = 0; j &lt; N; ++j)
1195             {
1196                 // for each non-null value in the sparse column
1197                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
1198                 {
1199                     // extract row index of non-null value
1200                     npy_int32 i = indices[i_idx];
1201                     // write resulting gradient to sparse output
1202                     zout[i_idx] = data[i_idx] + Db[i * Sb];
1203                 }
1204             }
1205         }
1206         """ % dict(locals(), **sub)
1207     def __str__(self):
1208         return self.__class__.__name__
1209 structured_add_s_v_csr = StructuredAddSVCSR()
1210 @gof.local_optimizer([sparse.structured_add_s_v])
1211 def local_structured_add_s_v(node):
1212     if node.op == sparse.structured_add_s_v:
1213         x, y = node.inputs
1214         x_is_sparse_variable = _is_sparse_variable(x)
1215         if x_is_sparse_variable:
1216             svar = x
1217             dvar = y
1218         else:
1219             svar = y
1220             dvar = x
1221         if dvar.type.ndim != 1:
1222             return False
1223         elif svar.type.format == 'csr':
1224             CSx = sparse.CSR
1225             structured_add_s_v_csx = structured_add_s_v_csr
1226         else:
1227             return False
1228         s_val, s_ind, s_ptr, s_shape = sparse.csm_properties(svar)
1229         c_data = structured_add_s_v_csx(s_val, s_ind, s_ptr, dvar)
1230         return [CSx(c_data, s_ind, s_ptr, s_shape)]
1231     return False
1232 register_specialize(local_structured_add_s_v, 'cxx_only')
1233 class SamplingDotCSR(gof.Op):
1234     """
1235     Operand optimized for calculating the dot product dot(`x`, `y`.T) = `z`
1236     when you only want to calculate a subset of `z`.
1237     It is equivalent to `p` o (`x` . `y`.T) where o is the element-wise
1238     product, `x` and `y` operands of the dot product and `p` is a matrix
1239     that contains 1 when the corresponding element of `z` should be
1240     calculated and 0 when it shouldn't. Note that SamplingDot has a different
1241     interface than `dot` because SamplingDot requires `x` to be a `m`x`k`
1242     matrix while `y` is a `n`x`k` matrix instead of the usual `k`x`n` matrix.
1243     Parameters
1244     ----------
1245     x
1246         Tensor matrix.
1247     y
1248         Tensor matrix.
1249     p_data
1250         Sparse matrix data.
1251     p_ind
1252         Sparse matrix indices.
1253     p_ptr
1254         Sparse matric indptr.
1255     p_ncols
1256         Sparse matrix number of columns.
1257     Returns
1258     -------
1259     A dense matrix containing the dot product of `x` by `y`.T only
1260     where `p` is 1.
1261     Notes
1262     -----
1263     It will work if the pattern is not binary value, but if the
1264     pattern doesn't have a high sparsity proportion it will be slower
1265     then a more optimized dot followed by a normal elemwise
1266     multiplication.
1267     If we have the input of mixed dtype, we insert cast elemwise
1268     in the graph to be able to call blas function as they don't
1269     allow mixed dtype.
1270     This op is used as an optimization for SamplingDot.
1271     """
1272     __props__ = ()
1273     <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, x, y, p_data, p_ind, p_ptr, p_ncols):
1274         x = tensor.as_tensor_variable(x)
1275         y = tensor.as_tensor_variable(y)
1276         p_data = tensor.as_tensor_variable(p_data)
1277         p_ind = tensor.as_tensor_variable(p_ind)
1278         p_ptr = tensor.as_tensor_variable(p_ptr)
1279         p_ncols = tensor.as_tensor_variable(</b></font>p_ncols)
1280         assert p_ncols.dtype == 'int32'
1281         dtype_out = scalar.upcast(x.type.dtype, y.type.dtype,
1282                                   p_data.type.dtype)
1283         dot_out = scalar.upcast(x.type.dtype, y.type.dtype)
1284         x = tensor.cast(x, dot_out)
1285         y = tensor.cast(y, dot_out)
1286         return gof.Apply(self, [x, y, p_data, p_ind, p_ptr, p_ncols], [
1287             tensor.tensor(dtype=dtype_out, broadcastable=(False,)),
1288             tensor.tensor(dtype=p_ind.type.dtype, broadcastable=(False,)),
1289             tensor.tensor(dtype=p_ptr.type.dtype, broadcastable=(False,))
1290     def c_code_cache_version(self):
1291         return (4, blas<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.blas_header_version())
1292     def c_support_code(self):
1293         return blas.blas_header_text()
1294     def c_libraries(self):
1295         return blas.ldflags()
1296     def c_compile_args(self):
1297         return blas.ldflags(</b></font>libs=False, flags=True)
1298     def c_lib_dirs(self):
1299         return blas.ldflags(libs=False, libs_dir=True)
1300     def c_header_dirs(self):
1301         return blas.ldflags(libs=False, include_dir=True)
1302     def c_code(self, node, name, inputs, outputs, sub):
1303         x, y, p_data, p_ind, p_ptr, p_ncols = inputs
1304         z_data, z_ind, z_ptr = outputs
1305         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
1306             raise NotImplementedError('Complex types are not supported for x')
1307         if node.inputs[1].type.dtype in ('complex64', 'complex128'):
1308             raise NotImplementedError('Complex types are not supported for y')
1309         if node.inputs[2].type.dtype in ('complex64', 'complex128'):
1310             raise NotImplementedError(
1311                 'Complex types are not supported for pattern')
1312         dot_out = scalar.upcast(node.inputs[0].type.dtype,
1313                                 node.inputs[1].type.dtype)
1314         if dot_out == "float32":
1315             conv_type = "float"
1316             cdot = "sdot_"
1317         else:
1318             conv_type = "double"
1319             cdot = "ddot_"
1320         typenum_x = node.inputs[0].type.dtype_specs()[2]
1321         typenum_y = node.inputs[1].type.dtype_specs()[2]
1322         typenum_p = node.inputs[2].type.dtype_specs()[2]
1323         typenum_zd = tensor.TensorType(node.outputs[0].dtype,
1324                                        []).dtype_specs()[2]
1325         typenum_zi = tensor.TensorType(node.outputs[1].dtype,
1326                                        []).dtype_specs()[2]
1327         typenum_zp = tensor.TensorType(node.outputs[2].dtype,
1328                                        []).dtype_specs()[2]
1329         rval = """
1330         if (PyArray_NDIM(%(x)s) != 2) {
1331 PyErr_SetString(PyExc_NotImplementedError, "rank(x) != 2"); %(fail)s;}
1332         if (PyArray_NDIM(%(y)s) != 2) {
1333 PyErr_SetString(PyExc_NotImplementedError, "rank(y) != 2"); %(fail)s;}
1334         if (PyArray_TYPE(%(x)s) != %(typenum_x)s) {
1335             PyErr_SetString(PyExc_NotImplementedError,
1336                             "Invalid type for x");
1337             %(fail)s;}
1338         if (PyArray_TYPE(%(y)s) != %(typenum_y)s) {
1339             PyErr_SetString(PyExc_NotImplementedError,
1340                             "Invalid type for y");
1341             %(fail)s;}
1342         if (PyArray_TYPE(%(p_data)s) != %(typenum_p)s) {
1343             PyErr_SetString(PyExc_NotImplementedError,
1344                             "Invalid type for pattern");
1345             %(fail)s;}
1346         if (PyArray_DIMS(%(x)s)[1] != PyArray_DIMS(%(y)s)[1]) {
1347             PyErr_SetString(PyExc_NotImplementedError,
1348               "x's number of columns doesn't match y's rows! Note: sampling_dot is different from dot because y is assumed to be transposed.");
1349             %(fail)s;}
1350         if (PyArray_DIMS(%(y)s)[0] != ((npy_int32 *)PyArray_DATA(%(p_ncols)s))[0] ||
1351             PyArray_DIMS(%(x)s)[0] != (PyArray_DIMS(%(p_ptr)s)[0] - 1))
1352         {PyErr_SetString(PyExc_NotImplementedError,
1353         "The dimension of the pattern and the output must match"); %(fail)s;}
1354         // Allocate output
1355         if (!%(z_data)s
1356             || (PyArray_DIMS(%(z_data)s)[0] != PyArray_DIMS(%(p_data)s)[0])
1357             || (PyArray_TYPE(%(z_data)s) != %(typenum_zd)s)
1358             || !(PyArray_ISCONTIGUOUS(%(z_data)s)))
1359          {
1360             {Py_XDECREF(%(z_data)s);}
1361             npy_intp dims[] = {0};
1362             dims[0] = PyArray_DIMS(%(p_data)s)[0];
1363             %(z_data)s = (PyArrayObject*) PyArray_SimpleNew(1, dims,
1364                                                             %(typenum_zd)s);
1365         }
1366         if (!%(z_ind)s
1367             || (PyArray_DIMS(%(z_ind)s)[0] != PyArray_DIMS(%(p_ind)s)[0])
1368             || (PyArray_TYPE(%(z_ind)s) != %(typenum_zi)s)
1369             || !(PyArray_ISCONTIGUOUS(%(z_ind)s)))
1370         {
1371             {Py_XDECREF(%(z_ind)s);}
1372             npy_intp dims[] = {0};
1373             dims[0] = PyArray_DIMS(%(p_ind)s)[0];
1374             %(z_ind)s = (PyArrayObject*) PyArray_SimpleNew(1, dims,
1375                                                            %(typenum_zi)s);
1376         }
1377         if (!%(z_ptr)s
1378             || (PyArray_DIMS(%(z_ptr)s)[0] != PyArray_DIMS(%(p_ptr)s)[0])
1379             || (PyArray_TYPE(%(z_ptr)s) != %(typenum_zp)s)
1380             || !(PyArray_ISCONTIGUOUS(%(z_ptr)s)))
1381         {
1382             {Py_XDECREF(%(z_ptr)s);}
1383             npy_intp dims[] = {0};
1384             dims[0] = PyArray_DIMS(%(p_ptr)s)[0];
1385             %(z_ptr)s = (PyArrayObject*) PyArray_SimpleNew(1, dims,
1386                                                            %(typenum_zp)s);
1387         }
1388         {
1389             // Product of MxK and NxK, output MxN
1390             npy_intp M = PyArray_DIMS(%(x)s)[0];
1391             npy_intp N = PyArray_DIMS(%(y)s)[0];
1392             npy_intp K = PyArray_DIMS(%(y)s)[1];
1393             // pointers to access actual data in the arrays passed as params.
1394             const dtype_%(x)s* __restrict__ Dx = (dtype_%(x)s*)PyArray_DATA(%(x)s);
1395             const dtype_%(y)s* __restrict__ Dy = (dtype_%(y)s*)PyArray_DATA(%(y)s);
1396             const dtype_%(p_data)s* __restrict__ Dpd = (dtype_%(p_data)s*)PyArray_DATA(%(p_data)s);
1397             const dtype_%(p_ind)s* __restrict__ Dpi = (dtype_%(p_ind)s*)PyArray_DATA(%(p_ind)s);
1398             const dtype_%(p_ptr)s* __restrict__ Dpp = (dtype_%(p_ptr)s*)PyArray_DATA(%(p_ptr)s);
1399             dtype_%(z_data)s* __restrict__ Dzd = (dtype_%(z_data)s*)PyArray_DATA(%(z_data)s);
1400             dtype_%(z_ind)s* __restrict__ Dzi = (dtype_%(z_ind)s*)PyArray_DATA(%(z_ind)s);
1401             dtype_%(z_ptr)s* __restrict__ Dzp = (dtype_%(z_ptr)s*)PyArray_DATA(%(z_ptr)s);
1402             const npy_intp Sdx = PyArray_STRIDES(%(x)s)[1]/PyArray_DESCR(%(x)s)-&gt;elsize;
1403             const npy_intp Sdy = PyArray_STRIDES(%(y)s)[1]/PyArray_DESCR(%(y)s)-&gt;elsize;
1404             const npy_intp Sdpd = PyArray_STRIDES(%(p_data)s)[0] / PyArray_DESCR(%(p_data)s)-&gt;elsize;
1405             const npy_intp Sdpi = PyArray_STRIDES(%(p_ind)s)[0] / PyArray_DESCR(%(p_ind)s)-&gt;elsize;
1406             const npy_intp Sdpp = PyArray_STRIDES(%(p_ptr)s)[0] / PyArray_DESCR(%(p_ptr)s)-&gt;elsize;
1407             const npy_intp Sdzd = PyArray_STRIDES(%(z_data)s)[0] / PyArray_DESCR(%(z_data)s)-&gt;elsize;
1408             const npy_intp Sdzi = PyArray_STRIDES(%(z_ind)s)[0] / PyArray_DESCR(%(z_ind)s)-&gt;elsize;
1409             const npy_intp Sdzp = PyArray_STRIDES(%(z_ptr)s)[0] / PyArray_DESCR(%(z_ptr)s)-&gt;elsize;
1410             memcpy(Dzi, Dpi, PyArray_DIMS(%(p_ind)s)[0]*sizeof(dtype_%(p_ind)s));
1411             memcpy(Dzp, Dpp, PyArray_DIMS(%(p_ptr)s)[0]*sizeof(dtype_%(p_ptr)s));
1412             // blas expects ints; convert here (rather than just making K etc ints) to avoid potential overflow in the negative-stride correction
1413             if ((K &gt; 0x7fffffffL)||(Sdx &gt; 0x7fffffffL)||(Sdy &gt; 0x7fffffffL)||(Sdx &lt; -0x7fffffffL)||(Sdy &lt; -0x7fffffffL))
1414             {PyErr_SetString(PyExc_NotImplementedError, "array too big for BLAS (overflows int32 index)"); %(fail)s;}
1415             int K32 = K;
1416             int Sdx32 = Sdx;
1417             int Sdy32 = Sdy;
1418             for (npy_intp m = 0; m &lt; M; ++m) {
1419                 for (npy_int32 n_idx = Dpp[m * Sdpp]; n_idx &lt; Dpp[(m+1)*Sdpp]; ++n_idx) {
1420                     const npy_int32 n = Dpi[n_idx * Sdpi]; // row index of non-null value for column K
1421                     const dtype_%(x)s* x_row = (dtype_%(x)s*)(PyArray_BYTES(%(x)s) + PyArray_STRIDES(%(x)s)[0] * m);
1422                     const dtype_%(y)s* y_col = (dtype_%(y)s*)(PyArray_BYTES(%(y)s) + PyArray_STRIDES(%(y)s)[0] * n);
1423                     // dot expects pointer to the beginning of memory arrays,
1424                     // so when the stride is negative, we need to get the
1425                     // last element
1426                     if (Sdx &lt; 0)
1427                         x_row += (K - 1) * Sdx;
1428                     if (Sdy &lt; 0)
1429                         y_col += (K - 1) * Sdy;
1430                     Dzd[n_idx * Sdzd] = Dpd[n_idx * Sdpd] * %(cdot)s(&amp;K32, (const %(conv_type)s*)x_row, &amp;Sdx32, (const %(conv_type)s*)y_col, &amp;Sdy32);
1431                 }
1432             }
1433         }
1434         """ % dict(locals(), **sub)
1435         return rval
1436 sampling_dot_csr = SamplingDotCSR()
1437 @gof.local_optimizer([sparse.sampling_dot])
1438 def local_sampling_dot_csr(node):
1439     if not theano.config.blas.ldflags:
1440         return
1441     if node.op == sparse.sampling_dot:
1442         x, y, p = node.inputs
1443         if p.type.format == 'csr':
1444             p_data, p_ind, p_ptr, p_shape = sparse.csm_properties(p)
1445             z_data, z_ind, z_ptr = sampling_dot_csr(x, y, p_data,
1446                                                     p_ind, p_ptr, p_shape[1])
1447             return [sparse.CSR(z_data, z_ind, z_ptr, p_shape)]
1448     return False
1449 register_specialize(local_sampling_dot_csr,
1450                     'cxx_only',
1451                     name='local_sampling_dot_csr')
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_blas_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 from copy import copy
3 from itertools import product as itertools_product
4 from unittest import TestCase
5 import numpy as np
6 from numpy import (arange, array, common_type, complex64, complex128, float32,
7                    float64, newaxis, shape, transpose, zeros)
8 from numpy.testing import assert_array_almost_equal
9 from itertools import product
10 from six.moves import xrange
11 import theano
12 import theano.tensor as T
13 from theano import tensor, In, shared, config
14 from theano.compat import exc_message
15 from theano.printing import pp
16 from theano.tensor.blas import (_dot22, _dot22scalar, res_is_a, _as_scalar,
17                                 _is_real_matrix, _gemm_canonicalize,
18                                 _factor_canonicalized, Gemm, Gemv,
19                                 gemm_inplace, gemm_no_inplace,
20                                 InconsistencyError, Ger, ger, ger_destructive)
21 from theano.tests import unittest_tools
22 from .test_basic import (as_tensor_variable, inplace_func,
23                          compile, inplace)
24 import theano.tensor.blas_scipy
25 from theano.tests.unittest_tools import attr
26 if config.mode == 'FAST_COMPILE':
27     mode_not_fast_compile = 'FAST_RUN'
28 else:
29     mode_not_fast_compile = config.mode
30 mode_blas_opt = theano.compile.get_default_mode().including(
31     'BlasOpt', 'specialize', 'InplaceBlasOpt')
32 mode_blas_opt = mode_blas_opt.excluding('c_blas')
33 def test_dot_eq():
34     assert T.Dot() == T.Dot()
35 def sharedX(x, name):
36     return theano.shared(np.asarray(x, config.floatX), name=name)
37 class t_gemm(TestCase):
38     """
39     This test suite is supposed to establish that gemm works as it is supposed to.
40     """
41     def setUp(self):
42         unittest_tools.seed_rng()
43         Gemm.debug = False
44     @staticmethod
45     def _gemm(z, a, x, y, b):
46         assert a.shape == ()
47         assert b.shape == ()
48         return b * z + a * np.dot(x, y)
49     @staticmethod
50     def rand(*args):
51         return np.random.rand(*args)
52     def cmp(self, z_, a_, x_, y_, b_):
53         for dtype in ['float32', 'float64', 'complex64', 'complex128']:
54             z = np.asarray(z_, dtype=dtype)
55             a = np.asarray(a_, dtype=dtype)
56             x = np.asarray(x_, dtype=dtype)
57             y = np.asarray(y_, dtype=dtype)
58             b = np.asarray(b_, dtype=dtype)
59             def cmp_linker(z, a, x, y, b, l):
60                 z, a, x, y, b = [np.asarray(p) for p in (z, a, x, y, b)]
61                 z_orig = z.copy()
62                 tz, ta, tx, ty, tb = [as_tensor_variable(p).type()
63                                       for p in (z, a, x, y, b)]
64                 f = inplace_func([tz, ta, tx, ty, tb],
65                                  gemm_inplace(tz, ta, tx, ty, tb),
66                                  mode=compile.Mode(optimizer=None, linker=l))
67                 f(z, a, x, y, b)
68                 z_after = self._gemm(z_orig, a, x, y, b)
69                 unittest_tools.assert_allclose(z_after, z)
70                 if a == 0.0 and b == 1.0:
71                     return
72                 elif z_orig.size == 0:
73                     self.assertTrue(z.size == 0)
74                 else:
75                     self.assertFalse(np.all(z_orig == z))
76             cmp_linker(copy(z), a, x, y, b, 'c|py')
77             cmp_linker(copy(z), a, x, y, b, 'py')
78             if (not dtype.startswith("complex") and theano.config.cxx):
79                 cmp_linker(copy(z), a, x, y, b, 'c')
80     def test0a(self):
81         Gemm.debug = True
82         try:
83             gemm_no_inplace([1.], 1., [1.], [1.], 1.)
84         except TypeError as e:
85             if exc_message(e) is Gemm.E_rank:
86                 return
87         self.fail()
88     def test0(self):
89         try:
90             self.cmp(1., 0., 1.0, 1.0, 1.0)
91         except TypeError as e:
92             if exc_message(e) is Gemm.E_rank:
93                 return
94         self.fail()
95     def test2(self):
96         try:
97             self.cmp(2., 1.0, [3, 2, 1.], [[1], [2], [3.]], 1.0)
98         except TypeError as e:
99             self.assertTrue(exc_message(e) == Gemm.E_rank)
100             return
101         self.fail()
102     def test4(self):
103         self.cmp(self.rand(3, 4), 1.0, self.rand(3, 5), self.rand(5, 4), 0.0)
104     def test5(self):
105         self.cmp(self.rand(3, 4), 1.0,
106                  self.rand(3, 5), self.rand(5, 4), 1.0)
107     def test6(self):
108         self.cmp(self.rand(3, 4), 1.0,
109                  self.rand(3, 5), self.rand(5, 4), -1.0)
110     def test7(self):
111         self.cmp(self.rand(3, 4), 0.0,
112                  self.rand(3, 5), self.rand(5, 4), 0.0)
113     def test8(self):
114         self.cmp(self.rand(3, 4), 0.0,
115                  self.rand(3, 5), self.rand(5, 4), 0.6)
116     def test9(self):
117         self.cmp(self.rand(3, 4), 0.0,
118                  self.rand(3, 5), self.rand(5, 4), -1.0)
119     def test10(self):
120         self.cmp(self.rand(3, 4), -1.0, self.rand(3, 5), self.rand(5, 4), 0.0)
121     def test11(self):
122         self.cmp(self.rand(3, 4), -1.0,
123                  self.rand(3, 5), self.rand(5, 4), 1.0)
124     def test12(self):
125         self.cmp(self.rand(3, 4), -1.0,
126                  self.rand(3, 5), self.rand(5, 4), -1.0)
127     def test_shape_0(self):
128         self.cmp(self.rand(0, 4), -1.0, self.rand(0, 5), self.rand(5, 4), -1.0)
129         self.cmp(self.rand(3, 0), -1.0, self.rand(3, 5), self.rand(5, 0), -1.0)
130         self.cmp(self.rand(3, 4), -1.0, self.rand(3, 0), self.rand(0, 4), -1.0)
131         self.cmp(self.rand(0, 0), -1.0, self.rand(0, 5), self.rand(5, 0), -1.0)
132         self.cmp(self.rand(0, 0), -1.0, self.rand(0, 0), self.rand(0, 0), -1.0)
133     def test_factorised_scalar(self):
134         a = T.matrix()
135         b = T.matrix()
136         s = theano.shared(np.zeros((5, 5)).astype(config.floatX))
137         lr1 = T.constant(0.01).astype(config.floatX)
138         lr2 = T.constant(2).astype(config.floatX)
139         l2_reg = T.constant(0.0001).astype(config.floatX)
140         f = theano.function([a, b], updates=[(s, lr1 * T.dot(a, b) +
141                             l2_reg * lr2 * s)],
142                             mode=mode_not_fast_compile).maker.fgraph.toposort()
143         assert len(f) == 1
144         assert f[0].op == gemm_inplace
145         f = theano.function([a, b], updates=[(s, lr1 * (T.dot(a, b) -
146                                                         l2_reg * s))],
147                             mode=mode_not_fast_compile).maker.fgraph.toposort()
148         assert len(f) == 1
149         assert f[0].op == gemm_inplace
150         f = theano.function([a, b],
151                             updates=[(s, s - lr1 * (s * .0002 + T.dot(a, b)))],
152                             mode=mode_not_fast_compile).maker.fgraph.toposort()
153         assert len(f) == 1
154         assert f[0].op == gemm_inplace
155     def test_destroy_map0(self):
156         Z = as_tensor_variable(self.rand(2, 2))
157         try:
158             gemm_inplace(Z, 1.0, Z, Z, 1.0)
159         except InconsistencyError as e:
160             if exc_message(e) == Gemm.E_z_uniq:
161                 return
162         self.fail()
163     def test_destroy_map1(self):
164         Z = as_tensor_variable(self.rand(2, 2))
165         A = as_tensor_variable(self.rand(2, 2))
166         try:
167             gemm_inplace(Z, 1.0, A, inplace.transpose_inplace(Z), 1.0)
168         except InconsistencyError as e:
169             if exc_message(e) == Gemm.E_z_uniq:
170                 return
171         self.fail()
172     def test_destroy_map2(self):
173         Z = as_tensor_variable(self.rand(2, 2))
174         A = as_tensor_variable(self.rand(2, 2))
175         try:
176             gemm_inplace(Z, 1.0, inplace.transpose_inplace(Z), A, 1.0)
177         except InconsistencyError as e:
178             if exc_message(e) == Gemm.E_z_uniq:
179                 return
180         self.fail()
181     def test_destroy_map3(self):
182         Z = as_tensor_variable(self.rand(2, 2))
183         A = as_tensor_variable(self.rand(2, 2))
184         try:
185             gemm_inplace(Z, 1.0, Z, A, 1.0)
186         except InconsistencyError as e:
187             if exc_message(e) == Gemm.E_z_uniq:
188                 return
189         self.fail()
190     def test_destroy_map4(self):
191         Z = shared(self.rand(2, 2), name='Z')
192         A = shared(self.rand(2, 2), name='A')
193         one = T.constant(1.0).astype(Z.dtype)
194         f = inplace_func([], gemm_inplace(Z, one, A, A, one))
195         f()
196         f = inplace_func([], gemm_inplace(Z, one, A, A.T, one))
197         f()
198     def test_transposes(self):
199         A = self.rand(4, 5)[:, :4]
200         B = self.rand(4, 5)[:, :4]
201         C = self.rand(4, 5)[:, :4]
202         def t(z, x, y, a=1.0, b=0.0, l='c|py', dt='float64'):
203             z, a, x, y, b = [theano._asarray(p, dtype=dt)
204                              for p in (z, a, x, y, b)]
205             z_after = self._gemm(z, a, x, y, b)
206             tz, ta, tx, ty, tb = [shared(p) for p in (z, a, x, y, b)]
207             f = inplace_func([], gemm_inplace(tz, ta, tx, ty, tb),
208                              mode=compile.Mode(optimizer=None, linker=l))
209             f()
210             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
211             f()
212             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
213             f()
214             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
215             y_T = ty.get_value(borrow=True).T
216             ty.set_value(tx.get_value(borrow=True).T, borrow=True)
217             tx.set_value(y_T, borrow=True)
218             f()
219             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True).T)
220         t(C, A, B)
221         t(C.T, A, B)
222         t(C, A.T, B, dt='float32')
223         t(C, A, B.T)
224         t(C.T, A.T, B)
225         t(C, A.T, B.T, dt='float32')
226         t(C.T, A, B.T)
227         t(C.T, A.T, B.T, dt='float32')
228         t(C, A[:, :2], B[:2, :])
229         t(C.T, A[:, :2], B[:2, :], dt='float32')
230         t(C, A[:2, :].T, B[:2, :])
231         t(C.T, A[:2, :].T, B[:2, :], dt='float32')
232         t(C, A[:2, :].T, B[:, :2].T)
233         t(C.T, A[:2, :].T, B[:, :2].T)
234         try:
235             t(C.T, A[:2, :], B[:, :2].T)
236         except ValueError as e:
237             if exc_message(e).find('aligned') &gt;= 0:
238                 return
239         self.fail()
240     def test_non_contiguous(self):
241         A = self.rand(4, 4, 3)
242         B = self.rand(4, 4, 3)
243         C = self.rand(4, 4, 3)
244         def t(z, x, y, a=1.0, b=0.0, l='c|py', dt='float64'):
245             z, a, x, y, b = [theano._asarray(p, dtype=dt)
246                              for p in (z, a, x, y, b)]
247             z_orig = z.copy()
248             z_after = np.zeros_like(z_orig)
249             for i in xrange(3):
250                 z_after[:, :, i] = self._gemm(z[:, :, i], a,
251                                               x[:, :, i], y[:, :, i], b)
252             tz, ta, tx, ty, tb = [shared(p) for p in (z, a, x, y, b)]
253             for i in xrange(3):
254                 f_i = inplace_func([],
255                                    gemm_inplace(tz[:, :, i],
256                                    ta, tx[:, :, i], ty[:, :, i], tb),
257                                    mode=compile.Mode(optimizer=None, linker=l))
258                 for j in xrange(3):
259                     z_i = f_i()
260                     z = tz.get_value(borrow=True, return_internal_type=True)
261                     z[:, :, i] = z_i
262                     unittest_tools.assert_allclose(z_after[:, :, i],
263                                                    tz.get_value(borrow=True)[:, :, i])
264                 tz_i = gemm_no_inplace(tz[:, :, i], ta, tx[
265                     :, :, i], ty[:, :, i], tb)
266                 g_i = theano.function(
267                     [], tz_i, updates=[(tz, T.set_subtensor(tz[:, :, i],
268                                                             tz_i))],
269                     mode=compile.Mode(optimizer=None, linker=l))
270                 for j in xrange(3):
271                     g_i()
272                     unittest_tools.assert_allclose(z_after[:, :, i],
273                                                    tz.get_value(borrow=True)[:, :, i])
274         t(C, A, B)
275         t(C.transpose((1, 0, 2)), A, B)
276         t(C, A.transpose((1, 0, 2)), B, dt='float32')
277         t(C, A, B.transpose((1, 0, 2)))
278         t(C.transpose((1, 0, 2)), A.transpose((1, 0, 2)), B)
279         t(C, A.transpose((1, 0, 2)), B.transpose((1, 0, 2)), dt='float32')
280         t(C.transpose((1, 0, 2)), A, B.transpose((1, 0, 2)))
281         t(C.transpose((1, 0, 2)), A.transpose((1, 0, 2)), B.transpose((
282             1, 0, 2)), dt='float32')
283 class TestGemmNoFlags(object):
284     gemm = gemm_no_inplace
285     M = 4
286     N = 5
287     K = 6
288     slice_step = 3
289     def setUp(self):
290         unittest_tools.seed_rng()
291     def get_variable(self, V, to_transpose, to_slice):
292         if to_transpose:
293             V = V.T
294         if to_slice:
295             V = V[::self.slice_step]
296         return V
297     def get_function(self, dtype,
298                      transpose_A=False, transpose_B=False, transpose_C=False,
299                      slice_A=False, slice_B=False, slice_C=False):
300         alpha = theano.tensor.scalar(dtype=dtype, name='alpha')
301         beta = theano.tensor.scalar(dtype=dtype, name='beta')
302         A = theano.tensor.matrix(dtype=dtype, name='A')
303         B = theano.tensor.matrix(dtype=dtype, name='B')
304         C = theano.tensor.matrix(dtype=dtype, name='C')
305         A1 = self.get_variable(A, transpose_A, slice_A)
306         B1 = self.get_variable(B, transpose_B, slice_B)
307         C1 = self.get_variable(C, transpose_C, slice_C)
308         return theano.function([alpha, A, B, beta, C], self.gemm(C1, alpha, A1, B1, beta))
309     def generate_value(self, dtype, width, height, to_transpose, to_slice):
310         if to_slice:
311             if to_transpose:
312                 shape = (height, width * self.slice_step)
313             else:
314                 shape = (width * self.slice_step, height)
315         else:
316             if to_transpose:
317                 shape = (height, width)
318             else:
319                 shape = (width, height)
320         return np.random.random(shape).astype(dtype)
321     def get_data(self, dtype, alpha, beta,
322                  transpose_A=False, transpose_B=False, transpose_C=False,
323                  slice_A=False, slice_B=False, slice_C=False):
324         A = self.generate_value(dtype, self.M, self.N, transpose_A, slice_A)
325         B = self.generate_value(dtype, self.N, self.K, transpose_B, slice_B)
326         C = self.generate_value(dtype, self.M, self.K, transpose_C, slice_C)
327         return (alpha, A, B, beta, C)
328     def get_value(self, V, to_transpose, to_slice):
329         if to_transpose:
330             V = V.T
331         if to_slice:
332             V = V[::self.slice_step]
333         return V
334     def compute_ref(self, alpha, A, B, beta, C,
335                     transpose_A, transpose_B, transpose_C,
336                     slice_A, slice_B, slice_C):
337         A = self.get_value(A, transpose_A, slice_A)
338         B = self.get_value(B, transpose_B, slice_B)
339         C = self.get_value(C, transpose_C, slice_C)
340         return alpha * np.dot(A, B) + beta * C
341     @theano.change_flags({'blas.ldflags': ''})
342     def run_gemm(self, dtype, ALPHA, BETA,
343                  transpose_A, transpose_B, transpose_C,
344                  slice_A, slice_B, slice_C):
345         f = self.get_function(dtype, transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)
346         values = self.get_data(dtype, ALPHA, BETA, transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)
347         z_val = f(*values)
348         assert z_val.dtype == dtype
349         assert tuple<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(z_val.shape) == (self.M, self.K)
350         ref_val = self.compute_ref(*(values + (transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)))
351         unittest_tools.assert_allclose(</b></font>ref_val, z_val)
352     def test_gemm(self):
353         dtypes = ('float32', 'float64')
354         scalars = (0, 1, -2)
355         booleans = (False, True)
356         iterables = [dtypes] + ([scalars] * 2) + ([booleans] * 6)
357         for dtype, alpha, beta, tA, tB, tC, sA, sB, sC in product(*iterables):
358             yield (self.run_gemm, dtype, alpha, beta, tA, tB, tC, sA, sB, sC)
359 def test_res_is_a():
360     X, Y, Z, a, b = XYZab()
361     assert not res_is_a(a, T.sqrt)
362     assert not res_is_a(a + a, T.sqrt)
363     assert res_is_a(T.sqrt(a + a), T.sqrt)
364 class t_as_scalar(TestCase):
365     def test0(self):
366         a = T.constant(2.5)
367         b = T.constant(np.asarray([[[0.5]]]))
368         b2 = b.dimshuffle()
369         assert b2.ndim == 0
370         d_a = T.DimShuffle([], [])(a)
371         d_b = T.DimShuffle([True, True, True], [0, 2, 1])(b)
372         d_a2 = T.DimShuffle([], ['x', 'x', 'x'])(a)
373         self.assertTrue(_as_scalar(a) == a)
374         self.assertTrue(_as_scalar(b) != b)
375         self.assertTrue(_as_scalar(d_a) != d_a)
376         self.assertTrue(_as_scalar(d_b) != d_b)
377         self.assertTrue(_as_scalar(d_a2) != d_a2)
378     def test1(self):
379         a = T.constant(np.ones(5))
380         self.assertTrue(_as_scalar(a) is None)
381         self.assertTrue(_as_scalar(T.DimShuffle([False], [0, 'x'])(a)) is None)
382     def test2(self):
383         a = T.dscalar()
384         d_a = T.DimShuffle([], [])(a)
385         d_a2 = T.DimShuffle([], ['x', 'x'])(a)
386         self.assertTrue(_as_scalar(a) is a)
387         self.assertTrue(_as_scalar(d_a) is a)
388         self.assertTrue(_as_scalar(d_a2) is a)
389     def test3(self):
390         a = T.matrix()
391         self.assertTrue(_as_scalar(a) is None)
392         self.assertTrue(_as_scalar(T.DimShuffle([False, False],
393                                                 [0, 'x', 1])(a)) is None)
394 class T_real_matrix(TestCase):
395     def test0(self):
396         self.assertTrue(_is_real_matrix(T.DimShuffle([False, False],
397                                                      [1, 0])(T.matrix())))
398         self.assertTrue(not _is_real_matrix(T.DimShuffle([False],
399                                                          ['x', 0])
400                                             (T.dvector())))
401 def fail(msg):
402     print('FAIL', msg)
403     assert False
404 """
405 This test suite ensures that Gemm is inserted where it belongs, and
406 that the resulting functions compute the same things as the originals.
407 """
408 def XYZab():
409     return T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
410 class Failure(Exception):
411     pass
412 def just_gemm(i, o, ishapes=[(4, 3), (3, 5), (4, 5), (), ()],
413               max_graphlen=0, expected_nb_gemm=1):
414     try:
415         f = inplace_func(
416             [In(ii, mutable=True, allow_downcast=True) for ii in i],
417             o,
418             mode='FAST_RUN',
419             on_unused_input='ignore')
420         nb_gemm = 0
421         for node in f.maker.fgraph.apply_nodes:
422             if isinstance(node.op, T.Dot):
423                 raise Failure('dot not changed to gemm_inplace in graph')
424             if node.op == _dot22:
425                 raise Failure('_dot22 not changed to gemm_inplace in graph')
426             if node.op == gemm_inplace:
427                 nb_gemm += 1
428         assert nb_gemm == expected_nb_gemm, (nb_gemm, expected_nb_gemm)
429         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
430                          allow_input_downcast=True, on_unused_input='ignore')
431         for node in g.maker.fgraph.apply_nodes:
432             if node.op == gemm_inplace:
433                 raise Exception('gemm_inplace in original graph')
434         graphlen = len(f.maker.fgraph.toposort())
435         if max_graphlen and (graphlen &lt;= max_graphlen):
436             assert False, 'graphlen=%i&gt;%i' % (graphlen, max_graphlen)
437         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
438         r0 = f(*[np.asarray(rng.randn(*sh), config.floatX)
439                  for sh in ishapes])
440         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
441         r1 = g(*[np.asarray(rng.randn(*sh), config.floatX)
442                  for sh in ishapes])
443         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
444         eps = 1.0e-8
445         if config.floatX == 'float32':
446             eps = 1.0e-6
447         if max_abs_err &gt; eps:
448             raise Failure('GEMM is computing the wrong output. max_rel_err =',
449                           max_abs_err)
450     except Failure:
451         for node in f.maker.fgraph.toposort():
452             print('GRAPH', node)
453         raise
454 @unittest_tools.assertFailure_fast
455 def test_gemm_opt0():
456     X, Y, Z, a, b = XYZab()
457     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a + Z * b])
458     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) + b * Z])
459     just_gemm([X, Y, Z, a, b], [b * Z + a * T.dot(X, Y)])
460     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a - Z * b])
461     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) - b * Z])
462     just_gemm([X, Y, Z, a, b], [b * Z - a * T.dot(X, Y)])
463     just_gemm([X, Y, Z, a, b], [b * Z.T - a * T.dot(Y.T, X.T)])
464     just_gemm([X, Y, Z, a, b], [b * Z.T + a * b * T.dot(X, Y).T])
465     just_gemm([<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>X, Y, Z, a, b], [b * Z + a * T.dot(X, Y).T],
466               ishapes=[(5, 3), (3, 4), (4, 5), (), ()])
467     just_gemm([X</b></font>, Y, Z, a, b], [(b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
468     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y)])
469     just_gemm([X, Y, Z, a, b], [Z * b + T.dot(X, Y)])
470     just_gemm([X, Y, Z, a, b], [Z + a * b * a * T.dot(X, Y)])
471     just_gemm([X, Y, Z, a, b], [(b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
472     just_gemm([X, Y, Z, a, b], [Z - T.dot(X, Y)])
473     just_gemm([X, Y, Z, a, b], [Z * b - T.dot(X, Y)])
474     just_gemm([X, Y, Z, a, b], [Z - a * b * a * T.dot(X, Y)])
475 @unittest_tools.assertFailure_fast
476 def test_gemm_opt_double_gemm():
477     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
478     R, S, c = T.matrix(), T.matrix(), T.scalar()
479     just_gemm([X, Y, Z, a, b, R, S, c],
480               [Z * c + a * T.dot(X, Y) + b * T.dot(R, S).T],
481               ishapes=[(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()],
482               expected_nb_gemm=2)
483     ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()]
484     i = [X, Y, Z, a, b, R, S, c]
485     o = [(a * T.dot(X, Y) +
486          gemm_inplace(Z, b, S.T, R.T, T.constant(1.0).astype(config.floatX)))]
487     try:
488         f = inplace_func([In(ii, mutable=True) for ii in i], o,
489                          mode='FAST_RUN', on_unused_input='ignore')
490         for node in f.maker.fgraph.apply_nodes:
491             if isinstance(node.op, T.Dot):
492                 raise Failure('dot in graph')
493             if node.op == _dot22:
494                 raise Failure('_dot22 in graph')
495         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
496                          on_unused_input='ignore')
497         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
498         r0 = f(*[np.asarray(rng.randn(*sh), config.floatX)
499                  for sh in ishapes])
500         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
501         r1 = g(*[np.asarray(rng.randn(*sh), config.floatX)
502                  for sh in ishapes])
503         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
504         eps = 1.0e-8
505         if config.floatX == 'float32':
506             eps = 1.0e-6
507         if max_abs_err &gt; eps:
508             raise Failure(
509                 'GEMM is computing the wrong output. max_rel_err =',
510                 max_abs_err)
511     except Failure:
512         for node in f.maker.fgraph.toposort():
513             print('GRAPH', node)
514         raise
515 def test_gemm_canonicalize():
516     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
517         'a'), T.scalar('b')
518     c, d = T.scalar('c'), T.scalar('d')
519     u = T.row('u')
520     v = T.vector('v')
521     w = T.col('w')
522     can = []
523     _gemm_canonicalize(X + Y + Z, 1.0, can, 0)
524     assert can == [(1.0, X), (1.0, Y), (1.0, Z)]
525     can = []
526     _gemm_canonicalize(X + Y + u, 1.0, can, 0)
527     assert can == [(1.0, X), (1.0, Y), (1.0, u)], can
528     can = []
529     _gemm_canonicalize(X + Y + v, 1.0, can, 0)
530     assert can[:2] == [(1.0, X), (1.0, Y)]
531     assert isinstance(can[2], tuple)
532     assert len(can[2]) == 2
533     assert can[2][0] == 1.0
534     assert can[2][1].owner
535     assert isinstance(can[2][1].owner.op, T.DimShuffle)
536     assert can[2][1].owner.inputs == [v]
537     can = []
538     _gemm_canonicalize(X + Y + w, 1.0, can, 0)
539     assert can == [(1.0, X), (1.0, Y), (1.0, w)], can
540     can = []
541     _gemm_canonicalize(a * X + Y - b * Z * c, 1.0, can, 0)
542     assert can[0] == (a, X)
543     assert can[1] == (1.0, Y)
544     assert can[2][0].owner.op == T.mul
545     assert can[2][0].owner.inputs[0].owner.op == T.neg
546     assert can[2][0].owner.inputs[0].owner.inputs[0] == c
547     assert can[2][0].owner.inputs[1] == b
548     can = []
549     _gemm_canonicalize((-d) * X - (a * X + Y - b * Z * c), 1.0, can, 0)
550     assert can[0][0].owner.op == T.neg
551     assert can[0][0].owner.inputs[0] == d
552     assert can[0][1] == X
553     assert can[1][0].owner.op == T.neg
554     assert can[1][0].owner.inputs[0] == a
555     assert can[2] == (-1.0, Y)
556     assert can[3][0].owner.op == T.mul
557     assert can[3][0].owner.inputs == [c, b]
558 def test_gemm_factor():
559     X, Y = T.matrix('X'), T.matrix('Y')
560     assert [(1.0, X), (1.0, Y)] == _factor_canonicalized([(1.0, X), (1.0, Y)])
561 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def test_upcasting_scalar_nogemm():
562     v = T.fmatrix('v')
563     w = T.fmatrix('w')
564     t = T.fmatrix('t')
565     alpha = T.dscalar('a')
566     rval = T.dot(w, v) * alpha + t
567     f = theano.function(</b></font>[w, v, t, alpha], rval)
568     t = f.maker.fgraph.toposort()
569     assert np.sum([isinstance(n.op, Gemm) for n in t]) == 0
570     v = T.fmatrix('v')
571     w = T.fmatrix('w')
572     t = T.fmatrix('t')
573     alpha = T.cscalar('a')
574     on_opt_error = config.on_opt_error
575     try:
576         config.on_opt_error = 'raise'
577         rval = T.dot(w, v) * alpha + t
578         f = theano.function([w, v, t, alpha], rval)
579     finally:
580         config.on_opt_error = on_opt_error
581     t = f.maker.fgraph.toposort()
582     assert np.sum([isinstance(n.op, Gemm) for n in t]) == 0
583 def test_gemm_nested():
584     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
585         'a'), T.scalar('b')
586     R, S, U, c, d = T.matrix('R'), T.matrix('S'), T.matrix('U'), T.scalar(
587         'c'), T.scalar('d')
588     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
589               [a * Z - b * (c * T.dot(X, Y) + d * Z)],
590               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
591                        (2, 4), (), (), (), ()],
592               max_graphlen=1)
593     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
594               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * Z)],
595               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
596                        (2, 4), (), (), (), ()],
597               max_graphlen=1)
598     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
599               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * U)],
600               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
601                        (2, 4), (), (), (), ()],
602               max_graphlen=3)
603 def test_gemm_opt_wishlist():
604     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
605     just_gemm([X, Y, Z, a, b],
606               [(b * b) * Z * a + (a * a) * T.dot(X, Y) + b * T.dot(X, Y)])
607     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y) + T.dot(X, Y)])
608 def test_gemm_with_vector():
609     X, Y, Z, a, b = XYZab()
610     v = T.vector()
611     def my_just_gemm(o):
612         i = [X, Y, Z, a, b, v]
613         ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, )]
614         just_gemm(i, o, ishapes=ishapes)
615     my_just_gemm([v + T.dot(X, Y) * a + Z * b])
616     my_just_gemm([v + a * T.dot(X, Y) + b * Z])
617     my_just_gemm([v + b * Z + a * T.dot(X, Y)])
618     my_just_gemm([v + T.dot(X, Y) * a - Z * b])
619     my_just_gemm([v + a * T.dot(X, Y) - b * Z])
620     my_just_gemm([v + b * Z - a * T.dot(X, Y)])
621     my_just_gemm([v + (b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
622     my_just_gemm([v + Z + T.dot(X, Y)])
623     my_just_gemm([v + Z * b + T.dot(X, Y)])
624     my_just_gemm([v + Z + a * b * a * T.dot(X, Y)])
625     my_just_gemm([v + (b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
626     my_just_gemm([Z - T.dot(X, Y) + v])
627     my_just_gemm([Z * b - T.dot(X, Y) + v])
628     my_just_gemm([Z - a * b * a * T.dot(X, Y) + v])
629 def test_gemm_opt_vector_stuff():
630     X, Y, a = T.matrix(), T.matrix(), T.scalar()
631     u, v = T.vector(), T.vector()
632     f = inplace_func([a, u, v], a + T.dot(u, v), mode='FAST_RUN')
633     if gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]:
634         raise Failure('gemm_inplace in graph')
635     f = inplace_func([a, u, X, Y], a * u + T.dot(X, Y), mode='FAST_RUN')
636     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
637         raise Failure('gemm_inplace in graph')
638 def test_gemm_unrolled():
639     batch_size = 100
640     rep_size = 40
641     rng = np.random.RandomState([1, 2, 3])
642     for num_rounds in range(1, 10):
643         W = sharedX(rng.randn(rep_size, rep_size), name='W')
644         V = sharedX(np.zeros((batch_size, rep_size)), name='V')
645         H = sharedX(np.zeros((batch_size, rep_size)), name='H')
646         G = sharedX(np.zeros((batch_size, rep_size)), name='G')
647         cur_V = V
648         cur_H = H
649         def update_V(cur_H):
650             return T.nnet.sigmoid(T.dot(cur_H, W.T))
651         def update_H(cur_V):
652             return T.nnet.sigmoid(T.dot(cur_V, W) + T.dot(G, W.T))
653         for i in xrange(num_rounds):
654             cur_V = update_V(cur_H)
655             cur_H = update_H(cur_V)
656         unrolled_theano = theano.function([], updates=[(V, cur_V), (H, cur_H)],
657                                           name='unrolled_theano')
658         nb_dot = sum([1 for node in unrolled_theano.maker.fgraph.toposort()
659                       if isinstance(node.op, (theano.tensor.Dot,
660                                               theano.tensor.blas.Dot22,
661                                               theano.tensor.blas.Gemm))])
662         assert nb_dot == num_rounds * 2 + 1, nb_dot
663         unrolled_theano()
664 def test_inplace0():
665     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
666         'a'), T.scalar('b')
667     R, S, c = T.matrix('R'), T.matrix('S'), T.scalar('c')
668     f = inplace_func([Z, b, R, S],
669                      [Z * (Z + b * T.dot(R, S).T)], mode='FAST_RUN')
670     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
671         print(pp(f.maker.fgraph.outputs[0]))
672         raise Failure('gemm_inplace in graph')
673     assert gemm_no_inplace in [n.op for n in f.maker.fgraph.apply_nodes]
674     f = inplace_func([X, Y, Z, a, b, R, S, c],
675                      [Z * (c * Z + a * T.dot(X, Y) + b * T.dot(R, S).T)],
676                      mode='FAST_RUN')
677     if (gemm_inplace not in [n.op for n in f.maker.fgraph.apply_nodes]):
678         theano.printing.debugprint(f)
679         raise Failure('no gemm_inplace in graph')
680 def test_inplace1():
681     X, Y, Z, a, b = XYZab()
682     f = inplace_func([X, Y, Z],
683                      [Z + Z + T.dot(X, Y)], mode='FAST_RUN')
684     assert [n.op for n in f.maker.fgraph.apply_nodes] == [gemm_no_inplace]
685 def test_dot22():
686     for dtype1 in ['float32', 'float64', 'complex64', 'complex128']:
687         a = T.matrix(dtype=dtype1)
688         for dtype2 in ['float32', 'float64', 'complex64', 'complex128']:
689             b = T.matrix(dtype=dtype2)
690             f = theano.function([a, b], T.dot(a, b), mode=mode_blas_opt)
691             topo = f.maker.fgraph.toposort()
692             if dtype1 == dtype2:
693                 assert _dot22 in [x.op for x in topo], (dtype1, dtype2)
694             else:
695                 check = [isinstance(x.op, T.Dot) for x in topo]
696             rng = np.random.RandomState(unittest_tools.fetch_seed())
697             <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def cmp(a_shp, b_shp):
698                 av = rng.uniform(size=a_shp).astype(dtype1)
699                 bv = rng.uniform(size=b_shp).astype(</b></font>dtype2)
700                 f(av, bv)
701             cmp((3, 4), (4, 5))
702             cmp((0, 4), (4, 5))
703             cmp((3, 0), (0, 5))
704             cmp((3, 4), (4, 0))
705             cmp((0, 4), (4, 0))
706             cmp((0, 0), (0, 0))
707 @attr('slow')
708 def test_dot22scalar():
709     rng = np.random.RandomState(unittest_tools.fetch_seed())
710     for dtype1 in ['complex64', 'complex128']:
711         a = T.matrix('a', dtype=dtype1)
712         for dtype2 in ['complex64', 'complex128']:
713             b = T.matrix('b', dtype=dtype2)
714             for dtype3 in ['complex64', 'complex128']:
715                 c = T.matrix('c', dtype=dtype3)
716                 for dtype4 in ['complex64', 'complex128']:
717                     cst = theano.tensor.basic.constant(.2, dtype=dtype4)
718                     cst2 = theano.tensor.basic.constant(.1, dtype=dtype4)
719                     def check_dot22scalar(func, len_topo_scalar=-1):
720                         topo = func.maker.fgraph.toposort()
721                         ops = [x.op for x in topo]
722                         dtype4_upcast = theano.scalar.upcast(dtype4, dtype1,
723                                                              dtype2)
724                         if dtype1 == dtype2 == dtype3 == dtype4_upcast:
725                             if len_topo_scalar &gt; 0:
726                                 assert len(topo) == len_topo_scalar
727                             assert _dot22scalar in ops, (dtype1, dtype2,
728                                                          dtype3, dtype4)
729                         elif dtype1 == dtype2 == dtype4_upcast:
730                             if not (len_topo_scalar &gt; 0):
731                                 assert len(topo) == len_topo_scalar
732                                 assert _dot22scalar in ops, (dtype1, dtype2,
733                                                              dtype3, dtype4)
734                             else:
735                                 assert _dot22scalar in ops or _dot22 in ops, (
736                                     dtype1, dtype2, dtype3, dtype4)
737                         elif dtype1 == dtype2:
738                             assert _dot22 in ops, (dtype1, dtype2,
739                                                    dtype3, dtype4)
740                         else:
741                             check = [isinstance(o, T.Dot) for o in ops]
742                             assert any(check), (dtype1, dtype2, dtype3, dtype4)
743                     def cmp(a_shp, b_shp, c_shp, sqr_shp=(5, 5)):
744                         av = rng.uniform(size=a_shp).astype(dtype1)
745                         bv = rng.uniform(size=b_shp).astype(dtype2)
746                         cv = rng.uniform(size=c_shp).astype(dtype3)
747                         sv = rng.uniform(size=sqr_shp).astype(dtype1)
748                         if False:
749                             f = theano.function([a, b], cst * T.dot(a, b),
750                                                 mode=mode_blas_opt)
751                             f.maker.fgraph.toposort()
752                             check_dot22scalar(f, 1)
753                             f(av, bv)
754                         if True:
755                             f = theano.function([a, b, c],
756                                                 cst * c * T.dot(a, b),
757                                                 mode=mode_blas_opt)
758                             f.maker.fgraph.toposort()
759                             check_dot22scalar(f, 2)
760                             f(av, bv, cv)
761                         f = theano.function([a, b, c],
762                                             c * cst * T.dot(a, b),
763                                             mode=mode_blas_opt)
764                         f.maker.fgraph.toposort()
765                         check_dot22scalar(f, 2)
766                         f(av, bv, cv)
767                         m2 = mode_blas_opt.including('canonicalize')
768                         f = theano.function([a, b, c],
769                                             cst2 * c * cst * T.dot(a, b),
770                                             mode=m2)
771                         f.maker.fgraph.toposort()
772                         check_dot22scalar(f, 2)
773                         f(av, bv, cv)
774                         if dtype1 == dtype2 == dtype3:
775                             f = theano.function([a, b, c],
776                                                 c * cst * a * T.dot(a, b),
777                                                 mode=m2)
778                             f.maker.fgraph.toposort()
779                             check_dot22scalar(f, 2)
780                             f(sv, sv, sv)
781                             f = theano.function([a, b, c],
782                                                 cst * c * a * T.dot(a, b),
783                                                 mode=mode_blas_opt)
784                             f.maker.fgraph.toposort()
785                             f(sv, sv, sv)
786                             f = theano.function([a, b, c],
787                                                 c * a * cst * T.dot(a, b),
788                                                 mode=m2)
789                             f.maker.fgraph.toposort()
790                             check_dot22scalar(f, 2)
791                             f(sv, sv, sv)
792                     cmp((3, 4), (4, 5), (3, 5))
793                     cmp((0, 4), (4, 5), (0, 5))
794                     cmp((3, 0), (0, 5), (3, 5))
795                     cmp((3, 4), (4, 0), (3, 0), (0, 0))
796                     cmp((0, 4), (4, 0), (0, 0))
797                     cmp((0, 0), (0, 0), (0, 0))
798 def test_dot22scalar_cast():
799     A = T.dmatrix()
800     for scalar_int_type in T.int_dtypes:
801         y = T.scalar(dtype=scalar_int_type)
802         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
803         assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
804     A = T.fmatrix()
805     for scalar_int_type in T.int_dtypes:
806         y = T.scalar(dtype=scalar_int_type)
807         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
808         if scalar_int_type in ['int32', 'int64']:
809             assert _dot22 in [x.op for x in f.maker.fgraph.toposort()]
810         else:
811             assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
812 def test_local_dot22_to_dot22scalar():
813     A = T.dmatrix()
814     mode = theano.compile.mode.get_default_mode()
815     opt = theano.tensor.opt.in2out(
816         theano.tensor.blas.local_dot22_to_dot22scalar)
817     mode = mode.__class__(optimizer=opt)
818     x = T.dscalar()
819     y = T.dscalar()
820     z = T.dscalar()
821     m = T.dmatrix()
822     r = T.drow()
823     for idx, node in enumerate([
824         T.mul(_dot22(A, A), x),
825         T.mul(_dot22(A, A), x, y),
826         T.mul(_dot22(A, A), x, r),
827         T.mul(_dot22(A, A), m, x),
828         T.mul(_dot22(A, A), x, m),
829         T.mul(_dot22(A, A), x, (m * y)),
830         T.mul(_dot22(A, A), (m * y), x),
831         T.mul(_dot22(A, A), x, (r * y)),
832         T.mul(_dot22(A, A), (r * y), x),
833         T.mul(_dot22(A, A), (x * y), (m * x)),
834         T.mul(_dot22(A, A), (r * y), (y * x)),
835         T.mul(_dot22(A, A), (m * y), m),
836         T.mul(_dot22(A, A), m, (m * y)),
837         T.mul(_dot22(A, A), (r * y), (m * x)),
838         T.mul(_dot22(A, A), (m * y * z), m),
839         T.mul(_dot22(A, A), m, (m * y * z)),
840         T.mul(_dot22(A, A), T.mul(m, y, z), m),
841         T.mul(_dot22(A, A), m, T.mul(m, y, z)),
842         T.mul(_dot22(A, A), (r * m), (m * x)),
843     ]):
844         node2 = theano.tensor.blas.local_dot22_to_dot22scalar.transform(
845             node.owner)
846         assert node2
847         f = theano.function([x, y, z, m, r, A], node,
848                             mode=mode, on_unused_input='ignore')
849         f(.1, .2, .3, [[1, 2], [3, 4]], [[5, 6]], [[7, 8], [9, 10]])
850 def test_dot_w_self():
851     A = shared(value=np.ones((2, 2)))
852     B = T.matrix()
853     p = T.dot(A, A) * B
854     grad = T.grad(T.mean(p), A)
855     f = theano.function([B], p, updates=[(A, A - grad)])
856     f(np.asarray([[0, 1], [2, 3]], dtype=config.floatX))
857 class TestGemv(TestCase, unittest_tools.TestOptimizationMixin):
858     def test_dot_vv(self):
859         rng = np.random.RandomState(unittest_tools.fetch_seed())
860         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
861         w = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
862         f = theano.function([], theano.dot(v, w), mode=mode_blas_opt)
863         self.assertFunctionContains0(f, T.dot)
864         self.assertFunctionContains1(f, Gemv(True))
865         assert np.allclose(f(), np.dot(v.get_value(), w.get_value()))
866     def test_dot_vm(self):
867         rng = np.random.RandomState(unittest_tools.fetch_seed())
868         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
869         m = theano.shared(np.array(rng.uniform(size=(2, 3)), dtype='float32'))
870         f = theano.function([], theano.dot(v, m), mode=mode_blas_opt)
871         self.assertFunctionContains0(f, T.dot)
872         self.assertFunctionContains1(f, Gemv(True))
873         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
874         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
875         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
876     def test_dot_mv(self):
877         rng = np.random.RandomState(unittest_tools.fetch_seed())
878         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
879         m = theano.shared(np.array(rng.uniform(size=(3, 2)), dtype='float32'))
880         f = theano.function([], theano.dot(m, v), mode=mode_blas_opt)
881         self.assertFunctionContains0(f, T.dot)
882         self.assertFunctionContains1(f, Gemv(True))
883         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
884         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
885         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
886     @staticmethod
887     def t_gemv1(m_shp):
888         rng = np.random.RandomState(unittest_tools.fetch_seed())
889         v1 = theano.shared(np.array(rng.uniform(size=(m_shp[1],)),
890                            dtype='float32'))
891         v2_orig = np.array(rng.uniform(size=(m_shp[0],)), dtype='float32')
892         v2 = theano.shared(v2_orig)
893         m = theano.shared(np.array(rng.uniform(size=m_shp), dtype='float32'))
894         f = theano.function([], v2 + theano.dot(m, v1), mode=mode_blas_opt)
895         assert np.allclose(f(), np.dot(m.get_value(), v1.get_value()) + v2_orig)
896         topo = f.maker.fgraph.toposort()
897         assert len(topo) == 1
898         assert isinstance(topo[0].op, Gemv)
899         assert topo[0].op.inplace is False
900         g = theano.function([], [], updates=[(v2, v2 + theano.dot(m, v1))],
901                             mode=mode_blas_opt)
902         g()
903         assert np.allclose(v2.get_value(), np.dot(m.get_value(),
904                            v1.get_value()) + v2_orig)
905         topo = g.maker.fgraph.toposort()
906         assert len(topo) == 1
907         assert isinstance(topo[0].op, Gemv)
908         if config.mode != 'FAST_COMPILE':
909             assert topo[0].op.inplace is True
910         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
911                     borrow=True)
912         v2.set_value(v2_orig)
913         assert np.allclose(f(),
914                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
915         g()
916         assert np.allclose(v2.get_value(),
917                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
918     @attr('slow')
919     def test_gemv1(self):
920         self.t_gemv1((3, 2))
921         self.t_gemv1((0, 2))
922         self.t_gemv1((3, 0))
923         self.t_gemv1((0, 0))
924     def test_gemv2(self):
925         rng = np.random.RandomState(unittest_tools.fetch_seed())
926         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
927                            dtype='float32'))
928         v2_orig = np.array(rng.uniform(size=(3,)), dtype='float32')
929         v2 = theano.shared(v2_orig)
930         m = theano.shared(np.array(rng.uniform(size=(2, 3)),
931                           dtype='float32'))
932         f = theano.function([], v2 + theano.dot(v1, m), mode=mode_blas_opt)
933         assert np.allclose(f(),
934                            np.dot(v1.get_value(), m.get_value()) +
935                            v2.get_value())
936         topo = f.maker.fgraph.toposort()
937         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
938         assert topo[-1].op.inplace is False
939         g = theano.function([], [], updates=[(v2, v2 + theano.dot(v1, m))],
940                             mode=mode_blas_opt)
941         g()
942         assert np.allclose(v2.get_value(),
943                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
944         topo = g.maker.fgraph.toposort()
945         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
946         if config.mode != 'FAST_COMPILE':
947             assert topo[-1].op.inplace is True
948         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
949                     borrow=True)
950         v2.set_value(v2_orig)
951         assert np.allclose(f(),
952                            np.dot(v1.get_value(), m.get_value()) +
953                            v2.get_value())
954         g()
955         assert np.allclose(v2.get_value(),
956                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
957     def test_gemv_broadcast(self):
958         rng = np.random.RandomState(unittest_tools.fetch_seed())
959         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
960                                     dtype='float32'))
961         v2_orig = np.array(rng.uniform(size=(1,)), dtype='float32')
962         v2 = theano.shared(v2_orig)
963         m = theano.shared(np.array(rng.uniform(size=(1, 2)),
964                                    dtype='float32'),
965                           broadcastable=(True, False))
966         o = theano.dot(m, v1)
967         f = theano.function([], o + v2, mode=mode_blas_opt)
968         assert np.allclose(
969             f(),
970             np.dot(m.get_value(), v1.get_value()) + v2.get_value())
971         topo = f.maker.fgraph.toposort()
972         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
973         o = theano.tensor.blas.gemv_no_inplace(v2, 0.5, m, v1, 0.25)
974         f = theano.function([], o, mode=mode_blas_opt)
975         assert np.allclose(
976             f(),
977             0.5 * np.dot(m.get_value(), v1.get_value()) + 0.25 * v2.get_value())
978         topo = f.maker.fgraph.toposort()
979         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
980     def test_gemv_dimensions(self):
981         A = T.matrix('A')
982         x, y = T.vectors('x', 'y')
983         alpha = theano.shared(theano._asarray(1.0, dtype=config.floatX),
984                               name='alpha')
985         beta = theano.shared(theano._asarray(1.0, dtype=config.floatX),
986                              name='beta')
987         z = beta * y + alpha * T.dot(A, x)
988         f = theano.function([A, x, y], z)
989         A_val = np.ones((5, 3), dtype=config.floatX)
990         ones_3 = np.ones(3, dtype=config.floatX)
991         ones_4 = np.ones(4, dtype=config.floatX)
992         ones_5 = np.ones(5, dtype=config.floatX)
993         ones_6 = np.ones(6, dtype=config.floatX)
994         f(A_val, ones_3, ones_5)
995         f(A_val[::-1, ::-1], ones_3, ones_5)
996         self.assertRaises(ValueError, f, A_val, ones_4, ones_5)
997         self.assertRaises(ValueError, f, A_val, ones_3, ones_6)
998         self.assertRaises(ValueError, f, A_val, ones_4, ones_6)
999 def matrixmultiply(a, b):
1000     if len(b.shape) == 1:
1001         b_is_vector = True
1002         b = b[:, newaxis]
1003     else:
1004         b_is_vector = False
1005     assert a.shape[1] == b.shape[0]
1006     c = zeros((a.shape[0], b.shape[1]), common_type(a, b))
1007     for i in xrange(a.shape[0]):
1008         for j in xrange(b.shape[1]):
1009             s = 0
1010             for k in xrange(a.shape[1]):
1011                 s += a[i, k] * b[k, j]
1012             c[i, j] = s
1013     if b_is_vector:
1014         c = c.reshape((a.shape[0],))
1015     return c
1016 class BaseGemv(object):
1017     mode = mode_blas_opt  # can be overridden with self.mode
1018     shared = staticmethod(theano.shared)
1019     def get_data(self, x_stride=1, y_stride=1):
1020         rng = np.random.RandomState(unittest_tools.fetch_seed())
1021         mult = array(1, dtype=self.dtype)
1022         if self.dtype in [complex64, complex128]:
1023             mult = array(1 + 1j, dtype=self.dtype)
1024         alpha = array(1., dtype=self.dtype) * mult
1025         beta = array(1., dtype=self.dtype) * mult
1026         a = rng.randn(3, 3).astype(self.dtype) * mult
1027         x = arange(shape(a)[0] * x_stride, dtype=self.dtype) * mult
1028         y = arange(shape(a)[1] * y_stride, dtype=self.dtype) * mult
1029         return alpha, beta, a, x, y
1030     def test_simple(self):
1031         alpha, beta, a, x, y = [self.shared(value)
1032                                 for value in self.get_data()]
1033         desired_oy = alpha.get_value() * matrixmultiply(a.get_value(), x.get_value()) + beta.get_value() * y.get_value()
1034         oy = alpha * T.dot(a, x) + beta * y
1035         oy_func = theano.function([], oy, mode=self.mode)
1036         oy_func.maker.fgraph.toposort()
1037         self.assertFunctionContains1(oy_func, self.gemv)
1038         oy_val = oy_func()
1039         assert_array_almost_equal(desired_oy, oy_val)
1040     def test_default_beta_y(self):
1041         vs = self.get_data()
1042         alpha_v, beta_v, a_v, x_v, y_v = vs
1043         a = self.shared(a_v)
1044         x = self.shared(x_v)
1045         desired_oy = matrixmultiply(a_v, x_v)
1046         oy = T.dot(a, x)
1047         oy_func = theano.function([], oy, mode=self.mode)
1048         self.assertFunctionContains1(oy_func, self.gemv_inplace)
1049         oy_v = oy_func()
1050         assert_array_almost_equal(desired_oy, oy_v)
1051     def test_simple_transpose(self):
1052         vs = self.get_data()
1053         alpha_v, beta_v, a_v, x_v, y_v = vs
1054         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1055         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1056                                               x_v) + beta_v * y_v
1057         oy = alpha * T.dot(a.T, x) + beta * y
1058         oy_func = theano.function([], oy, mode=self.mode)
1059         self.assertFunctionContains1(oy_func, self.gemv)
1060         oy_v = oy_func()
1061         assert_array_almost_equal(desired_oy, oy_v)
1062     def test_x_stride(self):
1063         vs = self.get_data(x_stride=2)
1064         alpha_v, beta_v, a_v, x_v, y_v = vs
1065         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1066         desired_oy = alpha_v * matrixmultiply(a_v, x_v[::2]) + beta_v * y_v
1067         oy = alpha * T.dot(a, x[::2]) + beta * y
1068         oy_func = theano.function([], oy, mode=self.mode)
1069         self.assertFunctionContains1(oy_func, self.gemv)
1070         oy_v = oy_func()
1071         assert_array_almost_equal(desired_oy, oy_v)
1072     def test_x_stride_transpose(self):
1073         vs = self.get_data(x_stride=2)
1074         alpha_v, beta_v, a_v, x_v, y_v = vs
1075         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1076         desired_oy = alpha_v * matrixmultiply(transpose(a_v), x_v[::2]) + \
1077             beta_v * y_v
1078         oy = alpha * T.dot(a.T, x[::2]) + beta * y
1079         oy_func = theano.function([], oy, mode=self.mode)
1080         self.assertFunctionContains1(oy_func, self.gemv)
1081         oy_v = oy_func()
1082         assert_array_almost_equal(desired_oy, oy_v)
1083     def test_y_stride(self):
1084         vs = self.get_data(y_stride=2)
1085         alpha_v, beta_v, a_v, x_v, y_v = vs
1086         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1087         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v[::2]
1088         oy = alpha * T.dot(a, x) + beta * y[::2]
1089         oy_func = theano.function([], oy, mode=self.mode)
1090         self.assertFunctionContains1(oy_func, self.gemv)
1091         oy_v = oy_func()
1092         assert_array_almost_equal(desired_oy, oy_v)
1093     def test_y_stride_transpose(self):
1094         vs = self.get_data(y_stride=2)
1095         alpha_v, beta_v, a_v, x_v, y_v = vs
1096         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1097         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1098                                               x_v) + beta_v * y_v[::2]
1099         oy = alpha * T.dot(a.T, x) + beta * y[::2]
1100         oy_func = theano.function([], oy, mode=self.mode)
1101         self.assertFunctionContains1(oy_func, self.gemv)
1102         oy_v = oy_func()
1103         assert_array_almost_equal(desired_oy, oy_v)
1104     def test_a_strides(self):
1105         vs = self.get_data()
1106         alpha_v, beta_v, a_v, x_v, y_v = vs
1107         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1108         a_v = a_v[::-1, ::-1]
1109         a.set_value(a.get_value(borrow=True,
1110                                 return_internal_type=True)[::-1, ::-1],
1111                     borrow=True)
1112         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v
1113         oy = alpha * T.dot(a, x) + beta * y
1114         oy_func = theano.function([], oy, mode=self.mode)
1115         self.assertFunctionContains1(oy_func, self.gemv)
1116         oy_v = oy_func()
1117         assert_array_almost_equal(desired_oy, oy_v)
1118     def test_a_strides_transpose(self):
1119         vs = self.get_data()
1120         alpha_v, beta_v, a_v, x_v, y_v = vs
1121         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1122         a_v = a_v[::-1, ::-1]
1123         a.set_value(a.get_value(borrow=True,
1124                                 return_internal_type=True)[::-1, ::-1],
1125                     borrow=True)
1126         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1127                                               x_v) + beta_v * y_v
1128         oy = alpha * T.dot(a.T, x) + beta * y
1129         oy_func = theano.function([], oy, mode=self.mode)
1130         self.assertFunctionContains1(oy_func, self.gemv)
1131         oy_v = oy_func()
1132         assert_array_almost_equal(desired_oy, oy_v)
1133     def test_upcasting_scalar_nogemv(self):
1134         vs = self.get_data()
1135         alpha_v, beta_v, a_v, x_v, y_v = vs
1136         alpha_v = alpha_v.astype("float64")
1137         a_v = a_v.astype("float32")
1138         x_v = x_v.astype("float32")
1139         y_v = y_v.astype("float32")
1140         alpha = T.dscalar('alpha')
1141         a = self.shared(a_v)
1142         x = self.shared(x_v)
1143         y = self.shared(y_v)
1144         rval = T.dot(a, x) * alpha + y
1145         f = theano.function([alpha], rval, mode=self.mode)
1146         n_gemvs = 0
1147         for node in f.maker.fgraph.toposort():
1148             if node.op == self.gemv_inplace:
1149                 n_gemvs += 1
1150                 assert node.outputs[0].dtype == 'float32'
1151         assert n_gemvs == 1, n_gemvs
1152         self.assertFunctionContains1(f, self.gemv_inplace)
1153         f(alpha_v)
1154 class TestSgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1155     dtype = float32
1156     gemv = theano.tensor.blas.gemv_no_inplace
1157     gemv_inplace = theano.tensor.blas.gemv_inplace
1158 class TestDgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1159     dtype = float64
1160     gemv = theano.tensor.blas.gemv_no_inplace
1161     gemv_inplace = theano.tensor.blas.gemv_inplace
1162 class TestGer_make_node(TestCase):
1163     def setUp(self):
1164         self.iv = T.tensor(dtype='int32', broadcastable=(False,))
1165         self.fv = T.tensor(dtype='float32', broadcastable=(False,))
1166         self.fv1 = T.tensor(dtype='float32', broadcastable=(True,))
1167         self.dv = T.tensor(dtype='float64', broadcastable=(False,))
1168         self.dv1 = T.tensor(dtype='float64', broadcastable=(True,))
1169         self.cv = T.tensor(dtype='complex64', broadcastable=(False,))
1170         self.zv = T.tensor(dtype='complex128', broadcastable=(False,))
1171         self.fv_2 = T.tensor(dtype='float32', broadcastable=(False,))
1172         self.fv1_2 = T.tensor(dtype='float32', broadcastable=(True,))
1173         self.dv_2 = T.tensor(dtype='float64', broadcastable=(False,))
1174         self.dv1_2 = T.tensor(dtype='float64', broadcastable=(True,))
1175         self.cv_2 = T.tensor(dtype='complex64', broadcastable=(False,))
1176         self.zv_2 = T.tensor(dtype='complex128', broadcastable=(False,))
1177         self.fm = T.fmatrix()
1178         self.dm = T.dmatrix()
1179         self.cm = T.cmatrix()
1180         self.zm = T.zmatrix()
1181         self.fa = T.fscalar()
1182         self.da = T.dscalar()
1183         self.ca = T.cscalar()
1184         self.za = T.zscalar()
1185     def test_works_on_all_valid_dtypes(self):
1186         self.assertEqual(self.fm.type,
1187                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1188         self.assertEqual(self.fm.type,
1189                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1190         self.assertEqual(self.fm.type,
1191                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1192         self.assertEqual(self.fm.type,
1193                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1194     def test_fails_on_invalid_dtypes(self):
1195         self.assertRaises(TypeError,
1196                           ger, T.imatrix(), T.iscalar(), T.ivector(),
1197                           T.ivector())
1198     def test_fails_for_nonscalar_alpha(self):
1199         self.assertRaises(TypeError,
1200                           ger, self.fm, self.fm, self.fv, self.fv_2)
1201         self.assertRaises(TypeError,
1202                           ger, self.fm, self.fv1, self.fv, self.fv_2)
1203         self.assertEqual(self.fm.type,
1204                          ger(self.fm, self.fv1.dimshuffle(), self.fv,
1205                              self.fv_2).type)
1206     def test_fails_for_nonmatrix_A(self):
1207         self.assertRaises(TypeError,
1208                           ger, self.fv, self.fa, self.fv, self.fv_2)
1209     def test_fails_for_nonvector_x_or_y(self):
1210         self.assertRaises(TypeError,
1211                           ger, self.fm, self.fa,
1212                           self.fv.dimshuffle('x', 0), self.fv_2)
1213         self.assertRaises(TypeError,
1214                           ger, self.fm, self.fa,
1215                           self.fv, self.fv_2.dimshuffle('x', 0))
1216     def test_fails_for_mixed_dtypes(self):
1217         self.assertRaises(TypeError, ger, self.dm, self.fa, self.fv, self.fv_2)
1218         self.assertRaises(TypeError, ger, self.fm, self.da, self.fv, self.fv_2)
1219         self.assertRaises(TypeError, ger, self.fm, self.fa, self.dv, self.fv_2)
1220         self.assertRaises(TypeError, ger, self.fm, self.fa, self.fv, self.dv_2)
1221         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.dv_2)
1222         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.zv_2)
1223 class TestGer_OpContract(TestCase, unittest_tools.T_OpContractMixin):
1224     def setUp(self):
1225         self.ops = [ger, ger_destructive]
1226     def clone(self, op):
1227         return Ger(op.destructive)
1228 class TestGer(TestCase, unittest_tools.TestOptimizationMixin):
1229     shared = staticmethod(theano.shared)
1230     def setUp(self):
1231         self.mode = theano.compile.get_default_mode().including('fast_run')
1232         self.mode = self.mode.excluding('c_blas', 'scipy_blas')
1233         dtype = self.dtype = 'float64'  # optimization isn't dtype-dependent
1234         self.A = T.tensor(dtype=dtype, broadcastable=(False, False))
1235         self.a = T.tensor(dtype=dtype, broadcastable=())
1236         self.x = T.tensor(dtype=dtype, broadcastable=(False,))
1237         self.y = T.tensor(dtype=dtype, broadcastable=(False,))
1238         self.ger = ger
1239         self.ger_destructive = ger_destructive
1240         self.gemm = gemm_no_inplace
1241     def function(self, inputs, outputs, updates=None):
1242         if updates is None:
1243             updates = []
1244         return theano.function(inputs, outputs, self.mode, updates=updates)
1245     def b(self, bval):
1246         return T.as_tensor_variable(np.asarray(bval, dtype=self.dtype))
1247     def test_b_0_triggers_ger(self):
1248         assert T.blas.local_gemm_to_ger.transform(
1249             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1250                             self.y.dimshuffle('x', 0), self.b(0)).owner)
1251     def test_b_1_triggers_ger(self):
1252         assert T.blas.local_gemm_to_ger.transform(
1253             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1254                             self.y.dimshuffle('x', 0), self.b(1)).owner)
1255     def test_b_other_does_not_triggers_ger(self):
1256         assert not T.blas.local_gemm_to_ger.transform(
1257             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1258                             self.y.dimshuffle('x', 0), self.b(1.5)).owner)
1259     def test_b_nonconst_does_not_triggers_ger(self):
1260         assert not T.blas.local_gemm_to_ger.transform(
1261             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1262                             self.y.dimshuffle('x', 0), self.a).owner)
1263     def test_outer(self):
1264         f = self.function([self.x, self.y], T.outer(self.x, self.y))
1265         self.assertFunctionContains(f, self.ger_destructive)
1266         f(np.random.rand(5).astype(self.dtype),
1267           np.random.rand(4).astype(self.dtype))
1268     def test_A_plus_outer(self):
1269         f = self.function([self.A, self.x, self.y],
1270                           self.A + T.outer(self.x, self.y))
1271         self.assertFunctionContains(f, self.ger)
1272         f(np.random.rand(5, 4).astype(self.dtype),
1273           np.random.rand(5).astype(self.dtype),
1274           np.random.rand(4).astype(self.dtype))
1275         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1276           np.random.rand(5).astype(self.dtype),
1277           np.random.rand(4).astype(self.dtype))
1278     def test_A_plus_scaled_outer(self):
1279         f = self.function([self.A, self.x, self.y],
1280                           self.A + 0.1 * T.outer(self.x, self.y))
1281         self.assertFunctionContains(f, self.ger)
1282         f(np.random.rand(5, 4).astype(self.dtype),
1283           np.random.rand(5).astype(self.dtype),
1284           np.random.rand(4).astype(self.dtype))
1285         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1286           np.random.rand(5).astype(self.dtype),
1287           np.random.rand(4).astype(self.dtype))
1288     def test_scaled_A_plus_scaled_outer(self):
1289         f = self.function([self.A, self.x, self.y],
1290                           np.asarray(0.2, self.dtype) * self.A +
1291                           np.asarray(0.1, self.dtype) * T.outer(
1292                           self.x, self.y))
1293         self.assertFunctionContains(f, self.gemm)
1294         f(np.random.rand(5, 4).astype(self.dtype),
1295           np.random.rand(5).astype(self.dtype),
1296           np.random.rand(4).astype(self.dtype))
1297         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1298           np.random.rand(5).astype(self.dtype),
1299           np.random.rand(4).astype(self.dtype))
1300     def given_dtype(self, dtype, M, N):
1301         f = self.function([self.A, self.x, self.y],
1302                           self.A + 0.1 * T.outer(self.x, self.y))
1303         self.assertFunctionContains(f, self.ger)
1304         f(np.random.rand(M, N).astype(self.dtype),
1305           np.random.rand(M).astype(self.dtype),
1306           np.random.rand(N).astype(self.dtype))
1307         f(np.random.rand(M, N).astype(self.dtype)[::-1, ::-1],
1308           np.random.rand(M).astype(self.dtype),
1309     def test_f32_0_0(self):
1310         return self<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.given_dtype('float32', 0, 0)
1311     def test_f32_1_0(self):
1312         return self.given_dtype('float32', 1, 0)
1313     def test_f32_0_1(self):
1314     def test_f32_1_1(self):
1315         return self.given_dtype(</b></font><font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'float32', 1, 1)
1316     def test_f32_4_4(self):
1317         return self.given_dtype('float32', 4, 4)
1318     def test_f32_7_1(self):
1319         return self.given_dtype('float32', 7, 1)
1320     def test_f32_1_2(self):
1321         return self.given_dtype(</b></font>'float32', 1, 2)
1322     def test_f64_4_5(self):
1323         return self.given_dtype('float64', 4, 5)
1324     def test_c64_7_1(self):
1325         return self.given_dtype('complex64', 7, 1)
1326     def test_c128_1_9(self):
1327         return self.given_dtype('complex128', 1, 9)
1328     def test_inplace(self):
1329         A = self.shared(np.random.rand(4, 5).astype(self.dtype))
1330         f = self.function([self.x, self.y], [],
1331                           updates=[(A, A + T.constant(0.1, dtype=self.dtype) *
1332                                    T.outer(self.x, self.y))])
1333         self.assertFunctionContains(f, self.ger_destructive)
1334         f(np.random.rand(4).astype(self.dtype),
1335           np.random.rand(5).astype(self.dtype))
1336         A.set_value(
1337             A.get_value(borrow=True, return_internal_type=True)[::-1, ::-1],
1338             borrow=True)
1339         f(np.random.rand(4).astype(self.dtype),
1340           np.random.rand(5).astype(self.dtype))
1341 class TestBlasStrides(TestCase):
1342     dtype = 'float64'
1343     shared = staticmethod(tensor._shared)
1344     mode = theano.compile.get_default_mode()
1345     mode = mode.including('fast_run').excluding('gpu', 'c_blas', 'scipy_blas')
1346     rng = np.random.RandomState(seed=unittest_tools.fetch_seed())
1347     def rand(self, *shape):
1348         return theano._asarray(self.rng.rand(*shape), dtype=self.dtype)
1349     def cmp_dot22(self, b_shp, c_shp):
1350         av = np.zeros((0, 0), dtype=self.dtype)
1351         bv = self.rand(*b_shp)
1352         cv = self.rand(*c_shp)
1353         a = self.shared(av, 'a')
1354         b = self.shared(bv, 'b')
1355         c = self.shared(cv, 'c')
1356         b_t = self.shared(bv.T, 'b.T')
1357         c_t = self.shared(cv.T, 'c.T')
1358         b_dev = b.get_value(borrow=False, return_internal_type=True)
1359         c_dev = c.get_value(borrow=False, return_internal_type=True)
1360         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1361         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1362         f_nn = theano.function([], [], updates=[(a, tensor.dot(b, c))],
1363                                mode=self.mode)
1364         f_nt = theano.function([], [], updates=[(a, tensor.dot(b, c_t.T))],
1365                                mode=self.mode)
1366         f_tn = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c))],
1367                                mode=self.mode)
1368         f_tt = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c_t.T))],
1369                                mode=self.mode)
1370         for step_signs in itertools_product((-1, 1), repeat=4):
1371             for step in (1, 2):
1372                 b_step1, b_step2, c_step1, c_step2 = (s * step
1373                                                       for s in step_signs)
1374                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1375                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1376                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1377                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1378                 a_n = np.dot(bv[::b_step1, ::b_step2],
1379                              cv[::c_step1, ::c_step2])
1380                 f_nn()
1381                 assert np.allclose(a.get_value(), a_n)
1382                 f_nt()
1383                 assert np.allclose(a.get_value(), a_n)
1384                 f_tn()
1385                 assert np.allclose(a.get_value(), a_n)
1386                 f_tt()
1387                 assert np.allclose(a.get_value(), a_n)
1388     def test_dot22(self):
1389         self.cmp_dot22((3, 4), (4, 5))
1390         self.cmp_dot22((1, 4), (4, 5))
1391         self.cmp_dot22((3, 4), (4, 1))
1392         self.cmp_dot22((3, 1), (1, 1))
1393         self.cmp_dot22((1, 4), (4, 1))
1394         self.cmp_dot22((3, 1), (1, 5))
1395         self.cmp_dot22((0, 4), (4, 5))
1396         self.cmp_dot22((0, 4), (4, 1))
1397         self.cmp_dot22((0, 1), (1, 5))
1398         self.cmp_dot22((3, 4), (4, 0))
1399         self.cmp_dot22((3, 0), (0, 5))
1400         self.cmp_dot22((0, 4), (4, 0))
1401         self.cmp_dot22((0, 0), (0, 0))
1402     def cmp_dot22scalar(self, b_shp, c_shp):
1403         av = np.zeros((0, 0), dtype=self.dtype)
1404         bv = self.rand(*b_shp)
1405         cv = self.rand(*c_shp)
1406         l = np.float32(0.2)
1407         a = self.shared(av, 'a')
1408         b = self.shared(bv, 'b')
1409         c = self.shared(cv, 'c')
1410         b_t = self.shared(bv.T, 'b.T')
1411         c_t = self.shared(cv.T, 'c.T')
1412         b_dev = b.get_value(borrow=False, return_internal_type=True)
1413         c_dev = c.get_value(borrow=False, return_internal_type=True)
1414         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1415         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1416         f_nn = theano.function([], [], updates=[(a, l * tensor.dot(b, c))],
1417                                mode=self.mode)
1418         f_nt = theano.function([], [], updates=[(a, l * tensor.dot(b, c_t.T))],
1419                                mode=self.mode)
1420         f_tn = theano.function([], [], updates=[(a, l * tensor.dot(b_t.T, c))],
1421                                mode=self.mode)
1422         f_tt = theano.function([], [],
1423                                updates=[(a, l * tensor.dot(b_t.T, c_t.T))],
1424                                mode=self.mode)
1425         for step_signs in itertools_product((-1, 1), repeat=4):
1426             for step in (1, 2):
1427                 b_step1, b_step2, c_step1, c_step2 = (s * step
1428                                                       for s in step_signs)
1429                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1430                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1431                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1432                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1433                 a_n = l * np.dot(bv[::b_step1, ::b_step2],
1434                                  cv[::c_step1, ::c_step2])
1435                 f_nn()
1436                 assert np.allclose(a.get_value(), a_n)
1437                 f_nt()
1438                 assert np.allclose(a.get_value(), a_n)
1439                 f_tn()
1440                 assert np.allclose(a.get_value(), a_n)
1441                 f_tt()
1442                 assert np.allclose(a.get_value(), a_n)
1443     def test_dot22scalar(self):
1444         self.cmp_dot22scalar((3, 4), (4, 5))
1445         self.cmp_dot22scalar((1, 4), (4, 5))
1446         self.cmp_dot22scalar((3, 4), (4, 1))
1447         self.cmp_dot22scalar((3, 1), (1, 1))
1448         self.cmp_dot22scalar((1, 4), (4, 1))
1449         self.cmp_dot22scalar((3, 1), (1, 5))
1450         self.cmp_dot22scalar((0, 4), (4, 5))
1451         self.cmp_dot22scalar((0, 4), (4, 1))
1452         self.cmp_dot22scalar((0, 1), (1, 5))
1453         self.cmp_dot22scalar((3, 0), (0, 5))
1454         self.cmp_dot22scalar((0, 4), (4, 0))
1455         self.cmp_dot22scalar((0, 0), (0<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0))
1456     def cmp_gemm(self, a_shp, b_shp, c_shp):
1457         av = self.rand(*a_shp)
1458         bv = self.rand(*b_shp)
1459         cv = self.rand(*c_shp)
1460         l = np.float32(0.2)
1461         a = self.shared(av, 'a')
1462         b = self.shared(bv, 'b')
1463         c = self.shared(</b></font>cv, 'c')
1464         a_t = self.shared(av.T, 'a.T')
1465         b_t = self.shared(bv.T, 'b.T')
1466         c_t = self.shared(cv.T, 'c.T')
1467         a_dev = a.get_value(borrow=False, return_internal_type=True)
1468         b_dev = b.get_value(borrow=False, return_internal_type=True)
1469         c_dev = c.get_value(borrow=False, return_internal_type=True)
1470         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1471         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1472         f_nnn = theano.function(
1473             [], [],
1474             updates=[(a, (l * a + tensor.dot(b, c)))],
1475             mode=self.mode)
1476         f_nnt = theano.function(
1477             [], [],
1478             updates=[(a, (l * a + tensor.dot(b, c_t.T)))],
1479             mode=self.mode)
1480         f_ntn = theano.function(
1481             [], [],
1482             updates=[(a, (l * a + tensor.dot(b_t.T, c)))],
1483         f_ntt = theano.function(
1484             [], [],
1485             updates=[(a, (l * a + tensor.dot(b_t<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.T, c_t.T)))],
1486             mode=self.mode)
1487         f_tnn = theano.function(
1488             [], [],
1489             updates=[(a_t, (l * a_t + tensor.</b></font>dot(b, c).T))],
1490             mode=self.mode)
1491         f_tnt = theano.function(
1492             [], [],
1493             updates=[(a_t, (l * a_t + tensor.dot(b, c_t.T).T))],
1494             mode=self.mode)
1495         f_ttn = theano.function(
1496             [], [],
1497             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c).T))],
1498             mode=self.mode)
1499         f_ttt = theano.function(
1500             [], [],
1501             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c_t.T).T))],
1502             mode=self.mode)
1503         for step_signs in itertools_product((-1, 1), repeat=6):
1504             for step in (1, 2):
1505                 a_step1, a_step2, b_step1, b_step2, c_step1, c_step2 = \
1506                     (s * step for s in step_signs)
1507                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1508                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1509                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1510                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1511                 a_n = (l * av[::a_step1, ::a_step2] +
1512                        np.dot(bv[::b_step1, ::b_step2],
1513                               cv[::c_step1, ::c_step2]))
1514                 at_n = (l * av[::a_step1, ::a_step2].T +
1515                         np.dot(bv[::b_step1, ::b_step2],
1516                                cv[::c_step1, ::c_step2]).T)
1517                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1518                 f_nnn()
1519                 assert np.allclose(a.get_value(), a_n)
1520                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1521                 f_nnt()
1522                 assert np.allclose(a.get_value(), a_n)
1523                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1524                 f_ntn()
1525                 assert np.allclose(a.get_value(), a_n)
1526                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1527                 f_ntt()
1528                 assert np.allclose(a.get_value(), a_n)
1529                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1530                               borrow=True)
1531                 f_tnn()
1532                 assert np.allclose(a_t.get_value(), at_n)
1533                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1534                               borrow=True)
1535                 f_tnt()
1536                 assert np.allclose(a_t.get_value(), at_n)
1537                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1538                               borrow=True)
1539                 f_ttn()
1540                 assert np.allclose(a_t.get_value(), at_n)
1541                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1542                               borrow=True)
1543                 f_ttt()
1544                 assert np.allclose(a_t.get_value(), at_n)
1545     def test_gemm(self):
1546         self.cmp_gemm((3, 5), (3, 4), (4, 5))
1547         self.cmp_gemm((1, 5), (1, 4), (4, 5))
1548         self.cmp_gemm((3, 1), (3, 4), (4, 1))
1549         self.cmp_gemm((3, 1), (3, 1), (1, 1))
1550         self.cmp_gemm((1, 1), (1, 4), (4, 1))
1551         self.cmp_gemm((3, 5), (3, 1), (1, 5))
1552         self.cmp_gemm((0, 5), (0, 4), (4, 5))
1553         self.cmp_gemm((0, 1), (0, 4), (4, 1))
1554         self.cmp_gemm((0, 5), (0, 1), (1, 5))
1555         self.cmp_gemm((3, 0), (3, 4), (4, 0))
1556         self.cmp_gemm((3, 5), (3, 0), (0, 5))
1557         self.cmp_gemm((0, 0), (0, 4), (4, 0))
1558         self.cmp_gemm((0, 0), (0, 0), (0, 0))
1559     def cmp_gemv(self, a_shp, b_shp, c_shp):
1560         av = self.rand(a_shp)
1561         bv = self.rand(*b_shp)
1562         cv = self.rand(c_shp)
1563         l = np.float32(0.2)
1564         a = self.shared(av, 'a')
1565         b = self.shared(bv, 'b')
1566         c = self.shared(cv, 'c')
1567         b_t = self.shared(bv.T, 'b.T')
1568         a_dev = a.get_value(borrow=False, return_internal_type=True)
1569         b_dev = b.get_value(borrow=False, return_internal_type=True)
1570         c_dev = c.get_value(borrow=False, return_internal_type=True)
1571         f_n = theano.function([], [], updates=[(a, (a + l * tensor.dot(b, c)))],
1572                               mode=self.mode)
1573         f_t = theano.function([], [],
1574                               updates=[(a, (a + l * tensor.dot(b_t.T, c)))],
1575                               mode=self.mode)
1576         for step_signs in itertools_product((1, -1), repeat=4):
1577             for step in (1, 2):
1578                 a_step, b_step1, b_step2, c_step = (s * step
1579                                                     for s in step_signs)
1580                 a.set_value(a_dev.copy()[::a_step], borrow=True)
1581                 b.set_value(b_dev.copy()[::b_step1, ::b_step2],
1582                             borrow=True)
1583                 b_t.set_value(transpose(b_dev.copy())[::b_step2, ::b_step1],
1584                               borrow=True)
1585                 c.set_value(c_dev.copy()[::c_step], borrow=True)
1586                 a_n = (av[::a_step] +
1587                        l * np.dot(bv[::b_step1, ::b_step2],
1588                                   cv[::c_step]))
1589                 f_n()
1590                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
1591                 a.set_value(a_dev.copy()[::a_step], borrow=True)
1592                 f_t()
1593                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
1594     def test_gemv(self):
1595         self.cmp_gemv(3, (3, 5), 5)
1596         self.cmp_gemv(1, (1, 5), 5)
1597         self.cmp_gemv(3, (3, 1), 1)
1598         self.cmp_gemv(0, (0, 5), 5)
1599         self.cmp_gemv(3, (3, 0), 0)
1600         self.cmp_gemv(0, (0, 1), 1)
1601         self.cmp_gemv(1, (1, 0), 0)
1602         self.cmp_gemv(0, (0, 0), 0)
1603     def cmp_ger(self, a_shp, b_shp, c_shp):
1604         av = self.rand(*a_shp)
1605         bv = self.rand(b_shp)
1606         cv = self.rand(c_shp)
1607         l = np.float32(0.2)
1608         a = self.shared(av, 'a')
1609         b = self.shared(bv, 'b')
1610         c = self.shared(cv, 'c')
1611         a_t = self.shared(av.T, 'a.T')
1612         a_dev = a.get_value(borrow=False, return_internal_type=True)
1613         b_dev = b.get_value(borrow=False, return_internal_type=True)
1614         c_dev = c.get_value(borrow=False, return_internal_type=True)
1615         f_n = theano.function(
1616             [], [],
1617             updates=[(a, (a + l * tensor.outer(b, c)))],
1618             mode=self.mode)
1619         f_t = theano.function(
1620             [], [],
1621             updates=[(a_t, (a_t + l * tensor.outer(b, c).T))],
1622             mode=self.mode)
1623         for step_signs in itertools_product((1, -1), repeat=4):
1624             for step in (1, 2):
1625                 a_step1, a_step2, b_step, c_step = (s * step
1626                                                     for s in step_signs)
1627                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1628                 a_t.set_value(transpose(a_dev.copy())[::a_step1, ::a_step2],
1629                               borrow=True)
1630                 b.set_value(b_dev.copy()[::b_step], borrow=True)
1631                 c.set_value(c_dev.copy()[::c_step], borrow=True)
1632                 f_n()
1633                 n_n = (av[::a_step1, ::a_step2] +
1634                        l * np.outer(bv[::b_step], cv[::c_step]))
1635                 assert np.allclose(a.get_value(), n_n), (a.get_value(), n_n)
1636                 f_t()
1637                 n_t = (av.T[::a_step1, ::a_step2] +
1638                        l * np.outer(bv[::b_step], cv[::c_step]).T)
1639                 assert np.allclose(a_t.get_value(), n_t), (a_t.get_value(), n_t)
1640     def test_ger_strides(self):
1641         self.cmp_ger((3, 5), 3, 5)
1642         self.cmp_ger((1, 5), 1, 5)
1643         self.cmp_ger((3, 1), 3, 1)
1644         self.cmp_ger((0, 5), 0, 5)
1645         self.cmp_ger((3, 0), 3, 0)
1646         self.cmp_ger((0, 1), 0, 1)
1647         self.cmp_ger((1, 0), 1, 0)
1648         self.cmp_ger((0, 0), 0, 0)
1649     def test_gemm_non_contiguous(self):
1650         aval = np.ones((6, 2))
1651         bval = np.ones((2, 7))
1652         cval = np.arange(7) + np.arange(0, .6, .1)[:, np.newaxis]
1653         a = theano.shared(aval[:3], borrow=True)
1654         b = theano.shared(bval[:, :5], borrow=True)
1655         c = theano.shared(cval[:3, :5], borrow=True)
1656         s = theano.tensor.scalar()
1657         upd_c = s * c + theano.tensor.dot(a, b)
1658         f = theano.function([s], [], updates={c: upd_c})
1659         f(0)
1660         ref_output = np.ones((3, 5)) * 2
1661         unittest_tools.assert_allclose(c.get_value(), ref_output)
1662 class test_infer_shape(unittest_tools.InferShapeTester):
1663     def test_dot22(self):
1664         x, y = T.matrices('xy')
1665         self._compile_and_check(
1666             [x, y], [T.blas._dot22(x, y)],
1667             [np.random.random((2, 3)).astype(config.floatX),
1668              np.random.random((3, 4)).astype(config.floatX)],
1669             T.blas.Dot22)
1670     def test_dot22scalar(self):
1671         x, y = T.matrices('xy')
1672         a = T.scalar('a')
1673         self._compile_and_check(
1674             [x, y, a], [T.blas._dot22scalar(x, y, a)],
1675             [np.random.random((2, 3)).astype(config.floatX),
1676              np.random.random((3, 4)).astype(config.floatX),
1677              np.asarray(0.5, dtype=config.floatX)],
1678             T.blas.Dot22Scalar)
1679     def test_gemm(self):
1680         x, y, z = T.matrices('xyz')
1681         a = T.scalar('a')
1682         b = T.scalar('b')
1683         self._compile_and_check(
1684             [x, y, a, z, b], [T.blas.gemm(z, a, x, y, b)],
1685             [np.random.random((2, 3)).astype(config.floatX),
1686              np.random.random((3, 4)).astype(config.floatX),
1687              np.asarray(0.5, dtype=config.floatX),
1688              np.random.random((2, 4)).astype(config.floatX),
1689              np.asarray(0.5, dtype=config.floatX)],
1690             T.blas.Gemm)
1691     def test_gemv(self):
1692         A = T.matrix('A')
1693         x, y = T.vectors('xy')
1694         a = T.scalar('a')
1695         b = T.scalar('b')
1696         self._compile_and_check(
1697             [y, a, A, x, b], [T.blas.gemv(y, a, A, x, b)],
1698             [np.random.random((2,)).astype(config.floatX),
1699              np.asarray(0.5, dtype=config.floatX),
1700              np.random.random((2, 3)).astype(config.floatX),
1701              np.random.random((3,)).astype(config.floatX),
1702              np.asarray(0.5, dtype=config.floatX)],
1703             T.blas.Gemv)
1704     def test_ger(self):
1705         A = T.matrix('A')
1706         x, y = T.vectors('xy')
1707         a = T.scalar('a')
1708         self._compile_and_check(
1709             [A, a, x, y], [T.blas.ger(A, a, x, y)],
1710             [np.random.random((2, 3)).astype(config.floatX),
1711              np.asarray(0.5, dtype=config.floatX),
1712              np.random.random((2,)).astype(config.floatX),
1713              np.random.random((3,)).astype(config.floatX)],
1714             T.blas.Ger)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
