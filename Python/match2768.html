<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for opt.py &amp; test_blas_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for opt.py &amp; test_blas_1.py
      </h3>
<h1 align="center">
        3.2%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>opt.py (8.013029%)<th>test_blas_1.py (2.0157325%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(662-671)<td><a href="#" name="0">(2001-2011)</a><td align="center"><font color="#ff0000">23</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1738-1744)<td><a href="#" name="1">(730-740)</a><td align="center"><font color="#d20000">19</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(1763-1772)<td><a href="#" name="2">(1808-1817)</a><td align="center"><font color="#bc0000">17</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(703-712)<td><a href="#" name="3">(1817-1826)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(1555-1559)<td><a href="#" name="4">(932-934)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(290-291)<td><a href="#" name="5">(608-612)</a><td align="center"><font color="#850000">12</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(264-267)<td><a href="#" name="6">(2037-2041)</a><td align="center"><font color="#850000">12</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(254-256)<td><a href="#" name="7">(453-455)</a><td align="center"><font color="#850000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>opt.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import numpy as np
3 import scipy
4 import theano
5 from theano import gof, scalar, tensor
6 from theano.compat import izip
7 from theano.tensor import blas
8 from theano.tensor.opt import register_specialize, register_canonicalize
9 from theano.sparse import (CSC, CSR, csm_properties,
10                            csm_grad, usmm, csm_indices, csm_indptr,
11                            csm_data)
12 from theano.sparse import basic as sparse
13 _is_sparse_variable = sparse._is_sparse_variable
14 _is_dense = sparse._is_dense
15 @gof.local_optimizer([csm_properties])
16 def local_csm_properties_csm(node):
17     if node.op == csm_properties:
18         csm, = node.inputs
19         if csm.owner and (csm.owner.op == CSC or csm.owner.op == CSR):
20             ret_var = [theano.tensor.patternbroadcast(i, o.broadcastable)
21                        for i, o in izip(csm.owner.inputs, node.outputs)]
22             return ret_var
23     return False
24 register_specialize(local_csm_properties_csm)
25 @gof.local_optimizer([sparse.Remove0])
26 def local_inplace_remove0(node):
27     if isinstance(node.op, sparse.Remove0) and not node.op.inplace:
28         new_op = node.op.__class__(inplace=True)
29         new_node = new_op(*node.inputs)
30         return [new_node]
31     return False
32 theano.compile.optdb.register(
33     'local_inplace_remove0',
34     gof.TopoOptimizer(local_inplace_remove0,
35                       failure_callback=gof.TopoOptimizer.warn_inplace),
36     60, 'fast_run', 'inplace')
37 class AddSD_ccode(gof.op.Op):
38     __props__ = ("format", "inplace")
39     def __init__(self, format, inplace=False, *args, **kwargs):
40         gof.Op.__init__(self, *args, **kwargs)
41         self.inplace = inplace
42         self.format = format
43         if self.inplace:
44             self.destroy_map = {0: [3]}
45     def __str__(self):
46         inp = ''
47         if self.inplace:
48             inp = ',inplace'
49         return "%s{%s%s}" % (self.__class__.__name__,
50                              self.format, inp)
51     def make_node(self, x, y):
52         x, y = sparse.as_sparse_variable(x), tensor.as_tensor_variable(y)
53         out_dtype = scalar.upcast(x.type.dtype, y.type.dtype)
54         if self.inplace:
55             assert out_dtype == y.dtype
56         indices, indptr, data = csm_indices(x), csm_indptr(x), csm_data(x)
57         assert self.format == x.type.format
58         assert y.type.ndim == 2
59         out = tensor.TensorType(dtype=out_dtype,
60                                 broadcastable=y.type.broadcastable)()
61         return gof.Apply(self,
62                          [data, indices, indptr, y],
63                          [out])
64     def c_code(self, node, name, inputs, outputs, sub):
65         (_data, _indices, _indptr, y) = inputs
66         (z,) = outputs
67         inplace = int(self.inplace)
68         format = {'csc': 0, 'csr': 1}[self.format]
69         out_typenum = node.outputs[0].type.dtype_specs()[2]
70         code = """
71                 Py_XDECREF(%(z)s);
72                 if (!%(inplace)s){
73                     if(PyArray_TYPE(%(y)s) != %(out_typenum)s){
74                         %(z)s = (PyArrayObject *) PyArray_FromArray(%(y)s,  PyArray_DescrFromType(%(out_typenum)s), 0);
75                     }else{
76                         %(z)s = (PyArrayObject *) PyArray_NewCopy(%(y)s, NPY_CORDER);
77                     }
78                 }else{
79                   %(z)s = %(y)s;
80                   Py_XINCREF(%(z)s);
81                 }
82                 npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
83                 const dtype_%(_indptr)s* __restrict__ indptr = (dtype_%(_indptr)s*)PyArray_DATA(%(_indptr)s);
84                 const dtype_%(_indices)s* __restrict__ indices = (dtype_%(_indices)s*)PyArray_DATA(%(_indices)s);
85                 const dtype_%(_data)s* __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
86                 dtype_%(y)s* ydata = (dtype_%(y)s*)PyArray_DATA(%(y)s);
87                 dtype_%(z)s* zdata = (dtype_%(z)s*)PyArray_DATA(%(z)s);
88                 npy_intp Yi = PyArray_STRIDES(%(y)s)[0]/PyArray_DESCR(%(y)s)-&gt;elsize;
89                 npy_intp Yj = PyArray_STRIDES(%(y)s)[1]/PyArray_DESCR(%(y)s)-&gt;elsize;
90                 npy_intp pos;
91                 if (%(format)s == 0){
92                 for (npy_intp col = 0; col &lt; N; ++col){
93                   for (dtype_%(_indptr)s ind = indptr[col]; ind &lt; indptr[col+1]; ++ind){
94                     npy_intp row = indices[ind];
95                     pos = row * Yi + col * Yj;
96                     zdata[pos] = ydata[pos] + data[ind];
97                   }
98                 }
99                 }else{
100                 for (npy_intp row = 0; row &lt; N; ++row){
101                   for (dtype_%(_indptr)s ind = indptr[row]; ind &lt; indptr[row+1]; ++ind){
102                     npy_intp col = indices[ind];
103                     pos = row * Yi + col * Yj;
104                     zdata[pos] = ydata[pos] + data[ind];
105                   }
106                  }
107                 }
108     Optimization to insert inplace versions of AddSD.
109     Convert AddSD to faster AddSD_ccode.
110     Structured Dot CSC is like dot, except that only the gradient wrt non-zero
111     elements of the sparse matrix `a` are calculated and propagated.
112     The output is presumed to be a dense matrix, and is represented by a
113     TensorType instance.
114     Parameters
115     ----------
116     a
117         A sparse matrix in csc format.
118     b
119         A sparse or dense matrix.
120     Returns
121     -------
122     The dot product of `a` and `b`.
123     Notes
124     -----
125     The grad implemented is structured.
126     This op is used as an optimization for StructuredDot.
127         dtype_out = scalar.upcast<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(a_val.type.dtype, b.type.dtype)
128         r = gof.Apply(self, [a_val, a_ind, a_ptr, a_nrows, b],
129                       [tensor.tensor(</b></font>dtype_out,
130                                      (False, b.type.broadcastable[1]))])
131         return r
132     def perform(self, node, inputs, outputs):
133 <a name="6"></a>        (a_val, a_ind, a_ptr, a_nrows, b) = inputs
134         (out,) = outputs
135         a = scipy.sparse.csc_matrix((a_val, a_ind, a_ptr),
136                                     (a_nrows, b<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.shape[0]),
137                                     copy=False)
138         out[0] = theano._asarray(a * b, dtype=node.outputs[0].type.</b></font>dtype)
139         assert _is_dense(out[0])  # scipy 0.7 automatically converts to dense
140     def c_code(self, node, name, inputs, outputs, sub):
141         (a_val, a_ind, a_ptr, a_nrows, b) = inputs
142         (z,) = outputs
143         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
144             raise NotImplementedError('Complex types are not supported for a_val')
145 <a name="5"></a>        if node.inputs[4].type.dtype in ('complex64', 'complex128'):
146             raise NotImplementedError('Complex types are not supported for b')
147         typenum_z = node.outputs<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[0].type.dtype_specs()[2]  # retrieve dtype number
148         typenum_a_val = node.inputs[0].type.dtype_specs()[</b></font>2]  # retrieve dtype number
149         typenum_b = node.inputs[4].type.dtype_specs()[2]  # retrieve dtype number
150         rval = """
151         if (PyArray_NDIM(%(a_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_val) != 1"); %(fail)s;}
152         if (PyArray_NDIM(%(a_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ind) != 1"); %(fail)s;}
153         if (PyArray_NDIM(%(a_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ptr) != 1"); %(fail)s;}
154         if (PyArray_NDIM(%(a_nrows)s) != 0) {PyErr_SetString(PyExc_NotImplementedError, "rank(nrows) != 0"); %(fail)s;}
155         if (PyArray_NDIM(%(b)s) != 2) {PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 2"); %(fail)s;}
156         if (PyArray_TYPE(%(a_val)s) != %(typenum_a_val)s) {
157         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for a_val"); %(fail)s;}
158         if (PyArray_TYPE(%(b)s) != %(typenum_b)s) {
159         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for b"); %(fail)s;}
160         if (PyArray_TYPE(%(a_ind)s) != NPY_INT32) {
161         PyErr_SetString(PyExc_NotImplementedError, "a_ind dtype not INT32"); %(fail)s;}
162         if (PyArray_TYPE(%(a_ptr)s) != NPY_INT32)
163         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr dtype not INT32"); %(fail)s;}
164         if (PyArray_TYPE(%(a_nrows)s) != NPY_INT32)
165         {PyErr_SetString(PyExc_NotImplementedError, "a_nrows dtype not INT32"); %(fail)s;}
166         if (PyArray_DIMS(%(a_val)s)[0] != PyArray_DIMS(%(a_ind)s)[0])
167         {PyErr_SetString(PyExc_NotImplementedError, "a_val and a_ind have different lengths"); %(fail)s;}
168         if (PyArray_DIMS(%(a_ptr)s)[0] != PyArray_DIMS(%(b)s)[0]+1)
169         {PyErr_SetString(PyExc_NotImplementedError, "a's number of columns doesn't match b's rows"); %(fail)s;}
170         if ((!%(z)s)
171             || (PyArray_DIMS(%(z)s)[0] != ((npy_int32 *)PyArray_DATA(%(a_nrows)s))[0])
172             || (PyArray_DIMS(%(z)s)[1] != PyArray_DIMS(%(b)s)[1])
173             )
174         {
175             {Py_XDECREF(%(z)s);}
176             npy_intp dims[] = {0, 0};
177             dims[0] = ((npy_int32 *)PyArray_DATA(%(a_nrows)s))[0];
178             dims[1] = PyArray_DIMS(%(b)s)[1];
179             %(z)s = (PyArrayObject*) PyArray_SimpleNew(2, dims, %(typenum_z)s);
180         }
181         {
182             // sparse array has size MxK, dense KxN, output MxN
183             npy_intp M = PyArray_DIMS(%(z)s)[0];
184             npy_intp N = PyArray_DIMS(%(z)s)[1];
185             npy_intp K = PyArray_DIMS(%(b)s)[0];
186             if (N &gt; 0x7fffffffL)
187             {PyErr_SetString(PyExc_NotImplementedError, "array too big (overflows int32 index)"); %(fail)s;}
188             // strides tell you how many bytes to skip to go to next column/row entry
189             npy_intp Szm = PyArray_STRIDES(%(z)s)[0] / PyArray_DESCR(%(z)s)-&gt;elsize;
190             npy_intp Szn = PyArray_STRIDES(%(z)s)[1] / PyArray_DESCR(%(z)s)-&gt;elsize;
191             //npy_intp Sbm = PyArray_STRIDES(%(b)s)[0] / PyArray_DESCR(%(b)s)-&gt;elsize;
192             npy_intp Sbn = PyArray_STRIDES(%(b)s)[1] / PyArray_DESCR(%(b)s)-&gt;elsize;
193             npy_intp Sval = PyArray_STRIDES(%(a_val)s)[0] / PyArray_DESCR(%(a_val)s)-&gt;elsize;
194             npy_intp Sind = PyArray_STRIDES(%(a_ind)s)[0] / PyArray_DESCR(%(a_ind)s)-&gt;elsize;
195             npy_intp Sptr = PyArray_STRIDES(%(a_ptr)s)[0] / PyArray_DESCR(%(a_ptr)s)-&gt;elsize;
196             // pointers to access actual data in the arrays passed as params.
197             dtype_%(z)s*     __restrict__ Dz   = (dtype_%(z)s*)PyArray_DATA(%(z)s);
198             const dtype_%(a_val)s* __restrict__ Dval = (dtype_%(a_val)s*)PyArray_DATA(%(a_val)s);
199             const npy_int32 * __restrict__ Dind = (npy_int32*)PyArray_DATA(%(a_ind)s);
200             const npy_int32 * __restrict__ Dptr = (npy_int32*)PyArray_DATA(%(a_ptr)s);
201             //npy_intp nnz = PyArray_DIMS(%(a_ind)s)[0];
202             //clear the output array
203             memset(Dz, 0, M*N*sizeof(dtype_%(z)s));
204             //iterate over the sparse array, making the most of an entry wherever we find it.
205             //
206             // Normal matrix matrix multiply: A MxK, B KxN =&gt;  Z = AB
207             // for m
208             //   for n
209             //     for k
210             //        z[m, n] += a[m, k] * b[k, n]
211             // Here instead: Z =
212             // for k
213             //   for m (sparse)
214             //     for n
215             //        z[m, n] += a[m, k] * b[k, n]
216             // loop over inner dimension
217             for (npy_int32 k = 0; k &lt; K; ++k)
218             {
219                 // get pointer to k-th row of dense matrix
220                 const dtype_%(b)s* __restrict__ bk = (dtype_%(b)s*)(PyArray_BYTES(%(b)s) + PyArray_STRIDES(%(b)s)[0] * k);
221                 // loop over sparse column indices through index pointer array
222                 // (amounts to looping over rows M of sparse matrix)
223                 for (npy_int32 m_idx = Dptr[k * Sptr]; m_idx &lt; Dptr[(k+1) * Sptr]; ++m_idx)
224                 {
225                     npy_int32 m = Dind[m_idx * Sind]; // row index of non-null value for column K
226                     const dtype_%(a_val)s Amk = Dval[m_idx * Sval]; // actual value at that location
227                     // pointer to m-th row of the output matrix Z
228                     dtype_%(z)s* __restrict__ zm = (dtype_%(z)s*)(PyArray_BYTES(%(z)s) + PyArray_STRIDES(%(z)s)[0] * m);
229                     //RESOLVE: a.shape[0] equals z.shape[0], why is this not an equality constraint?
230                     if (m &gt;= PyArray_DIMS(%(z)s)[0])
231                     {PyErr_SetString(PyExc_NotImplementedError, "illegal row index in a"); %(fail)s;}
232                     // loop over final dimension (cols of dense matrix) and perform dot product
233                     if ((Szn == 1) &amp;&amp; (Sbn == 1)) {
234                         for(npy_int32 n = 0; n &lt; N; ++n)
235                         {
236                             zm[n] += Amk * bk[n];
237                         }
238                     }
239                     else
240                     {
241                         for(npy_int32 n = 0; n &lt; N; ++n)
242                         {
243                             zm[n*Szn] += Amk * bk[n*Sbn];
244                         }
245                     }
246                 }
247             }
248         }
249     Structured Dot CSR is like dot, except that only the
250     gradient wrt non-zero elements of the sparse matrix
251     `a` are calculated and propagated.
252     The output is presumed to be a dense matrix, and is represented by a
253     TensorType instance.
254     Parameters
255     ----------
256     a
257         A sparse matrix in csr format.
258     b
259         A sparse or dense matrix.
260     Returns
261     -------
262     matrix
263         The dot product of `a` and `b`.
264     Notes
265     -----
266     The grad implemented is structured.
267     This op is used as an optimization for StructuredDot.
268         C-implementation of the dot product of the sparse matrix A and matrix B.
269         Parameters
270         ----------
271         a_val
272             Non-zero values of the sparse matrix.
273         a_ind
274             Column indices of the non-null values (.indices of a
275             scipy.csc_matrix).
276         a_ptr
277             Indicates col indices for col. i are in the range
278             a_ptr[i]:a_ptr[i+1].
279         n_cols
280             Number of columns of sparse matrix.
281         b
282             Dense matrix to perform dot product with, as in dot(a, b).
283         z
284             Return value.
285         sub
286             TODO, not too sure, something to do with weave probably.
287     def c_code_cache_version(self):
288         return (2,)
289 sd_csr = StructuredDotCSR()
290 @gof.local_optimizer([sparse._structured_dot])
291 def local_structured_dot(node):
292     if node.op == sparse._structured_dot:
293         a, b = node.inputs
294         if a.type.format == 'csc':
295             a_val, a_ind, a_ptr, a_shape = csm_properties(a)
296             a_nsparse = a_shape[0]
297             return [sd_csc(a_val, a_ind, a_ptr, a_nsparse, b)]
298         if a.type.format == 'csr':
299             a_val, a_ind, a_ptr, a_shape = csm_properties(a)
300             return [sd_csr(a_val, a_ind, a_ptr, b)]
301     return False
302 class UsmmCscDense(gof.Op):
303     __props__ = ("inplace",)
304     def __init__(self, inplace):
305         self.inplace = inplace
306         if inplace:
307             self.destroy_map = {0: [6]}
308     def __str__(self):
309 <a name="0"></a>        if self.inplace:
310             return 'UsmmCscDense{inplace}'
311         else:
312             re<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>turn 'UsmmCscDense{no_inplace}'
313     def make_node(self, alpha, x_val, x_ind, x_ptr, x_nrows, y, z):
314         alpha = tensor.as_tensor_variable(alpha)
315         x_val = tensor.as_tensor_variable(x_val)
316         x_ind = tensor.as_tensor_variable(x_ind)
317         x_ptr = tensor.as_tensor_variable(x_ptr)
318         x_nrows = tensor.as_tensor_variable(x_nrows)
319         y = tensor.as_tensor_variable(y)
320         z = tensor.as_tensor_variable(</b></font>z)
321         assert x_ind.dtype == 'int32'
322         assert x_ptr.dtype == 'int32'
323         assert x_nrows.dtype == 'int32'
324         assert alpha.ndim == 2 and alpha.type.broadcastable == (True, True)
325         assert x_val.ndim == 1
326         assert y.ndim == 2
327         assert z.ndim == 2
328         dtype_out = scalar.upcast(alpha.type.dtype, x_val.type.dtype,
329                                   y.type.dtype, z.type.dtype)
330         if dtype_out not in ('float32', 'float64'):
331             raise NotImplementedError('only float types are supported in '
332                                       'operands')
333         if self.inplace:
334             assert z.type.dtype == dtype_out
335         if dtype_out != alpha.type.dtype:
336             alpha = tensor.cast(alpha, dtype_out)
337         if dtype_out != x_val.type.dtype:
338             x_val = tensor.cast(x_val, dtype_out)
339         if dtype_out != y.type.dtype:
340             y = tensor.cast(y, dtype_out)
341         if dtype_out != z.type.dtype:
342             z = tensor.cast(z, dtype_out)
343 <a name="3"></a>        r = gof.Apply(
344             self, [alpha, x_val, x_ind, x_ptr, x_nrows, y, z],
345             [tensor.tensor(dtype_out, (False, y.type.broadcastable[1]))])
346         r<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>eturn r
347     def c_support_code(self):
348         return blas.blas_header_text()
349     def c_libraries(self):
350         return blas.ldflags()
351     def c_compile_args(self):
352         return blas.ldflags(</b></font>libs=False, flags=True)
353     def c_lib_dirs(self):
354         return blas.ldflags(libs=False, libs_dir=True)
355     def c_header_dirs(self):
356         return blas.ldflags(libs=False, include_dir=True)
357     def c_code(self, node, name, inputs, outputs, sub):
358         alpha, x_val, x_ind, x_ptr, x_nrows, y, z = inputs
359         zn = outputs[0]
360         if node.inputs[1].type.dtype in ('complex64', 'complex128'):
361             raise NotImplementedError('Complex types are not supported for '
362                                       'x_val')
363         if node.inputs[5].type.dtype in ('complex64', 'complex128'):
364             raise NotImplementedError('Complex types are not supported for y')
365         if node.inputs[6].type.dtype != node.outputs[0].type.dtype:
366             raise NotImplementedError('z and output must have same type')
367         if node.inputs[1].type.dtype == "float32":
368             conv_type = "float"
369             axpy = "saxpy_"
370         else:
371             conv_type = "double"
372             axpy = "daxpy_"
373         typenum_alpha = node.inputs[0].type.dtype_specs()[2]
374         typenum_x_val = node.inputs[1].type.dtype_specs()[2]
375         typenum_y = node.inputs[5].type.dtype_specs()[2]
376         typenum_z = node.inputs[6].type.dtype_specs()[2]
377         typenum_zn = node.outputs[0].type.dtype_specs()[2]
378         inplace = int(self.inplace)
379         rval = """
380         if (PyArray_NDIM(%(x_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(x_val) != 1"); %(fail)s;}
381         if (PyArray_NDIM(%(x_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(x_ind) != 1"); %(fail)s;}
382         if (PyArray_NDIM(%(x_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(x_ptr) != 1"); %(fail)s;}
383         if (PyArray_NDIM(%(x_nrows)s) != 0) {PyErr_SetString(PyExc_NotImplementedError, "rank(nrows) != 0"); %(fail)s;}
384         if (PyArray_NDIM(%(y)s) != 2) {PyErr_SetString(PyExc_NotImplementedError, "rank(y) != 2"); %(fail)s;}
385         if (PyArray_TYPE(%(x_val)s) != %(typenum_x_val)s) {
386         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for x_val"); %(fail)s;}
387         if (PyArray_TYPE(%(y)s) != %(typenum_y)s) {
388         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for y"); %(fail)s;}
389         if (PyArray_TYPE(%(z)s) != %(typenum_z)s) {
390         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for z"); %(fail)s;}
391         if (PyArray_TYPE(%(alpha)s) != %(typenum_alpha)s) {
392         PyErr_SetString(PyExc_NotImplementedError, "Invalid type for alpha"); %(fail)s;}
393         if (PyArray_TYPE(%(x_ind)s) != NPY_INT32) {
394         PyErr_SetString(PyExc_NotImplementedError, "x_ind dtype not INT32"); %(fail)s;}
395         if (PyArray_TYPE(%(x_ptr)s) != NPY_INT32)
396         {PyErr_SetString(PyExc_NotImplementedError, "x_ptr dtype not INT32"); %(fail)s;}
397         if (PyArray_TYPE(%(x_nrows)s) != NPY_INT32)
398         {PyErr_SetString(PyExc_NotImplementedError, "x_nrows dtype not INT32"); %(fail)s;}
399         if (PyArray_DIMS(%(x_val)s)[0] != PyArray_DIMS(%(x_ind)s)[0])
400         {PyErr_SetString(PyExc_NotImplementedError, "x_val and x_ind have different lengths"); %(fail)s;}
401         if (PyArray_DIMS(%(x_ptr)s)[0] != PyArray_DIMS(%(y)s)[0]+1)
402         {PyErr_SetString(PyExc_NotImplementedError, "x's number of columns doesn't match y's rows"); %(fail)s;}
403         if (PyArray_DIMS(%(z)s)[0] != ((npy_int32 *)PyArray_DATA(%(x_nrows)s))[0] || PyArray_DIMS(%(z)s)[1] != PyArray_DIMS(%(y)s)[1])
404         {PyErr_SetString(PyExc_NotImplementedError, "The dimension of the allocated output doesn't match the correct output size."); %(fail)s;}
405         if (PyArray_SIZE(%(alpha)s) != 1)
406         {PyErr_SetString(PyExc_NotImplementedError, "The number of element in alpha must be 1"); %(fail)s;}
407         if (PyArray_NDIM(%(alpha)s) != 2)
408         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of alpha must be 2"); %(fail)s;}
409         if (PyArray_NDIM(%(x_val)s) != 1)
410         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of x_val must be 1"); %(fail)s;}
411         if (PyArray_NDIM(%(y)s) != 2)
412         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of y must be 2"); %(fail)s;}
413         if (PyArray_NDIM(%(z)s) != 2)
414         {PyErr_SetString(PyExc_NotImplementedError, "The number dimension of z must be 2"); %(fail)s;}
415         if (%(inplace)s)
416         {
417             if (%(typenum_zn)s != %(typenum_z)s) {
418             PyErr_SetString(PyExc_NotImplementedError, "When inplace the output dtype must be the same as the input"); %(fail)s;}
419             Py_XDECREF(%(zn)s);
420             %(zn)s = %(z)s;
421             Py_INCREF(%(zn)s);
422         }
423         else if (!%(zn)s
424             || (PyArray_DIMS(%(zn)s)[0] != ((npy_int32 *)PyArray_DATA(%(x_nrows)s))[0])
425             || (PyArray_DIMS(%(zn)s)[1] != PyArray_DIMS(%(y)s)[1])
426             )
427         {
428             {Py_XDECREF(%(zn)s);}
429             npy_intp dims[] = {0, 0};
430             dims[0] = ((npy_int32 *)PyArray_DATA(%(x_nrows)s))[0];
431             dims[1] = PyArray_DIMS(%(y)s)[1];
432             %(zn)s = (PyArrayObject*) PyArray_SimpleNew(2, dims, %(typenum_zn)s);
433         }
434         {
435             // sparse array has size MxK, dense KxN, output MxN
436             npy_intp M = PyArray_DIMS(%(zn)s)[0];
437             npy_intp N = PyArray_DIMS(%(zn)s)[1];
438             npy_intp K = PyArray_DIMS(%(y)s)[0];
439             // pointers to access actual data in the arrays passed as params.
440             const dtype_%(x_val)s* __restrict__ Dval = (dtype_%(x_val)s*)PyArray_DATA(%(x_val)s);
441             const npy_int32 * __restrict__ Dind = (npy_int32*)PyArray_DATA(%(x_ind)s);
442             const npy_int32 * __restrict__ Dptr = (npy_int32*)PyArray_DATA(%(x_ptr)s);
443             const dtype_%(alpha)s alpha = ((dtype_%(alpha)s*)PyArray_DATA(%(alpha)s))[0];
444             npy_intp Sz = PyArray_STRIDES(%(z)s)[1] / PyArray_DESCR(%(z)s)-&gt;elsize;
445             npy_intp Szn = PyArray_STRIDES(%(zn)s)[1] / PyArray_DESCR(%(zn)s)-&gt;elsize;
446             npy_intp Sval = PyArray_STRIDES(%(x_val)s)[0] / PyArray_DESCR(%(x_val)s)-&gt;elsize;
447             npy_intp Sind = PyArray_STRIDES(%(x_ind)s)[0] / PyArray_DESCR(%(x_ind)s)-&gt;elsize;
448             npy_intp Sptr = PyArray_STRIDES(%(x_ptr)s)[0] / PyArray_DESCR(%(x_ptr)s)-&gt;elsize;
449             npy_intp Sy = PyArray_STRIDES(%(y)s)[1] / PyArray_DESCR(%(y)s)-&gt;elsize;
450             // blas expects ints; convert here (rather than just making N etc ints) to avoid potential overflow in the negative-stride correction
451             if ((N &gt; 0x7fffffffL)||(Sy &gt; 0x7fffffffL)||(Szn &gt; 0x7fffffffL)||(Sy &lt; -0x7fffffffL)||(Szn &lt; -0x7fffffffL))
452             {PyErr_SetString(PyExc_NotImplementedError, "array too big for BLAS (overflows int32 index)"); %(fail)s;}
453             int N32 = N;
454             int Sy32 = Sy;
455             int Szn32 = Szn;
456             if (!(%(inplace)s))
457             {
458                 if (PyArray_CopyInto(%(zn)s, %(z)s))
459                 {
460                     Py_XDECREF(%(zn)s);
461                     %(fail)s;
462                 }
463             }
464             for (npy_intp k = 0; k &lt; K; ++k)
465             {
466                 for (npy_int32 m_idx = Dptr[k * Sptr]; m_idx &lt; Dptr[(k+1)*Sptr]; ++m_idx)
467                 {
468                     const npy_int32 m = Dind[m_idx * Sind]; // row index of non-null value for column K
469                     const dtype_%(x_val)s Amk = alpha * Dval[m_idx * Sval]; // actual value at that location
470                     dtype_%(y)s* y_row = (dtype_%(y)s*)(PyArray_BYTES(%(y)s) + PyArray_STRIDES(%(y)s)[0] * k);
471                     // axpy expects pointer to the beginning of memory arrays,
472                     // so when the stride is negative, we need to get the
473                     // last element
474                     if (Sy &lt; 0)
475                         y_row += (K - 1) * Sy;
476                     dtype_%(zn)s* z_row = (dtype_%(zn)s*)(PyArray_BYTES(%(zn)s) + PyArray_STRIDES(%(zn)s)[0] * m);
477                     if (Szn &lt; 0)
478                         z_row += (N - 1) * Szn;
479                     %(axpy)s(&amp;N32, (%(conv_type)s*)&amp;Amk, (%(conv_type)s*)y_row, &amp;Sy32, (%(conv_type)s*)z_row, &amp;Szn32);
480                 }
481             }
482         }
483     usmm -&gt; usmm_csc_dense
484     """
485     if node.op == usmm:
486         alpha, x, y, z = node.inputs
487         x_is_sparse_variable = _is_sparse_variable(x)
488         y_is_sparse_variable = _is_sparse_variable(y)
489         if x_is_sparse_variable and not y_is_sparse_variable:
490             if x.type.format == 'csc':
491                 x_val, x_ind, x_ptr, x_shape = csm_properties(x)
492                 x_nsparse = x_shape[0]
493                 dtype_out = scalar.upcast(alpha.type.dtype, x.type.dtype,
494                                           y.type.dtype, z.type.dtype)
495                 if dtype_out not in ('float32', 'float64'):
496                     return False
497                 if y.type.dtype != dtype_out:
498                     return False
499                 return [usmm_csc_dense(alpha, x_val, x_ind, x_ptr,
500                                        x_nsparse, y, z)]
501     return False
502 register_specialize(local_usmm_csx, 'cxx_only')
503 class CSMGradC(gof.Op):
504     __props__ = ()
505     def make_node(self, a_val, a_ind, a_ptr, a_dim,
506                   b_val, b_ind, b_ptr, b_dim):
507         return gof.Apply(self, [a_val, a_ind, a_ptr, a_dim,
508                          b_val, b_ind, b_ptr, b_dim], [b_val.type()])
509     def c_code(self, node, name, inputs, outputs, sub):
510         (a_val, a_ind, a_ptr, a_dim,
511          b_val, b_ind, b_ptr, b_dim) = inputs
512         (z,) = outputs
513         typenum_z = node.outputs[0].type.dtype_specs()[2]
514         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
515             raise NotImplementedError('Complex types are not supported for a_val')
516         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
517             raise NotImplementedError('Complex types are not supported for b_val')
518         return """
519         if (PyArray_NDIM(%(a_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_val) != 1"); %(fail)s;}
520         if (PyArray_NDIM(%(a_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ind) != 1"); %(fail)s;}
521         if (PyArray_NDIM(%(a_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(a_ptr) != 1"); %(fail)s;}
522         if (PyArray_NDIM(%(b_val)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(b_val) != 1"); %(fail)s;}
523         if (PyArray_NDIM(%(b_ind)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(b_ind) != 1"); %(fail)s;}
524         if (PyArray_NDIM(%(b_ptr)s) != 1) {PyErr_SetString(PyExc_NotImplementedError, "rank(b_ptr) != 1"); %(fail)s;}
525         if (PyArray_TYPE(%(a_ind)s) != NPY_INT32) {
526         PyErr_SetString(PyExc_NotImplementedError, "a_ind dtype not INT32"); %(fail)s;}
527         if (PyArray_TYPE(%(a_ptr)s) != NPY_INT32)
528         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr dtype not INT32"); %(fail)s;}
529         if (PyArray_TYPE(%(b_ind)s) != NPY_INT32) {
530         PyErr_SetString(PyExc_NotImplementedError, "b_ind dtype not INT32"); %(fail)s;}
531         if (PyArray_TYPE(%(b_ptr)s) != NPY_INT32)
532         {PyErr_SetString(PyExc_NotImplementedError, "b_ptr dtype not INT32"); %(fail)s;}
533         if (PyArray_DIMS(%(a_val)s)[0] != PyArray_DIMS(%(a_ind)s)[0])
534         {PyErr_SetString(PyExc_NotImplementedError, "a_val and a_ind have different lengths"); %(fail)s;}
535         if (PyArray_DIMS(%(b_val)s)[0] != PyArray_DIMS(%(b_ind)s)[0])
536         {PyErr_SetString(PyExc_NotImplementedError, "b_val and b_ind have different lengths"); %(fail)s;}
537         if (PyArray_DIMS(%(a_ptr)s)[0] != PyArray_DIMS(%(b_ptr)s)[0])
538         {PyErr_SetString(PyExc_NotImplementedError, "a_ptr and b_ptr have different lengths"); %(fail)s;}
539         if ((!%(z)s) || (PyArray_DIMS(%(z)s)[0] != PyArray_DIMS(%(a_val)s)[0]))
540         {
541             {Py_XDECREF(%(z)s);}
542             npy_intp dims[] = {0};
543             dims[0] = PyArray_DIMS(%(a_val)s)[0];
544             %(z)s = (PyArrayObject*) PyArray_SimpleNew(1, dims, %(typenum_z)s);
545         }
546         {
547             // sparse array has size MxK, dense KxN, output MxN
548             npy_intp M = PyArray_DIMS(%(a_ptr)s)[0] - 1;
549             npy_intp a_dim_0 = ((npy_int32 *)PyArray_DATA(%(a_dim)s))[0];
550             npy_intp a_dim_1 = ((npy_int32 *)PyArray_DATA(%(a_dim)s))[1];
551             npy_intp sp_dim = (M == a_dim_0)?a_dim_1:a_dim_0;
552             // strides tell you how many bytes to skip to go to next column/row entry
553             npy_intp Sz = PyArray_STRIDES(%(z)s)[0] / PyArray_DESCR(%(z)s)-&gt;elsize;
554             npy_intp Sa_val = PyArray_STRIDES(%(a_val)s)[0] / PyArray_DESCR(%(a_val)s)-&gt;elsize;
555             npy_intp Sa_ind = PyArray_STRIDES(%(a_ind)s)[0] / PyArray_DESCR(%(a_ind)s)-&gt;elsize;
556             npy_intp Sa_ptr = PyArray_STRIDES(%(a_ptr)s)[0] / PyArray_DESCR(%(a_ptr)s)-&gt;elsize;
557             npy_intp Sb_val = PyArray_STRIDES(%(b_val)s)[0] / PyArray_DESCR(%(b_val)s)-&gt;elsize;
558             npy_intp Sb_ind = PyArray_STRIDES(%(b_ind)s)[0] / PyArray_DESCR(%(b_ind)s)-&gt;elsize;
559             npy_intp Sb_ptr = PyArray_STRIDES(%(b_ptr)s)[0] / PyArray_DESCR(%(b_ptr)s)-&gt;elsize;
560             // pointers to access actual data in the arrays passed as params.
561             dtype_%(z)s* __restrict__ Dz = (dtype_%(z)s*)PyArray_DATA(%(z)s);
562             const dtype_%(a_val)s* __restrict__ Da_val = (dtype_%(a_val)s*)PyArray_DATA(%(a_val)s);
563             const npy_int32 * __restrict__ Da_ind = (npy_int32*)PyArray_DATA(%(a_ind)s);
564             const npy_int32 * __restrict__ Da_ptr = (npy_int32*)PyArray_DATA(%(a_ptr)s);
565             const dtype_%(b_val)s* __restrict__ Db_val = (dtype_%(b_val)s*)PyArray_DATA(%(b_val)s);
566             const npy_int32 * __restrict__ Db_ind = (npy_int32*)PyArray_DATA(%(b_ind)s);
567             const npy_int32 * __restrict__ Db_ptr = (npy_int32*)PyArray_DATA(%(b_ptr)s);
568             npy_intp nnz = PyArray_DIMS(%(a_ind)s)[0];
569             dtype_%(b_val)s b_row[sp_dim];
570             //clear the output array
571             for (npy_int64 i = 0; i &lt; nnz; ++i)
572             {
573                 Dz[i*Sz] = 0;
574             }
575             memset(b_row, 0, sp_dim*sizeof(dtype_%(b_val)s));
576             // loop over inner dimension
577             for (npy_int64 m = 0; m &lt; M; ++m)
578             {
579                 for (npy_int32 j_ptr = Db_ptr[m * Sb_ptr];
580                     j_ptr &lt; Db_ptr[(m + 1) * Sb_ptr]; j_ptr++) {
581                     b_row[Db_ind[j_ptr * Sb_ind]] += Db_val[j_ptr*Sb_val];
582                 }
583                 for (npy_int32 j_ptr = Da_ptr[m * Sa_ptr];
584                     j_ptr &lt; Da_ptr[(m + 1) * Sa_ptr]; j_ptr++) {
585                     Dz[j_ptr*Sz] = b_row[Da_ind[j_ptr * Sa_ind]];
586                 }
587                 for (npy_int32 j_ptr = Db_ptr[m * Sb_ptr];
588                     j_ptr &lt; Db_ptr[(m + 1) * Sb_ptr]; j_ptr++) {
589                     b_row[Db_ind[j_ptr * Sb_ind]] = 0;
590                 }
591             }
592         }
593     csm_grad(None) -&gt; csm_grad_c
594     """
595     if node.op == csm_grad(None):
596         return [csm_grad_c(*node.inputs)]
597     return False
598 class MulSDCSC(gof.Op):
599     __props__ = ()
600     def make_node(self, a_data, a_indices, a_indptr, b):
601         assert b.type.ndim == 2
602         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
603                                [tensor.tensor(b.dtype, (False,))])
604     def c_code_cache_version(self):
605         return (3,)
606     def c_code(self, node, name, inputs, outputs, sub):
607         (_data, _indices, _indptr, _b,) = inputs
608         (_zout,) = outputs
609         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
610             raise NotImplementedError('Complex types are not supported for a')
611         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
612             raise NotImplementedError('Complex types are not supported for b')
613         return """
614         if (PyArray_NDIM(%(_b)s) != 2) {
615             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 2");
616             %(fail)s;}
617         if (PyArray_NDIM(%(_data)s) != 1) {
618             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
619             %(fail)s;}
620         if (PyArray_NDIM(%(_indices)s) != 1) {
621             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
622             %(fail)s;}
623         if (PyArray_NDIM(%(_indptr)s) != 1) {
624             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
625             %(fail)s;}
626         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
627         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
628         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
629         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
630         if (!%(_zout)s ||
631             (PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0]) ||
632             !(PyArray_ISCONTIGUOUS(%(_zout)s)))
633         {
634             Py_XDECREF(%(_zout)s);
635             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
636                   PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
637             if (!%(_zout)s)
638             {
639                 PyErr_SetString(PyExc_MemoryError,
640                     "Could not allocate output memory.");
641                 %(fail)s;
642             }
643         }
644         { //makes it compile even though labels jump over variable definitions.
645             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
646             //TODO: error checking with this
647             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
648             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
649             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
650             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
651             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
652             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0];
653             // loop over columns
654             for (npy_intp j = 0; j &lt; N; ++j)
655             {
656                 // for each non-null value in the sparse column
657                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
658                 {
659                     // extract row index of non-null value
660                     npy_int32 i = indices[i_idx];
661                     // extract i-th row of dense matrix
662                     const dtype_%(_b)s* __restrict__ b_row = (dtype_%(_b)s*)(PyArray_BYTES(%(_b)s) + Sb * i);
663                     // write resulting gradient to sparse output
664                     zout[i_idx] = data[i_idx] * b_row[j];
665                 }
666             }
667         }
668     Multiplication of sparse matrix by a broadcasted dense vector
669     element wise.
670     Parameters
671     ----------
672     a_data
673         Sparse matrix data.
674     a_indices
675         Sparse matrix indices.
676     a_indptr
677         Sparse matrix indptr.
678     b
679         Tensor type matrix.
680     Returns
681     -------
682     The multiplication of the two matrix element wise.
683     Notes
684     -----
685     `a_data`, `a_indices` and `a_indptr` must be the properties
686     of a sparse matrix in csr format.
687     The dtype of `a_data`, i.e. the dtype of the sparse matrix,
688     cannot be a complex type.
689     This op is used as an optimization of mul_s_d.
690     def __str__(self):
691         return self.__class__.__name__
692 mul_s_d_csr = MulSDCSR()
693 @gof.local_optimizer([sparse.mul_s_d])
694 def local_mul_s_d(node):
695     if node.op == sparse.mul_s_d:
696         x, y = node.inputs
697         x_is_sparse_variable = _is_sparse_variable(x)
698         if x_is_sparse_variable:
699             svar = x
700             dvar = y
701         else:
702             svar = y
703             dvar = x
704         if dvar.type.ndim != 2:
705             return False
706         if svar.type.format == 'csc':
707             CSx = sparse.CSC
708             mul_s_d_csx = mul_s_d_csc
709         elif svar.type.format == 'csr':
710             CSx = sparse.CSR
711             mul_s_d_csx = mul_s_d_csr
712         else:
713             raise NotImplementedError
714         if x.dtype != y.dtype:
715             return
716         c_data = mul_s_d_csx(sparse.csm_data(svar),
717                              sparse.csm_indices(svar),
718                              sparse.csm_indptr(svar), dvar)
719         return [CSx(c_data,
720                     sparse.csm_indices(svar),
721                     sparse.csm_indptr(svar),
722                     sparse.csm_shape(svar))]
723     return False
724 register_specialize(local_mul_s_d, 'cxx_only')
725 class MulSVCSR(gof.Op):
726     __props__ = ()
727     def make_node(self, a_data, a_indices, a_indptr, b):
728         assert b.type.ndim == 1
729         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
730                                [tensor.tensor(b.dtype, (False,))])
731     def c_code_cache_version(self):
732         return (2,)
733     def c_code(self, node, name, inputs, outputs, sub):
734         _data, _indices, _indptr, _b, = inputs
735         _zout, = outputs
736         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
737             raise NotImplementedError('Complex types are not supported for a')
738         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
739             raise NotImplementedError('Complex types are not supported for b')
740         return """
741         if (PyArray_NDIM(%(_b)s) != 1) {
742             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 1");
743             %(fail)s;
744         }
745         if (PyArray_NDIM(%(_data)s) != 1) {
746             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
747             %(fail)s;
748         }
749         if (PyArray_NDIM(%(_indices)s) != 1) {
750             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
751             %(fail)s;
752         }
753         if (PyArray_NDIM(%(_indptr)s) != 1) {
754             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
755             %(fail)s;
756         }
757         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
758         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
759         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
760         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
761         if (!%(_zout)s
762             || PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0]
763             || !PyArray_ISCONTIGUOUS(%(_zout)s))
764         {
765             Py_XDECREF(%(_zout)s);
766             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
767                     PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
768         }
769         { //makes it compile even though labels jump over variable definitions.
770             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
771             //TODO: error checking with this
772             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
773             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
774             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
775             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
776             const dtype_%(_b)s* __restrict__ Db = (dtype_%(_b)s*)PyArray_DATA(%(_b)s);
777             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
778             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0] / PyArray_DESCR(%(_b)s)-&gt;elsize;
779             // loop over rows
780             for (npy_intp j = 0; j &lt; N; ++j)
781             {
782                 // for each non-null value in the sparse column
783                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
784                 {
785                     // extract row index of non-null value
786                     npy_int32 i = indices[i_idx];
787                     zout[i_idx] = data[i_idx] * Db[i * Sb];
788                 }
789             }
790         }
791     Structured addition of a sparse matrix and a dense vector.
792     The elements of the vector are are only added to the corresponding
793     non-zero elements. Therefore, this operation outputs another sparse
794     matrix.
795     Parameters
796     ----------
797     a_data
798         Sparse matrix data.
799     a_indices
800         Sparse matrix indices.
801     a_indptr
802         Sparse matrix indptr.
803     b
804         Tensor type vector.
805     Returns
806     -------
807     A sparse matrix containing the addition of the vector to the data of the
808     sparse matrix.
809     Notes
810     -----
811     The a_* are the properties of a sparse matrix in csr format.
812     This op is used as an optimization for StructuredAddSV.
813     <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, a_data, a_indices, a_indptr, b):
814         b = tensor.as_tensor_variable(b)
815         a_data = tensor.as_tensor_variable(a_data)
816         a_indices = tensor.as_tensor_variable(a_indices)
817         a_indptr = tensor.as_tensor_variable(</b></font>a_indptr)
818         assert a_data.type.ndim == 1
819         assert a_indices.type.ndim == 1
820         assert a_indptr.type.ndim == 1
821         assert b.type.ndim == 1
822         return gof.Apply(self, [a_data, a_indices, a_indptr, b],
823                                [tensor.tensor(b.dtype, (False,))])
824     def c_code_cache_version(self):
825         return (3,)
826     def c_code(self, node, name, inputs, outputs, sub):
827         _data, _indices, _indptr, _b, = inputs
828         _zout, = outputs
829         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
830             raise NotImplementedError('Complex types are not supported for a')
831         if node.inputs[3].type.dtype in ('complex64', 'complex128'):
832             raise NotImplementedError('Complex types are not supported for b')
833         return """
834         if (PyArray_NDIM(%(_b)s) != 1) {
835             PyErr_SetString(PyExc_NotImplementedError, "rank(b) != 1");
836             %(fail)s;
837         }
838         if (PyArray_NDIM(%(_data)s) != 1) {
839             PyErr_SetString(PyExc_NotImplementedError, "rank(data) != 1");
840             %(fail)s;
841         }
842         if (PyArray_NDIM(%(_indices)s) != 1) {
843             PyErr_SetString(PyExc_NotImplementedError, "rank(indices) != 1");
844             %(fail)s;
845         }
846         if (PyArray_NDIM(%(_indptr)s) != 1) {
847             PyErr_SetString(PyExc_NotImplementedError, "rank(indptr) != 1");
848             %(fail)s;
849         }
850         if( PyArray_TYPE(%(_indices)s) != NPY_INT32) {
851         PyErr_SetString(PyExc_NotImplementedError, "C"); %(fail)s;}
852         if( PyArray_TYPE(%(_indptr)s) != NPY_INT32)
853         {PyErr_SetString(PyExc_NotImplementedError, "D"); %(fail)s;}
854         if (!%(_zout)s
855             || (PyArray_DIMS(%(_zout)s)[0] != PyArray_DIMS(%(_indices)s)[0])
856             || !(PyArray_ISCONTIGUOUS(%(_zout)s)))
857         {
858             Py_XDECREF(%(_zout)s);
859             %(_zout)s = (PyArrayObject*) PyArray_SimpleNew(1,
860                     PyArray_DIMS(%(_indices)s), PyArray_TYPE(%(_b)s));
861             if (!%(_zout)s)
862             {
863                 PyErr_SetString(PyExc_MemoryError,
864                     "Could not allocate output memory.");
865                 %(fail)s;
866             }
867         }
868         { //makes it compile even though labels jump over variable definitions.
869             const npy_intp nnz = PyArray_DIMS(%(_indices)s)[0];
870             //TODO: error checking with this
871             const npy_intp N =  PyArray_DIMS(%(_indptr)s)[0]-1;
872             const dtype_%(_data)s * const __restrict__ data = (dtype_%(_data)s*)PyArray_DATA(%(_data)s);
873             const npy_int32 * const __restrict__ indptr = (npy_int32 *)PyArray_DATA(%(_indptr)s);
874             const npy_int32 * const __restrict__ indices = (npy_int32 *)PyArray_DATA(%(_indices)s);
875             const dtype_%(_b)s* __restrict__ Db = (dtype_%(_b)s*)PyArray_DATA(%(_b)s);
876             dtype_%(_zout)s * const __restrict__ zout = (dtype_%(_zout)s*)PyArray_DATA(%(_zout)s);
877             const npy_intp Sb = PyArray_STRIDES(%(_b)s)[0] / PyArray_DESCR(%(_b)s)-&gt;elsize;
878             // loop over columns
879             for (npy_intp j = 0; j &lt; N; ++j)
880             {
881                 // for each non-null value in the sparse column
882                 for (npy_int32 i_idx = indptr[j]; i_idx &lt; indptr[j+1]; ++i_idx)
883                 {
884                     // extract row index of non-null value
885                     npy_int32 i = indices[i_idx];
886                     // write resulting gradient to sparse output
887                     zout[i_idx] = data[i_idx] + Db[i * Sb];
888                 }
889             }
890         }
891     Operand optimized for calculating the dot product dot(`x`, `y`.T) = `z`
892     when you only want to calculate a subset of `z`.
893     It is equivalent to `p` o (`x` . `y`.T) where o is the element-wise
894     product, `x` and `y` operands of the dot product and `p` is a matrix
895     that contains 1 when the corresponding element of `z` should be
896     calculated and 0 when it shouldn't. Note that SamplingDot has a different
897     interface than `dot` because SamplingDot requires `x` to be a `m`x`k`
898     matrix while `y` is a `n`x`k` matrix instead of the usual `k`x`n` matrix.
899     Parameters
900     ----------
901     x
902         Tensor matrix.
903     y
904         Tensor matrix.
905     p_data
906         Sparse matrix data.
907     p_ind
908         Sparse matrix indices.
909     p_ptr
910         Sparse matric indptr.
911     p_ncols
912         Sparse matrix number of columns.
913     Returns
914     -------
915     A dense matrix containing the dot product of `x` by `y`.T only
916     where `p` is 1.
917     Notes
918     -----
919     It will work if the pattern is not binary value, but if the
920     pattern doesn't have a high sparsity proportion it will be slower
921     then a more optimized dot followed by a normal elemwise
922     multiplication.
923     If we have the input of mixed dtype, we insert cast elemwise
924     in the graph to be able to call blas function as they don't
925     allow mixed dtype.
926     This op is used as an optimization for SamplingDot.
927     <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, x, y, p_data, p_ind, p_ptr, p_ncols):
928         x = tensor.as_tensor_variable(x)
929         y = tensor.as_tensor_variable(y)
930         p_data = tensor.as_tensor_variable(p_data)
931         p_ind = tensor.as_tensor_variable(p_ind)
932         p_ptr = tensor.as_tensor_variable(p_ptr)
933         p_ncols = tensor.as_tensor_variable(</b></font>p_ncols)
934         assert p_ncols.dtype == 'int32'
935         dtype_out = scalar.upcast(x.type.dtype, y.type.dtype,
936                                   p_data.type.dtype)
937         dot_out = scalar.upcast(x.type.dtype, y.type.dtype)
938         x = tensor.cast(x, dot_out)
939         y = tensor.cast(y, dot_out)
940         return gof.Apply(self, [x, y, p_data, p_ind, p_ptr, p_ncols], [
941             tensor.tensor(dtype=dtype_out, broadcastable=(False,)),
942             tensor.tensor(dtype=p_ind.type.dtype, broadcastable=(False,)),
943             tensor.tensor(dtype=p_ptr.type.dtype, broadcastable=(False,))
944 <a name="2"></a>        ])
945     def c_code_cache_version(self):
946         return (4, blas<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.blas_header_version())
947     def c_support_code(self):
948         return blas.blas_header_text()
949     def c_libraries(self):
950         return blas.ldflags()
951     def c_compile_args(self):
952         return blas.ldflags(</b></font>libs=False, flags=True)
953     def c_lib_dirs(self):
954         return blas.ldflags(libs=False, libs_dir=True)
955     def c_header_dirs(self):
956         return blas.ldflags(libs=False, include_dir=True)
957     def c_code(self, node, name, inputs, outputs, sub):
958         x, y, p_data, p_ind, p_ptr, p_ncols = inputs
959         z_data, z_ind, z_ptr = outputs
960         if node.inputs[0].type.dtype in ('complex64', 'complex128'):
961             raise NotImplementedError('Complex types are not supported for x')
962         if node.inputs[1].type.dtype in ('complex64', 'complex128'):
963             raise NotImplementedError('Complex types are not supported for y')
964         if node.inputs[2].type.dtype in ('complex64', 'complex128'):
965             raise NotImplementedError(
966                 'Complex types are not supported for pattern')
967         dot_out = scalar.upcast(node.inputs[0].type.dtype,
968                                 node.inputs[1].type.dtype)
969         if dot_out == "float32":
970             conv_type = "float"
971             cdot = "sdot_"
972         else:
973             conv_type = "double"
974             cdot = "ddot_"
975         typenum_x = node.inputs[0].type.dtype_specs()[2]
976         typenum_y = node.inputs[1].type.dtype_specs()[2]
977         typenum_p = node.inputs[2].type.dtype_specs()[2]
978         typenum_zd = tensor.TensorType(node.outputs[0].dtype,
979                                        []).dtype_specs()[2]
980         typenum_zi = tensor.TensorType(node.outputs[1].dtype,
981                                        []).dtype_specs()[2]
982         typenum_zp = tensor.TensorType(node.outputs[2].dtype,
983                                        []).dtype_specs()[2]
984         rval = """
985         if (PyArray_NDIM(%(x)s) != 2) {
986 PyErr_SetString(PyExc_NotImplementedError, "rank(x) != 2"); %(fail)s;}
987         if (PyArray_NDIM(%(y)s) != 2) {
988 PyErr_SetString(PyExc_NotImplementedError, "rank(y) != 2"); %(fail)s;}
989         if (PyArray_TYPE(%(x)s) != %(typenum_x)s) {
990             PyErr_SetString(PyExc_NotImplementedError,
991                             "Invalid type for x");
992             %(fail)s;}
993         if (PyArray_TYPE(%(y)s) != %(typenum_y)s) {
994             PyErr_SetString(PyExc_NotImplementedError,
995                             "Invalid type for y");
996             %(fail)s;}
997         if (PyArray_TYPE(%(p_data)s) != %(typenum_p)s) {
998             PyErr_SetString(PyExc_NotImplementedError,
999                             "Invalid type for pattern");
1000             %(fail)s;}
1001         if (PyArray_DIMS(%(x)s)[1] != PyArray_DIMS(%(y)s)[1]) {
1002             PyErr_SetString(PyExc_NotImplementedError,
1003               "x's number of columns doesn't match y's rows! Note: sampling_dot is different from dot because y is assumed to be transposed.");
1004             %(fail)s;}
1005         if (PyArray_DIMS(%(y)s)[0] != ((npy_int32 *)PyArray_DATA(%(p_ncols)s))[0] ||
1006             PyArray_DIMS(%(x)s)[0] != (PyArray_DIMS(%(p_ptr)s)[0] - 1))
1007         {PyErr_SetString(PyExc_NotImplementedError,
1008         "The dimension of the pattern and the output must match"); %(fail)s;}
1009         // Allocate output
1010         if (!%(z_data)s
1011             || (PyArray_DIMS(%(z_data)s)[0] != PyArray_DIMS(%(p_data)s)[0])
1012             || (PyArray_TYPE(%(z_data)s) != %(typenum_zd)s)
1013             || !(PyArray_ISCONTIGUOUS(%(z_data)s)))
1014          {
1015             {Py_XDECREF(%(z_data)s);}
1016             npy_intp dims[] = {0};
1017             dims[0] = PyArray_DIMS(%(p_data)s)[0];
1018             %(z_data)s = (PyArrayObject*) PyArray_SimpleNew(1, dims,
1019                                                             %(typenum_zd)s);
1020         }
1021         if (!%(z_ind)s
1022             || (PyArray_DIMS(%(z_ind)s)[0] != PyArray_DIMS(%(p_ind)s)[0])
1023             || (PyArray_TYPE(%(z_ind)s) != %(typenum_zi)s)
1024             || !(PyArray_ISCONTIGUOUS(%(z_ind)s)))
1025         {
1026             {Py_XDECREF(%(z_ind)s);}
1027             npy_intp dims[] = {0};
1028             dims[0] = PyArray_DIMS(%(p_ind)s)[0];
1029             %(z_ind)s = (PyArrayObject*) PyArray_SimpleNew(1, dims,
1030                                                            %(typenum_zi)s);
1031         }
1032         if (!%(z_ptr)s
1033             || (PyArray_DIMS(%(z_ptr)s)[0] != PyArray_DIMS(%(p_ptr)s)[0])
1034             || (PyArray_TYPE(%(z_ptr)s) != %(typenum_zp)s)
1035             || !(PyArray_ISCONTIGUOUS(%(z_ptr)s)))
1036         {
1037             {Py_XDECREF(%(z_ptr)s);}
1038             npy_intp dims[] = {0};
1039             dims[0] = PyArray_DIMS(%(p_ptr)s)[0];
1040             %(z_ptr)s = (PyArrayObject*) PyArray_SimpleNew(1, dims,
1041                                                            %(typenum_zp)s);
1042         }
1043         {
1044             // Product of MxK and NxK, output MxN
1045             npy_intp M = PyArray_DIMS(%(x)s)[0];
1046             npy_intp N = PyArray_DIMS(%(y)s)[0];
1047             npy_intp K = PyArray_DIMS(%(y)s)[1];
1048             // pointers to access actual data in the arrays passed as params.
1049             const dtype_%(x)s* __restrict__ Dx = (dtype_%(x)s*)PyArray_DATA(%(x)s);
1050             const dtype_%(y)s* __restrict__ Dy = (dtype_%(y)s*)PyArray_DATA(%(y)s);
1051             const dtype_%(p_data)s* __restrict__ Dpd = (dtype_%(p_data)s*)PyArray_DATA(%(p_data)s);
1052             const dtype_%(p_ind)s* __restrict__ Dpi = (dtype_%(p_ind)s*)PyArray_DATA(%(p_ind)s);
1053             const dtype_%(p_ptr)s* __restrict__ Dpp = (dtype_%(p_ptr)s*)PyArray_DATA(%(p_ptr)s);
1054             dtype_%(z_data)s* __restrict__ Dzd = (dtype_%(z_data)s*)PyArray_DATA(%(z_data)s);
1055             dtype_%(z_ind)s* __restrict__ Dzi = (dtype_%(z_ind)s*)PyArray_DATA(%(z_ind)s);
1056             dtype_%(z_ptr)s* __restrict__ Dzp = (dtype_%(z_ptr)s*)PyArray_DATA(%(z_ptr)s);
1057             const npy_intp Sdx = PyArray_STRIDES(%(x)s)[1]/PyArray_DESCR(%(x)s)-&gt;elsize;
1058             const npy_intp Sdy = PyArray_STRIDES(%(y)s)[1]/PyArray_DESCR(%(y)s)-&gt;elsize;
1059             const npy_intp Sdpd = PyArray_STRIDES(%(p_data)s)[0] / PyArray_DESCR(%(p_data)s)-&gt;elsize;
1060             const npy_intp Sdpi = PyArray_STRIDES(%(p_ind)s)[0] / PyArray_DESCR(%(p_ind)s)-&gt;elsize;
1061             const npy_intp Sdpp = PyArray_STRIDES(%(p_ptr)s)[0] / PyArray_DESCR(%(p_ptr)s)-&gt;elsize;
1062             const npy_intp Sdzd = PyArray_STRIDES(%(z_data)s)[0] / PyArray_DESCR(%(z_data)s)-&gt;elsize;
1063             const npy_intp Sdzi = PyArray_STRIDES(%(z_ind)s)[0] / PyArray_DESCR(%(z_ind)s)-&gt;elsize;
1064             const npy_intp Sdzp = PyArray_STRIDES(%(z_ptr)s)[0] / PyArray_DESCR(%(z_ptr)s)-&gt;elsize;
1065             memcpy(Dzi, Dpi, PyArray_DIMS(%(p_ind)s)[0]*sizeof(dtype_%(p_ind)s));
1066             memcpy(Dzp, Dpp, PyArray_DIMS(%(p_ptr)s)[0]*sizeof(dtype_%(p_ptr)s));
1067             // blas expects ints; convert here (rather than just making K etc ints) to avoid potential overflow in the negative-stride correction
1068             if ((K &gt; 0x7fffffffL)||(Sdx &gt; 0x7fffffffL)||(Sdy &gt; 0x7fffffffL)||(Sdx &lt; -0x7fffffffL)||(Sdy &lt; -0x7fffffffL))
1069             {PyErr_SetString(PyExc_NotImplementedError, "array too big for BLAS (overflows int32 index)"); %(fail)s;}
1070             int K32 = K;
1071             int Sdx32 = Sdx;
1072             int Sdy32 = Sdy;
1073             for (npy_intp m = 0; m &lt; M; ++m) {
1074                 for (npy_int32 n_idx = Dpp[m * Sdpp]; n_idx &lt; Dpp[(m+1)*Sdpp]; ++n_idx) {
1075                     const npy_int32 n = Dpi[n_idx * Sdpi]; // row index of non-null value for column K
1076                     const dtype_%(x)s* x_row = (dtype_%(x)s*)(PyArray_BYTES(%(x)s) + PyArray_STRIDES(%(x)s)[0] * m);
1077                     const dtype_%(y)s* y_col = (dtype_%(y)s*)(PyArray_BYTES(%(y)s) + PyArray_STRIDES(%(y)s)[0] * n);
1078                     // dot expects pointer to the beginning of memory arrays,
1079                     // so when the stride is negative, we need to get the
1080                     // last element
1081                     if (Sdx &lt; 0)
1082                         x_row += (K - 1) * Sdx;
1083                     if (Sdy &lt; 0)
1084                         y_col += (K - 1) * Sdy;
1085                     Dzd[n_idx * Sdzd] = Dpd[n_idx * Sdpd] * %(cdot)s(&amp;K32, (const %(conv_type)s*)x_row, &amp;Sdx32, (const %(conv_type)s*)y_col, &amp;Sdy32);
1086                 }
1087             }
1088         }
1089     This test suite is supposed to establish that gemm works as it is supposed to.
1090         assert tuple<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(z_val.shape) == (self.M, self.K)
1091         ref_val = self.compute_ref(*(values + (transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)))
1092         unittest_tools.assert_allclose(</b></font>ref_val, z_val)
1093     def test_gemm(self):
1094         dtypes = ('float32', 'float64')
1095         scalars = (0, 1, -2)
1096         booleans = (False, True)
1097         iterables = [dtypes] + ([scalars] * 2) + ([booleans] * 6)
1098         for dtype, alpha, beta, tA, tB, tC, sA, sB, sC in product(*iterables):
1099             yield (self.run_gemm, dtype, alpha, beta, tA, tB, tC, sA, sB, sC)
1100 def test_res_is_a():
1101     X, Y, Z, a, b = XYZab()
1102     assert not res_is_a(a, T.sqrt)
1103     assert not res_is_a(a + a, T.sqrt)
1104     assert res_is_a(T.sqrt(a + a), T.sqrt)
1105 class t_as_scalar(TestCase):
1106     def test0(self):
1107         a = T.constant(2.5)
1108         b = T.constant(np.asarray([[[0.5]]]))
1109         b2 = b.dimshuffle()
1110         assert b2.ndim == 0
1111         d_a = T.DimShuffle([], [])(a)
1112         d_b = T.DimShuffle([True, True, True], [0, 2, 1])(b)
1113         d_a2 = T.DimShuffle([], ['x', 'x', 'x'])(a)
1114         self.assertTrue(_as_scalar(a) == a)
1115         self.assertTrue(_as_scalar(b) != b)
1116         self.assertTrue(_as_scalar(d_a) != d_a)
1117         self.assertTrue(_as_scalar(d_b) != d_b)
1118         self.assertTrue(_as_scalar(d_a2) != d_a2)
1119     def test1(self):
1120         a = T.constant(np.ones(5))
1121         self.assertTrue(_as_scalar(a) is None)
1122         self.assertTrue(_as_scalar(T.DimShuffle([False], [0, 'x'])(a)) is None)
1123     def test2(self):
1124         a = T.dscalar()
1125         d_a = T.DimShuffle([], [])(a)
1126         d_a2 = T.DimShuffle([], ['x', 'x'])(a)
1127         self.assertTrue(_as_scalar(a) is a)
1128         self.assertTrue(_as_scalar(d_a) is a)
1129         self.assertTrue(_as_scalar(d_a2) is a)
1130     def test3(self):
1131         a = T.matrix()
1132         self.assertTrue(_as_scalar(a) is None)
1133         self.assertTrue(_as_scalar(T.DimShuffle([False, False],
1134                                                 [0, 'x', 1])(a)) is None)
1135 class T_real_matrix(TestCase):
1136     def test0(self):
1137         self.assertTrue(_is_real_matrix(T.DimShuffle([False, False],
1138                                                      [1, 0])(T.matrix())))
1139         self.assertTrue(not _is_real_matrix(T.DimShuffle([False],
1140                                                          ['x', 0])
1141                                             (T.dvector())))
1142 def fail(msg):
1143     print('FAIL', msg)
1144     assert False
1145 def XYZab():
1146     return T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
1147 class Failure(Exception):
1148     pass
1149 def just_gemm(i, o, ishapes=[(4, 3), (3, 5), (4, 5), (), ()],
1150               max_graphlen=0, expected_nb_gemm=1):
1151     try:
1152         f = inplace_func(
1153             [In(ii, mutable=True, allow_downcast=True) for ii in i],
1154             o,
1155             mode='FAST_RUN',
1156             on_unused_input='ignore')
1157         nb_gemm = 0
1158         for node in f.maker.fgraph.apply_nodes:
1159             if isinstance(node.op, T.Dot):
1160                 raise Failure('dot not changed to gemm_inplace in graph')
1161             if node.op == _dot22:
1162                 raise Failure('_dot22 not changed to gemm_inplace in graph')
1163             if node.op == gemm_inplace:
1164                 nb_gemm += 1
1165         assert nb_gemm == expected_nb_gemm, (nb_gemm, expected_nb_gemm)
1166         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
1167                          allow_input_downcast=True, on_unused_input='ignore')
1168         for node in g.maker.fgraph.apply_nodes:
1169             if node.op == gemm_inplace:
1170                 raise Exception('gemm_inplace in original graph')
1171         graphlen = len(f.maker.fgraph.toposort())
1172         if max_graphlen and (graphlen &lt;= max_graphlen):
1173             assert False, 'graphlen=%i&gt;%i' % (graphlen, max_graphlen)
1174         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
1175         r0 = f(*[np.asarray(rng.randn(*sh), config.floatX)
1176                  for sh in ishapes])
1177         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
1178         r1 = g(*[np.asarray(rng.randn(*sh), config.floatX)
1179                  for sh in ishapes])
1180         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
1181         eps = 1.0e-8
1182         if config.floatX == 'float32':
1183             eps = 1.0e-6
1184         if max_abs_err &gt; eps:
1185             raise Failure('GEMM is computing the wrong output. max_rel_err =',
1186                           max_abs_err)
1187     except Failure:
1188         for node in f.maker.fgraph.toposort():
1189             print('GRAPH', node)
1190         raise
1191 @unittest_tools.assertFailure_fast
1192 def test_gemm_opt0():
1193     X, Y, Z, a, b = XYZab()
1194     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a + Z * b])
1195     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) + b * Z])
1196     just_gemm([X, Y, Z, a, b], [b * Z + a * T.dot(X, Y)])
1197     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a - Z * b])
1198     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) - b * Z])
1199     just_gemm([X, Y, Z, a, b], [b * Z - a * T.dot(X, Y)])
1200     just_gemm([X, Y, Z, a, b], [b * Z.T - a * T.dot(Y.T, X.T)])
1201     just_gemm([X, Y, Z, a, b], [b * Z.T + a * b * T.dot(X, Y).T])
1202     just_gemm([<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>X, Y, Z, a, b], [b * Z + a * T.dot(X, Y).T],
1203               ishapes=[(5, 3), (3, 4), (4, 5), (), ()])
1204     just_gemm([X</b></font>, Y, Z, a, b], [(b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
1205     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y)])
1206     just_gemm([X, Y, Z, a, b], [Z * b + T.dot(X, Y)])
1207     just_gemm([X, Y, Z, a, b], [Z + a * b * a * T.dot(X, Y)])
1208     just_gemm([X, Y, Z, a, b], [(b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
1209     just_gemm([X, Y, Z, a, b], [Z - T.dot(X, Y)])
1210     just_gemm([X, Y, Z, a, b], [Z * b - T.dot(X, Y)])
1211     just_gemm([X, Y, Z, a, b], [Z - a * b * a * T.dot(X, Y)])
1212 @unittest_tools.assertFailure_fast
1213 def test_gemm_opt_double_gemm():
1214     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
1215     R, S, c = T.matrix(), T.matrix(), T.scalar()
1216     just_gemm([X, Y, Z, a, b, R, S, c],
1217               [Z * c + a * T.dot(X, Y) + b * T.dot(R, S).T],
1218               ishapes=[(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()],
1219               expected_nb_gemm=2)
1220     ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()]
1221     i = [X, Y, Z, a, b, R, S, c]
1222     o = [(a * T.dot(X, Y) +
1223          gemm_inplace(Z, b, S.T, R.T, T.constant(1.0).astype(config.floatX)))]
1224     try:
1225         f = inplace_func([In(ii, mutable=True) for ii in i], o,
1226                          mode='FAST_RUN', on_unused_input='ignore')
1227         for node in f.maker.fgraph.apply_nodes:
1228             if isinstance(node.op, T.Dot):
1229                 raise Failure('dot in graph')
1230             if node.op == _dot22:
1231                 raise Failure('_dot22 in graph')
1232         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
1233                          on_unused_input='ignore')
1234         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
1235         r0 = f(*[np.asarray(rng.randn(*sh), config.floatX)
1236                  for sh in ishapes])
1237         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
1238         r1 = g(*[np.asarray(rng.randn(*sh), config.floatX)
1239                  for sh in ishapes])
1240         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
1241         eps = 1.0e-8
1242         if config.floatX == 'float32':
1243             eps = 1.0e-6
1244         if max_abs_err &gt; eps:
1245             raise Failure(
1246                 'GEMM is computing the wrong output. max_rel_err =',
1247                 max_abs_err)
1248     except Failure:
1249         for node in f.maker.fgraph.toposort():
1250             print('GRAPH', node)
1251         raise
1252 def test_gemm_canonicalize():
1253     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
1254         'a'), T.scalar('b')
1255     c, d = T.scalar('c'), T.scalar('d')
1256     u = T.row('u')
1257     v = T.vector('v')
1258     w = T.col('w')
1259     can = []
1260     _gemm_canonicalize(X + Y + Z, 1.0, can, 0)
1261     assert can == [(1.0, X), (1.0, Y), (1.0, Z)]
1262     can = []
1263     _gemm_canonicalize(X + Y + u, 1.0, can, 0)
1264     assert can == [(1.0, X), (1.0, Y), (1.0, u)], can
1265     can = []
1266     _gemm_canonicalize(X + Y + v, 1.0, can, 0)
1267     assert can[:2] == [(1.0, X), (1.0, Y)]
1268     assert isinstance(can[2], tuple)
1269     assert len(can[2]) == 2
1270     assert can[2][0] == 1.0
1271     assert can[2][1].owner
1272     assert isinstance(can[2][1].owner.op, T.DimShuffle)
1273     assert can[2][1].owner.inputs == [v]
1274     can = []
1275     _gemm_canonicalize(X + Y + w, 1.0, can, 0)
1276     assert can == [(1.0, X), (1.0, Y), (1.0, w)], can
1277     can = []
1278     _gemm_canonicalize(a * X + Y - b * Z * c, 1.0, can, 0)
1279     assert can[0] == (a, X)
1280     assert can[1] == (1.0, Y)
1281     assert can[2][0].owner.op == T.mul
1282     assert can[2][0].owner.inputs[0].owner.op == T.neg
1283     assert can[2][0].owner.inputs[0].owner.inputs[0] == c
1284     assert can[2][0].owner.inputs[1] == b
1285     can = []
1286     _gemm_canonicalize((-d) * X - (a * X + Y - b * Z * c), 1.0, can, 0)
1287     assert can[0][0].owner.op == T.neg
1288     assert can[0][0].owner.inputs[0] == d
1289     assert can[0][1] == X
1290     assert can[1][0].owner.op == T.neg
1291     assert can[1][0].owner.inputs[0] == a
1292     assert can[2] == (-1.0, Y)
1293     assert can[3][0].owner.op == T.mul
1294     assert can[3][0].owner.inputs == [c, b]
1295 def test_gemm_factor():
1296     X, Y = T.matrix('X'), T.matrix('Y')
1297     assert [(1.0, X), (1.0, Y)] == _factor_canonicalized([(1.0, X), (1.0, Y)])
1298 <a name="1"></a>    assert [(2.0, X)] == _factor_canonicalized([(1.0, X), (1.0, X)])
1299 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def test_upcasting_scalar_nogemm():
1300     v = T.fmatrix('v')
1301     w = T.fmatrix('w')
1302     t = T.fmatrix('t')
1303     alpha = T.dscalar('a')
1304     rval = T.dot(w, v) * alpha + t
1305     f = theano.function(</b></font>[w, v, t, alpha], rval)
1306     t = f.maker.fgraph.toposort()
1307     assert np.sum([isinstance(n.op, Gemm) for n in t]) == 0
1308     v = T.fmatrix('v')
1309     w = T.fmatrix('w')
1310     t = T.fmatrix('t')
1311     alpha = T.cscalar('a')
1312     on_opt_error = config.on_opt_error
1313     try:
1314         config.on_opt_error = 'raise'
1315         rval = T.dot(w, v) * alpha + t
1316         f = theano.function([w, v, t, alpha], rval)
1317     finally:
1318         config.on_opt_error = on_opt_error
1319     t = f.maker.fgraph.toposort()
1320     assert np.sum([isinstance(n.op, Gemm) for n in t]) == 0
1321 def test_gemm_nested():
1322     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
1323         'a'), T.scalar('b')
1324     R, S, U, c, d = T.matrix('R'), T.matrix('S'), T.matrix('U'), T.scalar(
1325         'c'), T.scalar('d')
1326     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
1327               [a * Z - b * (c * T.dot(X, Y) + d * Z)],
1328               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
1329                        (2, 4), (), (), (), ()],
1330               max_graphlen=1)
1331     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
1332               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * Z)],
1333               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
1334                        (2, 4), (), (), (), ()],
1335               max_graphlen=1)
1336     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
1337               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * U)],
1338               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
1339                        (2, 4), (), (), (), ()],
1340               max_graphlen=3)
1341 def test_gemm_opt_wishlist():
1342     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
1343     just_gemm([X, Y, Z, a, b],
1344               [(b * b) * Z * a + (a * a) * T.dot(X, Y) + b * T.dot(X, Y)])
1345     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y) + T.dot(X, Y)])
1346 def test_gemm_with_vector():
1347     X, Y, Z, a, b = XYZab()
1348     v = T.vector()
1349     def my_just_gemm(o):
1350         i = [X, Y, Z, a, b, v]
1351         ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, )]
1352         just_gemm(i, o, ishapes=ishapes)
1353     my_just_gemm([v + T.dot(X, Y) * a + Z * b])
1354     my_just_gemm([v + a * T.dot(X, Y) + b * Z])
1355     my_just_gemm([v + b * Z + a * T.dot(X, Y)])
1356     my_just_gemm([v + T.dot(X, Y) * a - Z * b])
1357     my_just_gemm([v + a * T.dot(X, Y) - b * Z])
1358     my_just_gemm([v + b * Z - a * T.dot(X, Y)])
1359     my_just_gemm([v + (b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
1360     my_just_gemm([v + Z + T.dot(X, Y)])
1361     my_just_gemm([v + Z * b + T.dot(X, Y)])
1362     my_just_gemm([v + Z + a * b * a * T.dot(X, Y)])
1363     my_just_gemm([v + (b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
1364     my_just_gemm([Z - T.dot(X, Y) + v])
1365     my_just_gemm([Z * b - T.dot(X, Y) + v])
1366     my_just_gemm([Z - a * b * a * T.dot(X, Y) + v])
1367 def test_gemm_opt_vector_stuff():
1368     X, Y, a = T.matrix(), T.matrix(), T.scalar()
1369     u, v = T.vector(), T.vector()
1370     f = inplace_func([a, u, v], a + T.dot(u, v), mode='FAST_RUN')
1371     if gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]:
1372         raise Failure('gemm_inplace in graph')
1373     f = inplace_func([a, u, X, Y], a * u + T.dot(X, Y), mode='FAST_RUN')
1374     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
1375         raise Failure('gemm_inplace in graph')
1376 def test_gemm_unrolled():
1377     batch_size = 100
1378     rep_size = 40
1379     rng = np.random.RandomState([1, 2, 3])
1380     for num_rounds in range(1, 10):
1381         W = sharedX(rng.randn(rep_size, rep_size), name='W')
1382         V = sharedX(np.zeros((batch_size, rep_size)), name='V')
1383         H = sharedX(np.zeros((batch_size, rep_size)), name='H')
1384         G = sharedX(np.zeros((batch_size, rep_size)), name='G')
1385         cur_V = V
1386         cur_H = H
1387         def update_V(cur_H):
1388             return T.nnet.sigmoid(T.dot(cur_H, W.T))
1389         def update_H(cur_V):
1390             return T.nnet.sigmoid(T.dot(cur_V, W) + T.dot(G, W.T))
1391         for i in xrange(num_rounds):
1392             cur_V = update_V(cur_H)
1393             cur_H = update_H(cur_V)
1394         unrolled_theano = theano.function([], updates=[(V, cur_V), (H, cur_H)],
1395                                           name='unrolled_theano')
1396         nb_dot = sum([1 for node in unrolled_theano.maker.fgraph.toposort()
1397                       if isinstance(node.op, (theano.tensor.Dot,
1398                                               theano.tensor.blas.Dot22,
1399                                               theano.tensor.blas.Gemm))])
1400         assert nb_dot == num_rounds * 2 + 1, nb_dot
1401         unrolled_theano()
1402 def test_inplace0():
1403     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
1404         'a'), T.scalar('b')
1405     R, S, c = T.matrix('R'), T.matrix('S'), T.scalar('c')
1406     f = inplace_func([Z, b, R, S],
1407                      [Z * (Z + b * T.dot(R, S).T)], mode='FAST_RUN')
1408     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
1409         print(pp(f.maker.fgraph.outputs[0]))
1410         raise Failure('gemm_inplace in graph')
1411     assert gemm_no_inplace in [n.op for n in f.maker.fgraph.apply_nodes]
1412     f = inplace_func([X, Y, Z, a, b, R, S, c],
1413                      [Z * (c * Z + a * T.dot(X, Y) + b * T.dot(R, S).T)],
1414                      mode='FAST_RUN')
1415     if (gemm_inplace not in [n.op for n in f.maker.fgraph.apply_nodes]):
1416         theano.printing.debugprint(f)
1417         raise Failure('no gemm_inplace in graph')
1418 def test_inplace1():
1419     X, Y, Z, a, b = XYZab()
1420     f = inplace_func([X, Y, Z],
1421                      [Z + Z + T.dot(X, Y)], mode='FAST_RUN')
1422     assert [n.op for n in f.maker.fgraph.apply_nodes] == [gemm_no_inplace]
1423 def test_dot22():
1424     for dtype1 in ['float32', 'float64', 'complex64', 'complex128']:
1425         a = T.matrix(dtype=dtype1)
1426         for dtype2 in ['float32', 'float64', 'complex64', 'complex128']:
1427             b = T.matrix(dtype=dtype2)
1428             f = theano.function([a, b], T.dot(a, b), mode=mode_blas_opt)
1429             topo = f.maker.fgraph.toposort()
1430             if dtype1 == dtype2:
1431                 assert _dot22 in [x.op for x in topo], (dtype1, dtype2)
1432             else:
1433                 check = [isinstance(x.op, T.Dot) for x in topo]
1434 <a name="4"></a>                assert any(check), (dtype1, dtype2)
1435             rng = np.random.RandomState(unittest_tools.fetch_seed())
1436             <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def cmp(a_shp, b_shp):
1437                 av = rng.uniform(size=a_shp).astype(dtype1)
1438                 bv = rng.uniform(size=b_shp).astype(</b></font>dtype2)
1439                 f(av, bv)
1440             cmp((3, 4), (4, 5))
1441             cmp((0, 4), (4, 5))
1442             cmp((3, 0), (0, 5))
1443             cmp((3, 4), (4, 0))
1444             cmp((0, 4), (4, 0))
1445             cmp((0, 0), (0, 0))
1446 @attr('slow')
1447 def test_dot22scalar():
1448     rng = np.random.RandomState(unittest_tools.fetch_seed())
1449     for dtype1 in ['complex64', 'complex128']:
1450         a = T.matrix('a', dtype=dtype1)
1451         for dtype2 in ['complex64', 'complex128']:
1452             b = T.matrix('b', dtype=dtype2)
1453             for dtype3 in ['complex64', 'complex128']:
1454                 c = T.matrix('c', dtype=dtype3)
1455                 for dtype4 in ['complex64', 'complex128']:
1456                     cst = theano.tensor.basic.constant(.2, dtype=dtype4)
1457                     cst2 = theano.tensor.basic.constant(.1, dtype=dtype4)
1458                     def check_dot22scalar(func, len_topo_scalar=-1):
1459                         topo = func.maker.fgraph.toposort()
1460                         ops = [x.op for x in topo]
1461                         dtype4_upcast = theano.scalar.upcast(dtype4, dtype1,
1462                                                              dtype2)
1463                         if dtype1 == dtype2 == dtype3 == dtype4_upcast:
1464                             if len_topo_scalar &gt; 0:
1465                                 assert len(topo) == len_topo_scalar
1466                             assert _dot22scalar in ops, (dtype1, dtype2,
1467                                                          dtype3, dtype4)
1468                         elif dtype1 == dtype2 == dtype4_upcast:
1469                             if not (len_topo_scalar &gt; 0):
1470                                 assert len(topo) == len_topo_scalar
1471                                 assert _dot22scalar in ops, (dtype1, dtype2,
1472                                                              dtype3, dtype4)
1473                             else:
1474                                 assert _dot22scalar in ops or _dot22 in ops, (
1475                                     dtype1, dtype2, dtype3, dtype4)
1476                         elif dtype1 == dtype2:
1477                             assert _dot22 in ops, (dtype1, dtype2,
1478                                                    dtype3, dtype4)
1479                         else:
1480                             check = [isinstance(o, T.Dot) for o in ops]
1481                             assert any(check), (dtype1, dtype2, dtype3, dtype4)
1482                     def cmp(a_shp, b_shp, c_shp, sqr_shp=(5, 5)):
1483                         av = rng.uniform(size=a_shp).astype(dtype1)
1484                         bv = rng.uniform(size=b_shp).astype(dtype2)
1485                         cv = rng.uniform(size=c_shp).astype(dtype3)
1486                         sv = rng.uniform(size=sqr_shp).astype(dtype1)
1487                         if False:
1488                             f = theano.function([a, b], cst * T.dot(a, b),
1489                                                 mode=mode_blas_opt)
1490                             f.maker.fgraph.toposort()
1491                             check_dot22scalar(f, 1)
1492                             f(av, bv)
1493                         if True:
1494                             f = theano.function([a, b, c],
1495                                                 cst * c * T.dot(a, b),
1496                                                 mode=mode_blas_opt)
1497                             f.maker.fgraph.toposort()
1498                             check_dot22scalar(f, 2)
1499                             f(av, bv, cv)
1500                         f = theano.function([a, b, c],
1501                                             c * cst * T.dot(a, b),
1502                                             mode=mode_blas_opt)
1503                         f.maker.fgraph.toposort()
1504                         check_dot22scalar(f, 2)
1505                         f(av, bv, cv)
1506                         m2 = mode_blas_opt.including('canonicalize')
1507                         f = theano.function([a, b, c],
1508                                             cst2 * c * cst * T.dot(a, b),
1509                                             mode=m2)
1510                         f.maker.fgraph.toposort()
1511                         check_dot22scalar(f, 2)
1512                         f(av, bv, cv)
1513                         if dtype1 == dtype2 == dtype3:
1514                             f = theano.function([a, b, c],
1515                                                 c * cst * a * T.dot(a, b),
1516                                                 mode=m2)
1517                             f.maker.fgraph.toposort()
1518                             check_dot22scalar(f, 2)
1519                             f(sv, sv, sv)
1520                             f = theano.function([a, b, c],
1521                                                 cst * c * a * T.dot(a, b),
1522                                                 mode=mode_blas_opt)
1523                             f.maker.fgraph.toposort()
1524                             f(sv, sv, sv)
1525                             f = theano.function([a, b, c],
1526                                                 c * a * cst * T.dot(a, b),
1527                                                 mode=m2)
1528                             f.maker.fgraph.toposort()
1529                             check_dot22scalar(f, 2)
1530                             f(sv, sv, sv)
1531                     cmp((3, 4), (4, 5), (3, 5))
1532                     cmp((0, 4), (4, 5), (0, 5))
1533                     cmp((3, 0), (0, 5), (3, 5))
1534                     cmp((3, 4), (4, 0), (3, 0), (0, 0))
1535                     cmp((0, 4), (4, 0), (0, 0))
1536                     cmp((0, 0), (0, 0), (0, 0))
1537 def test_dot22scalar_cast():
1538     A = T.dmatrix()
1539     for scalar_int_type in T.int_dtypes:
1540         y = T.scalar(dtype=scalar_int_type)
1541         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
1542         assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
1543     A = T.fmatrix()
1544     for scalar_int_type in T.int_dtypes:
1545         y = T.scalar(dtype=scalar_int_type)
1546         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
1547         if scalar_int_type in ['int32', 'int64']:
1548             assert _dot22 in [x.op for x in f.maker.fgraph.toposort()]
1549         else:
1550             assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
1551 def test_local_dot22_to_dot22scalar():
1552     A = T.dmatrix()
1553     mode = theano.compile.mode.get_default_mode()
1554     opt = theano.tensor.opt.in2out(
1555         theano.tensor.blas.local_dot22_to_dot22scalar)
1556     mode = mode.__class__(optimizer=opt)
1557     x = T.dscalar()
1558     y = T.dscalar()
1559     z = T.dscalar()
1560     m = T.dmatrix()
1561     r = T.drow()
1562     for idx, node in enumerate([
1563         T.mul(_dot22(A, A), x),
1564         T.mul(_dot22(A, A), x, y),
1565         T.mul(_dot22(A, A), x, r),
1566         T.mul(_dot22(A, A), m, x),
1567         T.mul(_dot22(A, A), x, m),
1568         T.mul(_dot22(A, A), x, (m * y)),
1569         T.mul(_dot22(A, A), (m * y), x),
1570         T.mul(_dot22(A, A), x, (r * y)),
1571         T.mul(_dot22(A, A), (r * y), x),
1572         T.mul(_dot22(A, A), (x * y), (m * x)),
1573         T.mul(_dot22(A, A), (r * y), (y * x)),
1574         T.mul(_dot22(A, A), (m * y), m),
1575         T.mul(_dot22(A, A), m, (m * y)),
1576         T.mul(_dot22(A, A), (r * y), (m * x)),
1577         T.mul(_dot22(A, A), (m * y * z), m),
1578         T.mul(_dot22(A, A), m, (m * y * z)),
1579         T.mul(_dot22(A, A), T.mul(m, y, z), m),
1580         T.mul(_dot22(A, A), m, T.mul(m, y, z)),
1581         T.mul(_dot22(A, A), (r * m), (m * x)),
1582     ]):
1583         node2 = theano.tensor.blas.local_dot22_to_dot22scalar.transform(
1584             node.owner)
1585         assert node2
1586         f = theano.function([x, y, z, m, r, A], node,
1587                             mode=mode, on_unused_input='ignore')
1588         f(.1, .2, .3, [[1, 2], [3, 4]], [[5, 6]], [[7, 8], [9, 10]])
1589 def test_dot_w_self():
1590     A = shared(value=np.ones((2, 2)))
1591     B = T.matrix()
1592     p = T.dot(A, A) * B
1593     grad = T.grad(T.mean(p), A)
1594     f = theano.function([B], p, updates=[(A, A - grad)])
1595     f(np.asarray([[0, 1], [2, 3]], dtype=config.floatX))
1596 class TestGemv(TestCase, unittest_tools.TestOptimizationMixin):
1597     def test_dot_vv(self):
1598         rng = np.random.RandomState(unittest_tools.fetch_seed())
1599         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
1600         w = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
1601         f = theano.function([], theano.dot(v, w), mode=mode_blas_opt)
1602         self.assertFunctionContains0(f, T.dot)
1603         self.assertFunctionContains1(f, Gemv(True))
1604         assert np.allclose(f(), np.dot(v.get_value(), w.get_value()))
1605     def test_dot_vm(self):
1606         rng = np.random.RandomState(unittest_tools.fetch_seed())
1607         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
1608         m = theano.shared(np.array(rng.uniform(size=(2, 3)), dtype='float32'))
1609         f = theano.function([], theano.dot(v, m), mode=mode_blas_opt)
1610         self.assertFunctionContains0(f, T.dot)
1611         self.assertFunctionContains1(f, Gemv(True))
1612         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
1613         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
1614         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
1615     def test_dot_mv(self):
1616         rng = np.random.RandomState(unittest_tools.fetch_seed())
1617         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
1618         m = theano.shared(np.array(rng.uniform(size=(3, 2)), dtype='float32'))
1619         f = theano.function([], theano.dot(m, v), mode=mode_blas_opt)
1620         self.assertFunctionContains0(f, T.dot)
1621         self.assertFunctionContains1(f, Gemv(True))
1622         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
1623         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
1624         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
1625     @staticmethod
1626     def t_gemv1(m_shp):
1627         rng = np.random.RandomState(unittest_tools.fetch_seed())
1628         v1 = theano.shared(np.array(rng.uniform(size=(m_shp[1],)),
1629                            dtype='float32'))
1630         v2_orig = np.array(rng.uniform(size=(m_shp[0],)), dtype='float32')
1631         v2 = theano.shared(v2_orig)
1632         m = theano.shared(np.array(rng.uniform(size=m_shp), dtype='float32'))
1633         f = theano.function([], v2 + theano.dot(m, v1), mode=mode_blas_opt)
1634         assert np.allclose(f(), np.dot(m.get_value(), v1.get_value()) + v2_orig)
1635         topo = f.maker.fgraph.toposort()
1636         assert len(topo) == 1
1637         assert isinstance(topo[0].op, Gemv)
1638         assert topo[0].op.inplace is False
1639         g = theano.function([], [], updates=[(v2, v2 + theano.dot(m, v1))],
1640                             mode=mode_blas_opt)
1641         g()
1642         assert np.allclose(v2.get_value(), np.dot(m.get_value(),
1643                            v1.get_value()) + v2_orig)
1644         topo = g.maker.fgraph.toposort()
1645         assert len(topo) == 1
1646         assert isinstance(topo[0].op, Gemv)
1647         if config.mode != 'FAST_COMPILE':
1648             assert topo[0].op.inplace is True
1649         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
1650                     borrow=True)
1651         v2.set_value(v2_orig)
1652         assert np.allclose(f(),
1653                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
1654         g()
1655         assert np.allclose(v2.get_value(),
1656                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
1657     @attr('slow')
1658     def test_gemv1(self):
1659         self.t_gemv1((3, 2))
1660         self.t_gemv1((0, 2))
1661         self.t_gemv1((3, 0))
1662         self.t_gemv1((0, 0))
1663     def test_gemv2(self):
1664         rng = np.random.RandomState(unittest_tools.fetch_seed())
1665         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
1666                            dtype='float32'))
1667         v2_orig = np.array(rng.uniform(size=(3,)), dtype='float32')
1668         v2 = theano.shared(v2_orig)
1669         m = theano.shared(np.array(rng.uniform(size=(2, 3)),
1670                           dtype='float32'))
1671         f = theano.function([], v2 + theano.dot(v1, m), mode=mode_blas_opt)
1672         assert np.allclose(f(),
1673                            np.dot(v1.get_value(), m.get_value()) +
1674                            v2.get_value())
1675         topo = f.maker.fgraph.toposort()
1676         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
1677         assert topo[-1].op.inplace is False
1678         g = theano.function([], [], updates=[(v2, v2 + theano.dot(v1, m))],
1679                             mode=mode_blas_opt)
1680         g()
1681         assert np.allclose(v2.get_value(),
1682                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
1683         topo = g.maker.fgraph.toposort()
1684         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
1685         if config.mode != 'FAST_COMPILE':
1686             assert topo[-1].op.inplace is True
1687         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
1688                     borrow=True)
1689         v2.set_value(v2_orig)
1690         assert np.allclose(f(),
1691                            np.dot(v1.get_value(), m.get_value()) +
1692                            v2.get_value())
1693         g()
1694         assert np.allclose(v2.get_value(),
1695                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
1696     def test_gemv_broadcast(self):
1697         rng = np.random.RandomState(unittest_tools.fetch_seed())
1698         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
1699                                     dtype='float32'))
1700         v2_orig = np.array(rng.uniform(size=(1,)), dtype='float32')
1701         v2 = theano.shared(v2_orig)
1702         m = theano.shared(np.array(rng.uniform(size=(1, 2)),
1703                                    dtype='float32'),
1704                           broadcastable=(True, False))
1705         o = theano.dot(m, v1)
1706         f = theano.function([], o + v2, mode=mode_blas_opt)
1707         assert np.allclose(
1708             f(),
1709             np.dot(m.get_value(), v1.get_value()) + v2.get_value())
1710         topo = f.maker.fgraph.toposort()
1711         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
1712         o = theano.tensor.blas.gemv_no_inplace(v2, 0.5, m, v1, 0.25)
1713         f = theano.function([], o, mode=mode_blas_opt)
1714         assert np.allclose(
1715             f(),
1716             0.5 * np.dot(m.get_value(), v1.get_value()) + 0.25 * v2.get_value())
1717         topo = f.maker.fgraph.toposort()
1718         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
1719     def test_gemv_dimensions(self):
1720         A = T.matrix('A')
1721         x, y = T.vectors('x', 'y')
1722         alpha = theano.shared(theano._asarray(1.0, dtype=config.floatX),
1723                               name='alpha')
1724         beta = theano.shared(theano._asarray(1.0, dtype=config.floatX),
1725                              name='beta')
1726         z = beta * y + alpha * T.dot(A, x)
1727         f = theano.function([A, x, y], z)
1728         A_val = np.ones((5, 3), dtype=config.floatX)
1729         ones_3 = np.ones(3, dtype=config.floatX)
1730         ones_4 = np.ones(4, dtype=config.floatX)
1731         ones_5 = np.ones(5, dtype=config.floatX)
1732         ones_6 = np.ones(6, dtype=config.floatX)
1733         f(A_val, ones_3, ones_5)
1734         f(A_val[::-1, ::-1], ones_3, ones_5)
1735         self.assertRaises(ValueError, f, A_val, ones_4, ones_5)
1736         self.assertRaises(ValueError, f, A_val, ones_3, ones_6)
1737         self.assertRaises(ValueError, f, A_val, ones_4, ones_6)
1738 def matrixmultiply(a, b):
1739     if len(b.shape) == 1:
1740         b_is_vector = True
1741         b = b[:, newaxis]
1742     else:
1743         b_is_vector = False
1744     assert a.shape[1] == b.shape[0]
1745     c = zeros((a.shape[0], b.shape[1]), common_type(a, b))
1746     for i in xrange(a.shape[0]):
1747         for j in xrange(b.shape[1]):
1748             s = 0
1749             for k in xrange(a.shape[1]):
1750                 s += a[i, k] * b[k, j]
1751             c[i, j] = s
1752     if b_is_vector:
1753         c = c.reshape((a.shape[0],))
1754     return c
1755 class BaseGemv(object):
1756     mode = mode_blas_opt  # can be overridden with self.mode
1757     shared = staticmethod(theano.shared)
1758     def get_data(self, x_stride=1, y_stride=1):
1759         rng = np.random.RandomState(unittest_tools.fetch_seed())
1760         mult = array(1, dtype=self.dtype)
1761         if self.dtype in [complex64, complex128]:
1762             mult = array(1 + 1j, dtype=self.dtype)
1763         alpha = array(1., dtype=self.dtype) * mult
1764         beta = array(1., dtype=self.dtype) * mult
1765         a = rng.randn(3, 3).astype(self.dtype) * mult
1766         x = arange(shape(a)[0] * x_stride, dtype=self.dtype) * mult
1767         y = arange(shape(a)[1] * y_stride, dtype=self.dtype) * mult
1768         return alpha, beta, a, x, y
1769     def test_simple(self):
1770         alpha, beta, a, x, y = [self.shared(value)
1771                                 for value in self.get_data()]
1772         desired_oy = alpha.get_value() * matrixmultiply(a.get_value(), x.get_value()) + beta.get_value() * y.get_value()
1773         oy = alpha * T.dot(a, x) + beta * y
1774         oy_func = theano.function([], oy, mode=self.mode)
1775         oy_func.maker.fgraph.toposort()
1776         self.assertFunctionContains1(oy_func, self.gemv)
1777         oy_val = oy_func()
1778         assert_array_almost_equal(desired_oy, oy_val)
1779     def test_default_beta_y(self):
1780         vs = self.get_data()
1781         alpha_v, beta_v, a_v, x_v, y_v = vs
1782         a = self.shared(a_v)
1783         x = self.shared(x_v)
1784         desired_oy = matrixmultiply(a_v, x_v)
1785         oy = T.dot(a, x)
1786         oy_func = theano.function([], oy, mode=self.mode)
1787         self.assertFunctionContains1(oy_func, self.gemv_inplace)
1788         oy_v = oy_func()
1789         assert_array_almost_equal(desired_oy, oy_v)
1790     def test_simple_transpose(self):
1791         vs = self.get_data()
1792         alpha_v, beta_v, a_v, x_v, y_v = vs
1793         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1794         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1795                                               x_v) + beta_v * y_v
1796         oy = alpha * T.dot(a.T, x) + beta * y
1797         oy_func = theano.function([], oy, mode=self.mode)
1798         self.assertFunctionContains1(oy_func, self.gemv)
1799         oy_v = oy_func()
1800         assert_array_almost_equal(desired_oy, oy_v)
1801     def test_x_stride(self):
1802         vs = self.get_data(x_stride=2)
1803         alpha_v, beta_v, a_v, x_v, y_v = vs
1804         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1805         desired_oy = alpha_v * matrixmultiply(a_v, x_v[::2]) + beta_v * y_v
1806         oy = alpha * T.dot(a, x[::2]) + beta * y
1807         oy_func = theano.function([], oy, mode=self.mode)
1808         self.assertFunctionContains1(oy_func, self.gemv)
1809         oy_v = oy_func()
1810         assert_array_almost_equal(desired_oy, oy_v)
1811     def test_x_stride_transpose(self):
1812         vs = self.get_data(x_stride=2)
1813         alpha_v, beta_v, a_v, x_v, y_v = vs
1814         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1815         desired_oy = alpha_v * matrixmultiply(transpose(a_v), x_v[::2]) + \
1816             beta_v * y_v
1817         oy = alpha * T.dot(a.T, x[::2]) + beta * y
1818         oy_func = theano.function([], oy, mode=self.mode)
1819         self.assertFunctionContains1(oy_func, self.gemv)
1820         oy_v = oy_func()
1821         assert_array_almost_equal(desired_oy, oy_v)
1822     def test_y_stride(self):
1823         vs = self.get_data(y_stride=2)
1824         alpha_v, beta_v, a_v, x_v, y_v = vs
1825         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1826         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v[::2]
1827         oy = alpha * T.dot(a, x) + beta * y[::2]
1828         oy_func = theano.function([], oy, mode=self.mode)
1829         self.assertFunctionContains1(oy_func, self.gemv)
1830         oy_v = oy_func()
1831         assert_array_almost_equal(desired_oy, oy_v)
1832     def test_y_stride_transpose(self):
1833         vs = self.get_data(y_stride=2)
1834         alpha_v, beta_v, a_v, x_v, y_v = vs
1835         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1836         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1837                                               x_v) + beta_v * y_v[::2]
1838         oy = alpha * T.dot(a.T, x) + beta * y[::2]
1839         oy_func = theano.function([], oy, mode=self.mode)
1840         self.assertFunctionContains1(oy_func, self.gemv)
1841         oy_v = oy_func()
1842         assert_array_almost_equal(desired_oy, oy_v)
1843     def test_a_strides(self):
1844         vs = self.get_data()
1845         alpha_v, beta_v, a_v, x_v, y_v = vs
1846         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1847         a_v = a_v[::-1, ::-1]
1848         a.set_value(a.get_value(borrow=True,
1849                                 return_internal_type=True)[::-1, ::-1],
1850                     borrow=True)
1851         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v
1852         oy = alpha * T.dot(a, x) + beta * y
1853         oy_func = theano.function([], oy, mode=self.mode)
1854         self.assertFunctionContains1(oy_func, self.gemv)
1855         oy_v = oy_func()
1856         assert_array_almost_equal(desired_oy, oy_v)
1857     def test_a_strides_transpose(self):
1858         vs = self.get_data()
1859         alpha_v, beta_v, a_v, x_v, y_v = vs
1860         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1861         a_v = a_v[::-1, ::-1]
1862         a.set_value(a.get_value(borrow=True,
1863                                 return_internal_type=True)[::-1, ::-1],
1864                     borrow=True)
1865         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1866                                               x_v) + beta_v * y_v
1867         oy = alpha * T.dot(a.T, x) + beta * y
1868         oy_func = theano.function([], oy, mode=self.mode)
1869         self.assertFunctionContains1(oy_func, self.gemv)
1870         oy_v = oy_func()
1871         assert_array_almost_equal(desired_oy, oy_v)
1872     def test_upcasting_scalar_nogemv(self):
1873         vs = self.get_data()
1874         alpha_v, beta_v, a_v, x_v, y_v = vs
1875         alpha_v = alpha_v.astype("float64")
1876         a_v = a_v.astype("float32")
1877         x_v = x_v.astype("float32")
1878         y_v = y_v.astype("float32")
1879         alpha = T.dscalar('alpha')
1880         a = self.shared(a_v)
1881         x = self.shared(x_v)
1882         y = self.shared(y_v)
1883         rval = T.dot(a, x) * alpha + y
1884         f = theano.function([alpha], rval, mode=self.mode)
1885         n_gemvs = 0
1886         for node in f.maker.fgraph.toposort():
1887             if node.op == self.gemv_inplace:
1888                 n_gemvs += 1
1889                 assert node.outputs[0].dtype == 'float32'
1890         assert n_gemvs == 1, n_gemvs
1891         self.assertFunctionContains1(f, self.gemv_inplace)
1892         f(alpha_v)
1893 class TestSgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1894     dtype = float32
1895     gemv = theano.tensor.blas.gemv_no_inplace
1896     gemv_inplace = theano.tensor.blas.gemv_inplace
1897 class TestDgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1898     dtype = float64
1899     gemv = theano.tensor.blas.gemv_no_inplace
1900     gemv_inplace = theano.tensor.blas.gemv_inplace
1901 class TestGer_make_node(TestCase):
1902     def setUp(self):
1903         self.iv = T.tensor(dtype='int32', broadcastable=(False,))
1904         self.fv = T.tensor(dtype='float32', broadcastable=(False,))
1905         self.fv1 = T.tensor(dtype='float32', broadcastable=(True,))
1906         self.dv = T.tensor(dtype='float64', broadcastable=(False,))
1907         self.dv1 = T.tensor(dtype='float64', broadcastable=(True,))
1908         self.cv = T.tensor(dtype='complex64', broadcastable=(False,))
1909         self.zv = T.tensor(dtype='complex128', broadcastable=(False,))
1910         self.fv_2 = T.tensor(dtype='float32', broadcastable=(False,))
1911         self.fv1_2 = T.tensor(dtype='float32', broadcastable=(True,))
1912         self.dv_2 = T.tensor(dtype='float64', broadcastable=(False,))
1913         self.dv1_2 = T.tensor(dtype='float64', broadcastable=(True,))
1914         self.cv_2 = T.tensor(dtype='complex64', broadcastable=(False,))
1915         self.zv_2 = T.tensor(dtype='complex128', broadcastable=(False,))
1916         self.fm = T.fmatrix()
1917         self.dm = T.dmatrix()
1918         self.cm = T.cmatrix()
1919         self.zm = T.zmatrix()
1920         self.fa = T.fscalar()
1921         self.da = T.dscalar()
1922         self.ca = T.cscalar()
1923         self.za = T.zscalar()
1924     def test_works_on_all_valid_dtypes(self):
1925         self.assertEqual(self.fm.type,
1926                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1927         self.assertEqual(self.fm.type,
1928                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1929         self.assertEqual(self.fm.type,
1930                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1931         self.assertEqual(self.fm.type,
1932                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1933     def test_fails_on_invalid_dtypes(self):
1934         self.assertRaises(TypeError,
1935                           ger, T.imatrix(), T.iscalar(), T.ivector(),
1936                           T.ivector())
1937     def test_fails_for_nonscalar_alpha(self):
1938         self.assertRaises(TypeError,
1939                           ger, self.fm, self.fm, self.fv, self.fv_2)
1940         self.assertRaises(TypeError,
1941                           ger, self.fm, self.fv1, self.fv, self.fv_2)
1942         self.assertEqual(self.fm.type,
1943                          ger(self.fm, self.fv1.dimshuffle(), self.fv,
1944                              self.fv_2).type)
1945     def test_fails_for_nonmatrix_A(self):
1946         self.assertRaises(TypeError,
1947                           ger, self.fv, self.fa, self.fv, self.fv_2)
1948     def test_fails_for_nonvector_x_or_y(self):
1949         self.assertRaises(TypeError,
1950                           ger, self.fm, self.fa,
1951                           self.fv.dimshuffle('x', 0), self.fv_2)
1952         self.assertRaises(TypeError,
1953                           ger, self.fm, self.fa,
1954                           self.fv, self.fv_2.dimshuffle('x', 0))
1955     def test_fails_for_mixed_dtypes(self):
1956         self.assertRaises(TypeError, ger, self.dm, self.fa, self.fv, self.fv_2)
1957         self.assertRaises(TypeError, ger, self.fm, self.da, self.fv, self.fv_2)
1958         self.assertRaises(TypeError, ger, self.fm, self.fa, self.dv, self.fv_2)
1959         self.assertRaises(TypeError, ger, self.fm, self.fa, self.fv, self.dv_2)
1960         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.dv_2)
1961         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.zv_2)
1962 class TestGer_OpContract(TestCase, unittest_tools.T_OpContractMixin):
1963     def setUp(self):
1964         self.ops = [ger, ger_destructive]
1965     def clone(self, op):
1966         return Ger(op.destructive)
1967 class TestGer(TestCase, unittest_tools.TestOptimizationMixin):
1968     shared = staticmethod(theano.shared)
1969     def setUp(self):
1970         self.mode = theano.compile.get_default_mode().including('fast_run')
1971         self.mode = self.mode.excluding('c_blas', 'scipy_blas')
1972         dtype = self.dtype = 'float64'  # optimization isn't dtype-dependent
1973         self.A = T.tensor(dtype=dtype, broadcastable=(False, False))
1974         self.a = T.tensor(dtype=dtype, broadcastable=())
1975         self.x = T.tensor(dtype=dtype, broadcastable=(False,))
1976         self.y = T.tensor(dtype=dtype, broadcastable=(False,))
1977         self.ger = ger
1978         self.ger_destructive = ger_destructive
1979         self.gemm = gemm_no_inplace
1980     def function(self, inputs, outputs, updates=None):
1981         if updates is None:
1982             updates = []
1983         return theano.function(inputs, outputs, self.mode, updates=updates)
1984     def b(self, bval):
1985         return T.as_tensor_variable(np.asarray(bval, dtype=self.dtype))
1986     def test_b_0_triggers_ger(self):
1987         assert T.blas.local_gemm_to_ger.transform(
1988             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1989                             self.y.dimshuffle('x', 0), self.b(0)).owner)
1990     def test_b_1_triggers_ger(self):
1991         assert T.blas.local_gemm_to_ger.transform(
1992             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1993                             self.y.dimshuffle('x', 0), self.b(1)).owner)
1994     def test_b_other_does_not_triggers_ger(self):
1995         assert not T.blas.local_gemm_to_ger.transform(
1996             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1997                             self.y.dimshuffle('x', 0), self.b(1.5)).owner)
1998     def test_b_nonconst_does_not_triggers_ger(self):
1999         assert not T.blas.local_gemm_to_ger.transform(
2000             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
2001                             self.y.dimshuffle('x', 0), self.a).owner)
2002     def test_outer(self):
2003         f = self.function([self.x, self.y], T.outer(self.x, self.y))
2004         self.assertFunctionContains(f, self.ger_destructive)
2005         f(np.random.rand(5).astype(self.dtype),
2006           np.random.rand(4).astype(self.dtype))
2007     def test_A_plus_outer(self):
2008         f = self.function([self.A, self.x, self.y],
2009                           self.A + T.outer(self.x, self.y))
2010         self.assertFunctionContains(f, self.ger)
2011         f(np.random.rand(5, 4).astype(self.dtype),
2012           np.random.rand(5).astype(self.dtype),
2013           np.random.rand(4).astype(self.dtype))
2014         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
2015           np.random.rand(5).astype(self.dtype),
2016           np.random.rand(4).astype(self.dtype))
2017     def test_A_plus_scaled_outer(self):
2018         f = self.function([self.A, self.x, self.y],
2019                           self.A + 0.1 * T.outer(self.x, self.y))
2020         self.assertFunctionContains(f, self.ger)
2021         f(np.random.rand(5, 4).astype(self.dtype),
2022           np.random.rand(5).astype(self.dtype),
2023           np.random.rand(4).astype(self.dtype))
2024         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
2025           np.random.rand(5).astype(self.dtype),
2026           np.random.rand(4).astype(self.dtype))
2027     def test_scaled_A_plus_scaled_outer(self):
2028         f = self.function([self.A, self.x, self.y],
2029                           np.asarray(0.2, self.dtype) * self.A +
2030                           np.asarray(0.1, self.dtype) * T.outer(
2031                           self.x, self.y))
2032         self.assertFunctionContains(f, self.gemm)
2033         f(np.random.rand(5, 4).astype(self.dtype),
2034           np.random.rand(5).astype(self.dtype),
2035           np.random.rand(4).astype(self.dtype))
2036         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
2037           np.random.rand(5).astype(self.dtype),
2038           np.random.rand(4).astype(self.dtype))
2039     def given_dtype(self, dtype, M, N):
2040         f = self.function([self.A, self.x, self.y],
2041                           self.A + 0.1 * T.outer(self.x, self.y))
2042         self.assertFunctionContains(f, self.ger)
2043         f(np.random.rand(M, N).astype(self.dtype),
2044           np.random.rand(M).astype(self.dtype),
2045           np.random.rand(N).astype(self.dtype))
2046         f(np.random.rand(M, N).astype(self.dtype)[::-1, ::-1],
2047           np.random.rand(M).astype(self.dtype),
2048 <a name="2"></a>          np.random.rand(N).astype(self.dtype))
2049     def test_f32_0_0(self):
2050         return self<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.given_dtype('float32', 0, 0)
2051     def test_f32_1_0(self):
2052         return self.given_dtype('float32', 1, 0)
2053     def test_f32_0_1(self):
2054 <a name="3"></a>        return self.given_dtype('float32', 0, 1)
2055     def test_f32_1_1(self):
2056         return self.given_dtype(</b></font><font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'float32', 1, 1)
2057     def test_f32_4_4(self):
2058         return self.given_dtype('float32', 4, 4)
2059     def test_f32_7_1(self):
2060         return self.given_dtype('float32', 7, 1)
2061     def test_f32_1_2(self):
2062         return self.given_dtype(</b></font>'float32', 1, 2)
2063     def test_f64_4_5(self):
2064         return self.given_dtype('float64', 4, 5)
2065     def test_c64_7_1(self):
2066         return self.given_dtype('complex64', 7, 1)
2067     def test_c128_1_9(self):
2068         return self.given_dtype('complex128', 1, 9)
2069     def test_inplace(self):
2070         A = self.shared(np.random.rand(4, 5).astype(self.dtype))
2071         f = self.function([self.x, self.y], [],
2072                           updates=[(A, A + T.constant(0.1, dtype=self.dtype) *
2073                                    T.outer(self.x, self.y))])
2074         self.assertFunctionContains(f, self.ger_destructive)
2075         f(np.random.rand(4).astype(self.dtype),
2076           np.random.rand(5).astype(self.dtype))
2077         A.set_value(
2078             A.get_value(borrow=True, return_internal_type=True)[::-1, ::-1],
2079             borrow=True)
2080         f(np.random.rand(4).astype(self.dtype),
2081           np.random.rand(5).astype(self.dtype))
2082 class TestBlasStrides(TestCase):
2083     dtype = 'float64'
2084     shared = staticmethod(tensor._shared)
2085     mode = theano.compile.get_default_mode()
2086     mode = mode.including('fast_run').excluding('gpu', 'c_blas', 'scipy_blas')
2087     rng = np.random.RandomState(seed=unittest_tools.fetch_seed())
2088     def rand(self, *shape):
2089         return theano._asarray(self.rng.rand(*shape), dtype=self.dtype)
2090     def cmp_dot22(self, b_shp, c_shp):
2091         av = np.zeros((0, 0), dtype=self.dtype)
2092         bv = self.rand(*b_shp)
2093         cv = self.rand(*c_shp)
2094         a = self.shared(av, 'a')
2095         b = self.shared(bv, 'b')
2096         c = self.shared(cv, 'c')
2097         b_t = self.shared(bv.T, 'b.T')
2098         c_t = self.shared(cv.T, 'c.T')
2099         b_dev = b.get_value(borrow=False, return_internal_type=True)
2100         c_dev = c.get_value(borrow=False, return_internal_type=True)
2101         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
2102         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
2103         f_nn = theano.function([], [], updates=[(a, tensor.dot(b, c))],
2104                                mode=self.mode)
2105         f_nt = theano.function([], [], updates=[(a, tensor.dot(b, c_t.T))],
2106                                mode=self.mode)
2107         f_tn = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c))],
2108                                mode=self.mode)
2109         f_tt = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c_t.T))],
2110                                mode=self.mode)
2111         for step_signs in itertools_product((-1, 1), repeat=4):
2112             for step in (1, 2):
2113                 b_step1, b_step2, c_step1, c_step2 = (s * step
2114                                                       for s in step_signs)
2115                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
2116                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
2117                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
2118                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
2119                 a_n = np.dot(bv[::b_step1, ::b_step2],
2120                              cv[::c_step1, ::c_step2])
2121                 f_nn()
2122                 assert np.allclose(a.get_value(), a_n)
2123                 f_nt()
2124                 assert np.allclose(a.get_value(), a_n)
2125                 f_tn()
2126                 assert np.allclose(a.get_value(), a_n)
2127                 f_tt()
2128                 assert np.allclose(a.get_value(), a_n)
2129     def test_dot22(self):
2130         self.cmp_dot22((3, 4), (4, 5))
2131         self.cmp_dot22((1, 4), (4, 5))
2132         self.cmp_dot22((3, 4), (4, 1))
2133         self.cmp_dot22((3, 1), (1, 1))
2134         self.cmp_dot22((1, 4), (4, 1))
2135         self.cmp_dot22((3, 1), (1, 5))
2136         self.cmp_dot22((0, 4), (4, 5))
2137         self.cmp_dot22((0, 4), (4, 1))
2138         self.cmp_dot22((0, 1), (1, 5))
2139         self.cmp_dot22((3, 4), (4, 0))
2140         self.cmp_dot22((3, 0), (0, 5))
2141         self.cmp_dot22((0, 4), (4, 0))
2142         self.cmp_dot22((0, 0), (0, 0))
2143     def cmp_dot22scalar(self, b_shp, c_shp):
2144         av = np.zeros((0, 0), dtype=self.dtype)
2145         bv = self.rand(*b_shp)
2146         cv = self.rand(*c_shp)
2147         l = np.float32(0.2)
2148         a = self.shared(av, 'a')
2149         b = self.shared(bv, 'b')
2150         c = self.shared(cv, 'c')
2151         b_t = self.shared(bv.T, 'b.T')
2152         c_t = self.shared(cv.T, 'c.T')
2153         b_dev = b.get_value(borrow=False, return_internal_type=True)
2154         c_dev = c.get_value(borrow=False, return_internal_type=True)
2155         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
2156         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
2157         f_nn = theano.function([], [], updates=[(a, l * tensor.dot(b, c))],
2158                                mode=self.mode)
2159         f_nt = theano.function([], [], updates=[(a, l * tensor.dot(b, c_t.T))],
2160                                mode=self.mode)
2161         f_tn = theano.function([], [], updates=[(a, l * tensor.dot(b_t.T, c))],
2162                                mode=self.mode)
2163         f_tt = theano.function([], [],
2164                                updates=[(a, l * tensor.dot(b_t.T, c_t.T))],
2165                                mode=self.mode)
2166         for step_signs in itertools_product((-1, 1), repeat=4):
2167             for step in (1, 2):
2168                 b_step1, b_step2, c_step1, c_step2 = (s * step
2169                                                       for s in step_signs)
2170                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
2171                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
2172                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
2173                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
2174                 a_n = l * np.dot(bv[::b_step1, ::b_step2],
2175                                  cv[::c_step1, ::c_step2])
2176                 f_nn()
2177                 assert np.allclose(a.get_value(), a_n)
2178                 f_nt()
2179                 assert np.allclose(a.get_value(), a_n)
2180                 f_tn()
2181                 assert np.allclose(a.get_value(), a_n)
2182                 f_tt()
2183                 assert np.allclose(a.get_value(), a_n)
2184     def test_dot22scalar(self):
2185         self.cmp_dot22scalar((3, 4), (4, 5))
2186         self.cmp_dot22scalar((1, 4), (4, 5))
2187         self.cmp_dot22scalar((3, 4), (4, 1))
2188         self.cmp_dot22scalar((3, 1), (1, 1))
2189         self.cmp_dot22scalar((1, 4), (4, 1))
2190         self.cmp_dot22scalar((3, 1), (1, 5))
2191         self.cmp_dot22scalar((0, 4), (4, 5))
2192         self.cmp_dot22scalar((0, 4), (4, 1))
2193         self.cmp_dot22scalar((0, 1), (1, 5))
2194 <a name="0"></a>        self.cmp_dot22scalar((3, 4), (4, 0))
2195         self.cmp_dot22scalar((3, 0), (0, 5))
2196         self.cmp_dot22scalar((0, 4), (4, 0))
2197         self.cmp_dot22scalar((0, 0), (0<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0))
2198     def cmp_gemm(self, a_shp, b_shp, c_shp):
2199         av = self.rand(*a_shp)
2200         bv = self.rand(*b_shp)
2201         cv = self.rand(*c_shp)
2202         l = np.float32(0.2)
2203         a = self.shared(av, 'a')
2204         b = self.shared(bv, 'b')
2205         c = self.shared(</b></font>cv, 'c')
2206         a_t = self.shared(av.T, 'a.T')
2207         b_t = self.shared(bv.T, 'b.T')
2208         c_t = self.shared(cv.T, 'c.T')
2209         a_dev = a.get_value(borrow=False, return_internal_type=True)
2210         b_dev = b.get_value(borrow=False, return_internal_type=True)
2211         c_dev = c.get_value(borrow=False, return_internal_type=True)
2212         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
2213         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
2214         f_nnn = theano.function(
2215             [], [],
2216             updates=[(a, (l * a + tensor.dot(b, c)))],
2217             mode=self.mode)
2218         f_nnt = theano.function(
2219             [], [],
2220             updates=[(a, (l * a + tensor.dot(b, c_t.T)))],
2221             mode=self.mode)
2222         f_ntn = theano.function(
2223             [], [],
2224             updates=[(a, (l * a + tensor.dot(b_t.T, c)))],
2225 <a name="6"></a>            mode=self.mode)
2226         f_ntt = theano.function(
2227             [], [],
2228             updates=[(a, (l * a + tensor.dot(b_t<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.T, c_t.T)))],
2229             mode=self.mode)
2230         f_tnn = theano.function(
2231             [], [],
2232             updates=[(a_t, (l * a_t + tensor.</b></font>dot(b, c).T))],
2233             mode=self.mode)
2234         f_tnt = theano.function(
2235             [], [],
2236             updates=[(a_t, (l * a_t + tensor.dot(b, c_t.T).T))],
2237             mode=self.mode)
2238         f_ttn = theano.function(
2239             [], [],
2240             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c).T))],
2241             mode=self.mode)
2242         f_ttt = theano.function(
2243             [], [],
2244             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c_t.T).T))],
2245             mode=self.mode)
2246         for step_signs in itertools_product((-1, 1), repeat=6):
2247             for step in (1, 2):
2248                 a_step1, a_step2, b_step1, b_step2, c_step1, c_step2 = \
2249                     (s * step for s in step_signs)
2250                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
2251                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
2252                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
2253                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
2254                 a_n = (l * av[::a_step1, ::a_step2] +
2255                        np.dot(bv[::b_step1, ::b_step2],
2256                               cv[::c_step1, ::c_step2]))
2257                 at_n = (l * av[::a_step1, ::a_step2].T +
2258                         np.dot(bv[::b_step1, ::b_step2],
2259                                cv[::c_step1, ::c_step2]).T)
2260                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
2261                 f_nnn()
2262                 assert np.allclose(a.get_value(), a_n)
2263                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
2264                 f_nnt()
2265                 assert np.allclose(a.get_value(), a_n)
2266                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
2267                 f_ntn()
2268                 assert np.allclose(a.get_value(), a_n)
2269                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
2270                 f_ntt()
2271                 assert np.allclose(a.get_value(), a_n)
2272                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
2273                               borrow=True)
2274                 f_tnn()
2275                 assert np.allclose(a_t.get_value(), at_n)
2276                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
2277                               borrow=True)
2278                 f_tnt()
2279                 assert np.allclose(a_t.get_value(), at_n)
2280                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
2281                               borrow=True)
2282                 f_ttn()
2283                 assert np.allclose(a_t.get_value(), at_n)
2284                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
2285                               borrow=True)
2286                 f_ttt()
2287                 assert np.allclose(a_t.get_value(), at_n)
2288     def test_gemm(self):
2289         self.cmp_gemm((3, 5), (3, 4), (4, 5))
2290         self.cmp_gemm((1, 5), (1, 4), (4, 5))
2291         self.cmp_gemm((3, 1), (3, 4), (4, 1))
2292         self.cmp_gemm((3, 1), (3, 1), (1, 1))
2293         self.cmp_gemm((1, 1), (1, 4), (4, 1))
2294         self.cmp_gemm((3, 5), (3, 1), (1, 5))
2295         self.cmp_gemm((0, 5), (0, 4), (4, 5))
2296         self.cmp_gemm((0, 1), (0, 4), (4, 1))
2297         self.cmp_gemm((0, 5), (0, 1), (1, 5))
2298         self.cmp_gemm((3, 0), (3, 4), (4, 0))
2299         self.cmp_gemm((3, 5), (3, 0), (0, 5))
2300         self.cmp_gemm((0, 0), (0, 4), (4, 0))
2301         self.cmp_gemm((0, 0), (0, 0), (0, 0))
2302     def cmp_gemv(self, a_shp, b_shp, c_shp):
2303         av = self.rand(a_shp)
2304         bv = self.rand(*b_shp)
2305         cv = self.rand(c_shp)
2306         l = np.float32(0.2)
2307         a = self.shared(av, 'a')
2308         b = self.shared(bv, 'b')
2309         c = self.shared(cv, 'c')
2310         b_t = self.shared(bv.T, 'b.T')
2311         a_dev = a.get_value(borrow=False, return_internal_type=True)
2312         b_dev = b.get_value(borrow=False, return_internal_type=True)
2313         c_dev = c.get_value(borrow=False, return_internal_type=True)
2314         f_n = theano.function([], [], updates=[(a, (a + l * tensor.dot(b, c)))],
2315                               mode=self.mode)
2316         f_t = theano.function([], [],
2317                               updates=[(a, (a + l * tensor.dot(b_t.T, c)))],
2318                               mode=self.mode)
2319         for step_signs in itertools_product((1, -1), repeat=4):
2320             for step in (1, 2):
2321                 a_step, b_step1, b_step2, c_step = (s * step
2322                                                     for s in step_signs)
2323                 a.set_value(a_dev.copy()[::a_step], borrow=True)
2324                 b.set_value(b_dev.copy()[::b_step1, ::b_step2],
2325                             borrow=True)
2326                 b_t.set_value(transpose(b_dev.copy())[::b_step2, ::b_step1],
2327                               borrow=True)
2328                 c.set_value(c_dev.copy()[::c_step], borrow=True)
2329                 a_n = (av[::a_step] +
2330                        l * np.dot(bv[::b_step1, ::b_step2],
2331                                   cv[::c_step]))
2332                 f_n()
2333                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
2334                 a.set_value(a_dev.copy()[::a_step], borrow=True)
2335                 f_t()
2336                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
2337     def test_gemv(self):
2338         self.cmp_gemv(3, (3, 5), 5)
2339         self.cmp_gemv(1, (1, 5), 5)
2340         self.cmp_gemv(3, (3, 1), 1)
2341         self.cmp_gemv(0, (0, 5), 5)
2342         self.cmp_gemv(3, (3, 0), 0)
2343         self.cmp_gemv(0, (0, 1), 1)
2344         self.cmp_gemv(1, (1, 0), 0)
2345         self.cmp_gemv(0, (0, 0), 0)
2346     def cmp_ger(self, a_shp, b_shp, c_shp):
2347         av = self.rand(*a_shp)
2348         bv = self.rand(b_shp)
2349         cv = self.rand(c_shp)
2350         l = np.float32(0.2)
2351         a = self.shared(av, 'a')
2352         b = self.shared(bv, 'b')
2353         c = self.shared(cv, 'c')
2354         a_t = self.shared(av.T, 'a.T')
2355         a_dev = a.get_value(borrow=False, return_internal_type=True)
2356         b_dev = b.get_value(borrow=False, return_internal_type=True)
2357         c_dev = c.get_value(borrow=False, return_internal_type=True)
2358         f_n = theano.function(
2359             [], [],
2360             updates=[(a, (a + l * tensor.outer(b, c)))],
2361             mode=self.mode)
2362         f_t = theano.function(
2363             [], [],
2364             updates=[(a_t, (a_t + l * tensor.outer(b, c).T))],
2365             mode=self.mode)
2366         for step_signs in itertools_product((1, -1), repeat=4):
2367             for step in (1, 2):
2368                 a_step1, a_step2, b_step, c_step = (s * step
2369                                                     for s in step_signs)
2370                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
2371                 a_t.set_value(transpose(a_dev.copy())[::a_step1, ::a_step2],
2372                               borrow=True)
2373                 b.set_value(b_dev.copy()[::b_step], borrow=True)
2374                 c.set_value(c_dev.copy()[::c_step], borrow=True)
2375                 f_n()
2376                 n_n = (av[::a_step1, ::a_step2] +
2377                        l * np.outer(bv[::b_step], cv[::c_step]))
2378                 assert np.allclose(a.get_value(), n_n), (a.get_value(), n_n)
2379                 f_t()
2380                 n_t = (av.T[::a_step1, ::a_step2] +
2381                        l * np.outer(bv[::b_step], cv[::c_step]).T)
2382                 assert np.allclose(a_t.get_value(), n_t), (a_t.get_value(), n_t)
2383     def test_ger_strides(self):
2384         self.cmp_ger((3, 5), 3, 5)
2385         self.cmp_ger((1, 5), 1, 5)
2386         self.cmp_ger((3, 1), 3, 1)
2387         self.cmp_ger((0, 5), 0, 5)
2388         self.cmp_ger((3, 0), 3, 0)
2389         self.cmp_ger((0, 1), 0, 1)
2390         self.cmp_ger((1, 0), 1, 0)
2391         self.cmp_ger((0, 0), 0, 0)
2392     def test_gemm_non_contiguous(self):
2393         aval = np.ones((6, 2))
2394         bval = np.ones((2, 7))
2395         cval = np.arange(7) + np.arange(0, .6, .1)[:, np.newaxis]
2396         a = theano.shared(aval[:3], borrow=True)
2397         b = theano.shared(bval[:, :5], borrow=True)
2398         c = theano.shared(cval[:3, :5], borrow=True)
2399         s = theano.tensor.scalar()
2400         upd_c = s * c + theano.tensor.dot(a, b)
2401         f = theano.function([s], [], updates={c: upd_c})
2402         f(0)
2403         ref_output = np.ones((3, 5)) * 2
2404         unittest_tools.assert_allclose(c.get_value(), ref_output)
2405 class test_infer_shape(unittest_tools.InferShapeTester):
2406     def test_dot22(self):
2407         x, y = T.matrices('xy')
2408         self._compile_and_check(
2409             [x, y], [T.blas._dot22(x, y)],
2410             [np.random.random((2, 3)).astype(config.floatX),
2411              np.random.random((3, 4)).astype(config.floatX)],
2412             T.blas.Dot22)
2413     def test_dot22scalar(self):
2414         x, y = T.matrices('xy')
2415         a = T.scalar('a')
2416         self._compile_and_check(
2417             [x, y, a], [T.blas._dot22scalar(x, y, a)],
2418             [np.random.random((2, 3)).astype(config.floatX),
2419              np.random.random((3, 4)).astype(config.floatX),
2420              np.asarray(0.5, dtype=config.floatX)],
2421             T.blas.Dot22Scalar)
2422     def test_gemm(self):
2423         x, y, z = T.matrices('xyz')
2424         a = T.scalar('a')
2425         b = T.scalar('b')
2426         self._compile_and_check(
2427             [x, y, a, z, b], [T.blas.gemm(z, a, x, y, b)],
2428             [np.random.random((2, 3)).astype(config.floatX),
2429              np.random.random((3, 4)).astype(config.floatX),
2430              np.asarray(0.5, dtype=config.floatX),
2431              np.random.random((2, 4)).astype(config.floatX),
2432              np.asarray(0.5, dtype=config.floatX)],
2433             T.blas.Gemm)
2434     def test_gemv(self):
2435         A = T.matrix('A')
2436         x, y = T.vectors('xy')
2437         a = T.scalar('a')
2438         b = T.scalar('b')
2439         self._compile_and_check(
2440             [y, a, A, x, b], [T.blas.gemv(y, a, A, x, b)],
2441             [np.random.random((2,)).astype(config.floatX),
2442              np.asarray(0.5, dtype=config.floatX),
2443              np.random.random((2, 3)).astype(config.floatX),
2444              np.random.random((3,)).astype(config.floatX),
2445              np.asarray(0.5, dtype=config.floatX)],
2446             T.blas.Gemv)
2447     def test_ger(self):
2448         A = T.matrix('A')
2449         x, y = T.vectors('xy')
2450         a = T.scalar('a')
2451         self._compile_and_check(
2452             [A, a, x, y], [T.blas.ger(A, a, x, y)],
2453             [np.random.random((2, 3)).astype(config.floatX),
2454              np.asarray(0.5, dtype=config.floatX),
2455              np.random.random((2,)).astype(config.floatX),
2456              np.random.random((3,)).astype(config.floatX)],
2457             T.blas.Ger)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
