
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 24, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-replicator_test.py</h3>
            <pre><code>1  import os
2  from absl import logging
3  from absl.testing import parameterized
4  from sonnet.src import initializers
5  from sonnet.src import test_utils
6  from sonnet.src.distribute import replicator as snt_replicator
7  from sonnet.src.distribute import replicator_test_utils as replicator_utils
8  import tensorflow as tf
9  def _create_variable_in_cross_replica_context(replicator):
10    with replicator.scope():
11      v = tf.Variable(1.)
12    return v
13  class TrainableVariable:
14    def __call__(self):
15      if not hasattr(self, &quot;v&quot;):
16        self.v = tf.Variable(1.)
17      return self.v
18  def _create_variable_in_replica_context(replicator):
19    o = TrainableVariable()
20    def create_var():
21      replicator.run(o)
22    if isinstance(replicator, snt_replicator.TpuReplicator):
23      create_var = tf.function(create_var)
24    create_var()
25    return o.v
26  def all_variable_creators():
27    return ((&quot;cross_replica_context&quot;, _create_variable_in_cross_replica_context),
28            (&quot;replica_context&quot;, _create_variable_in_replica_context))
29  class ReplicatorTest(test_utils.TestCase, parameterized.TestCase):
30    ENTER_PRIMARY_DEVICE = False
31    @test_utils.combined_named_parameters(replicator_utils.named_replicators(),
32                                          all_variable_creators())
33    def test_variable_synchronization_default(self, replicator_fn, create_var):
34      replicator = replicator_fn()
35      if replicator is None:
36        self.skipTest(&quot;No replicator supplied.&quot;)
37      v = create_var(replicator)
38      self.assertEqual(tf.VariableSynchronization.ON_READ,
39                       v.values[0].synchronization)
40    @test_utils.combined_named_parameters(replicator_utils.named_replicators(),
41                                          all_variable_creators())
42    def test_variable_aggregation_default(self, replicator_fn, create_var):
43      replicator = replicator_fn()
44      if replicator is None:
45        self.skipTest(&quot;No replicator supplied.&quot;)
46      v = create_var(replicator)
47      self.assertEqual(tf.VariableAggregation.ONLY_FIRST_REPLICA, v.aggregation)
48    @test_utils.combined_named_parameters(replicator_utils.named_replicators(),
49                                          all_variable_creators())
50    def test_variable_trainable_default(self, replicator_fn, create_var):
51      replicator = replicator_fn()
52      if replicator is None:
53        self.skipTest(&quot;No replicator supplied.&quot;)
54      v = create_var(replicator)
55      self.assertTrue(v.trainable)
56    @test_utils.combined_named_parameters(replicator_utils.named_replicators(),
57                                          test_utils.named_bools(&quot;trainable&quot;))
58    def test_variable_trainable(self, replicator_fn, trainable):
59      replicator = replicator_fn()
60      if replicator is None:
61        self.skipTest(&quot;No replicator supplied.&quot;)
62      with replicator.scope():
63        v = tf.Variable(1., trainable=trainable)
64      self.assertEqual(trainable, v.trainable)
65    @test_utils.combined_named_parameters(replicator_utils.named_replicators(),
66                                          ((&quot;assign&quot;, &quot;assign&quot;, 1.),
67                                           (&quot;assign_add&quot;, &quot;assign_add&quot;, 1.),
68                                           (&quot;assign_sub&quot;, &quot;assign_sub&quot;, -1.)),
69                                          test_utils.named_bools(&quot;cross_replica&quot;))
70    def test_assign(self, replicator_fn, method_name, value, cross_replica):
71      replicator = replicator_fn()
72      if replicator is None:
73        self.skipTest(&quot;No replicator supplied.&quot;)
74      with replicator.scope():
75        v = tf.Variable(0.)
76      update_fn = lambda: getattr(v, method_name)(value)
77      if cross_replica:
78        update_fn()
79      else:
80        if isinstance(replicator, snt_replicator.TpuReplicator):
81          update_fn = tf.function(update_fn)
82        replicator.run(update_fn)
83      for component in v._values:
84        self.assertAllEqual(component.read_value(), tf.ones_like(component))
85    @test_utils.combined_named_parameters(replicator_utils.named_replicators(),
86                                          test_utils.named_bools(&quot;cross_replica&quot;))
87    def test_read_value(self, replicator_fn, cross_replica):
88      replicator = replicator_fn()
89      if replicator is None:
90        self.skipTest(&quot;No replicator supplied.&quot;)
91      with replicator.scope():
92        v = tf.Variable(0.)
93      if cross_replica:
94        values = [v.read_value()]
95      else:
96        if isinstance(replicator, snt_replicator.TpuReplicator):
97          read_value_fn = tf.function(v.read_value)
98        else:
99          read_value_fn = v.read_value
100        values = replicator.run(read_value_fn)
101        values = replicator.experimental_local_results(values)
102      for component in v._values:
103        for value in values:
104          self.assertAllEqual(component.read_value(), value)
105    @parameterized.parameters(True, False)
106    def test_falls_back_to_graph(self, autograph):
107      if os.environ.get(&quot;GITHUB_ACTIONS&quot;, &quot;&quot;) == &quot;true&quot; and autograph:
108        self.skipTest(&quot;Autograph generated code has syntax error.&quot;)
109      init = FailsInEagerMode()
110      value = tf.function(
111          snt_replicator.create_variables_eagerly(
112              lambda: init([], tf.float32)), autograph=autograph)()
113      self.assertEqual(value.numpy(), 1.)
114    @parameterized.parameters(True, False)
115    def test_requires_eager(self, autograph):
116      init = MyOnesInitializer()
117      value = tf.function(
118          snt_replicator.create_variables_eagerly(
119              lambda: init([], tf.float32)), autograph=autograph)()
120      self.assertEqual(value.numpy(), 1.)
121    @parameterized.parameters(True, False)
122    def test_eager_variable_creator(self, autograph):
123      variables = [None, None]
124      eager_ones = tf.ones([])
125      @snt_replicator.create_variables_eagerly
126      def f():
127        if variables[0] is None:
128          graph_ones = tf.ones([])
129          v1 = tf.Variable(graph_ones)
130          v2 = tf.Variable(eager_ones)
131          with tf.init_scope():
132            self.assertEqual(v1.numpy(), 1.)
133            self.assertEqual(v2.numpy(), 1.)
134          variables[0] = v1
135          variables[1] = v2
136      tf.function(f, autograph=autograph)()
137  class MyOnesInitializer(initializers.Initializer):
138    def __call__(self, shape, dtype):
139      assert tf.executing_eagerly()
140      return tf.ones(shape, dtype)
141  class FailsInEagerMode(initializers.Initializer):
142    def __call__(self, shape, dtype):
143      if tf.executing_eagerly():
144        raise ValueError(&quot;Eager mode&quot;)
145      return tf.ones(shape, dtype)
146  def setUpModule():
147    gpus = tf.config.experimental.list_physical_devices(device_type=&quot;GPU&quot;)
148    if len(gpus) == 1:
149      logging.info(&quot;Splitting one physical GPU into two logical GPUs.&quot;)
150      tf.config.experimental.set_virtual_device_configuration(
151          gpus[0], [
152              tf.config.experimental.VirtualDeviceConfiguration(
<span onclick='openModal()' class='match'>153                  memory_limit=1024),
154              tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)
155          ])
156  if __name__ == &quot;__main__&quot;:
157    tf.test.main()
</span></code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-distributed_batch_norm_test.py</h3>
            <pre><code>1  from absl import logging
2  from absl.testing import parameterized
3  from sonnet.src import test_utils
4  from sonnet.src.distribute import distributed_batch_norm as batch_norm
5  from sonnet.src.distribute import replicator
6  import tensorflow as tf
7  class CrossReplicaBatchNormTest(test_utils.TestCase, parameterized.TestCase):
8    ENTER_PRIMARY_DEVICE = False
9    def testDefaultReplicaContext(self):
10      layer = batch_norm.CrossReplicaBatchNorm(False, False, TestMetric(),
11                                               TestMetric())
12      inputs = tf.ones([2, 3, 3, 5])
13      scale = tf.constant(0.5, shape=(5,))
14      offset = tf.constant(2.0, shape=(5,))
15      outputs = layer(inputs, True, scale=scale, offset=offset).numpy()
16      self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))
17    def testWithMultipleDevicesMirrored(self):
18      if self.primary_device == &quot;CPU&quot;:
19        self.skipTest(&quot;No devices to mirror across.&quot;)
20      elif self.primary_device == &quot;GPU&quot;:
21        devices = tf.config.experimental.list_logical_devices(&quot;GPU&quot;)
22      else:
23        devices = tf.config.experimental.list_logical_devices(&quot;TPU&quot;)
24      strategy = replicator.Replicator([device.name for device in devices])
25      with strategy.scope():
26        mean_metric = TestMetric()
27        var_metric = TestMetric()
28        layer = batch_norm.CrossReplicaBatchNorm(False, False, mean_metric,
29                                                 var_metric)
30      scale = tf.constant(0.5, shape=(5,))
31      offset = tf.constant(2.0, shape=(5,))
32      def foo():
33        inputs = tf.random.normal([2, 3, 3, 5])
34        outputs = layer(inputs, True, False, scale, offset)
35        return inputs, outputs
36      inputs, outputs = strategy.run(foo)
37      local_mean_metric = strategy.experimental_local_results(mean_metric.value)
38      local_var_metric = strategy.experimental_local_results(var_metric.value)
39      self.assertAllEqual(local_mean_metric[0].numpy(),
40                          local_mean_metric[1].numpy())
41      self.assertAllEqual(local_var_metric[0].numpy(),
42                          local_var_metric[1].numpy())
43      mean = local_mean_metric[0]
44      var = local_var_metric[0]
45      for inp, out in zip(
46          strategy.experimental_local_results(inputs),
47          strategy.experimental_local_results(outputs)):
48        expected_out = (inp - mean) * tf.math.rsqrt(var + 1e-5) * scale + offset
49        self.assertAllClose(out, expected_out)
50    def testWithTpuStrategy(self):
51      if self.primary_device != &quot;TPU&quot;:
52        self.skipTest(&quot;TPU strategy only runs on TPU&#x27;s&quot;)
53      strategy = replicator.TpuReplicator()
54      with strategy.scope():
55        mean_metric = TestMetric()
56        var_metric = TestMetric()
57        layer = batch_norm.CrossReplicaBatchNorm(False, False,
58                                                 mean_metric, var_metric)
59      scale = tf.constant(0.5, shape=(5,))
60      offset = tf.constant(2.0, shape=(5,))
61      @tf.function
62      def run():
63        def compute():
64          inputs = tf.ones([2, 3, 3, 5])
65          outputs = layer(inputs, True, False, scale, offset)
66          return inputs, outputs
67        return strategy.run(compute)
68      inputs, outputs = run()
69      local_mean_metric = strategy.experimental_local_results(mean_metric.value)
70      local_var_metric = strategy.experimental_local_results(var_metric.value)
71      self.assertAllEqual(local_mean_metric[0].numpy(),
72                          local_mean_metric[1].numpy())
73      self.assertAllEqual(local_var_metric[0].numpy(),
74                          local_var_metric[1].numpy())
75      mean = local_mean_metric[0]
76      var = local_var_metric[0]
77      for inp, out in zip(
78          strategy.experimental_local_results(inputs),
79          strategy.experimental_local_results(outputs)):
80        expected_out = (inp - mean) * tf.math.rsqrt(var + 1e-5) * scale + offset
81        self.assertAllClose(out, expected_out)
82  class TestMetric:
83    def __init__(self):
84      self._foo = None
85      self._built = False
86    def update(self, x):
87      if self._built:
88        self._foo.assign(x)
89      else:
90        self._foo = tf.Variable(x)
91        self._built = True
92    @property
93    def value(self):
94      return self._foo
95    def initialize(self, x):
96      self._foo = tf.Variable(x)
97      self._built = True
98  def setUpModule():
99    gpus = tf.config.experimental.list_physical_devices(device_type=&quot;GPU&quot;)
100    if len(gpus) == 1:
101      logging.info(&quot;Splitting one physical GPU into two logical GPUs.&quot;)
102      tf.config.experimental.set_virtual_device_configuration(
103          gpus[0], [
104              tf.config.experimental.VirtualDeviceConfiguration(
<span onclick='openModal()' class='match'>105                  memory_limit=1024),
106              tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)
107          ])
108  if __name__ == &quot;__main__&quot;:
109    tf.test.main()
</span></code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-replicator_test.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-distributed_batch_norm_test.py</div>
                </div>
                <div class="column column_space"><pre><code>153                  memory_limit=1024),
154              tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)
155          ])
156  if __name__ == &quot;__main__&quot;:
157    tf.test.main()
</pre></code></div>
                <div class="column column_space"><pre><code>105                  memory_limit=1024),
106              tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)
107          ])
108  if __name__ == &quot;__main__&quot;:
109    tf.test.main()
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    