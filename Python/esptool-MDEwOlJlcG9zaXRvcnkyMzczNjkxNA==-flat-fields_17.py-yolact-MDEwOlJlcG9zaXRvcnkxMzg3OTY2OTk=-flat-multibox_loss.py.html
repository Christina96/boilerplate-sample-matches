
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 5.637982195845697%, Tokens: 9</h2>
        <div class="column">
            <h3>esptool-MDEwOlJlcG9zaXRvcnkyMzczNjkxNA==-flat-fields_17.py</h3>
            <pre><code>1  import binascii
2  import struct
3  import time
4  from bitstring import BitArray
5  import esptool
6  import reedsolo
7  from .mem_definition import EfuseDefineBlocks, EfuseDefineFields, EfuseDefineRegisters
8  from .. import base_fields
9  from .. import util
10  class EfuseBlock(base_fields.EfuseBlockBase):
11      def len_of_burn_unit(self):
12          return 8 * 4
13      def __init__(self, parent, param, skip_read=False):
14          parent.read_coding_scheme()
15          super(EfuseBlock, self).__init__(parent, param, skip_read=skip_read)
16      def apply_coding_scheme(self):
17          data = self.get_raw(from_read=False)[::-1]
18          if len(data) < self.len_of_burn_unit():
19              add_empty_bytes = self.len_of_burn_unit() - len(data)
20              data = data + (b"\x00" * add_empty_bytes)
21          if self.get_coding_scheme() == self.parent.REGS.CODING_SCHEME_RS:
22              rs = reedsolo.RSCodec(12)
23              encoded_data = rs.encode([x for x in data])
24              words = struct.unpack("<" + "I" * 11, encoded_data)
25          else:
26              words = struct.unpack("<" + ("I" * (len(data) // 4)), data)
27          return words
28  class EspEfuses(base_fields.EspEfusesBase):
29      debug = False
30      do_not_confirm = False
31      def __init__(self, esp, skip_connect=False, debug=False, do_not_confirm=False):
32          self.Blocks = EfuseDefineBlocks()
33          self.Fields = EfuseDefineFields()
34          self.REGS = EfuseDefineRegisters
35          self.BURN_BLOCK_DATA_NAMES = self.Blocks.get_burn_block_data_names()
36          self.BLOCKS_FOR_KEYS = self.Blocks.get_blocks_for_keys()
37          self._esp = esp
38          self.debug = debug
39          self.do_not_confirm = do_not_confirm
40          if esp.CHIP_NAME != "ESP32-C6":
41              raise esptool.FatalError(
42                  "Expected the 'esp' param for ESP32-C6 chip but got for '%s'."
43                  % (esp.CHIP_NAME)
44              )
45          if not skip_connect:
46              flags = self._esp.get_security_info()["flags"]
47              GET_SECURITY_INFO_FLAG_SECURE_DOWNLOAD_ENABLE = 1 << 2
48              if flags & GET_SECURITY_INFO_FLAG_SECURE_DOWNLOAD_ENABLE:
49                  raise esptool.FatalError(
50                      "Secure Download Mode is enabled. The tool can not read eFuses."
51                  )
52          self.blocks = [
53              EfuseBlock(self, self.Blocks.get(block), skip_read=skip_connect)
54              for block in self.Blocks.BLOCKS
55          ]
56          if not skip_connect:
57              self.get_coding_scheme_warnings()
<span onclick='openModal()' class='match'>58          self.efuses = [EfuseField.convert(self, efuse) for efuse in self.Fields.EFUSES]
59          self.efuses += [
60              EfuseField.convert(self, efuse) for efuse in self.Fields.KEYBLOCKS
</span>61          ]
62          if skip_connect:
63              self.efuses += [
64                  EfuseField.convert(self, efuse)
65                  for efuse in self.Fields.BLOCK2_CALIBRATION_EFUSES
66              ]
67          else:
68              if self["BLK_VERSION_MINOR"].get() == 1:
69                  self.efuses += [
70                      EfuseField.convert(self, efuse)
71                      for efuse in self.Fields.BLOCK2_CALIBRATION_EFUSES
72                  ]
73              self.efuses += [
74                  EfuseField.convert(self, efuse) for efuse in self.Fields.CALC
75              ]
76      def __getitem__(self, efuse_name):
77          for e in self.efuses:
78              if efuse_name == e.name or any(x == efuse_name for x in e.alt_names):
79                  return e
80          new_fields = False
81          for efuse in self.Fields.BLOCK2_CALIBRATION_EFUSES:
82              if efuse.name == efuse_name or any(
83                  x == efuse_name for x in efuse.alt_names
84              ):
85                  self.efuses += [
86                      EfuseField.convert(self, efuse)
87                      for efuse in self.Fields.BLOCK2_CALIBRATION_EFUSES
88                  ]
89                  new_fields = True
90          if new_fields:
91              for e in self.efuses:
92                  if efuse_name == e.name or any(x == efuse_name for x in e.alt_names):
93                      return e
94          raise KeyError
95      def read_coding_scheme(self):
96          self.coding_scheme = self.REGS.CODING_SCHEME_RS
97      def print_status_regs(self):
98          print("")
99          self.blocks[0].print_block(self.blocks[0].err_bitarray, "err__regs", debug=True)
100          print(
101              "{:27} 0x{:08x}".format(
102                  "EFUSE_RD_RS_ERR0_REG", self.read_reg(self.REGS.EFUSE_RD_RS_ERR0_REG)
103              )
104          )
105          print(
106              "{:27} 0x{:08x}".format(
107                  "EFUSE_RD_RS_ERR1_REG", self.read_reg(self.REGS.EFUSE_RD_RS_ERR1_REG)
108              )
109          )
110      def efuse_controller_setup(self):
111          self.set_efuse_timing()
112          self.clear_pgm_registers()
113          self.wait_efuse_idle()
114      def write_efuses(self, block):
115          self.efuse_program(block)
116          return self.get_coding_scheme_warnings(silent=True)
117      def clear_pgm_registers(self):
118          self.wait_efuse_idle()
119          for r in range(
120              self.REGS.EFUSE_PGM_DATA0_REG, self.REGS.EFUSE_PGM_DATA0_REG + 32, 4
121          ):
122              self.write_reg(r, 0)
123      def wait_efuse_idle(self):
124          deadline = time.time() + self.REGS.EFUSE_BURN_TIMEOUT
125          while time.time() < deadline:
126              if self.read_reg(self.REGS.EFUSE_STATUS_REG) & 0x7 == 1:
127                  return
128          raise esptool.FatalError(
129              "Timed out waiting for Efuse controller command to complete"
130          )
131      def efuse_program(self, block):
132          self.wait_efuse_idle()
133          self.write_reg(self.REGS.EFUSE_CONF_REG, self.REGS.EFUSE_WRITE_OP_CODE)
134          self.write_reg(self.REGS.EFUSE_CMD_REG, self.REGS.EFUSE_PGM_CMD | (block << 2))
135          self.wait_efuse_idle()
136          self.clear_pgm_registers()
137          self.efuse_read()
138      def efuse_read(self):
139          self.wait_efuse_idle()
140          self.write_reg(self.REGS.EFUSE_CONF_REG, self.REGS.EFUSE_READ_OP_CODE)
141          try:
142              self.write_reg(
143                  self.REGS.EFUSE_CMD_REG, self.REGS.EFUSE_READ_CMD, delay_after_us=1000
144              )
145              self.wait_efuse_idle()
146          except esptool.FatalError:
147              secure_download_mode_before = self._esp.secure_download_mode
148              try:
149                  self._esp = self.reconnect_chip(self._esp)
150              except esptool.FatalError:
151                  print("Can not re-connect to the chip")
152                  if not self["DIS_DOWNLOAD_MODE"].get() and self[
153                      "DIS_DOWNLOAD_MODE"
154                  ].get(from_read=False):
155                      print(
156                          "This is the correct behavior as we are actually burning "
157                          "DIS_DOWNLOAD_MODE which disables the connection to the chip"
158                      )
159                      print("DIS_DOWNLOAD_MODE is enabled")
160                      print("Successful")
161                      exit(0)  # finish without errors
162                  raise
163              print("Established a connection with the chip")
164              if self._esp.secure_download_mode and not secure_download_mode_before:
165                  print("Secure download mode is enabled")
166                  if not self["ENABLE_SECURITY_DOWNLOAD"].get() and self[
167                      "ENABLE_SECURITY_DOWNLOAD"
168                  ].get(from_read=False):
169                      print(
170                          "espefuse tool can not continue to work in Secure download mode"
171                      )
172                      print("ENABLE_SECURITY_DOWNLOAD is enabled")
173                      print("Successful")
174                      exit(0)  # finish without errors
175              raise
176      def set_efuse_timing(self):
177          apb_freq = self.get_crystal_freq()
178          if apb_freq != 40:
179              raise esptool.FatalError(
180                  "The eFuse supports only xtal=40M (xtal was %d)" % apb_freq
181              )
182          self.update_reg(self.REGS.EFUSE_DAC_CONF_REG, self.REGS.EFUSE_DAC_NUM_M, 0xFF)
183          self.update_reg(
184              self.REGS.EFUSE_DAC_CONF_REG, self.REGS.EFUSE_DAC_CLK_DIV_M, 0x28
185          )
186          self.update_reg(
187              self.REGS.EFUSE_WR_TIM_CONF1_REG, self.REGS.EFUSE_PWR_ON_NUM_M, 0x3000
188          )
189          self.update_reg(
190              self.REGS.EFUSE_WR_TIM_CONF2_REG, self.REGS.EFUSE_PWR_OFF_NUM_M, 0x190
191          )
192      def get_coding_scheme_warnings(self, silent=False):
193          old_addr_reg = 0
194          reg_value = 0
195          ret_fail = False
196          for block in self.blocks:
197              if block.id == 0:
198                  words = [
199                      self.read_reg(self.REGS.EFUSE_RD_REPEAT_ERR0_REG + offs * 4)
200                      for offs in range(5)
201                  ]
202                  block.err_bitarray.pos = 0
203                  for word in reversed(words):
204                      block.err_bitarray.overwrite(BitArray("uint:32=%d" % word))
205                  block.num_errors = block.err_bitarray.count(True)
206                  block.fail = block.num_errors != 0
207              else:
208                  addr_reg, err_num_mask, err_num_offs, fail_bit = self.REGS.BLOCK_ERRORS[
209                      block.id
210                  ]
211                  if err_num_mask is None or err_num_offs is None or fail_bit is None:
212                      continue
213                  if addr_reg != old_addr_reg:
214                      old_addr_reg = addr_reg
215                      reg_value = self.read_reg(addr_reg)
216                  block.fail = reg_value & (1 << fail_bit) != 0
217                  block.num_errors = (reg_value >> err_num_offs) & err_num_mask
218              ret_fail |= block.fail
219              if not silent and (block.fail or block.num_errors):
220                  print(
221                      "Error(s) in BLOCK%d [ERRORS:%d FAIL:%d]"
222                      % (block.id, block.num_errors, block.fail)
223                  )
224          if (self.debug or ret_fail) and not silent:
225              self.print_status_regs()
226          return ret_fail
227      def summary(self):
228          return ""
229  class EfuseField(base_fields.EfuseFieldBase):
230      @staticmethod
231      def convert(parent, efuse):
232          return {
233              "mac": EfuseMacField,
234              "keypurpose": EfuseKeyPurposeField,
235              "t_sensor": EfuseTempSensor,
236              "adc_tp": EfuseAdcPointCalibration,
237              "wafer": EfuseWafer,
238          }.get(efuse.class_type, EfuseField)(parent, efuse)
239  class EfuseWafer(EfuseField):
240      def get(self, from_read=True):
241          hi_bits = self.parent["WAFER_VERSION_MINOR_HI"].get(from_read)
242          assert self.parent["WAFER_VERSION_MINOR_HI"].bit_len == 1
243          lo_bits = self.parent["WAFER_VERSION_MINOR_LO"].get(from_read)
244          assert self.parent["WAFER_VERSION_MINOR_LO"].bit_len == 3
245          return (hi_bits << 3) + lo_bits
246      def save(self, new_value):
247          raise esptool.FatalError("Burning %s is not supported" % self.name)
248  class EfuseTempSensor(EfuseField):
249      def get(self, from_read=True):
250          value = self.get_bitstring(from_read)
251          sig = -1 if value[0] else 1
252          return sig * value[1:].uint * 0.1
253  class EfuseAdcPointCalibration(EfuseField):
254      def get(self, from_read=True):
255          STEP_SIZE = 4
256          value = self.get_bitstring(from_read)
257          sig = -1 if value[0] else 1
258          return sig * value[1:].uint * STEP_SIZE
259  class EfuseMacField(EfuseField):
260      def check_format(self, new_value_str):
261          if new_value_str is None:
262              raise esptool.FatalError(
263                  "Required MAC Address in AA:CD:EF:01:02:03 format!"
264              )
265          if new_value_str.count(":") != 5:
266              raise esptool.FatalError(
267                  "MAC Address needs to be a 6-byte hexadecimal format "
268                  "separated by colons (:)!"
269              )
270          hexad = new_value_str.replace(":", "")
271          if len(hexad) != 12:
272              raise esptool.FatalError(
273                  "MAC Address needs to be a 6-byte hexadecimal number "
274                  "(12 hexadecimal characters)!"
275              )
276          bindata = binascii.unhexlify(hexad)
277          if esptool.util.byte(bindata, 0) & 0x01:
278              raise esptool.FatalError("Custom MAC must be a unicast MAC!")
279          return bindata
280      def check(self):
281          errs, fail = self.parent.get_block_errors(self.block)
282          if errs != 0 or fail:
283              output = "Block%d has ERRORS:%d FAIL:%d" % (self.block, errs, fail)
284          else:
285              output = "OK"
286          return "(" + output + ")"
287      def get(self, from_read=True):
288          if self.name == "CUSTOM_MAC":
289              mac = self.get_raw(from_read)[::-1]
290          else:
291              mac = self.get_raw(from_read)
292          return "%s %s" % (util.hexify(mac, ":"), self.check())
293      def save(self, new_value):
294          def print_field(e, new_value):
295              print(
296                  "    - '{}' ({}) {} -> {}".format(
297                      e.name, e.description, e.get_bitstring(), new_value
298                  )
299              )
300          if self.name == "CUSTOM_MAC":
301              bitarray_mac = self.convert_to_bitstring(new_value)
302              print_field(self, bitarray_mac)
303              super(EfuseMacField, self).save(new_value)
304          else:
305              raise esptool.FatalError("Writing Factory MAC address is not supported")
306  class EfuseKeyPurposeField(EfuseField):
307      KEY_PURPOSES = [
308          ("USER",                         0,  None,       None,      "no_need_rd_protect"),   # User purposes (software-only use)
309          ("RESERVED",                     1,  None,       None,      "no_need_rd_protect"),   # Reserved
310          ("XTS_AES_128_KEY",              4,  None,       "Reverse", "need_rd_protect"),      # XTS_AES_128_KEY (flash/PSRAM encryption)
311          ("HMAC_DOWN_ALL",                5,  None,       None,      "need_rd_protect"),      # HMAC Downstream mode
312          ("HMAC_DOWN_JTAG",               6,  None,       None,      "need_rd_protect"),      # JTAG soft enable key (uses HMAC Downstream mode)
313          ("HMAC_DOWN_DIGITAL_SIGNATURE",  7,  None,       None,      "need_rd_protect"),      # Digital Signature peripheral key (uses HMAC Downstream mode)
314          ("HMAC_UP",                      8,  None,       None,      "need_rd_protect"),      # HMAC Upstream mode
315          ("SECURE_BOOT_DIGEST0",          9,  "DIGEST",   None,      "no_need_rd_protect"),   # SECURE_BOOT_DIGEST0 (Secure Boot key digest)
316          ("SECURE_BOOT_DIGEST1",          10, "DIGEST",   None,      "no_need_rd_protect"),   # SECURE_BOOT_DIGEST1 (Secure Boot key digest)
317          ("SECURE_BOOT_DIGEST2",          11, "DIGEST",   None,      "no_need_rd_protect"),   # SECURE_BOOT_DIGEST2 (Secure Boot key digest)
318      ]
319      KEY_PURPOSES_NAME = [name[0] for name in KEY_PURPOSES]
320      DIGEST_KEY_PURPOSES = [name[0] for name in KEY_PURPOSES if name[2] == "DIGEST"]
321      def check_format(self, new_value_str):
322          raw_val = new_value_str
323          for purpose_name in self.KEY_PURPOSES:
324              if purpose_name[0] == new_value_str:
325                  raw_val = str(purpose_name[1])
326                  break
327          if raw_val.isdigit():
328              if int(raw_val) not in [p[1] for p in self.KEY_PURPOSES if p[1] > 0]:
329                  raise esptool.FatalError("'%s' can not be set (value out of range)" % raw_val)
330          else:
331              raise esptool.FatalError("'%s' unknown name" % raw_val)
332          return raw_val
333      def need_reverse(self, new_key_purpose):
334          for key in self.KEY_PURPOSES:
335              if key[0] == new_key_purpose:
336                  return key[3] == "Reverse"
337      def need_rd_protect(self, new_key_purpose):
338          for key in self.KEY_PURPOSES:
339              if key[0] == new_key_purpose:
340                  return key[4] == "need_rd_protect"
341      def get(self, from_read=True):
342          for p in self.KEY_PURPOSES:
343              if p[1] == self.get_raw(from_read):
344                  return p[0]
345          return "FORBIDDEN_STATE"
346      def get_name(self, raw_val):
347          for key in self.KEY_PURPOSES:
348              if key[1] == raw_val:
349                  return key[0]
350      def save(self, new_value):
351          raw_val = int(self.check_format(str(new_value)))
352          str_new_value = self.get_name(raw_val)
353          if self.name == "KEY_PURPOSE_5" and str_new_value.startswith("XTS_AES"):
354              raise esptool.FatalError(f"{self.name} can not have {str_new_value} key due to a hardware bug (please see TRM for more details)")
355          return super(EfuseKeyPurposeField, self).save(raw_val)
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-multibox_loss.py</h3>
            <pre><code>1  import torch
2  import torch.nn as nn
3  import torch.nn.functional as F
4  from torch.autograd import Variable
5  from ..box_utils import match, log_sum_exp, decode, center_size, crop, elemwise_mask_iou, elemwise_box_iou
6  from data import cfg, mask_type, activation_func
7  class MultiBoxLoss(nn.Module):
8      def __init__(self, num_classes, pos_threshold, neg_threshold, negpos_ratio):
9          super(MultiBoxLoss, self).__init__()
10          self.num_classes = num_classes
11          self.pos_threshold = pos_threshold
12          self.neg_threshold = neg_threshold
13          self.negpos_ratio = negpos_ratio
14          self.l1_expected_area = 20*20/70/70
15          self.l1_alpha = 0.1
16          if cfg.use_class_balanced_conf:
17              self.class_instances = None
18              self.total_instances = 0
19      def forward(self, net, predictions, targets, masks, num_crowds):
20          loc_data  = predictions['loc']
21          conf_data = predictions['conf']
22          mask_data = predictions['mask']
23          priors    = predictions['priors']
24          if cfg.mask_type == mask_type.lincomb:
25              proto_data = predictions['proto']
26          score_data = predictions['score'] if cfg.use_mask_scoring   else None   
27          inst_data  = predictions['inst']  if cfg.use_instance_coeff else None
28          labels = [None] * len(targets) # Used in sem segm loss
29          batch_size = loc_data.size(0)
30          num_priors = priors.size(0)
31          num_classes = self.num_classes
32          loc_t = loc_data.new(batch_size, num_priors, 4)
33          gt_box_t = loc_data.new(batch_size, num_priors, 4)
34          conf_t = loc_data.new(batch_size, num_priors).long()
35          idx_t = loc_data.new(batch_size, num_priors).long()
36          if cfg.use_class_existence_loss:
37              class_existence_t = loc_data.new(batch_size, num_classes-1)
38          for idx in range(batch_size):
39              truths      = targets[idx][:, :-1].data
40              labels[idx] = targets[idx][:, -1].data.long()
41              if cfg.use_class_existence_loss:
42                  class_existence_t[idx, :] = torch.eye(num_classes-1, device=conf_t.get_device())[labels[idx]].max(dim=0)[0]
43              cur_crowds = num_crowds[idx]
44              if cur_crowds > 0:
45                  split = lambda x: (x[-cur_crowds:], x[:-cur_crowds])
46                  crowd_boxes, truths = split(truths)
47                  _, labels[idx] = split(labels[idx])
48                  _, masks[idx]  = split(masks[idx])
49              else:
50                  crowd_boxes = None
51              match(self.pos_threshold, self.neg_threshold,
52                    truths, priors.data, labels[idx], crowd_boxes,
53                    loc_t, conf_t, idx_t, idx, loc_data[idx])
54              gt_box_t[idx, :, :] = truths[idx_t[idx]]
55          loc_t = Variable(loc_t, requires_grad=False)
56          conf_t = Variable(conf_t, requires_grad=False)
57          idx_t = Variable(idx_t, requires_grad=False)
58          pos = conf_t > 0
59          num_pos = pos.sum(dim=1, keepdim=True)
60          pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)
61          losses = {}
62          if cfg.train_boxes:
63              loc_p = loc_data[pos_idx].view(-1, 4)
64              loc_t = loc_t[pos_idx].view(-1, 4)
65              losses['B'] = F.smooth_l1_loss(loc_p, loc_t, reduction='sum') * cfg.bbox_alpha
66          if cfg.train_masks:
67              if cfg.mask_type == mask_type.direct:
68                  if cfg.use_gt_bboxes:
69                      pos_masks = []
70                      for idx in range(batch_size):
71                          pos_masks.append(masks[idx][idx_t[idx, pos[idx]]])
72                      masks_t = torch.cat(pos_masks, 0)
73                      masks_p = mask_data[pos, :].view(-1, cfg.mask_dim)
74                      losses['M'] = F.binary_cross_entropy(torch.clamp(masks_p, 0, 1), masks_t, reduction='sum') * cfg.mask_alpha
75                  else:
76                      losses['M'] = self.direct_mask_loss(pos_idx, idx_t, loc_data, mask_data, priors, masks)
77              elif cfg.mask_type == mask_type.lincomb:
78                  ret = self.lincomb_mask_loss(pos, idx_t, loc_data, mask_data, priors, proto_data, masks, gt_box_t, score_data, inst_data, labels)
79                  if cfg.use_maskiou:
80                      loss, maskiou_targets = ret
81                  else:
82                      loss = ret
83                  losses.update(loss)
84                  if cfg.mask_proto_loss is not None:
85                      if cfg.mask_proto_loss == 'l1':
86                          losses['P'] = torch.mean(torch.abs(proto_data)) / self.l1_expected_area * self.l1_alpha
87                      elif cfg.mask_proto_loss == 'disj':
88                          losses['P'] = -torch.mean(torch.max(F.log_softmax(proto_data, dim=-1), dim=-1)[0])
89          if cfg.use_focal_loss:
90              if cfg.use_sigmoid_focal_loss:
91                  losses['C'] = self.focal_conf_sigmoid_loss(conf_data, conf_t)
92              elif cfg.use_objectness_score:
93                  losses['C'] = self.focal_conf_objectness_loss(conf_data, conf_t)
94              else:
95                  losses['C'] = self.focal_conf_loss(conf_data, conf_t)
96          else:
97              if cfg.use_objectness_score:
98                  losses['C'] = self.conf_objectness_loss(conf_data, conf_t, batch_size, loc_p, loc_t, priors)
99              else:
100                  losses['C'] = self.ohem_conf_loss(conf_data, conf_t, pos, batch_size)
101          if cfg.use_maskiou and maskiou_targets is not None:
102              losses['I'] = self.mask_iou_loss(net, maskiou_targets)
103          if cfg.use_class_existence_loss:
104              losses['E'] = self.class_existence_loss(predictions['classes'], class_existence_t)
105          if cfg.use_semantic_segmentation_loss:
106              losses['S'] = self.semantic_segmentation_loss(predictions['segm'], masks, labels)
107          total_num_pos = num_pos.data.sum().float()
108          for k in losses:
109              if k not in ('P', 'E', 'S'):
110                  losses[k] /= total_num_pos
111              else:
112                  losses[k] /= batch_size
113          return losses
114      def class_existence_loss(self, class_data, class_existence_t):
115          return cfg.class_existence_alpha * F.binary_cross_entropy_with_logits(class_data, class_existence_t, reduction='sum')
116      def semantic_segmentation_loss(self, segment_data, mask_t, class_t, interpolation_mode='bilinear'):
117          batch_size, num_classes, mask_h, mask_w = segment_data.size()
118          loss_s = 0
119          for idx in range(batch_size):
120              cur_segment = segment_data[idx]
121              cur_class_t = class_t[idx]
122              with torch.no_grad():
123                  downsampled_masks = F.interpolate(mask_t[idx].unsqueeze(0), (mask_h, mask_w),
124                                                    mode=interpolation_mode, align_corners=False).squeeze(0)
125                  downsampled_masks = downsampled_masks.gt(0.5).float()
126                  segment_t = torch.zeros_like(cur_segment, requires_grad=False)
127                  for obj_idx in range(downsampled_masks.size(0)):
128                      segment_t[cur_class_t[obj_idx]] = torch.max(segment_t[cur_class_t[obj_idx]], downsampled_masks[obj_idx])
129              loss_s += F.binary_cross_entropy_with_logits(cur_segment, segment_t, reduction='sum')
130          return loss_s / mask_h / mask_w * cfg.semantic_segmentation_alpha
131      def ohem_conf_loss(self, conf_data, conf_t, pos, num):
132          batch_conf = conf_data.view(-1, self.num_classes)
133          if cfg.ohem_use_most_confident:
134              batch_conf = F.softmax(batch_conf, dim=1)
135              loss_c, _ = batch_conf[:, 1:].max(dim=1)
136          else:
137              loss_c = log_sum_exp(batch_conf) - batch_conf[:, 0]
138          loss_c = loss_c.view(num, -1)
139          loss_c[pos]        = 0 # filter out pos boxes
140          loss_c[conf_t < 0] = 0 # filter out neutrals (conf_t = -1)
141          _, loss_idx = loss_c.sort(1, descending=True)
142          _, idx_rank = loss_idx.sort(1)
143          num_pos = pos.long().sum(1, keepdim=True)
144          num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)
145          neg = idx_rank < num_neg.expand_as(idx_rank)
146          neg[pos]        = 0
147          neg[conf_t < 0] = 0 # Filter out neutrals
148          pos_idx = pos.unsqueeze(2).expand_as(conf_data)
149          neg_idx = neg.unsqueeze(2).expand_as(conf_data)
150          conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)
151          targets_weighted = conf_t[(pos+neg).gt(0)]
152          loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='none')
153          if cfg.use_class_balanced_conf:
154              if self.class_instances is None:
155                  self.class_instances = torch.zeros(self.num_classes, device=targets_weighted.device)
156              classes, counts = targets_weighted.unique(return_counts=True)
157              for _cls, _cnt in zip(classes.cpu().numpy(), counts.cpu().numpy()):
158                  self.class_instances[_cls] += _cnt
159              self.total_instances += targets_weighted.size(0)
160              weighting = 1 - (self.class_instances[targets_weighted] / self.total_instances)
161              weighting = torch.clamp(weighting, min=1/self.num_classes)
162              avg_weight = (self.num_classes - 1) / self.num_classes
163              loss_c = (loss_c * weighting).sum() / avg_weight
164          else:
165              loss_c = loss_c.sum()
166          return cfg.conf_alpha * loss_c
167      def focal_conf_loss(self, conf_data, conf_t):
168          conf_t = conf_t.view(-1) # [batch_size*num_priors]
169          conf_data = conf_data.view(-1, conf_data.size(-1)) # [batch_size*num_priors, num_classes]
170          keep = (conf_t >= 0).float()
171          conf_t[conf_t < 0] = 0 # so that gather doesn't drum up a fuss
172          logpt = F.log_softmax(conf_data, dim=-1)
173          logpt = logpt.gather(1, conf_t.unsqueeze(-1))
174          logpt = logpt.view(-1)
175          pt    = logpt.exp()
176          background = (conf_t == 0).float()
177          at = (1 - cfg.focal_loss_alpha) * background + cfg.focal_loss_alpha * (1 - background)
178          loss = -at * (1 - pt) ** cfg.focal_loss_gamma * logpt
179          return cfg.conf_alpha * (loss * keep).sum()
180      def focal_conf_sigmoid_loss(self, conf_data, conf_t):
181          num_classes = conf_data.size(-1)
182          conf_t = conf_t.view(-1) # [batch_size*num_priors]
183          conf_data = conf_data.view(-1, num_classes) # [batch_size*num_priors, num_classes]
184          keep = (conf_t >= 0).float()
185          conf_t[conf_t < 0] = 0 # can't mask with -1, so filter that out
186          conf_one_t = torch.eye(num_classes, device=conf_t.get_device())[conf_t]
187          conf_pm_t  = conf_one_t * 2 - 1 # -1 if background, +1 if forground for specific class
188          logpt = F.logsigmoid(conf_data * conf_pm_t) # note: 1 - sigmoid(x) = sigmoid(-x)
189          pt    = logpt.exp()
190          at = cfg.focal_loss_alpha * conf_one_t + (1 - cfg.focal_loss_alpha) * (1 - conf_one_t)
191          at[..., 0] = 0 # Set alpha for the background class to 0 because sigmoid focal loss doesn't use it
192          loss = -at * (1 - pt) ** cfg.focal_loss_gamma * logpt
193          loss = keep * loss.sum(dim=-1)
194          return cfg.conf_alpha * loss.sum()
195      def focal_conf_objectness_loss(self, conf_data, conf_t):
196          conf_t = conf_t.view(-1) # [batch_size*num_priors]
197          conf_data = conf_data.view(-1, conf_data.size(-1)) # [batch_size*num_priors, num_classes]
198          keep = (conf_t >= 0).float()
199          conf_t[conf_t < 0] = 0 # so that gather doesn't drum up a fuss
200          background = (conf_t == 0).float()
201          at = (1 - cfg.focal_loss_alpha) * background + cfg.focal_loss_alpha * (1 - background)
202          logpt = F.logsigmoid(conf_data[:, 0]) * (1 - background) + F.logsigmoid(-conf_data[:, 0]) * background
203          pt    = logpt.exp()
204          obj_loss = -at * (1 - pt) ** cfg.focal_loss_gamma * logpt
205          pos_mask = conf_t > 0
206          conf_data_pos = (conf_data[:, 1:])[pos_mask] # Now this has just 80 classes
207          conf_t_pos    = conf_t[pos_mask] - 1         # So subtract 1 here
208          class_loss = F.cross_entropy(conf_data_pos, conf_t_pos, reduction='sum')
209          return cfg.conf_alpha * (class_loss + (obj_loss * keep).sum())
210      def conf_objectness_loss(self, conf_data, conf_t, batch_size, loc_p, loc_t, priors):
211          conf_t = conf_t.view(-1) # [batch_size*num_priors]
212          conf_data = conf_data.view(-1, conf_data.size(-1)) # [batch_size*num_priors, num_classes]
213          pos_mask = (conf_t > 0)
214          neg_mask = (conf_t == 0)
215          obj_data = conf_data[:, 0]
216          obj_data_pos = obj_data[pos_mask]
217          obj_data_neg = obj_data[neg_mask]
218          obj_neg_loss = - F.logsigmoid(-obj_data_neg).sum()
219          with torch.no_grad():
220              pos_priors = priors.unsqueeze(0).expand(batch_size, -1, -1).reshape(-1, 4)[pos_mask, :]
221              boxes_pred = decode(loc_p, pos_priors, cfg.use_yolo_regressors)
222              boxes_targ = decode(loc_t, pos_priors, cfg.use_yolo_regressors)
223              iou_targets = elemwise_box_iou(boxes_pred, boxes_targ)
224          obj_pos_loss = - iou_targets * F.logsigmoid(obj_data_pos) - (1 - iou_targets) * F.logsigmoid(-obj_data_pos)
225          obj_pos_loss = obj_pos_loss.sum()
226          conf_data_pos = (conf_data[:, 1:])[pos_mask] # Now this has just 80 classes
227          conf_t_pos    = conf_t[pos_mask] - 1         # So subtract 1 here
228          class_loss = F.cross_entropy(conf_data_pos, conf_t_pos, reduction='sum')
229          return cfg.conf_alpha * (class_loss + obj_pos_loss + obj_neg_loss)
230      def direct_mask_loss(self, pos_idx, idx_t, loc_data, mask_data, priors, masks):
231          loss_m = 0
232          for idx in range(mask_data.size(0)):
233              with torch.no_grad():
234                  cur_pos_idx = pos_idx[idx, :, :]
235                  cur_pos_idx_squeezed = cur_pos_idx[:, 1]
<span onclick='openModal()' class='match'>236                  pos_bboxes = decode(loc_data[idx, :, :], priors.data, cfg.use_yolo_regressors)
237                  pos_bboxes = pos_bboxes[cur_pos_idx].view(-1, 4).clamp(0, 1)
</span>238                  pos_lookup = idx_t[idx, cur_pos_idx_squeezed]
239                  cur_masks = masks[idx]
240                  pos_masks = cur_masks[pos_lookup, :, :]
241                  num_pos, img_height, img_width = pos_masks.size()
242                  x1, x2 = sanitize_coordinates(pos_bboxes[:, 0], pos_bboxes[:, 2], img_width)
243                  y1, y2 = sanitize_coordinates(pos_bboxes[:, 1], pos_bboxes[:, 3], img_height)
244                  scaled_masks = []
245                  for jdx in range(num_pos):
246                      tmp_mask = pos_masks[jdx, y1[jdx]:y2[jdx], x1[jdx]:x2[jdx]]
247                      while tmp_mask.dim() < 2:
248                          tmp_mask = tmp_mask.unsqueeze(0)
249                      new_mask = F.adaptive_avg_pool2d(tmp_mask.unsqueeze(0), cfg.mask_size)
250                      scaled_masks.append(new_mask.view(1, -1))
251                  mask_t = torch.cat(scaled_masks, 0).gt(0.5).float() # Threshold downsampled mask
252              pos_mask_data = mask_data[idx, cur_pos_idx_squeezed, :]
253              loss_m += F.binary_cross_entropy(torch.clamp(pos_mask_data, 0, 1), mask_t, reduction='sum') * cfg.mask_alpha
254          return loss_m
255      def coeff_diversity_loss(self, coeffs, instance_t):
256          num_pos = coeffs.size(0)
257          instance_t = instance_t.view(-1) # juuuust to make sure
258          coeffs_norm = F.normalize(coeffs, dim=1)
259          cos_sim = coeffs_norm @ coeffs_norm.t()
260          inst_eq = (instance_t[:, None].expand_as(cos_sim) == instance_t[None, :].expand_as(cos_sim)).float()
261          cos_sim = (cos_sim + 1) / 2
262          loss = (1 - cos_sim) * inst_eq + cos_sim * (1 - inst_eq)
263          return cfg.mask_proto_coeff_diversity_alpha * loss.sum() / num_pos
264      def lincomb_mask_loss(self, pos, idx_t, loc_data, mask_data, priors, proto_data, masks, gt_box_t, score_data, inst_data, labels, interpolation_mode='bilinear'):
265          mask_h = proto_data.size(1)
266          mask_w = proto_data.size(2)
267          process_gt_bboxes = cfg.mask_proto_normalize_emulate_roi_pooling or cfg.mask_proto_crop
268          if cfg.mask_proto_remove_empty_masks:
269              pos = pos.clone()
270          loss_m = 0
271          loss_d = 0 # Coefficient diversity loss
272          maskiou_t_list = []
273          maskiou_net_input_list = []
274          label_t_list = []
275          for idx in range(mask_data.size(0)):
276              with torch.no_grad():
277                  downsampled_masks = F.interpolate(masks[idx].unsqueeze(0), (mask_h, mask_w),
278                                                    mode=interpolation_mode, align_corners=False).squeeze(0)
279                  downsampled_masks = downsampled_masks.permute(1, 2, 0).contiguous()
280                  if cfg.mask_proto_binarize_downsampled_gt:
281                      downsampled_masks = downsampled_masks.gt(0.5).float()
282                  if cfg.mask_proto_remove_empty_masks:
283                      very_small_masks = (downsampled_masks.sum(dim=(0,1)) <= 0.0001)
284                      for i in range(very_small_masks.size(0)):
285                          if very_small_masks[i]:
286                              pos[idx, idx_t[idx] == i] = 0
287                  if cfg.mask_proto_reweight_mask_loss:
288                      if not cfg.mask_proto_binarize_downsampled_gt:
289                          bin_gt = downsampled_masks.gt(0.5).float()
290                      else:
291                          bin_gt = downsampled_masks
292                      gt_foreground_norm = bin_gt     / (torch.sum(bin_gt,   dim=(0,1), keepdim=True) + 0.0001)
293                      gt_background_norm = (1-bin_gt) / (torch.sum(1-bin_gt, dim=(0,1), keepdim=True) + 0.0001)
294                      mask_reweighting   = gt_foreground_norm * cfg.mask_proto_reweight_coeff + gt_background_norm
295                      mask_reweighting  *= mask_h * mask_w
296              cur_pos = pos[idx]
297              pos_idx_t = idx_t[idx, cur_pos]
298              if process_gt_bboxes:
299                  if cfg.mask_proto_crop_with_pred_box:
300                      pos_gt_box_t = decode(loc_data[idx, :, :], priors.data, cfg.use_yolo_regressors)[cur_pos]
301                  else:
302                      pos_gt_box_t = gt_box_t[idx, cur_pos]
303              if pos_idx_t.size(0) == 0:
304                  continue
305              proto_masks = proto_data[idx]
306              proto_coef  = mask_data[idx, cur_pos, :]
307              if cfg.use_mask_scoring:
308                  mask_scores = score_data[idx, cur_pos, :]
309              if cfg.mask_proto_coeff_diversity_loss:
310                  if inst_data is not None:
311                      div_coeffs = inst_data[idx, cur_pos, :]
312                  else:
313                      div_coeffs = proto_coef
314                  loss_d += self.coeff_diversity_loss(div_coeffs, pos_idx_t)
315              old_num_pos = proto_coef.size(0)
316              if old_num_pos > cfg.masks_to_train:
317                  perm = torch.randperm(proto_coef.size(0))
318                  select = perm[:cfg.masks_to_train]
319                  proto_coef = proto_coef[select, :]
320                  pos_idx_t  = pos_idx_t[select]
321                  if process_gt_bboxes:
322                      pos_gt_box_t = pos_gt_box_t[select, :]
323                  if cfg.use_mask_scoring:
324                      mask_scores = mask_scores[select, :]
325              num_pos = proto_coef.size(0)
326              mask_t = downsampled_masks[:, :, pos_idx_t]     
327              label_t = labels[idx][pos_idx_t]     
328              pred_masks = proto_masks @ proto_coef.t()
329              pred_masks = cfg.mask_proto_mask_activation(pred_masks)
330              if cfg.mask_proto_double_loss:
331                  if cfg.mask_proto_mask_activation == activation_func.sigmoid:
332                      pre_loss = F.binary_cross_entropy(torch.clamp(pred_masks, 0, 1), mask_t, reduction='sum')
333                  else:
334                      pre_loss = F.smooth_l1_loss(pred_masks, mask_t, reduction='sum')
335                  loss_m += cfg.mask_proto_double_loss_alpha * pre_loss
336              if cfg.mask_proto_crop:
337                  pred_masks = crop(pred_masks, pos_gt_box_t)
338              if cfg.mask_proto_mask_activation == activation_func.sigmoid:
339                  pre_loss = F.binary_cross_entropy(torch.clamp(pred_masks, 0, 1), mask_t, reduction='none')
340              else:
341                  pre_loss = F.smooth_l1_loss(pred_masks, mask_t, reduction='none')
342              if cfg.mask_proto_normalize_mask_loss_by_sqrt_area:
343                  gt_area  = torch.sum(mask_t, dim=(0, 1), keepdim=True)
344                  pre_loss = pre_loss / (torch.sqrt(gt_area) + 0.0001)
345              if cfg.mask_proto_reweight_mask_loss:
346                  pre_loss = pre_loss * mask_reweighting[:, :, pos_idx_t]
347              if cfg.mask_proto_normalize_emulate_roi_pooling:
348                  weight = mask_h * mask_w if cfg.mask_proto_crop else 1
349                  pos_gt_csize = center_size(pos_gt_box_t)
350                  gt_box_width  = pos_gt_csize[:, 2] * mask_w
351                  gt_box_height = pos_gt_csize[:, 3] * mask_h
352                  pre_loss = pre_loss.sum(dim=(0, 1)) / gt_box_width / gt_box_height * weight
353              if old_num_pos > num_pos:
354                  pre_loss *= old_num_pos / num_pos
355              loss_m += torch.sum(pre_loss)
356              if cfg.use_maskiou:
357                  if cfg.discard_mask_area > 0:
358                      gt_mask_area = torch.sum(mask_t, dim=(0, 1))
359                      select = gt_mask_area > cfg.discard_mask_area
360                      if torch.sum(select) < 1:
361                          continue
362                      pos_gt_box_t = pos_gt_box_t[select, :]
363                      pred_masks = pred_masks[:, :, select]
364                      mask_t = mask_t[:, :, select]
365                      label_t = label_t[select]
366                  maskiou_net_input = pred_masks.permute(2, 0, 1).contiguous().unsqueeze(1)
367                  pred_masks = pred_masks.gt(0.5).float()                
368                  maskiou_t = self._mask_iou(pred_masks, mask_t)
369                  maskiou_net_input_list.append(maskiou_net_input)
370                  maskiou_t_list.append(maskiou_t)
371                  label_t_list.append(label_t)
372          losses = {'M': loss_m * cfg.mask_alpha / mask_h / mask_w}
373          if cfg.mask_proto_coeff_diversity_loss:
374              losses['D'] = loss_d
375          if cfg.use_maskiou:
376              if len(maskiou_t_list) == 0:
377                  return losses, None
378              maskiou_t = torch.cat(maskiou_t_list)
379              label_t = torch.cat(label_t_list)
380              maskiou_net_input = torch.cat(maskiou_net_input_list)
381              num_samples = maskiou_t.size(0)
382              if cfg.maskious_to_train > 0 and num_samples > cfg.maskious_to_train:
383                  perm = torch.randperm(num_samples)
384                  select = perm[:cfg.masks_to_train]
385                  maskiou_t = maskiou_t[select]
386                  label_t = label_t[select]
387                  maskiou_net_input = maskiou_net_input[select]
388              return losses, [maskiou_net_input, maskiou_t, label_t]
389          return losses
390      def _mask_iou(self, mask1, mask2):
391          intersection = torch.sum(mask1*mask2, dim=(0, 1))
392          area1 = torch.sum(mask1, dim=(0, 1))
393          area2 = torch.sum(mask2, dim=(0, 1))
394          union = (area1 + area2) - intersection
395          ret = intersection / union
396          return ret
397      def mask_iou_loss(self, net, maskiou_targets):
398          maskiou_net_input, maskiou_t, label_t = maskiou_targets
399          maskiou_p = net.maskiou_net(maskiou_net_input)
400          label_t = label_t[:, None]
401          maskiou_p = torch.gather(maskiou_p, dim=1, index=label_t).view(-1)
402          loss_i = F.smooth_l1_loss(maskiou_p, maskiou_t, reduction='sum')
403          return loss_i * cfg.maskiou_alpha
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from esptool-MDEwOlJlcG9zaXRvcnkyMzczNjkxNA==-flat-fields_17.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-multibox_loss.py</div>
                </div>
                <div class="column column_space"><pre><code>58          self.efuses = [EfuseField.convert(self, efuse) for efuse in self.Fields.EFUSES]
59          self.efuses += [
60              EfuseField.convert(self, efuse) for efuse in self.Fields.KEYBLOCKS
</pre></code></div>
                <div class="column column_space"><pre><code>236                  pos_bboxes = decode(loc_data[idx, :, :], priors.data, cfg.use_yolo_regressors)
237                  pos_bboxes = pos_bboxes[cur_pos_idx].view(-1, 4).clamp(0, 1)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    