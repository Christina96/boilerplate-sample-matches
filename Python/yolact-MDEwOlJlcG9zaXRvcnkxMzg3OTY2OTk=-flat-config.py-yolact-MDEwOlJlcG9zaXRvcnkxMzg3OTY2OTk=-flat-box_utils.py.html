
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 14.285714285714285%, Tokens: 9</h2>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-config.py</h3>
            <pre><code>1  from backbone import ResNetBackbone, VGGBackbone, ResNetBackboneGN, DarkNetBackbone
2  from math import sqrt
3  import torch
4  COLORS = ((244,  67,  54),
5            (233,  30,  99),
6            (156,  39, 176),
7            (103,  58, 183),
8            ( 63,  81, 181),
9            ( 33, 150, 243),
10            (  3, 169, 244),
11            (  0, 188, 212),
12            (  0, 150, 136),
13            ( 76, 175,  80),
14            (139, 195,  74),
15            (205, 220,  57),
16            (255, 235,  59),
17            (255, 193,   7),
18            (255, 152,   0),
19            (255,  87,  34),
20            (121,  85,  72),
21            (158, 158, 158),
22            ( 96, 125, 139))
23  MEANS = (103.94, 116.78, 123.68)
24  STD   = (57.38, 57.12, 58.40)
25  COCO_CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
26                  'train', 'truck', 'boat', 'traffic light', 'fire hydrant',
27                  'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',
28                  'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',
29                  'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
30                  'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',
31                  'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
32                  'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
33                  'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',
34                  'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
35                  'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
36                  'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
37                  'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
38                  'scissors', 'teddy bear', 'hair drier', 'toothbrush')
39  COCO_LABEL_MAP = { 1:  1,  2:  2,  3:  3,  4:  4,  5:  5,  6:  6,  7:  7,  8:  8,
40                     9:  9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16,
41                    18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24,
42                    27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32,
43                    37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40,
44                    46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48,
45                    54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56,
46                    62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64,
47                    74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72,
48                    82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}
49  class Config(object):
50      def __init__(self, config_dict):
51          for key, val in config_dict.items():
52              self.__setattr__(key, val)
53      def copy(self, new_config_dict={}):
54          ret = Config(vars(self))
55          for key, val in new_config_dict.items():
56              ret.__setattr__(key, val)
57          return ret
58      def replace(self, new_config_dict):
59          if isinstance(new_config_dict, Config):
60              new_config_dict = vars(new_config_dict)
61          for key, val in new_config_dict.items():
62              self.__setattr__(key, val)
63      def print(self):
64          for k, v in vars(self).items():
65              print(k, ' = ', v)
66  dataset_base = Config({
67      'name': 'Base Dataset',
68      'train_images': './data/coco/images/',
69      'train_info':   'path_to_annotation_file',
70      'valid_images': './data/coco/images/',
71      'valid_info':   'path_to_annotation_file',
72      'has_gt': True,
73      'class_names': COCO_CLASSES,
74      'label_map': None
75  })
76  coco2014_dataset = dataset_base.copy({
77      'name': 'COCO 2014',
78      'train_info': './data/coco/annotations/instances_train2014.json',
79      'valid_info': './data/coco/annotations/instances_val2014.json',
80      'label_map': COCO_LABEL_MAP
81  })
82  coco2017_dataset = dataset_base.copy({
83      'name': 'COCO 2017',
84      'train_info': './data/coco/annotations/instances_train2017.json',
85      'valid_info': './data/coco/annotations/instances_val2017.json',
86      'label_map': COCO_LABEL_MAP
87  })
88  coco2017_testdev_dataset = dataset_base.copy({
89      'name': 'COCO 2017 Test-Dev',
90      'valid_info': './data/coco/annotations/image_info_test-dev2017.json',
91      'has_gt': False,
92      'label_map': COCO_LABEL_MAP
93  })
94  PASCAL_CLASSES = ("aeroplane", "bicycle", "bird", "boat", "bottle",
95                    "bus", "car", "cat", "chair", "cow", "diningtable",
96                    "dog", "horse", "motorbike", "person", "pottedplant",
97                    "sheep", "sofa", "train", "tvmonitor")
98  pascal_sbd_dataset = dataset_base.copy({
99      'name': 'Pascal SBD 2012',
100      'train_images': './data/sbd/img',
101      'valid_images': './data/sbd/img',
102      'train_info': './data/sbd/pascal_sbd_train.json',
103      'valid_info': './data/sbd/pascal_sbd_val.json',
104      'class_names': PASCAL_CLASSES,
105  })
106  resnet_transform = Config({
107      'channel_order': 'RGB',
108      'normalize': True,
109      'subtract_means': False,
110      'to_float': False,
111  })
112  vgg_transform = Config({
113      'channel_order': 'RGB',
114      'normalize': False,
115      'subtract_means': True,
116      'to_float': False,
117  })
118  darknet_transform = Config({
119      'channel_order': 'RGB',
120      'normalize': False,
121      'subtract_means': False,
122      'to_float': True,
123  })
124  backbone_base = Config({
125      'name': 'Base Backbone',
126      'path': 'path/to/pretrained/weights',
127      'type': object,
128      'args': tuple(),
129      'transform': resnet_transform,
130      'selected_layers': list(),
131      'pred_scales': list(),
132      'pred_aspect_ratios': list(),
133      'use_pixel_scales': False,
134      'preapply_sqrt': True,
135      'use_square_anchors': False,
136  })
137  resnet101_backbone = backbone_base.copy({
138      'name': 'ResNet101',
139      'path': 'resnet101_reducedfc.pth',
140      'type': ResNetBackbone,
141      'args': ([3, 4, 23, 3],),
142      'transform': resnet_transform,
143      'selected_layers': list(range(2, 8)),
144      'pred_scales': [[1]]*6,
145      'pred_aspect_ratios': [ [[0.66685089, 1.7073535, 0.87508774, 1.16524493, 0.49059086]] ] * 6,
146  })
147  resnet101_gn_backbone = backbone_base.copy({
148      'name': 'ResNet101_GN',
149      'path': 'R-101-GN.pkl',
150      'type': ResNetBackboneGN,
151      'args': ([3, 4, 23, 3],),
152      'transform': resnet_transform,
153      'selected_layers': list(range(2, 8)),
154      'pred_scales': [[1]]*6,
155      'pred_aspect_ratios': [ [[0.66685089, 1.7073535, 0.87508774, 1.16524493, 0.49059086]] ] * 6,
156  })
157  resnet101_dcn_inter3_backbone = resnet101_backbone.copy({
158      'name': 'ResNet101_DCN_Interval3',
159      'args': ([3, 4, 23, 3], [0, 4, 23, 3], 3),
160  })
161  resnet50_backbone = resnet101_backbone.copy({
162      'name': 'ResNet50',
163      'path': 'resnet50-19c8e357.pth',
164      'type': ResNetBackbone,
165      'args': ([3, 4, 6, 3],),
166      'transform': resnet_transform,
167  })
168  resnet50_dcnv2_backbone = resnet50_backbone.copy({
169      'name': 'ResNet50_DCNv2',
170      'args': ([3, 4, 6, 3], [0, 4, 6, 3]),
171  })
172  darknet53_backbone = backbone_base.copy({
173      'name': 'DarkNet53',
174      'path': 'darknet53.pth',
175      'type': DarkNetBackbone,
176      'args': ([1, 2, 8, 8, 4],),
177      'transform': darknet_transform,
178      'selected_layers': list(range(3, 9)),
179      'pred_scales': [[3.5, 4.95], [3.6, 4.90], [3.3, 4.02], [2.7, 3.10], [2.1, 2.37], [1.8, 1.92]],
<span onclick='openModal()' class='match'>180      'pred_aspect_ratios': [ [[1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n], [1]] for n in [3, 5, 5, 5, 3, 3] ],
181  })
182  vgg16_arch = [[64, 64],
183                [ 'M', 128, 128],
184                [ 'M', 256, 256, 256],
185                [('M', {'kernel_size': 2, 'stride': 2, 'ceil_mode': True}), 512, 512, 512],
</span>186                [ 'M', 512, 512, 512],
187                [('M',  {'kernel_size': 3, 'stride':  1, 'padding':  1}),
188                 (1024, {'kernel_size': 3, 'padding': 6, 'dilation': 6}),
189                 (1024, {'kernel_size': 1})]]
190  vgg16_backbone = backbone_base.copy({
191      'name': 'VGG16',
192      'path': 'vgg16_reducedfc.pth',
193      'type': VGGBackbone,
194      'args': (vgg16_arch, [(256, 2), (128, 2), (128, 1), (128, 1)], [3]),
195      'transform': vgg_transform,
196      'selected_layers': [3] + list(range(5, 10)),
197      'pred_scales': [[5, 4]]*6,
198      'pred_aspect_ratios': [ [[1], [1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n]] for n in [3, 5, 5, 5, 3, 3] ],
199  })
200  mask_type = Config({
201      'direct': 0,
202      'lincomb': 1,
203  })
204  activation_func = Config({
205      'tanh':    torch.tanh,
206      'sigmoid': torch.sigmoid,
207      'softmax': lambda x: torch.nn.functional.softmax(x, dim=-1),
208      'relu':    lambda x: torch.nn.functional.relu(x, inplace=True),
209      'none':    lambda x: x,
210  })
211  fpn_base = Config({
212      'num_features': 256,
213      'interpolation_mode': 'bilinear',
214      'num_downsample': 1,
215      'use_conv_downsample': False,
216      'pad': True,
217      'relu_downsample_layers': False,
218      'relu_pred_layers': True,
219  })
220  coco_base_config = Config({
221      'dataset': coco2014_dataset,
222      'num_classes': 81, # This should include the background class
223      'max_iter': 400000,
224      'max_num_detections': 100,
225      'lr': 1e-3,
226      'momentum': 0.9,
227      'decay': 5e-4,
228      'gamma': 0.1,
229      'lr_steps': (280000, 360000, 400000),
230      'lr_warmup_init': 1e-4,
231      'lr_warmup_until': 500,
232      'conf_alpha': 1,
233      'bbox_alpha': 1.5,
234      'mask_alpha': 0.4 / 256 * 140 * 140, # Some funky equation. Don't worry about it.
235      'eval_mask_branch': True,
236      'nms_top_k': 200,
237      'nms_conf_thresh': 0.05,
238      'nms_thresh': 0.5,
239      'mask_type': mask_type.direct,
240      'mask_size': 16,
241      'masks_to_train': 100,
242      'mask_proto_src': None,
243      'mask_proto_net': [(256, 3, {}), (256, 3, {})],
244      'mask_proto_bias': False,
245      'mask_proto_prototype_activation': activation_func.relu,
246      'mask_proto_mask_activation': activation_func.sigmoid,
247      'mask_proto_coeff_activation': activation_func.tanh,
248      'mask_proto_crop': True,
249      'mask_proto_crop_expand': 0,
250      'mask_proto_loss': None,
251      'mask_proto_binarize_downsampled_gt': True,
252      'mask_proto_normalize_mask_loss_by_sqrt_area': False,
253      'mask_proto_reweight_mask_loss': False,
254      'mask_proto_grid_file': 'data/grid.npy',
255      'mask_proto_use_grid':  False,
256      'mask_proto_coeff_gate': False,
257      'mask_proto_prototypes_as_features': False,
258      'mask_proto_prototypes_as_features_no_grad': False,
259      'mask_proto_remove_empty_masks': False,
260      'mask_proto_reweight_coeff': 1,
261      'mask_proto_coeff_diversity_loss': False,
262      'mask_proto_coeff_diversity_alpha': 1,
263      'mask_proto_normalize_emulate_roi_pooling': False,
264      'mask_proto_double_loss': False,
265      'mask_proto_double_loss_alpha': 1,
266      'mask_proto_split_prototypes_by_head': False,
267      'mask_proto_crop_with_pred_box': False,
268      'augment_photometric_distort': True,
269      'augment_expand': True,
270      'augment_random_sample_crop': True,
271      'augment_random_mirror': True,
272      'augment_random_flip': False,
273      'augment_random_rot90': False,
274      'discard_box_width': 4 / 550,
275      'discard_box_height': 4 / 550,
276      'freeze_bn': False,
277      'fpn': None,
278      'share_prediction_module': False,
279      'ohem_use_most_confident': False,
280      'use_focal_loss': False,
281      'focal_loss_alpha': 0.25,
282      'focal_loss_gamma': 2,
283      'focal_loss_init_pi': 0.01,
284      'use_class_balanced_conf': False,
285      'use_sigmoid_focal_loss': False,
286      'use_objectness_score': False,
287      'use_class_existence_loss': False,
288      'class_existence_alpha': 1,
289      'use_semantic_segmentation_loss': False,
290      'semantic_segmentation_alpha': 1,
291      'use_mask_scoring': False,
292      'mask_scoring_alpha': 1,
293      'use_change_matching': False,
294      'extra_head_net': None,
295      'head_layer_params': {'kernel_size': 3, 'padding': 1},
296      'extra_layers': (0, 0, 0),
297      'positive_iou_threshold': 0.5,
298      'negative_iou_threshold': 0.5,
299      'ohem_negpos_ratio': 3,
300      'crowd_iou_threshold': 1,
301      'mask_dim': None,
302      'max_size': 300,
303      'force_cpu_nms': True,
304      'use_coeff_nms': False,
305      'use_instance_coeff': False,
306      'num_instance_coeffs': 64,
307      'train_masks': True,
308      'train_boxes': True,
309      'use_gt_bboxes': False,
310      'preserve_aspect_ratio': False,
311      'use_prediction_module': False,
312      'use_yolo_regressors': False,
313      'use_prediction_matching': False,
314      'delayed_settings': [],
315      'no_jit': False,
316      'backbone': None,
317      'name': 'base_config',
318      'use_maskiou': False,
319      'maskiou_net': [],
320      'discard_mask_area': -1,
321      'maskiou_alpha': 1.0,
322      'rescore_mask': False,
323      'rescore_bbox': False,
324      'maskious_to_train': -1,
325  })
326  yolact_base_config = coco_base_config.copy({
327      'name': 'yolact_base',
328      'dataset': coco2017_dataset,
329      'num_classes': len(coco2017_dataset.class_names) + 1,
330      'max_size': 550,
331      'lr_steps': (280000, 600000, 700000, 750000),
332      'max_iter': 800000,
333      'backbone': resnet101_backbone.copy({
334          'selected_layers': list(range(1, 4)),
335          'use_pixel_scales': True,
336          'preapply_sqrt': False,
337          'use_square_anchors': True, # This is for backward compatability with a bug
338          'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,
339          'pred_scales': [[24], [48], [96], [192], [384]],
340      }),
341      'fpn': fpn_base.copy({
342          'use_conv_downsample': True,
343          'num_downsample': 2,
344      }),
345      'mask_type': mask_type.lincomb,
346      'mask_alpha': 6.125,
347      'mask_proto_src': 0,
348      'mask_proto_net': [(256, 3, {'padding': 1})] * 3 + [(None, -2, {}), (256, 3, {'padding': 1})] + [(32, 1, {})],
349      'mask_proto_normalize_emulate_roi_pooling': True,
350      'share_prediction_module': True,
351      'extra_head_net': [(256, 3, {'padding': 1})],
352      'positive_iou_threshold': 0.5,
353      'negative_iou_threshold': 0.4,
354      'crowd_iou_threshold': 0.7,
355      'use_semantic_segmentation_loss': True,
356  })
357  yolact_im400_config = yolact_base_config.copy({
358      'name': 'yolact_im400',
359      'max_size': 400,
360      'backbone': yolact_base_config.backbone.copy({
361          'pred_scales': [[int(x[0] / yolact_base_config.max_size * 400)] for x in yolact_base_config.backbone.pred_scales],
362      }),
363  })
364  yolact_im700_config = yolact_base_config.copy({
365      'name': 'yolact_im700',
366      'masks_to_train': 300,
367      'max_size': 700,
368      'backbone': yolact_base_config.backbone.copy({
369          'pred_scales': [[int(x[0] / yolact_base_config.max_size * 700)] for x in yolact_base_config.backbone.pred_scales],
370      }),
371  })
372  yolact_darknet53_config = yolact_base_config.copy({
373      'name': 'yolact_darknet53',
374      'backbone': darknet53_backbone.copy({
375          'selected_layers': list(range(2, 5)),
376          'pred_scales': yolact_base_config.backbone.pred_scales,
377          'pred_aspect_ratios': yolact_base_config.backbone.pred_aspect_ratios,
378          'use_pixel_scales': True,
379          'preapply_sqrt': False,
380          'use_square_anchors': True, # This is for backward compatability with a bug
381      }),
382  })
383  yolact_resnet50_config = yolact_base_config.copy({
384      'name': 'yolact_resnet50',
385      'backbone': resnet50_backbone.copy({
386          'selected_layers': list(range(1, 4)),
387          'pred_scales': yolact_base_config.backbone.pred_scales,
388          'pred_aspect_ratios': yolact_base_config.backbone.pred_aspect_ratios,
389          'use_pixel_scales': True,
390          'preapply_sqrt': False,
391          'use_square_anchors': True, # This is for backward compatability with a bug
392      }),
393  })
394  yolact_resnet50_pascal_config = yolact_resnet50_config.copy({
395      'name': None, # Will default to yolact_resnet50_pascal
396      'dataset': pascal_sbd_dataset,
397      'num_classes': len(pascal_sbd_dataset.class_names) + 1,
398      'max_iter': 120000,
399      'lr_steps': (60000, 100000),
400      'backbone': yolact_resnet50_config.backbone.copy({
401          'pred_scales': [[32], [64], [128], [256], [512]],
402          'use_square_anchors': False,
403      })
404  })
405  yolact_plus_base_config = yolact_base_config.copy({
406      'name': 'yolact_plus_base',
407      'backbone': resnet101_dcn_inter3_backbone.copy({
408          'selected_layers': list(range(1, 4)),
409          'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,
410          'pred_scales': [[i * 2 ** (j / 3.0) for j in range(3)] for i in [24, 48, 96, 192, 384]],
411          'use_pixel_scales': True,
412          'preapply_sqrt': False,
413          'use_square_anchors': False,
414      }),
415      'use_maskiou': True,
416      'maskiou_net': [(8, 3, {'stride': 2}), (16, 3, {'stride': 2}), (32, 3, {'stride': 2}), (64, 3, {'stride': 2}), (128, 3, {'stride': 2})],
417      'maskiou_alpha': 25,
418      'rescore_bbox': False,
419      'rescore_mask': True,
420      'discard_mask_area': 5*5,
421  })
422  yolact_plus_resnet50_config = yolact_plus_base_config.copy({
423      'name': 'yolact_plus_resnet50',
424      'backbone': resnet50_dcnv2_backbone.copy({
425          'selected_layers': list(range(1, 4)),
426          'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,
427          'pred_scales': [[i * 2 ** (j / 3.0) for j in range(3)] for i in [24, 48, 96, 192, 384]],
428          'use_pixel_scales': True,
429          'preapply_sqrt': False,
430          'use_square_anchors': False,
431      }),
432  })
433  cfg = yolact_base_config.copy()
434  def set_cfg(config_name:str):
435      global cfg
436      cfg.replace(eval(config_name))
437      if cfg.name is None:
438          cfg.name = config_name.split('_config')[0]
439  def set_dataset(dataset_name:str):
440      cfg.dataset = eval(dataset_name)
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</h3>
            <pre><code>1  import torch
2  from utils import timer
3  from data import cfg
4  @torch.jit.script
5  def point_form(boxes):
6      return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin
7                       boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax
8  @torch.jit.script
9  def center_size(boxes):
10      return torch.cat(( (boxes[:, 2:] + boxes[:, :2])/2,     # cx, cy
11                          boxes[:, 2:] - boxes[:, :2]  ), 1)  # w, h
12  @torch.jit.script
13  def intersect(box_a, box_b):
14      n = box_a.size(0)
15      A = box_a.size(1)
16      B = box_b.size(1)
17      max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),
18                         box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))
19      min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),
20                         box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))
21      return torch.clamp(max_xy - min_xy, min=0).prod(3)  # inter
22  def jaccard(box_a, box_b, iscrowd:bool=False):
23      use_batch = True
24      if box_a.dim() == 2:
25          use_batch = False
26          box_a = box_a[None, ...]
27          box_b = box_b[None, ...]
28      inter = intersect(box_a, box_b)
29      area_a = ((box_a[:, :, 2]-box_a[:, :, 0]) *
30                (box_a[:, :, 3]-box_a[:, :, 1])).unsqueeze(2).expand_as(inter)  # [A,B]
31      area_b = ((box_b[:, :, 2]-box_b[:, :, 0]) *
32                (box_b[:, :, 3]-box_b[:, :, 1])).unsqueeze(1).expand_as(inter)  # [A,B]
33      union = area_a + area_b - inter
34      out = inter / area_a if iscrowd else inter / union
35      return out if use_batch else out.squeeze(0)
36  def elemwise_box_iou(box_a, box_b):
37      max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])
38      min_xy = torch.max(box_a[:, :2], box_b[:, :2])
39      inter = torch.clamp((max_xy - min_xy), min=0)
<span onclick='openModal()' class='match'>40      inter = inter[:, 0] * inter[:, 1]
41      area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])
</span>42      area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])
43      union = area_a + area_b - inter
44      union = torch.clamp(union, min=0.1)
45      return torch.clamp(inter / union, max=1)
46  def mask_iou(masks_a, masks_b, iscrowd=False):
47      masks_a = masks_a.view(masks_a.size(0), -1)
48      masks_b = masks_b.view(masks_b.size(0), -1)
49      intersection = masks_a @ masks_b.t()
50      area_a = masks_a.sum(dim=1).unsqueeze(1)
51      area_b = masks_b.sum(dim=1).unsqueeze(0)
52      return intersection / (area_a + area_b - intersection) if not iscrowd else intersection / area_a
53  def elemwise_mask_iou(masks_a, masks_b):
54      masks_a = masks_a.view(-1, masks_a.size(-1))
55      masks_b = masks_b.view(-1, masks_b.size(-1))
56      intersection = (masks_a * masks_b).sum(dim=0)
57      area_a = masks_a.sum(dim=0)
58      area_b = masks_b.sum(dim=0)
59      return torch.clamp(intersection / torch.clamp(area_a + area_b - intersection, min=0.1), max=1)
60  def change(gt, priors):
61      num_priors = priors.size(0)
62      num_gt     = gt.size(0)
63      gt_w = (gt[:, 2] - gt[:, 0])[:, None].expand(num_gt, num_priors)
64      gt_h = (gt[:, 3] - gt[:, 1])[:, None].expand(num_gt, num_priors)
65      gt_mat =     gt[:, None, :].expand(num_gt, num_priors, 4)
66      pr_mat = priors[None, :, :].expand(num_gt, num_priors, 4)
67      diff = gt_mat - pr_mat
68      diff[:, :, 0] /= gt_w
69      diff[:, :, 2] /= gt_w
70      diff[:, :, 1] /= gt_h
71      diff[:, :, 3] /= gt_h
72      return -torch.sqrt( (diff ** 2).sum(dim=2) )
73  def match(pos_thresh, neg_thresh, truths, priors, labels, crowd_boxes, loc_t, conf_t, idx_t, idx, loc_data):
74      decoded_priors = decode(loc_data, priors, cfg.use_yolo_regressors) if cfg.use_prediction_matching else point_form(priors)
75      overlaps = jaccard(truths, decoded_priors) if not cfg.use_change_matching else change(truths, decoded_priors)
76      best_truth_overlap, best_truth_idx = overlaps.max(0)
77      for _ in range(overlaps.size(0)):
78          best_prior_overlap, best_prior_idx = overlaps.max(1)
79          j = best_prior_overlap.max(0)[1]
80          i = best_prior_idx[j]
81          overlaps[:, i] = -1
82          overlaps[j, :] = -1
83          best_truth_overlap[i] = 2
84          best_truth_idx[i] = j
85      matches = truths[best_truth_idx]            # Shape: [num_priors,4]
86      conf = labels[best_truth_idx] + 1           # Shape: [num_priors]
87      conf[best_truth_overlap < pos_thresh] = -1  # label as neutral
88      conf[best_truth_overlap < neg_thresh] =  0  # label as background
89      if crowd_boxes is not None and cfg.crowd_iou_threshold < 1:
90          crowd_overlaps = jaccard(decoded_priors, crowd_boxes, iscrowd=True)
91          best_crowd_overlap, best_crowd_idx = crowd_overlaps.max(1)
92          conf[(conf <= 0) & (best_crowd_overlap > cfg.crowd_iou_threshold)] = -1
93      loc = encode(matches, priors, cfg.use_yolo_regressors)
94      loc_t[idx]  = loc    # [num_priors,4] encoded offsets to learn
95      conf_t[idx] = conf   # [num_priors] top class label for each prior
96      idx_t[idx]  = best_truth_idx # [num_priors] indices for lookup
97  @torch.jit.script
98  def encode(matched, priors, use_yolo_regressors:bool=False):
99      if use_yolo_regressors:
100          boxes = center_size(matched)
101          loc = torch.cat((
102              boxes[:, :2] - priors[:, :2],
103              torch.log(boxes[:, 2:] / priors[:, 2:])
104          ), 1)
105      else:
106          variances = [0.1, 0.2]
107          g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]
108          g_cxcy /= (variances[0] * priors[:, 2:])
109          g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]
110          g_wh = torch.log(g_wh) / variances[1]
111          loc = torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]
112      return loc
113  @torch.jit.script
114  def decode(loc, priors, use_yolo_regressors:bool=False):
115      if use_yolo_regressors:
116          boxes = torch.cat((
117              loc[:, :2] + priors[:, :2],
118              priors[:, 2:] * torch.exp(loc[:, 2:])
119          ), 1)
120          boxes = point_form(boxes)
121      else:
122          variances = [0.1, 0.2]
123          boxes = torch.cat((
124              priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],
125              priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)
126          boxes[:, :2] -= boxes[:, 2:] / 2
127          boxes[:, 2:] += boxes[:, :2]
128      return boxes
129  def log_sum_exp(x):
130      x_max = x.data.max()
131      return torch.log(torch.sum(torch.exp(x-x_max), 1)) + x_max
132  @torch.jit.script
133  def sanitize_coordinates(_x1, _x2, img_size:int, padding:int=0, cast:bool=True):
134      _x1 = _x1 * img_size
135      _x2 = _x2 * img_size
136      if cast:
137          _x1 = _x1.long()
138          _x2 = _x2.long()
139      x1 = torch.min(_x1, _x2)
140      x2 = torch.max(_x1, _x2)
141      x1 = torch.clamp(x1-padding, min=0)
142      x2 = torch.clamp(x2+padding, max=img_size)
143      return x1, x2
144  @torch.jit.script
145  def crop(masks, boxes, padding:int=1):
146      h, w, n = masks.size()
147      x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding, cast=False)
148      y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding, cast=False)
149      rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)
150      cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)
151      masks_left  = rows >= x1.view(1, 1, -1)
152      masks_right = rows <  x2.view(1, 1, -1)
153      masks_up    = cols >= y1.view(1, 1, -1)
154      masks_down  = cols <  y2.view(1, 1, -1)
155      crop_mask = masks_left * masks_right * masks_up * masks_down
156      return masks * crop_mask.float()
157  def index2d(src, idx):
158      offs = torch.arange(idx.size(0), device=idx.device)[:, None].expand_as(idx)
159      idx  = idx + offs * idx.size(1)
160      return src.view(-1)[idx.view(-1)].view(idx.size())
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-config.py</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</div>
                <div class="column column_space"><pre><code>180      'pred_aspect_ratios': [ [[1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n], [1]] for n in [3, 5, 5, 5, 3, 3] ],
181  })
182  vgg16_arch = [[64, 64],
183                [ 'M', 128, 128],
184                [ 'M', 256, 256, 256],
185                [('M', {'kernel_size': 2, 'stride': 2, 'ceil_mode': True}), 512, 512, 512],
</pre></code></div>
                <div class="column column_space"><pre><code>40      inter = inter[:, 0] * inter[:, 1]
41      area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    