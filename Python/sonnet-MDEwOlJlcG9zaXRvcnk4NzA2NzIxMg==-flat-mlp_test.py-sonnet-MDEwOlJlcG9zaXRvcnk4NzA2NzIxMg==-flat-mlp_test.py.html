
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 18, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-mlp_test.py</h3>
            <pre><code>1  import itertools
2  from absl.testing import parameterized
3  from sonnet.src import test_utils
4  from sonnet.src.nets import mlp
5  import tensorflow as tf
6  class MLPTest(test_utils.TestCase, parameterized.TestCase):
7    def test_b_init_when_with_bias_false(self):
8      with self.assertRaisesRegex(ValueError, "b_init must not be set"):
9        mlp.MLP([1], with_bias=False, b_init=object())
10    @parameterized.parameters(itertools.product((1, 2, 3), (0.1, 0.0, None)))
11    def test_submodules(self, num_layers, dropout_rate):
12      mod = mlp.MLP([1] * num_layers, dropout_rate=dropout_rate)
13      self.assertLen(mod.submodules, num_layers)
14    @parameterized.parameters(1, 2, 3)
15    def test_applies_activation(self, num_layers):
16      activation = CountingActivation()
17      mod = mlp.MLP([1] * num_layers, activation=activation)
18      mod(tf.ones([1, 1]))
19      self.assertEqual(activation.count, num_layers - 1)
20    @parameterized.parameters(1, 2, 3)
21    def test_activate_final(self, num_layers):
22      activation = CountingActivation()
23      mod = mlp.MLP([1] * num_layers, activate_final=True, activation=activation)
24      mod(tf.ones([1, 1]))
25      self.assertEqual(activation.count, num_layers)
26    @parameterized.parameters(1, 2, 3)
27    def test_adds_index_to_layer_names(self, num_layers):
28      mod = mlp.MLP([1] * num_layers)
29      for index, linear in enumerate(mod.submodules):
30        self.assertEqual(linear.name, "linear_%d" % index)
31    @parameterized.parameters(False, True)
32    def test_passes_with_bias_to_layers(self, with_bias):
33      mod = mlp.MLP([1, 1, 1], with_bias=with_bias)
34      for linear in mod.submodules:
35        self.assertEqual(linear.with_bias, with_bias)
36    def test_repeat_initializer(self):
37      w_init = CountingInitializer()
38      b_init = CountingInitializer()
39      mod = mlp.MLP([1, 1, 1], w_init=w_init, b_init=b_init)
40      mod(tf.ones([1, 1]))
41      self.assertEqual(w_init.count, 3)
42      self.assertEqual(b_init.count, 3)
43    def test_default_name(self):
44      mod = mlp.MLP([1])
45      self.assertEqual(mod.name, "mlp")
46    def test_custom_name(self):
47      mod = mlp.MLP([1], name="foobar")
48      self.assertEqual(mod.name, "foobar")
49    def test_reverse_default_name(self):
50      mod = reversed_mlp()
51      self.assertEqual(mod.name, "mlp_reversed")
52    def test_reverse_custom_name(self):
53      mod = reversed_mlp(name="foobar")
54      self.assertEqual(mod.name, "foobar_reversed")
55    def test_reverse_override_name(self):
56      mod = mlp.MLP([2, 3, 4])
57      mod(tf.ones([1, 1]))
58      rev = mod.reverse(name="foobar")
59      self.assertEqual(rev.name, "foobar")
60    def test_reverse(self):
61      mod = reversed_mlp()
62      self.assertEqual([l.output_size for l in mod.submodules], [3, 2, 1])
63    @parameterized.parameters(True, False)
64    def test_reverse_passed_with_bias(self, with_bias):
65      mod = reversed_mlp(with_bias=with_bias)
66      for linear in mod.submodules:
67        self.assertEqual(linear.with_bias, with_bias)
68    def test_reverse_w_init(self):
69      w_init = CountingInitializer()
70      mod = reversed_mlp(w_init=w_init)
71      for linear in mod.submodules:
72        self.assertIs(linear.w_init, w_init)
73    def test_reverse_b_init(self):
74      b_init = CountingInitializer()
75      mod = reversed_mlp(b_init=b_init)
76      for linear in mod.submodules:
77        self.assertIs(linear.b_init, b_init)
78    def test_reverse_activation(self):
79      activation = CountingActivation()
80      mod = reversed_mlp(activation=activation)
<span onclick='openModal()' class='match'>81      activation.count = 0
82      mod(tf.ones([1, 1]))
83      self.assertEqual(activation.count, 2)
</span>84    def test_dropout_requires_is_training(self):
85      mod = mlp.MLP([1, 1], dropout_rate=0.5)
86      with self.assertRaisesRegex(ValueError, "is_training.* is required"):
87        mod(tf.ones([1, 1]))
88    @parameterized.parameters(False, True)
89    def test_no_dropout_rejects_is_training(self, is_training):
90      mod = mlp.MLP([1, 1])
91      with self.assertRaisesRegex(ValueError, "is_training.*only.*with dropout"):
92        mod(tf.ones([1, 1]), is_training=is_training)
93    @parameterized.parameters(False, True)
94    def test_reverse_activate_final(self, activate_final):
95      activation = CountingActivation()
96      mod = reversed_mlp(activation=activation, activate_final=activate_final)
97      activation.count = 0
98      mod(tf.ones([1, 1]))
99      self.assertEqual(activation.count, 3 if activate_final else 2)
100    @parameterized.parameters(itertools.product((False, True), (False, True)))
101    def test_applies_activation_with_dropout(self, use_dropout, is_training):
102      activation = CountingActivation()
103      mod = mlp.MLP([1, 1, 1],
104                    dropout_rate=(0.5 if use_dropout else None),
105                    activation=activation)
106      mod(tf.ones([1, 1]), is_training=(is_training if use_dropout else None))
107      self.assertEqual(activation.count, 2)
108    def test_repr(self):
109      mod = mlp.MLP([1, 2, 3])
110      for index, linear in enumerate(mod.submodules):
111        self.assertEqual(
112            repr(linear),
113            "Linear(output_size={}, name='linear_{}')".format(index + 1, index))
114  def reversed_mlp(**kwargs):
115    mod = mlp.MLP([2, 3, 4], **kwargs)
116    mod(tf.ones([1, 1]))
117    return mod.reverse()
118  class CountingActivation:
119    def __init__(self):
120      self.count = 0
121    def __call__(self, x):
122      self.count += 1
123      return x
124  class CountingInitializer:
125    def __init__(self):
126      self.count = 0
127    def __call__(self, shape, dtype=tf.float32):
128      self.count += 1
129      return tf.ones(shape, dtype=dtype)
130  if __name__ == "__main__":
131    tf.test.main()
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-mlp_test.py</h3>
            <pre><code>1  import itertools
2  from absl.testing import parameterized
3  from sonnet.src import test_utils
4  from sonnet.src.nets import mlp
5  import tensorflow as tf
6  class MLPTest(test_utils.TestCase, parameterized.TestCase):
7    def test_b_init_when_with_bias_false(self):
8      with self.assertRaisesRegex(ValueError, "b_init must not be set"):
9        mlp.MLP([1], with_bias=False, b_init=object())
10    @parameterized.parameters(itertools.product((1, 2, 3), (0.1, 0.0, None)))
11    def test_submodules(self, num_layers, dropout_rate):
12      mod = mlp.MLP([1] * num_layers, dropout_rate=dropout_rate)
13      self.assertLen(mod.submodules, num_layers)
14    @parameterized.parameters(1, 2, 3)
15    def test_applies_activation(self, num_layers):
16      activation = CountingActivation()
17      mod = mlp.MLP([1] * num_layers, activation=activation)
18      mod(tf.ones([1, 1]))
19      self.assertEqual(activation.count, num_layers - 1)
20    @parameterized.parameters(1, 2, 3)
21    def test_activate_final(self, num_layers):
22      activation = CountingActivation()
23      mod = mlp.MLP([1] * num_layers, activate_final=True, activation=activation)
24      mod(tf.ones([1, 1]))
25      self.assertEqual(activation.count, num_layers)
26    @parameterized.parameters(1, 2, 3)
27    def test_adds_index_to_layer_names(self, num_layers):
28      mod = mlp.MLP([1] * num_layers)
29      for index, linear in enumerate(mod.submodules):
30        self.assertEqual(linear.name, "linear_%d" % index)
31    @parameterized.parameters(False, True)
32    def test_passes_with_bias_to_layers(self, with_bias):
33      mod = mlp.MLP([1, 1, 1], with_bias=with_bias)
34      for linear in mod.submodules:
35        self.assertEqual(linear.with_bias, with_bias)
36    def test_repeat_initializer(self):
37      w_init = CountingInitializer()
38      b_init = CountingInitializer()
39      mod = mlp.MLP([1, 1, 1], w_init=w_init, b_init=b_init)
40      mod(tf.ones([1, 1]))
41      self.assertEqual(w_init.count, 3)
42      self.assertEqual(b_init.count, 3)
43    def test_default_name(self):
44      mod = mlp.MLP([1])
45      self.assertEqual(mod.name, "mlp")
46    def test_custom_name(self):
47      mod = mlp.MLP([1], name="foobar")
48      self.assertEqual(mod.name, "foobar")
49    def test_reverse_default_name(self):
50      mod = reversed_mlp()
51      self.assertEqual(mod.name, "mlp_reversed")
52    def test_reverse_custom_name(self):
53      mod = reversed_mlp(name="foobar")
54      self.assertEqual(mod.name, "foobar_reversed")
55    def test_reverse_override_name(self):
56      mod = mlp.MLP([2, 3, 4])
57      mod(tf.ones([1, 1]))
58      rev = mod.reverse(name="foobar")
59      self.assertEqual(rev.name, "foobar")
60    def test_reverse(self):
61      mod = reversed_mlp()
62      self.assertEqual([l.output_size for l in mod.submodules], [3, 2, 1])
63    @parameterized.parameters(True, False)
64    def test_reverse_passed_with_bias(self, with_bias):
65      mod = reversed_mlp(with_bias=with_bias)
66      for linear in mod.submodules:
67        self.assertEqual(linear.with_bias, with_bias)
68    def test_reverse_w_init(self):
69      w_init = CountingInitializer()
70      mod = reversed_mlp(w_init=w_init)
71      for linear in mod.submodules:
72        self.assertIs(linear.w_init, w_init)
73    def test_reverse_b_init(self):
74      b_init = CountingInitializer()
75      mod = reversed_mlp(b_init=b_init)
76      for linear in mod.submodules:
77        self.assertIs(linear.b_init, b_init)
78    def test_reverse_activation(self):
79      activation = CountingActivation()
80      mod = reversed_mlp(activation=activation)
81      activation.count = 0
82      mod(tf.ones([1, 1]))
83      self.assertEqual(activation.count, 2)
84    def test_dropout_requires_is_training(self):
85      mod = mlp.MLP([1, 1], dropout_rate=0.5)
86      with self.assertRaisesRegex(ValueError, "is_training.* is required"):
87        mod(tf.ones([1, 1]))
88    @parameterized.parameters(False, True)
89    def test_no_dropout_rejects_is_training(self, is_training):
90      mod = mlp.MLP([1, 1])
91      with self.assertRaisesRegex(ValueError, "is_training.*only.*with dropout"):
92        mod(tf.ones([1, 1]), is_training=is_training)
93    @parameterized.parameters(False, True)
94    def test_reverse_activate_final(self, activate_final):
95      activation = CountingActivation()
96      mod = reversed_mlp(activation=activation, activate_final=activate_final)
<span onclick='openModal()' class='match'>97      activation.count = 0
98      mod(tf.ones([1, 1]))
99      self.assertEqual(activation.count, 3 if activate_final else 2)
</span>100    @parameterized.parameters(itertools.product((False, True), (False, True)))
101    def test_applies_activation_with_dropout(self, use_dropout, is_training):
102      activation = CountingActivation()
103      mod = mlp.MLP([1, 1, 1],
104                    dropout_rate=(0.5 if use_dropout else None),
105                    activation=activation)
106      mod(tf.ones([1, 1]), is_training=(is_training if use_dropout else None))
107      self.assertEqual(activation.count, 2)
108    def test_repr(self):
109      mod = mlp.MLP([1, 2, 3])
110      for index, linear in enumerate(mod.submodules):
111        self.assertEqual(
112            repr(linear),
113            "Linear(output_size={}, name='linear_{}')".format(index + 1, index))
114  def reversed_mlp(**kwargs):
115    mod = mlp.MLP([2, 3, 4], **kwargs)
116    mod(tf.ones([1, 1]))
117    return mod.reverse()
118  class CountingActivation:
119    def __init__(self):
120      self.count = 0
121    def __call__(self, x):
122      self.count += 1
123      return x
124  class CountingInitializer:
125    def __init__(self):
126      self.count = 0
127    def __call__(self, shape, dtype=tf.float32):
128      self.count += 1
129      return tf.ones(shape, dtype=dtype)
130  if __name__ == "__main__":
131    tf.test.main()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-mlp_test.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-mlp_test.py</div>
                </div>
                <div class="column column_space"><pre><code>81      activation.count = 0
82      mod(tf.ones([1, 1]))
83      self.assertEqual(activation.count, 2)
</pre></code></div>
                <div class="column column_space"><pre><code>97      activation.count = 0
98      mod(tf.ones([1, 1]))
99      self.assertEqual(activation.count, 3 if activate_final else 2)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    