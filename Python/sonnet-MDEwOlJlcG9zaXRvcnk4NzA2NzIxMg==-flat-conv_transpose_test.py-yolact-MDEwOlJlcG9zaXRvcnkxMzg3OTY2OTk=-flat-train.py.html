
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 3.670886075949367%, Tokens: 9</h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-conv_transpose_test.py</h3>
            <pre><code>1  import itertools
2  from absl.testing import parameterized
3  import numpy as np
4  from sonnet.src import conv_transpose
5  from sonnet.src import initializers as lib_initializers
6  from sonnet.src import test_utils
7  import tensorflow as tf
8  def create_constant_initializers(w, b, with_bias):
9    if with_bias:
10      return {
11          "w_init": lib_initializers.Constant(w),
12          "b_init": lib_initializers.Constant(b)
13      }
14    else:
15      return {"w_init": lib_initializers.Constant(w)}
16  class ConvTransposeTest(test_utils.TestCase, parameterized.TestCase):
17    @parameterized.parameters(0, 4)
18    def testIncorrectN(self, n):
19      with self.assertRaisesRegex(
20          ValueError,
21          "only support transpose convolution operations for num_spatial_dims"):
22        conv_transpose.ConvNDTranspose(
23            num_spatial_dims=n,
24            output_channels=1,
25            output_shape=None,
26            kernel_shape=3,
27            data_format="NHWC")
28    def testIncorrectPadding(self):
29      with self.assertRaisesRegex(
30          TypeError,
31          "ConvNDTranspose only takes string padding, please provide either"):
32        conv_transpose.ConvNDTranspose(
33            2, output_channels=1, kernel_shape=3, padding=None)
34    def testBiasInitNoBias(self):
35      with self.assertRaisesRegex(
36          ValueError, "When not using a bias the b_init must be None."):
37        conv_transpose.ConvNDTranspose(
38            2, output_channels=1, kernel_shape=3, with_bias=False,
39            b_init=lib_initializers.Ones(), data_format="NHWC")
40    def testIncorrectOutputShape(self):
41      c = conv_transpose.ConvNDTranspose(
42          num_spatial_dims=2,
43          output_channels=3,
44          kernel_shape=2,
45          output_shape=[1],
46          data_format="NHWC")
47      with self.assertRaisesRegex(
48          ValueError, "The output_shape must be of length 2 but instead was 1."):
49        c(tf.ones([3, 5, 5, 3]))
50    @parameterized.parameters(*itertools.product(
51        [True, False],  # with_bias
52        ["SAME", "VALID"]))  # padding
53    def testGraphConv(self, with_bias, padding):
<span onclick='openModal()' class='match'>54      conv1 = conv_transpose.ConvNDTranspose(
55          num_spatial_dims=2,
56          output_channels=1,
57          output_shape=None,
58          kernel_shape=3,
59          stride=1,
60          padding=padding,
</span>61          with_bias=with_bias,
62          data_format="NHWC",
63          **create_constant_initializers(1.0, 1.0, with_bias))
64      conv2 = conv_transpose.ConvNDTranspose(
65          num_spatial_dims=2,
66          output_channels=1,
67          output_shape=None,
68          kernel_shape=3,
69          stride=1,
70          padding=padding,
71          with_bias=with_bias,
72          data_format="NHWC",
73          **create_constant_initializers(1.0, 1.0, with_bias))
74      defun_conv = tf.function(conv2)
75      iterations = 5
76      for _ in range(iterations):
77        x = tf.random.uniform([1, 3, 3, 1])
78        y1 = conv1(x)
79        y2 = defun_conv(x)
80        self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)
81    def testUnknownBatchSizeNHWC(self):
82      x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)
83      c = conv_transpose.ConvNDTranspose(
84          num_spatial_dims=2,
85          output_channels=2,
86          kernel_shape=3,
87          data_format="NHWC")
88      defun_conv = tf.function(c).get_concrete_function(x)
89      out1 = defun_conv(tf.ones([3, 5, 5, 3]))
90      self.assertEqual(out1.shape, [3, 5, 5, 2])
91      out2 = defun_conv(tf.ones([5, 5, 5, 3]))
92      self.assertEqual(out2.shape, [5, 5, 5, 2])
93    def testUnknownBatchSizeNCHW(self):
94      if self.primary_device == "CPU":
95        self.skipTest("NCHW not supported on CPU")
96      x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)
97      c = conv_transpose.ConvNDTranspose(
98          num_spatial_dims=2,
99          output_channels=2,
100          kernel_shape=3,
101          data_format="NCHW")
102      defun_conv = tf.function(c).get_concrete_function(x)
103      out1 = defun_conv(tf.ones([3, 3, 5, 5]))
104      self.assertEqual(out1.shape, [3, 2, 5, 5])
105      out2 = defun_conv(tf.ones([5, 3, 5, 5]))
106      self.assertEqual(out2.shape, [5, 2, 5, 5])
107    def testUnknownShapeDims(self):
108      x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)
109      c = conv_transpose.ConvNDTranspose(
110          num_spatial_dims=2,
111          output_channels=2,
112          kernel_shape=3,
113          data_format="NHWC")
114      defun_conv = tf.function(c).get_concrete_function(x)
115      out1 = defun_conv(tf.ones([3, 5, 5, 3]))
116      self.assertEqual(out1.shape, [3, 5, 5, 2])
117      out1 = defun_conv(tf.ones([3, 3, 3, 3]))
118      self.assertEqual(out1.shape, [3, 3, 3, 2])
119    def testGivenOutputShape(self):
120      c = conv_transpose.ConvNDTranspose(
121          num_spatial_dims=2,
122          output_channels=2,
123          kernel_shape=3,
124          output_shape=[5, 5],
125          data_format="NHWC")
126      out1 = c(tf.ones([3, 5, 5, 3]))
127      self.assertEqual(out1.shape, [3, 5, 5, 2])
128    @parameterized.parameters(True, False)
129    def testUnknownChannels(self, autograph):
130      x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)
131      c = conv_transpose.ConvNDTranspose(
132          num_spatial_dims=2,
133          output_channels=1,
134          kernel_shape=3,
135          data_format="NHWC")
136      defun_conv = tf.function(c, autograph=autograph)
137      with self.assertRaisesRegex(ValueError,
138                                  "The number of input channels must be known"):
139        defun_conv.get_concrete_function(x)
140    @parameterized.parameters(
141        (1, (3,), 128, 5, "NWC"),
142        (2, (4, 4), 64, 3, "NHWC"),
143        (3, (4, 4, 4), 64, 3, "NDHWC"))
144    def testInitializerVariance(self, num_spatial_dims, kernel_shape,
145                                in_channels, output_channels, data_format):
146      inputs = tf.random.uniform([16] + ([32] * num_spatial_dims) + [in_channels])
147      c = conv_transpose.ConvNDTranspose(
148          num_spatial_dims=num_spatial_dims,
149          kernel_shape=kernel_shape,
150          output_channels=output_channels,
151          data_format=data_format)
152      c(inputs)
153      actual_std = c.w.numpy().std()
154      expected_std = 1 / (np.sqrt(np.prod(kernel_shape + (in_channels,))))
155      rel_diff = np.abs(actual_std - expected_std) / expected_std
156      self.assertLess(rel_diff, 0.5)
157  class Conv2DTransposeTest(test_utils.TestCase, parameterized.TestCase):
158    @parameterized.parameters(True, False)
159    def testComputationPaddingSame(self, with_bias):
160      expected_out = [[4, 6, 4], [6, 9, 6], [4, 6, 4]]
161      expected_out = np.asarray(expected_out, dtype=np.float32)
162      if with_bias:
163        expected_out += 1
164      conv_transpose1 = conv_transpose.Conv2DTranspose(
165          output_channels=1,
166          output_shape=None,
167          kernel_shape=3,
168          stride=1,
169          padding="SAME",
170          with_bias=with_bias,
171          **create_constant_initializers(1.0, 1.0, with_bias))
172      out = conv_transpose1(tf.ones([1, 3, 3, 1], dtype=tf.float32))
173      self.assertEqual(out.shape, [1, 3, 3, 1])
174      out = tf.squeeze(out, axis=(0, 3))
175      self.assertAllClose(self.evaluate(out), expected_out)
176    @parameterized.parameters(True, False)
177    def testComputationPaddingValid(self, with_bias):
178      expected_out = [[1, 2, 3, 2, 1], [2, 4, 6, 4, 2], [3, 6, 9, 6, 3],
179                      [2, 4, 6, 4, 2], [1, 2, 3, 2, 1]]
180      expected_out = np.asarray(expected_out, dtype=np.float32)
181      if with_bias:
182        expected_out += 1
183      conv1 = conv_transpose.Conv2DTranspose(
184          output_channels=1,
185          output_shape=None,
186          kernel_shape=3,
187          stride=1,
188          padding="VALID",
189          with_bias=with_bias,
190          **create_constant_initializers(1.0, 1.0, with_bias))
191      out = conv1(tf.ones([1, 3, 3, 1], dtype=tf.float32))
192      self.assertEqual(out.shape, [1, 5, 5, 1])
193      out = tf.squeeze(out, axis=(0, 3))
194      self.assertAllClose(self.evaluate(out), expected_out)
195    def testShapeDilated(self):
196      if "CPU" == self.primary_device:
197        self.skipTest("Not supported on CPU")
198      conv1 = conv_transpose.Conv2DTranspose(
199          output_channels=1,
200          output_shape=None,
201          kernel_shape=3,
202          rate=2,
203          padding="VALID")
204      out = conv1(tf.ones([1, 3, 3, 1]))
205      self.assertEqual(out.shape, [1, 7, 7, 1])
206  class Conv1DTransposeTest(test_utils.TestCase, parameterized.TestCase):
207    @parameterized.parameters(True, False)
208    def testComputationPaddingSame(self, with_bias):
209      expected_out = [2, 3, 2]
210      expected_out = np.asarray(expected_out, dtype=np.float32)
211      if with_bias:
212        expected_out += 1
213      conv1 = conv_transpose.Conv1DTranspose(
214          output_channels=1,
215          output_shape=None,
216          kernel_shape=3,
217          stride=1,
218          padding="SAME",
219          with_bias=with_bias,
220          **create_constant_initializers(1.0, 1.0, with_bias))
221      out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))
222      self.assertEqual(out.shape, [1, 3, 1])
223      out = tf.squeeze(out, axis=(0, 2))
224      self.assertAllClose(self.evaluate(out), expected_out)
225    @parameterized.parameters(True, False)
226    def testComputationPaddingValid(self, with_bias):
227      expected_out = [1, 2, 3, 2, 1]
228      expected_out = np.asarray(expected_out, dtype=np.float32)
229      if with_bias:
230        expected_out += 1
231      conv1 = conv_transpose.Conv1DTranspose(
232          output_channels=1,
233          output_shape=None,
234          kernel_shape=3,
235          stride=1,
236          padding="VALID",
237          with_bias=with_bias,
238          **create_constant_initializers(1.0, 1.0, with_bias))
239      out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))
240      self.assertEqual(out.shape, [1, 5, 1])
241      out = tf.squeeze(out, axis=(0, 2))
242      self.assertAllClose(self.evaluate(out), expected_out)
243  class Conv3DTransposeTest(test_utils.TestCase, parameterized.TestCase):
244    @parameterized.parameters(True, False)
245    def testComputationPaddingSame(self, with_bias):
246      expected_out = np.asarray([
247          8, 12, 8, 12, 18, 12, 8, 12, 8, 12, 18, 12, 18, 27, 18, 12, 18, 12, 8,
248          12, 8, 12, 18, 12, 8, 12, 8
249      ]).reshape((3, 3, 3))
250      if with_bias:
251        expected_out += 1
252      conv_transpose1 = conv_transpose.Conv3DTranspose(
253          output_channels=1,
254          output_shape=None,
255          kernel_shape=3,
256          stride=1,
257          padding="SAME",
258          with_bias=with_bias,
259          **create_constant_initializers(1.0, 1.0, with_bias))
260      out = conv_transpose1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))
261      self.assertEqual(out.shape, [1, 3, 3, 3, 1])
262      out = tf.squeeze(out, axis=(0, 4))
263      self.assertAllClose(self.evaluate(out), expected_out)
264    @parameterized.parameters(True, False)
265    def testComputationPaddingValid(self, with_bias):
266      expected_out = np.asarray([
267          1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2,
268          1, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12, 6, 4, 8, 12, 8, 4, 2,
269          4, 6, 4, 2, 3, 6, 9, 6, 3, 6, 12, 18, 12, 6, 9, 18, 27, 18, 9, 6, 12,
270          18, 12, 6, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12,
271          6, 4, 8, 12, 8, 4, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9,
272          6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1.
273      ]).reshape((5, 5, 5))
274      if with_bias:
275        expected_out += 1
276      conv1 = conv_transpose.Conv3DTranspose(
277          output_channels=1,
278          output_shape=None,
279          kernel_shape=3,
280          stride=1,
281          padding="VALID",
282          with_bias=with_bias,
283          **create_constant_initializers(1.0, 1.0, with_bias))
284      out = conv1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))
285      self.assertEqual(out.shape, [1, 5, 5, 5, 1])
286      out = tf.squeeze(out, axis=(0, 4))
287      self.assertAllClose(self.evaluate(out), expected_out)
288  if __name__ == "__main__":
289    tf.test.main()
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-train.py</h3>
            <pre><code>1  from data import *
2  from utils.augmentations import SSDAugmentation, BaseTransform
3  from utils.functions import MovingAverage, SavePath
4  from utils.logger import Log
5  from utils import timer
6  from layers.modules import MultiBoxLoss
7  from yolact import Yolact
8  import os
9  import sys
10  import time
11  import math, random
12  from pathlib import Path
13  import torch
14  from torch.autograd import Variable
15  import torch.nn as nn
16  import torch.optim as optim
17  import torch.backends.cudnn as cudnn
18  import torch.nn.init as init
19  import torch.utils.data as data
20  import numpy as np
21  import argparse
22  import datetime
23  import eval as eval_script
24  def str2bool(v):
25      return v.lower() in ("yes", "true", "t", "1")
26  parser = argparse.ArgumentParser(
27      description='Yolact Training Script')
28  parser.add_argument('--batch_size', default=8, type=int,
29                      help='Batch size for training')
30  parser.add_argument('--resume', default=None, type=str,
31                      help='Checkpoint state_dict file to resume training from. If this is "interrupt"'\
32                           ', the model will resume training from the interrupt file.')
33  parser.add_argument('--start_iter', default=-1, type=int,
34                      help='Resume training at this iter. If this is -1, the iteration will be'\
35                           'determined from the file name.')
36  parser.add_argument('--num_workers', default=4, type=int,
37                      help='Number of workers used in dataloading')
38  parser.add_argument('--cuda', default=True, type=str2bool,
39                      help='Use CUDA to train model')
40  parser.add_argument('--lr', '--learning_rate', default=None, type=float,
41                      help='Initial learning rate. Leave as None to read this from the config.')
42  parser.add_argument('--momentum', default=None, type=float,
43                      help='Momentum for SGD. Leave as None to read this from the config.')
44  parser.add_argument('--decay', '--weight_decay', default=None, type=float,
45                      help='Weight decay for SGD. Leave as None to read this from the config.')
46  parser.add_argument('--gamma', default=None, type=float,
47                      help='For each lr step, what to multiply the lr by. Leave as None to read this from the config.')
48  parser.add_argument('--save_folder', default='weights/',
49                      help='Directory for saving checkpoint models.')
50  parser.add_argument('--log_folder', default='logs/',
51                      help='Directory for saving logs.')
52  parser.add_argument('--config', default=None,
53                      help='The config object to use.')
54  parser.add_argument('--save_interval', default=10000, type=int,
55                      help='The number of iterations between saving the model.')
56  parser.add_argument('--validation_size', default=5000, type=int,
57                      help='The number of images to use for validation.')
58  parser.add_argument('--validation_epoch', default=2, type=int,
59                      help='Output validation information every n iterations. If -1, do no validation.')
60  parser.add_argument('--keep_latest', dest='keep_latest', action='store_true',
61                      help='Only keep the latest checkpoint instead of each one.')
62  parser.add_argument('--keep_latest_interval', default=100000, type=int,
63                      help='When --keep_latest is on, don\'t delete the latest file at these intervals. This should be a multiple of save_interval or 0.')
64  parser.add_argument('--dataset', default=None, type=str,
65                      help='If specified, override the dataset specified in the config with this one (example: coco2017_dataset).')
66  parser.add_argument('--no_log', dest='log', action='store_false',
67                      help='Don\'t log per iteration information into log_folder.')
68  parser.add_argument('--log_gpu', dest='log_gpu', action='store_true',
69                      help='Include GPU information in the logs. Nvidia-smi tends to be slow, so set this with caution.')
70  parser.add_argument('--no_interrupt', dest='interrupt', action='store_false',
71                      help='Don\'t save an interrupt when KeyboardInterrupt is caught.')
72  parser.add_argument('--batch_alloc', default=None, type=str,
73                      help='If using multiple GPUS, you can set this to be a comma separated list detailing which GPUs should get what local batch size (It should add up to your total batch size).')
74  parser.add_argument('--no_autoscale', dest='autoscale', action='store_false',
<span onclick='openModal()' class='match'>75                      help='YOLACT will automatically scale the lr and the number of iterations depending on the batch size. Set this if you want to disable that.')
76  parser.set_defaults(keep_latest=False, log=True, log_gpu=False, interrupt=True, autoscale=True)
77  args = parser.parse_args()
</span>78  if args.config is not None:
79      set_cfg(args.config)
80  if args.dataset is not None:
81      set_dataset(args.dataset)
82  if args.autoscale and args.batch_size != 8:
83      factor = args.batch_size / 8
84      if __name__ == '__main__':
85          print('Scaling parameters by %.2f to account for a batch size of %d.' % (factor, args.batch_size))
86      cfg.lr *= factor
87      cfg.max_iter //= factor
88      cfg.lr_steps = [x // factor for x in cfg.lr_steps]
89  def replace(name):
90      if getattr(args, name) == None: setattr(args, name, getattr(cfg, name))
91  replace('lr')
92  replace('decay')
93  replace('gamma')
94  replace('momentum')
95  cur_lr = args.lr
96  if torch.cuda.device_count() == 0:
97      print('No GPUs detected. Exiting...')
98      exit(-1)
99  if args.batch_size // torch.cuda.device_count() < 6:
100      if __name__ == '__main__':
101          print('Per-GPU batch size is less than the recommended limit for batch norm. Disabling batch norm.')
102      cfg.freeze_bn = True
103  loss_types = ['B', 'C', 'M', 'P', 'D', 'E', 'S', 'I']
104  if torch.cuda.is_available():
105      if args.cuda:
106          torch.set_default_tensor_type('torch.cuda.FloatTensor')
107      if not args.cuda:
108          print("WARNING: It looks like you have a CUDA device, but aren't " +
109                "using CUDA.\nRun with --cuda for optimal training speed.")
110          torch.set_default_tensor_type('torch.FloatTensor')
111  else:
112      torch.set_default_tensor_type('torch.FloatTensor')
113  class NetLoss(nn.Module):
114      def __init__(self, net:Yolact, criterion:MultiBoxLoss):
115          super().__init__()
116          self.net = net
117          self.criterion = criterion
118      def forward(self, images, targets, masks, num_crowds):
119          preds = self.net(images)
120          losses = self.criterion(self.net, preds, targets, masks, num_crowds)
121          return losses
122  class CustomDataParallel(nn.DataParallel):
123      def scatter(self, inputs, kwargs, device_ids):
124          devices = ['cuda:' + str(x) for x in device_ids]
125          splits = prepare_data(inputs[0], devices, allocation=args.batch_alloc)
126          return [[split[device_idx] for split in splits] for device_idx in range(len(devices))], \
127              [kwargs] * len(devices)
128      def gather(self, outputs, output_device):
129          out = {}
130          for k in outputs[0]:
131              out[k] = torch.stack([output[k].to(output_device) for output in outputs])
132          return out
133  def train():
134      if not os.path.exists(args.save_folder):
135          os.mkdir(args.save_folder)
136      dataset = COCODetection(image_path=cfg.dataset.train_images,
137                              info_file=cfg.dataset.train_info,
138                              transform=SSDAugmentation(MEANS))
139      if args.validation_epoch > 0:
140          setup_eval()
141          val_dataset = COCODetection(image_path=cfg.dataset.valid_images,
142                                      info_file=cfg.dataset.valid_info,
143                                      transform=BaseTransform(MEANS))
144      yolact_net = Yolact()
145      net = yolact_net
146      net.train()
147      if args.log:
148          log = Log(cfg.name, args.log_folder, dict(args._get_kwargs()),
149              overwrite=(args.resume is None), log_gpu_stats=args.log_gpu)
150      timer.disable_all()
151      if args.resume == 'interrupt':
152          args.resume = SavePath.get_interrupt(args.save_folder)
153      elif args.resume == 'latest':
154          args.resume = SavePath.get_latest(args.save_folder, cfg.name)
155      if args.resume is not None:
156          print('Resuming training, loading {}...'.format(args.resume))
157          yolact_net.load_weights(args.resume)
158          if args.start_iter == -1:
159              args.start_iter = SavePath.from_str(args.resume).iteration
160      else:
161          print('Initializing weights...')
162          yolact_net.init_weights(backbone_path=args.save_folder + cfg.backbone.path)
163      optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,
164                            weight_decay=args.decay)
165      criterion = MultiBoxLoss(num_classes=cfg.num_classes,
166                               pos_threshold=cfg.positive_iou_threshold,
167                               neg_threshold=cfg.negative_iou_threshold,
168                               negpos_ratio=cfg.ohem_negpos_ratio)
169      if args.batch_alloc is not None:
170          args.batch_alloc = [int(x) for x in args.batch_alloc.split(',')]
171          if sum(args.batch_alloc) != args.batch_size:
172              print('Error: Batch allocation (%s) does not sum to batch size (%s).' % (args.batch_alloc, args.batch_size))
173              exit(-1)
174      net = CustomDataParallel(NetLoss(net, criterion))
175      if args.cuda:
176          net = net.cuda()
177      if not cfg.freeze_bn: yolact_net.freeze_bn() # Freeze bn so we don't kill our means
178      yolact_net(torch.zeros(1, 3, cfg.max_size, cfg.max_size).cuda())
179      if not cfg.freeze_bn: yolact_net.freeze_bn(True)
180      loc_loss = 0
181      conf_loss = 0
182      iteration = max(args.start_iter, 0)
183      last_time = time.time()
184      epoch_size = len(dataset) // args.batch_size
185      num_epochs = math.ceil(cfg.max_iter / epoch_size)
186      step_index = 0
187      data_loader = data.DataLoader(dataset, args.batch_size,
188                                    num_workers=args.num_workers,
189                                    shuffle=True, collate_fn=detection_collate,
190                                    pin_memory=True)
191      save_path = lambda epoch, iteration: SavePath(cfg.name, epoch, iteration).get_path(root=args.save_folder)
192      time_avg = MovingAverage()
193      global loss_types # Forms the print order
194      loss_avgs  = { k: MovingAverage(100) for k in loss_types }
195      print('Begin training!')
196      print()
197      try:
198          for epoch in range(num_epochs):
199              if (epoch+1)*epoch_size < iteration:
200                  continue
201              for datum in data_loader:
202                  if iteration == (epoch+1)*epoch_size:
203                      break
204                  if iteration == cfg.max_iter:
205                      break
206                  changed = False
207                  for change in cfg.delayed_settings:
208                      if iteration >= change[0]:
209                          changed = True
210                          cfg.replace(change[1])
211                          for avg in loss_avgs:
212                              avg.reset()
213                  if changed:
214                      cfg.delayed_settings = [x for x in cfg.delayed_settings if x[0] > iteration]
215                  if cfg.lr_warmup_until > 0 and iteration <= cfg.lr_warmup_until:
216                      set_lr(optimizer, (args.lr - cfg.lr_warmup_init) * (iteration / cfg.lr_warmup_until) + cfg.lr_warmup_init)
217                  while step_index < len(cfg.lr_steps) and iteration >= cfg.lr_steps[step_index]:
218                      step_index += 1
219                      set_lr(optimizer, args.lr * (args.gamma ** step_index))
220                  optimizer.zero_grad()
221                  losses = net(datum)
222                  losses = { k: (v).mean() for k,v in losses.items() } # Mean here because Dataparallel
223                  loss = sum([losses[k] for k in losses])
224                  loss.backward() # Do this to free up vram even if loss is not finite
225                  if torch.isfinite(loss).item():
226                      optimizer.step()
227                  for k in losses:
228                      loss_avgs[k].add(losses[k].item())
229                  cur_time  = time.time()
230                  elapsed   = cur_time - last_time
231                  last_time = cur_time
232                  if iteration != args.start_iter:
233                      time_avg.add(elapsed)
234                  if iteration % 10 == 0:
235                      eta_str = str(datetime.timedelta(seconds=(cfg.max_iter-iteration) * time_avg.get_avg())).split('.')[0]
236                      total = sum([loss_avgs[k].get_avg() for k in losses])
237                      loss_labels = sum([[k, loss_avgs[k].get_avg()] for k in loss_types if k in losses], [])
238                      print(('[%3d] %7d ||' + (' %s: %.3f |' * len(losses)) + ' T: %.3f || ETA: %s || timer: %.3f')
239                              % tuple([epoch, iteration] + loss_labels + [total, eta_str, elapsed]), flush=True)
240                  if args.log:
241                      precision = 5
242                      loss_info = {k: round(losses[k].item(), precision) for k in losses}
243                      loss_info['T'] = round(loss.item(), precision)
244                      if args.log_gpu:
245                          log.log_gpu_stats = (iteration % 10 == 0) # nvidia-smi is sloooow
246                      log.log('train', loss=loss_info, epoch=epoch, iter=iteration,
247                          lr=round(cur_lr, 10), elapsed=elapsed)
248                      log.log_gpu_stats = args.log_gpu
249                  iteration += 1
250                  if iteration % args.save_interval == 0 and iteration != args.start_iter:
251                      if args.keep_latest:
252                          latest = SavePath.get_latest(args.save_folder, cfg.name)
253                      print('Saving state, iter:', iteration)
254                      yolact_net.save_weights(save_path(epoch, iteration))
255                      if args.keep_latest and latest is not None:
256                          if args.keep_latest_interval <= 0 or iteration % args.keep_latest_interval != args.save_interval:
257                              print('Deleting old save...')
258                              os.remove(latest)
259              if args.validation_epoch > 0:
260                  if epoch % args.validation_epoch == 0 and epoch > 0:
261                      compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)
262          compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)
263      except KeyboardInterrupt:
264          if args.interrupt:
265              print('Stopping early. Saving network...')
266              SavePath.remove_interrupt(args.save_folder)
267              yolact_net.save_weights(save_path(epoch, repr(iteration) + '_interrupt'))
268          exit()
269      yolact_net.save_weights(save_path(epoch, iteration))
270  def set_lr(optimizer, new_lr):
271      for param_group in optimizer.param_groups:
272          param_group['lr'] = new_lr
273      global cur_lr
274      cur_lr = new_lr
275  def gradinator(x):
276      x.requires_grad = False
277      return x
278  def prepare_data(datum, devices:list=None, allocation:list=None):
279      with torch.no_grad():
280          if devices is None:
281              devices = ['cuda:0'] if args.cuda else ['cpu']
282          if allocation is None:
283              allocation = [args.batch_size // len(devices)] * (len(devices) - 1)
284              allocation.append(args.batch_size - sum(allocation)) # The rest might need more/less
285          images, (targets, masks, num_crowds) = datum
286          cur_idx = 0
287          for device, alloc in zip(devices, allocation):
288              for _ in range(alloc):
289                  images[cur_idx]  = gradinator(images[cur_idx].to(device))
290                  targets[cur_idx] = gradinator(targets[cur_idx].to(device))
291                  masks[cur_idx]   = gradinator(masks[cur_idx].to(device))
292                  cur_idx += 1
293          if cfg.preserve_aspect_ratio:
294              _, h, w = images[random.randint(0, len(images)-1)].size()
295              for idx, (image, target, mask, num_crowd) in enumerate(zip(images, targets, masks, num_crowds)):
296                  images[idx], targets[idx], masks[idx], num_crowds[idx] \
297                      = enforce_size(image, target, mask, num_crowd, w, h)
298          cur_idx = 0
299          split_images, split_targets, split_masks, split_numcrowds \
300              = [[None for alloc in allocation] for _ in range(4)]
301          for device_idx, alloc in enumerate(allocation):
302              split_images[device_idx]    = torch.stack(images[cur_idx:cur_idx+alloc], dim=0)
303              split_targets[device_idx]   = targets[cur_idx:cur_idx+alloc]
304              split_masks[device_idx]     = masks[cur_idx:cur_idx+alloc]
305              split_numcrowds[device_idx] = num_crowds[cur_idx:cur_idx+alloc]
306              cur_idx += alloc
307          return split_images, split_targets, split_masks, split_numcrowds
308  def no_inf_mean(x:torch.Tensor):
309      no_inf = [a for a in x if torch.isfinite(a)]
310      if len(no_inf) > 0:
311          return sum(no_inf) / len(no_inf)
312      else:
313          return x.mean()
314  def compute_validation_loss(net, data_loader, criterion):
315      global loss_types
316      with torch.no_grad():
317          losses = {}
318          iterations = 0
319          for datum in data_loader:
320              images, targets, masks, num_crowds = prepare_data(datum)
321              out = net(images)
322              wrapper = ScatterWrapper(targets, masks, num_crowds)
323              _losses = criterion(out, wrapper, wrapper.make_mask())
324              for k, v in _losses.items():
325                  v = v.mean().item()
326                  if k in losses:
327                      losses[k] += v
328                  else:
329                      losses[k] = v
330              iterations += 1
331              if args.validation_size <= iterations * args.batch_size:
332                  break
333          for k in losses:
334              losses[k] /= iterations
335          loss_labels = sum([[k, losses[k]] for k in loss_types if k in losses], [])
336          print(('Validation ||' + (' %s: %.3f |' * len(losses)) + ')') % tuple(loss_labels), flush=True)
337  def compute_validation_map(epoch, iteration, yolact_net, dataset, log:Log=None):
338      with torch.no_grad():
339          yolact_net.eval()
340          start = time.time()
341          print()
342          print("Computing validation mAP (this may take a while)...", flush=True)
343          val_info = eval_script.evaluate(yolact_net, dataset, train_mode=True)
344          end = time.time()
345          if log is not None:
346              log.log('val', val_info, elapsed=(end - start), epoch=epoch, iter=iteration)
347          yolact_net.train()
348  def setup_eval():
349      eval_script.parse_args(['--no_bar', '--max_images='+str(args.validation_size)])
350  if __name__ == '__main__':
351      train()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-conv_transpose_test.py</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-train.py</div>
                <div class="column column_space"><pre><code>54      conv1 = conv_transpose.ConvNDTranspose(
55          num_spatial_dims=2,
56          output_channels=1,
57          output_shape=None,
58          kernel_shape=3,
59          stride=1,
60          padding=padding,
</pre></code></div>
                <div class="column column_space"><pre><code>75                      help='YOLACT will automatically scale the lr and the number of iterations depending on the batch size. Set this if you want to disable that.')
76  parser.set_defaults(keep_latest=False, log=True, log_gpu=False, interrupt=True, autoscale=True)
77  args = parser.parse_args()
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    